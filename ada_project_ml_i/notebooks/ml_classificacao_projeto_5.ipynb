{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C14faTVSb-LK"
      },
      "source": [
        "# Classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbheOUWYvUH9"
      },
      "source": [
        "# Instalando dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0TuBUIJTt3S-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install sidetable\n",
        "!pip install missingno\n",
        "!pip install pycaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkqJR_GSvO3H"
      },
      "source": [
        "# Importando modulos necessários"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4tuQEKkUruSB",
        "outputId": "0f50fb97-9e76-44b7-dcbb-a3af7c1a849f"
      },
      "outputs": [],
      "source": [
        "# data analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sidetable as stb\n",
        "import missingno as msno\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "#data viz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#supress warnings\n",
        "import warnings\n",
        "\n",
        "\n",
        "#time_serie\n",
        "\n",
        "import pycaret\n",
        "from sklearn.preprocessing import scale\n",
        "from pycaret.time_series import *\n",
        "from pycaret.time_series import TSForecastingExperiment\n",
        "\n",
        "# from google.colab import drive \n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqs3f7EuvObe"
      },
      "source": [
        "# Funçōes necessárias para o projeto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RwJ5j7_kvNm8"
      },
      "outputs": [],
      "source": [
        "def eda(df:pd.DataFrame)->None:\n",
        "  print(\"-\"*15,\"DataFrame shape\", \"-\"*15 )\n",
        "  print(f\"Linhas: {df.shape[0]}, Colunas: {df.shape[1]}\")\n",
        "  print(\"-\"*15,\"DataFrame info\", \"-\"*15 )\n",
        "  print(df.info())\n",
        "  print(\"-\"*15,\"DataFrame isna\", \"-\"*15 )\n",
        "  print(df.isna().sum())\n",
        "  print(\"-\"*15,\"DataFrame is duplicated\", \"-\"*15 )\n",
        "  print(df.duplicated().sum())\n",
        "  print(\"-\"*15,\"DataFrame describe numeric\", \"-\"*15 )\n",
        "  print(df.describe(percentiles=[.01,0.1,.25,.5,.75,.90,.99]))\n",
        "  print(\"-\"*15,\"DataFrame desribe categorical\", \"-\"*15 )\n",
        "  print(df.describe(exclude=[np.number]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def date_engineering(df: pd.DataFrame)->pd.DataFrame:\n",
        "    df[\"data_inversa\"]=pd.to_datetime(df[\"data_inversa\"],format='mixed')\n",
        "    df[\"year\"]=pd.to_datetime(df[\"data_inversa\"]).dt.year\n",
        "    df[\"month\"]=pd.to_datetime(df[\"data_inversa\"]).dt.month\n",
        "    df[\"quarter\"]=pd.to_datetime(df[\"data_inversa\"]).dt.quarter\n",
        "    df[\"quarter_end\"]=pd.to_datetime(df[\"data_inversa\"]).dt.is_quarter_end\n",
        "    return df\n",
        "\n",
        "\n",
        "def date_col(col:pd.Series):\n",
        "  df[\"dia_semana\"]=df['dia_semana'].str.split(\"-\").str[0]\n",
        "  return df[\"dia_semana\"]\n",
        "\n",
        "def time_col(col:pd.Series):\n",
        "  df[\"hour\"] = df[\"horario\"].str.split(\":\").str[0]\n",
        "\n",
        "  return df[\"hour\"]\n",
        "\n",
        "def convert_lower_case(column):\n",
        "  '''\n",
        "  Funcao que retorna a coluna selecionada para letra minuscula\n",
        "  -param column: coluna da pd.Dataframe\n",
        "  '''\n",
        "  return column.lower()\n",
        "\n",
        "\n",
        "def adf_test(timeseries):\n",
        "    print ('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print (dfoutput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi8e4r60zjqj"
      },
      "source": [
        "# Importando dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsY71MU-5vF"
      },
      "source": [
        "Escolha do dataset - Acidentes de Trânsito nas Rodovias Federais do Brasil (de 2007 a 2022). Disponível no link: https://www.kaggle.com/datasets/rafaelborgesgraunke/traffic-accidents-brazil-pt-br/data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "EiG9je0css8g",
        "outputId": "6c121778-77c0-4c16-c64e-932ef14839b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_inversa</th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>horario</th>\n",
              "      <th>uf</th>\n",
              "      <th>br</th>\n",
              "      <th>km</th>\n",
              "      <th>municipio</th>\n",
              "      <th>causa_acidente</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <th>...</th>\n",
              "      <th>feridos_graves</th>\n",
              "      <th>ilesos</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>regional</th>\n",
              "      <th>delegacia</th>\n",
              "      <th>uop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/11/2009</td>\n",
              "      <td>Terça</td>\n",
              "      <td>14:10:00</td>\n",
              "      <td>MG</td>\n",
              "      <td>381.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>BETIM</td>\n",
              "      <td>Velocidade incompatível</td>\n",
              "      <td>Colisão traseira</td>\n",
              "      <td>Com Vítimas Feridas</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16/10/2009</td>\n",
              "      <td>Sexta</td>\n",
              "      <td>04:35:00</td>\n",
              "      <td>MG</td>\n",
              "      <td>251.0</td>\n",
              "      <td>419.6</td>\n",
              "      <td>GRAO MOGOL</td>\n",
              "      <td>Outras</td>\n",
              "      <td>Colisão frontal</td>\n",
              "      <td>Com Vítimas Fatais</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20/09/2009</td>\n",
              "      <td>Domingo</td>\n",
              "      <td>20:00:00</td>\n",
              "      <td>PI</td>\n",
              "      <td>316.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>TERESINA</td>\n",
              "      <td>Dormindo</td>\n",
              "      <td>Saída de Pista</td>\n",
              "      <td>Com Vítimas Feridas</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17/05/2009</td>\n",
              "      <td>Domingo</td>\n",
              "      <td>15:45:00</td>\n",
              "      <td>RJ</td>\n",
              "      <td>101.0</td>\n",
              "      <td>429.0</td>\n",
              "      <td>MANGARATIBA</td>\n",
              "      <td>Falta de atenção</td>\n",
              "      <td>Colisão Transversal</td>\n",
              "      <td>Sem Vítimas</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>05/07/2009</td>\n",
              "      <td>Domingo</td>\n",
              "      <td>17:30:00</td>\n",
              "      <td>PE</td>\n",
              "      <td>101.0</td>\n",
              "      <td>59.2</td>\n",
              "      <td>RECIFE</td>\n",
              "      <td>Outras</td>\n",
              "      <td>Queda de motocicleta / bicicleta / veículo</td>\n",
              "      <td>Com Vítimas Feridas</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  data_inversa dia_semana   horario  uf     br     km    municipio  \\\n",
              "0   10/11/2009      Terça  14:10:00  MG  381.0  495.0        BETIM   \n",
              "1   16/10/2009      Sexta  04:35:00  MG  251.0  419.6   GRAO MOGOL   \n",
              "2   20/09/2009    Domingo  20:00:00  PI  316.0   24.0     TERESINA   \n",
              "3   17/05/2009    Domingo  15:45:00  RJ  101.0  429.0  MANGARATIBA   \n",
              "4   05/07/2009    Domingo  17:30:00  PE  101.0   59.2       RECIFE   \n",
              "\n",
              "            causa_acidente                               tipo_acidente  \\\n",
              "0  Velocidade incompatível                            Colisão traseira   \n",
              "1                   Outras                             Colisão frontal   \n",
              "2                 Dormindo                              Saída de Pista   \n",
              "3         Falta de atenção                         Colisão Transversal   \n",
              "4                   Outras  Queda de motocicleta / bicicleta / veículo   \n",
              "\n",
              "  classificacao_acidente  ... feridos_graves ilesos ignorados feridos  \\\n",
              "0    Com Vítimas Feridas  ...              0      1         0       4   \n",
              "1     Com Vítimas Fatais  ...              5      0         0       5   \n",
              "2    Com Vítimas Feridas  ...              4      0         0       6   \n",
              "3            Sem Vítimas  ...              0      2         0       0   \n",
              "4    Com Vítimas Feridas  ...              0      0         0       1   \n",
              "\n",
              "  veiculos latitude  longitude  regional  delegacia  uop  \n",
              "0        2      NaN        NaN       NaN        NaN  NaN  \n",
              "1        2      NaN        NaN       NaN        NaN  NaN  \n",
              "2        1      NaN        NaN       NaN        NaN  NaN  \n",
              "3        2      NaN        NaN       NaN        NaN  NaN  \n",
              "4        1      NaN        NaN       NaN        NaN  NaN  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.read_csv(\"datatran2007-2022.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnrP9ai8zn7X"
      },
      "source": [
        "# Tratamento de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZvbQQYFpywnt"
      },
      "outputs": [],
      "source": [
        "# transformando e criando colunas adicionais para data\n",
        "df=date_engineering(df)\n",
        "df[\"hour\"]=time_col(df[\"horario\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dE-85bhct17z",
        "outputId": "d123d8cb-4630-4ad8-8dc2-f1cd5f651b43"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>missing</th>\n",
              "      <th>total</th>\n",
              "      <th>percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>delegacia</th>\n",
              "      <td>1563088</td>\n",
              "      <td>1981317</td>\n",
              "      <td>78.891364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uop</th>\n",
              "      <td>1563008</td>\n",
              "      <td>1981317</td>\n",
              "      <td>78.887326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>regional</th>\n",
              "      <td>1562206</td>\n",
              "      <td>1981317</td>\n",
              "      <td>78.846848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>latitude</th>\n",
              "      <td>1562200</td>\n",
              "      <td>1981317</td>\n",
              "      <td>78.846545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>longitude</th>\n",
              "      <td>1562200</td>\n",
              "      <td>1981317</td>\n",
              "      <td>78.846545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ano</th>\n",
              "      <td>515480</td>\n",
              "      <td>1981317</td>\n",
              "      <td>26.017038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>km</th>\n",
              "      <td>894</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.045122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>br</th>\n",
              "      <td>894</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.045122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fase_dia</th>\n",
              "      <td>70</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.003533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condicao_metereologica</th>\n",
              "      <td>61</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.003079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tipo_acidente</th>\n",
              "      <td>40</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.002019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <td>26</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.001312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uf</th>\n",
              "      <td>12</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tipo_pista</th>\n",
              "      <td>10</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tracado_via</th>\n",
              "      <td>10</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uso_solo</th>\n",
              "      <td>10</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>causa_acidente</th>\n",
              "      <td>2</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data_inversa</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>veiculos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quarter</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quarter_end</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pessoas</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feridos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ignorados</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ilesos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feridos_graves</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feridos_leves</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mortos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dia_semana</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentido_via</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>municipio</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>horario</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        missing    total    percent\n",
              "delegacia               1563088  1981317  78.891364\n",
              "uop                     1563008  1981317  78.887326\n",
              "regional                1562206  1981317  78.846848\n",
              "latitude                1562200  1981317  78.846545\n",
              "longitude               1562200  1981317  78.846545\n",
              "ano                      515480  1981317  26.017038\n",
              "km                          894  1981317   0.045122\n",
              "br                          894  1981317   0.045122\n",
              "fase_dia                     70  1981317   0.003533\n",
              "condicao_metereologica       61  1981317   0.003079\n",
              "tipo_acidente                40  1981317   0.002019\n",
              "classificacao_acidente       26  1981317   0.001312\n",
              "uf                           12  1981317   0.000606\n",
              "tipo_pista                   10  1981317   0.000505\n",
              "tracado_via                  10  1981317   0.000505\n",
              "uso_solo                     10  1981317   0.000505\n",
              "causa_acidente                2  1981317   0.000101\n",
              "data_inversa                  0  1981317   0.000000\n",
              "year                          0  1981317   0.000000\n",
              "veiculos                      0  1981317   0.000000\n",
              "quarter                       0  1981317   0.000000\n",
              "quarter_end                   0  1981317   0.000000\n",
              "month                         0  1981317   0.000000\n",
              "pessoas                       0  1981317   0.000000\n",
              "feridos                       0  1981317   0.000000\n",
              "ignorados                     0  1981317   0.000000\n",
              "ilesos                        0  1981317   0.000000\n",
              "feridos_graves                0  1981317   0.000000\n",
              "feridos_leves                 0  1981317   0.000000\n",
              "mortos                        0  1981317   0.000000\n",
              "dia_semana                    0  1981317   0.000000\n",
              "sentido_via                   0  1981317   0.000000\n",
              "municipio                     0  1981317   0.000000\n",
              "horario                       0  1981317   0.000000\n",
              "hour                          0  1981317   0.000000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#percentual de dados vazios\n",
        "df.stb.missing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czca3SYDyV1a",
        "outputId": "863a2348-3c49-43b2-e67a-406c390d42ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        data_inversa dia_semana   horario  uf     br     km  \\\n",
            "1256430   2007-01-01    Segunda  06:30:00  PA  316.0    9.0   \n",
            "1257103   2007-01-01    Segunda  08:40:00  MG  381.0  397.4   \n",
            "1256115   2007-01-01    Segunda  15:20:00  SC  282.0   23.0   \n",
            "1260278   2007-01-01    Segunda  14:00:00  MG   40.0  120.8   \n",
            "1257102   2007-01-01    Segunda  12:00:00  MG   50.0    0.2   \n",
            "\n",
            "                         municipio               causa_acidente  \\\n",
            "1256430                 ANANINDEUA  Desobediência à sinalização   \n",
            "1257103                 NOVA UNIAO                       Outras   \n",
            "1256115  SANTO AMARO DA IMPERATRIZ  Desobediência à sinalização   \n",
            "1260278               LAGOA GRANDE                     Dormindo   \n",
            "1257102                   ARAGUARI                       Outras   \n",
            "\n",
            "               tipo_acidente classificacao_acidente  ... latitude longitude  \\\n",
            "1256430  Colisão Transversal    Com Vítimas Feridas  ...      NaN       NaN   \n",
            "1257103       Saída de Pista            Sem Vítimas  ...      NaN       NaN   \n",
            "1256115  Colisão Transversal    Com Vítimas Feridas  ...      NaN       NaN   \n",
            "1260278       Saída de Pista            Sem Vítimas  ...      NaN       NaN   \n",
            "1257102       Saída de Pista    Com Vítimas Feridas  ...      NaN       NaN   \n",
            "\n",
            "        regional delegacia  uop  year  month  quarter  quarter_end  hour  \n",
            "1256430      NaN       NaN  NaN  2007      1        1        False    06  \n",
            "1257103      NaN       NaN  NaN  2007      1        1        False    08  \n",
            "1256115      NaN       NaN  NaN  2007      1        1        False    15  \n",
            "1260278      NaN       NaN  NaN  2007      1        1        False    14  \n",
            "1257102      NaN       NaN  NaN  2007      1        1        False    12  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "        data_inversa dia_semana   horario  uf     br     km  \\\n",
            "1788483   2022-12-31     sábado  16:25:00  PR  153.0   51,9   \n",
            "1788482   2022-12-31     sábado  17:00:00  RS  285.0  211,5   \n",
            "1788481   2022-12-31     sábado  16:40:00  SC  470.0   21,7   \n",
            "1788506   2022-12-31     sábado  18:50:00  AL  316.0  165,2   \n",
            "1788519   2022-12-31     sábado  20:40:00  SC  116.0  231,7   \n",
            "\n",
            "                        municipio  \\\n",
            "1788483  SANTO ANTONIO DA PLATINA   \n",
            "1788482                  CASEIROS   \n",
            "1788481                    ILHOTA   \n",
            "1788506       PALMEIRA DOS INDIOS   \n",
            "1788519             CORREIA PINTO   \n",
            "\n",
            "                                            causa_acidente  \\\n",
            "1788483                   Ingestão de álcool pelo condutor   \n",
            "1788482  Acessar a via sem observar a presença dos outr...   \n",
            "1788481            Avarias e/ou desgaste excessivo no pneu   \n",
            "1788506                                   Animais na Pista   \n",
            "1788519                            Velocidade Incompatível   \n",
            "\n",
            "                     tipo_acidente classificacao_acidente  ...      latitude  \\\n",
            "1788483  Saída de leito carroçável    Com Vítimas Feridas  ...  -23,37173099   \n",
            "1788482        Colisão transversal            Sem Vítimas  ...  -28,23704643   \n",
            "1788481                 Tombamento    Com Vítimas Feridas  ...    -26,876698   \n",
            "1788506    Atropelamento de Animal    Com Vítimas Feridas  ...   -9,48239127   \n",
            "1788519  Saída de leito carroçável    Com Vítimas Feridas  ...  -27,67428901   \n",
            "\n",
            "            longitude regional delegacia             uop  year  month  \\\n",
            "1788483  -50,06416197  SPRF-PR  DEL07-PR  UOP04-DEL07-PR  2022     12   \n",
            "1788482  -51,63974404  SPRF-RS  DEL08-RS  UOP05-DEL08-RS  2022     12   \n",
            "1788481    -48,827362  SPRF-SC  DEL04-SC  UOP04-DEL04-SC  2022     12   \n",
            "1788506  -36,57309034  SPRF-AL  DEL03-AL  UOP01-DEL03-AL  2022     12   \n",
            "1788519  -50,33598326  SPRF-SC  DEL05-SC  UOP03-DEL05-SC  2022     12   \n",
            "\n",
            "         quarter  quarter_end  hour  \n",
            "1788483        4         True    16  \n",
            "1788482        4         True    17  \n",
            "1788481        4         True    16  \n",
            "1788506        4         True    18  \n",
            "1788519        4         True    20  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "#ordenando datas\n",
        "df=df.sort_values(by=\"data_inversa\")\n",
        "print(f\"{df.head()}\")\n",
        "print(f\"{df.tail()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "ywEMrhtVtpUR",
        "outputId": "cc08cfb6-3e23-4f7a-efde-b2af16cbecaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2500x1000 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "msno.matrix(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "g4zMowZJxP7V",
        "outputId": "7753f1c3-193d-4426-ac03-696f9bd17b86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAACFIAAAQuCAYAAAD1QIt3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3xN9x/H8fe9N7lGEiNi73kitqCIvakqnfihKErtWTW61Chq761ilKpqlapds7VKbbFi7xAryb338/tD7mmuJFbJvefr/Xw8+mh7l+/Luefccb73HJOICIiIiIiIiIiIiIiIiIiIiIgIZncPgIiIiIiIiIiIiIiIiIiIiMhTcCIFERERERERERERERERERERUSxOpCAiIiIiIiIiIiIiIiIiIiKKxYkURERERERERERERERERERERLE4kYKIiIiIiIiIiIiIiIiIiIgoFidSEBEREREREREREREREREREcXiRAoiIiIiIiIiIiIiIiIiIiKiWJxIQURERERERERERERERERERBSLEymIiIiIiIiIiIiIiIiIiIiIYnEiBREREREREREREREREREREVEsTqQgIiIiIiIiIiIiIiIiIiIiisWJFGRodrvd3UN4qVTuU7mNyBNxnTMuEXH3EOg5qbzsuE0xLofD4e4hvDQqtwFq93F7aVwq97HNuFTuY5txqd5HRPSiqLy9VLkNULuPba8uTqQgQ/rxxx8RFRUFi8Xi7qG8FCr3qdwWExPj7iEQxaPyOgcA169fd/cQXpoDBw7A4XDAZDK5eyj0jLZv3w6bzabkslN9m6KyNWvWIDo6Gmazeh8BVW4D1O5T+bVO9e2lyn1sMy6V+9hmXKr3ERG9KCpvL1VuA9TuYxup900MKe/bb79Fv379MG7cOADq/TJL5T6V2z777DN8/fXXiI6OdvdQXri4vxB0/rcqvxpUuQ1Qe50DgC+++AJffvklLl++7O6hvHDDhw9Hr169cPjwYXcP5aVQaT171IgRI9C1a1ds2rQJgFqtqm9TVDZy5Eh07twZixcvBqDWslO5DVC7T+XXOtW3lyr3sc24VO5jm3Gp3kdE9KKovL1UuQ1Qu49tBABe7h4A0bMYOnQo5s6dCwB48OABACj1yyyV+1Ru++abb7B48WKEhIQgMjIS6dKlc/eQXqiIiAjcunULZrMZyZMnR4YMGWAymeBwOAy/DFVuU3mdA4Bhw4Zh0aJFqFixonK/Yh02bBjmzJkDADh9+jQKFSrk3gG9QLt27UKpUqVgMpkgIkovu4sXLwKAMo0qb1P27NmD8PBw2Gw2pE+fHpUrV3b3kF6ouMsuIiICgDrLTuU2QO0+lV/rVN5eAmr3sc24VO5jm3Gp3kdE9KKovL1UuQ1Qu49t5MS/GTIM58pdqlQpWCwWHDlyBIA6M6VU7lO5bciQIZg9ezZq166Nfv36KTeJYv78+fj444/RoEEDNGrUCB07dsQvv/wCwPgvriq3qbzOAQ/Xuzlz5qBWrVro27cvMmTI4O4hvTBDhw7FnDlzEBgYCODfU5eocFSDefPmoVmzZvobdedkClU4l12RIkUAAOfPnwegxnqn8jZl3Lhx+PDDD9G3b18MGDAAH330ESZMmACbzebuob0QzmVXvHhxAMCJEycAqLHsVG4D1O5T+bVO5e0loHYf24xL5T62GZfqfUREL4rK20uV2wC1+9hGcRl7TxG9MoYMGYK5c+eiZs2aGDhwIEqXLo2zZ8/i+vXrht/hCajdp3Lb0KFD8d1336FmzZro0qUL8ubNC0CdF50RI0Zg0KBBOHnyJMqVK4cCBQrgn3/+wWeffYbt27e7e3j/icptKq9zgOt617Vr18eud0bbIRN32TVu3BjJkiXDrl27AKixXUmdOjUAYNasWfqvkFWZTBF32bVr1w4BAQH4+++/ERMTY/gjUqi8TZkwYQImTZqEbNmy4ZNPPkGvXr1QoUIF1K1bF15e/x64z6jP0bjLrmfPnsifPz+OHj2KO3fuGH7ZqdwGqN2n8mudyttLQO0+thmXyn1sMy7V+4iIXhSVt5cqtwFq97GNHsW/GfJ4Q4YM0XeadevWDZqmIWXKlLh+/bp+2GyjfsENqN2nctvw4cMxd+5c1KtXz2USBQBER0fj+vXrOH/+vEufkb4cXrp0KWbOnIkKFSpg7ty5mDp1KqZPn44OHTrg/v37OHr0KADov9g10nJUuU3ldQ54eBqduXPn4vXXX0fnzp1d1rtbt27hwoULOHLkCCIjI+FwOPTTtBhB3GXXp08fvPfee0iXLh0uXboEALBYLG4e4fNzPueSJ08OALhy5QqmTp2K0NBQAMafTBF32fXq1Qs1a9ZE2rRpcePGDdhsNkNPpFB5m7Jz507MmTMHhQsXxrfffotWrVqhTZs2mDRpEvLmzeuy7XAuQyO1xl123bt3R+nSpeHn54cbN27gzp07AIzVE5fKbYDafSq/1qm8vQTU7mObMdsAtfvYZsw2QP0+IqIXReXtpcptgNp9bDNm28vGiRTk0QYNGuSycjt3mpUuXRp2ux2bNm0CYNzzj6vcp3LbmjVrMGvWLGTIkAGtWrVC/vz59euWLVuG7t27o2HDhnjrrbfw8ccfY+bMmYiOjobZbDbETl273Y5t27bBy8sL7du31w+7nDJlShQpUgQmkwn37t2DiODWrVsAjLOTSeU2ldc5ANi+fTtmz54NPz8/NG/eHAUKFNCvW7JkCTp37oy3334b77zzDtq0aYOhQ4fq653dbnfjyJ/s0Tey2bNnx71795AmTRrs378fu3fvdvcQX4h79+4BAHLkyIGIiAh8++23hp9MMXjwYJdllzNnTsTExCBr1qw4c+YMNm/e7O4hPjfVtynh4eG4e/cumjVr5rI9OXv2LH777Tf07NkTAwYMwJw5c3Dy5El9cpYRnqdff/21y7LLkycPAKBgwYKIiIjAhg0bABhz2ancBqjdp/JrnerbS5X72GbMNkDtPrYZsw1Qv4+I6EVReXupchugdh/bjNmWFDiRgjzWypUrMX/+fFSpUsVl5QaAEiVKAAD27t2LmJgYQ3yp/SiV+1RuA4AqVaqgePHiuHLlCnbs2IGYmBgAwLRp0/Dpp59iw4YN8PHxAQBs3boVI0aMQN++ffWdup7eHB0djWPHjiFTpkwoWbKkfpnZbNYPzb9y5Uq0bdsWdevWxSeffIJVq1bBbrd7/E4mVdtUX+cAoEyZMqhVqxYiIyOxfPlyPHjwAAAwffp0DBw4ELt370b27NmRJUsWnDp1CvPmzUPLli0RFRUFi8XisZOYxo4di++++w41atTQl52IwNfXF/Xr1wcAnD592r2D/I+cb8Jv374NAOjYsSM+++wzPHjwAKNGjTLsZIpZs2Zh3rx5qF69usuy8/b2Rt26dQEAYWFhbh7l81F5m+Ic7/79+/V1zWn37t0YMmQIevXqhVWrVuGHH37AsGHD0KdPHyxYsEA/wognNy9evBihoaGoVq1avGVXuXJlAMChQ4cAeP4EwUep3Aao3Tdu3DhlX+tU3l4CavexzZhtgNp9bDNmG6Bmn1HG+TxUbiPydCpuL51UbgPU7mObMduSiteTb0LkHnny5MHnn3+OMmXK6Cu3c0XOly8fihQpgq1bt+LIkSMoUqSIO4f6XFTuU7nNZrPB29sbgwcPRtu2bfHjjz8iJCQEV69exeTJk1GiRAl06NABZcuWRXh4OPbu3YtJkyZh5cqVSJUqFQYMGOBy3nVPlTx5chw/fhw7duxA+fLlYbVacePGDYwfPx4igoiICNy7dw/37t3D8uXLsXPnTly9ehXNmjXz+PNpqdim8joHPDySiMViweDBg3HixAmsXLkSISEhSJ48OWbMmIHSpUujQ4cOKF++PC5fvoyTJ0/iyy+/xJ49e9C1a1eMHz8e3t7e7s5IUPLkyfHWW2/hww8/dHkjCwDZs2cHACxYsABVq1aFv7+/O4b4Qjx48AA7duwA8PCIFPny5UNkZCRGjx6NUaNGAQCaNWum76Q2wgzo1KlTo1WrVnjnnXfiLbts2bIBeHi0lIYNGyJLlizuGOJzU3mb4nA4YLFY9K6zZ88CACIiIjB37lxs3boVH374ITRNg9lsxooVK/DXX39h6tSpSJYsGd555x2Pfn5mypQJ3bp1Q82aNeMtu1y5ciFdunRYuXIlWrVqpR/twChUbgPU7vP29sbbb7+N1q1bK/dap/L2ElC7j23GbAPU7mObMdsA9fpiYmLg7e1tmM9mz0LlNjK+V+F5qdr2Mi6V2wC1+9hmzLak4pl7hOiV5lyJAwMD8fbbb7t82WYymWAymeDr64vy5ctDRLBmzRo4HA6P/bXxo1TuU7nNOUbnJIjs2bOjRYsWCA8Px/Tp07F//36kSZMGffr0QaVKlWC1WpEvXz688847GD58ONKnT49169bhwIED7sxIVNzZhilSpEBISAhSpEgBq9Wqt2/ZsgU7duxASEgIZs6cieXLl2Pp0qVo164dbt26hR9//BGnTp1yV0KiXoU2Fde5uCwWC+x2O/z8/NC9e3c4HA7Mnz8fO3bsgJeXF7p3747y5csDADJmzIhy5cph1qxZyJYtGzZt2qQfCt2T/PLLL7h27Ro++ugj9O7dO96yA4CaNWuibNmyuHjxon6qGSMsu4RmL1ssFty9exc5c+ZE8eLF4evri8aNG6N79+64f/++oY5M4Rzb22+/jY4dOya47IKDg1G7dm1cvXoVJ0+eBACPP8UMoPY25cSJEwAePhcBICgoCACwceNGREREIDw8HOvWrUOvXr3Qu3dvNGjQAPXr18fAgQPRpk0bREZG4pdffsHNmzfd1vA4zudXpUqV0KJFiwSXXY4cOVCjRg3cvXsXu3btcrmfJ1O5DVC7zznGDh06KPtap+L2ElC7j23GbAPU7mObMdsANfuGDRumH91RtR26KreRsUVFRemnk1SVittLJ5XbALX72GbMtqTGiRTkcUwmk76yWq3WeNc7NwBvvfUWMmbMiPXr1+POnTuGOGUCoHafym2PHonAarWiZs2aKF++PFatWoWpU6eievXq+uGQnH8PJpMJRYsWRd26dXH16lX9y29P8+jff61atbBkyRKUKlVKby9fvjyaNm2KcePGQdM0+Pn5QdM0NGnSBBUrVsSRI0ewdetWdwz/sVRuU3mdc3KO07nzMzg4GPXr18f27dsxa9YsVKhQQT9Ni/O2drsdWbJkQcuWLSEi2L9/v3sGn4jhw4ejd+/emDlzJgAgbdq08W7jcDhgt9tRtGhR3LhxA9OnTwcQf1vkiR794O88kk/Xrl3RvHlz2Gw2AECqVKnQpEkT9OjRwzCTKe7du+cytrinhXBy7jgMCQmBzWbTm5zPYU+m6jZl2LBh6N27Nw4fPqxfVqpUKTRq1Ai7d+/GihUrcODAAVitVtSuXRsioi/HrFmz4o033kDJkiXx119/eeTELMD1+ZUyZcp4H3yd/1+nTh1YrVb8+OOP+tE5PHnZAWq3AWr3xW1LkyZNvPEa/bVOxe2lk8p9bDNmG6B2H9uM2Qao1/fNN99gzpw5CAsLw/Xr1909nBdK5TbV/fXXX5g/fz4+++wzfPnll9iwYYM+Ud7o1qxZgxEjRuDDDz9EmzZtMGLECGzbts3dw3opVNtexqVyG6B2H9uM2ZbUPP8bCnplLFy4EH/88QcAPHZlde6gSZ8+PYoWLYqwsDBMmzbN5TpPpHKfym27du3CTz/9hG+//RYbNmzA8ePH9euyZs2KDh06IHny5LDZbMiQIYN+XdwvgJMlS4bXXnsNAHDx4sWkG/xTWLt2LcaNG4fmzZvjq6++wtKlSwE8/KVuvnz59NvZ7XYEBASgf//+8PHxcfmSP3PmzKhRowYA4PLly0kb8Bgqt6m8zgGufY/uUE+bNi3ef/995MyZEw6Hw2W9czY5d9wEBgYCeHjofk+ZTTtkyBDMmjULAHDjxg0ACR/BwWw2w2KxoGnTpggICMAff/yBffv2JXp7T3DmzBls2bIFixYtwrZt2xAWFgbg3yP5FC9eHP/73//g5eWlLw8/Pz80btzYEJMpvvrqK7Rr1w43btx47PrjfP5Vr14d2bNnx8aNG/H7778n1TCfi8rblKFDh2LOnDnIli0bUqdO7XJdnTp1EBAQgG+++QZr1qxBQEAAsmfPDpPJ5PI6nj17djRo0AAAEB4enqTjf5LE3qc8uiPa+f+FChWCpmn4+++/MX/+fACeu+xUbgPU7jtx4gT++OMPLFq0CAcOHMDdu3cBxB+vEV/rVN5eAmr3sc2YbYDafWwzZhugZt/gwYMxe/Zs+Pn54datW7hw4YK7h/TCqNwGANHR0e4ewkszbtw4dO7cGYMGDcKKFSuwcOFCdOjQAR9//DHWr1/v7uH9J6NHj0avXr3w3Xff4fr16zhz5gxmzpyJ1q1bY/Lkybh06ZK7h/hCqLi9dFK5DVC7j23GbHMbIXIzh8MhZ86cEU3TpE2bNrJ161aX6xK7j4hIWFiYlClTRurXry8HDx5MkvE+K5X7VG4TERk9erQUK1ZMNE3T/ylXrpxMmTJF7t+/r7fMnj1bmjZtKufOnYv3GDExMSIi8s8//4imafLll18macPjjBo1SgoWLCiBgYEujd98841+G7vd/tjHiI6OFhGRDRs2iKZpMmzYsJc65qelapvq69yz9P3666/SpEkTCQ8Pj3e9c707f/68BAYGSp8+fZJg9E82ZMgQ0TRNPvjgAylevLjUr19fIiMjE729zWYTEZHJkyeLpmkyatQo/brElre7TJ06VapVq+ayvhUrVkzGjBkjt27deuL9b9++LdOmTZPAwEApUaKEhIaGJsGon47D4ZDIyEgpX768aJomnTt3luvXrz/2Ps7tyy+//CKFCxeWvn37ujyep1B9m+Jc5zp37ixhYWH65XHbZsyYIYUKFdKft2vWrHG5nfO1YPPmzaJpmnz99ddJF/AEj3uf8uDBg3i3dz4vd+7cKcWLF5fWrVvLtWvXRMSznpciareJqN03ZcoUqVSpkt5VsGBB+eyzz+TYsWMJ3t4or3Wqby9V7mNb/PuIeH6biNp9bIt/HxHPbxNRt8/5vrlHjx4yevRo0TRN1q5d6+5hvRAqt4k8fF85a9YsiYiIcPdQXrgpU6aIpmnSvHlz2bZtm0RERMihQ4fkq6++0t9rLl682N3DfC6zZ88WTdOkVatWsm/fPv3yhQsXSsmSJUXTNPniiy/kyJEjbhzlf6Pq9lJE7TYRtfvYFv8+Ip7f5m48IgW5nYggTZo08PHxwZYtWzB37lz9EFaJ/RrVeViaXLly4b333sPx48fx008/6dd7yi+PAbX7VG6bNm0apkyZgsDAQAwfPhxff/21/kvk0aNHo3///jh06BAcDgdatmyJOXPmIGvWrABcz1nt/CX2ypUrATw8lLgnGD9+PKZOnYoiRYpgwoQJCA0NxeDBgwEAs2bN0n8xH/dXkQktG29vbwDA+vXr4eXl5RF9KrepvM4Bz9ZXr149zJ8/H9mzZweQ8Hq3ZMkSiAiCg4OTsCJhQ4YMwdy5c1GrVi18/fXXqFChAm7fvq2f5iIhziMblC9fHgEBAZg6dSrWrVsHwLOO1jB27FiMGjUK6dKlw6hRozBr1ix89tlnSJYsGSZPnowvv/wS+/fvf+x4nUem6N69O2w2GwYNGoTFixcnYUXiRAS+vr76NuD3339Hv3799COKJCTuL8iDgoKwbNky/PjjjwA8a9mpvE0ZOnQo5s6di5o1a6Jr1676uSDtdrvLzPoPP/wQLVq0gMVigdlsxurVq3HkyBEAD1udrwXO2fzFixdP2pBEPOl9Sr9+/XD48GF9GYqI/rzMkycPqlatiq1bt3rk81LlNkDtvlGjRmH06NFIlSoV+vbti9atWyNfvnxYtmwZ/vzzzwTvY5TXOpW3l4DafWxzZZQ2QO0+trkyShugZp/zs2qNGjXw8ccfo2rVqgCAjRs3AnD9rG00KrcBD0+juXz5cnz33XdYvXo1bt265e4hvTAHDx7EvHnzkDZtWvTt2xflypVD6tSpUbBgQQwcOBAffvghAGDgwIH47rvv3DzaZ3P27FksXboUyZMnR9euXVG0aFF9O9C4cWP07dsXwMNflc+ZM8ewpzFRcXvppHIboHYf21wZpc3tXui0DKLn5HA4pGbNmlKwYEHRNE1atmwp27Ztc7k+Mfv27ZO6deuKpmmyZMmSpBjuM1O5T8W2M2fOSOXKleXNN9+U48ePu1y3efNmefPNN0XTNGndurXs3LnTpTEyMlIWLVokGzduFIfDITExMTJr1iwpXbq01K9fXy5dupTUOfFs27ZNihcvLg0bNpTDhw+7XLdixQrRNE3q1asnV65cibf8IiMjZevWrS6/7J07d64UK1ZMGjVqJFeuXEmShsSo3Oak4joX1/P0RUZGytSpU+Wnn36SO3fuyN27d2Xq1KlSpkwZqV27tly8eDEpE+IZPHiwaJomnTp10n9N0LdvX9E0TX799deneowFCxaIpmny9ttvu/xawd02btwoBQsWlLp168Zb57Zt2yYhISGiaZp06NAh3vYyIbdv35Zx48ZJmTJl5MSJEy9z6M9s2LBhommaVK5cWTRNk48//viJR6YQEVm5cqVomiZly5Z1eS57ChW3Kc5fKHXq1Cne8+jKlSty8uRJOXbsmMsv5MeOHSuFCxeWoKAg6dixo6xcudJle1KyZEmPeR1/3vcpcZflli1b9CMirF69OknH/zgqt4mo3bd+/XrRNE2aNGni8ss55/uvxo0bS1RUlMt9Ht2+eOprnZOK28u4VO5jW8I8vU1E7T62JczT20TU6hs6dKj+vtn5Xcj169elQoUK0qxZMzeP7r9Ruc3JZrPJ+++/L5qmSdWqVWXhwoVPdURII1i3bp0EBgbKwIED9cscDod+pLaTJ09KxYoVJSgoyGPWp6f1zz//SLFixaR169b6ZXHbRETee+89CQ4OFk3TZODAgXLz5k39dkai0vbyUSq3iajdx7aEeXqbO3m5eyIHkYjg2rVruHjxIrJnz47cuXNj06ZN+vXlypXTZ0wldG6eokWLon379ujTpw8GDBgAPz8/1K5dOykTHkvlPlXbrly5gkuXLqFp06bIly8fbDYbzGYzzGYzKlSogFSpUmH8+PHYvHkz7HY7unfvjmLFisHhcGDlypUYP348rl27hiJFiuDOnTs4deoUAgICMHr0aGTMmNHdeTh+/Dju37+Ptm3bIjAwECICh8MBk8mEihUrIn/+/Dh58iRu3ryJ9OnT6/ez2+348ccfMWfOHABA4cKFce3aNezZswdp06bFiBEjXG7vDiq3Aequc07P0+dwOLB582bMnTsX169fR44cORAVFYXLly8jQ4YMmDBhAjJlyuSuJAwdOhTfffcdatasiW7duum/iq9evTqWLVuGM2fOPPb+ztYmTZpg//79WL58OcaOHYt+/frpj+VOx44dg8PhwAcffIDAwEA4HA6ICCwWC8qVK4devXqhb9++WL9+PR48eIBPPvkEmqYl+nh+fn5o1aoVmjVrhrRp0yZhSeKcyyBfvnxImzYtunXrhilTpui/mB40aBD8/f3j3c/hcMBsNqNu3bo4cuQIpk6dii+++AIjRoxA0aJFkzojQSpuU06cOIHly5cjefLkaNq0KfLkyaOPf/HixVi0aBFOnjwJu90Ob29vvPXWW2jdujW6dOmC9OnT4/vvv8fatWuxdu1aZMuWDQ8ePMC1a9eQKVMmj3kdf973KXGXZUhICHr06IEhQ4agW7dumD59OkJCQtydpnQboHaf80guH330ETRNQ0xMDLy9vVGhQgXkyZMH/v7+CAsL04+uERgYqG9X7HY7LBaLx77WAWpuL+NSuY9txmwD1O5jmzHbALX6Bg8ejHnz5qF27dro0qUL8ubNC4fDAS8vL6RPnx47d+7E9u3bUa5cObeM779QuS0ui8WClClTwmKx4PLly5gwYQKAh0fwTJUqlZtH999cvHjRZT2y2Wz60UcBIHPmzEiePDnKlSuH3bt3Y8CAAciQIQMqVarkriE/tZs3b+LBgwe4e/cu7t27h+TJk8NsNsNkMunvix0OB4oVKwYAWLx4MXLlyoXWrVsnuF3xVCptLx+lchugdh/bjNnmbpxIQW5nMpkQHh6OmJgY1K5dGzVq1IDZbMaGDRv02yS2kjv/v0GDBggPD8eECROQL18+d2QkSuU+ldsA4Pbt2wD+PU2Ac8xFixZFjx49YLFYsHHjRsyYMQOff/45AgICkDdvXtSpUwdLlizB4cOHkSVLFrz++uvo2rUrcuTI4c4cfafe3r17Afx76grg38Mqp0qVCjly5MDx48dx5coVFChQwOU2WbJkQe7cubF161ZcuHAB6dKlQ9WqVfHJJ58gV65cSdoTl8ptcam+zj1Pn9lsRuHChfHBBx9gwYIFiIyMRIYMGVCxYkV89NFH+qk/3GHFihX66TycpxYQEYgIsmTJAuDhB+JGjRolOtkj7jJs3rw5jhw5goMHD8LPzy9JGhLj/HD/119/Afh3PXMuE+cEphw5ciB16tQoVaoU1q1bh2nTpmHo0KGwWq36evsoX1/fJG15EucyCAoKws2bN5EhQwZMnjwZ7du3jzeZIjw8HMmTJ0eGDBlgNpv15+kHH3yAI0eO4K+//kK6dOncmeNCxW1KxowZ0axZM0ydOhUrVqzQxz916lSMHj0aJpMJJUqUgMViwc6dOxEaGoqwsDD07t0bTZo0QZEiRbBr1y4sWrQIDx48QLZs2VCvXj20aNEC2bJlc3eei+d5n+LcBpnNZrRo0QJhYWH48ccf2ZbEVOpzvh4cPnwYAGC1WgH8e4qja9eu4erVqwgPD8fRo0dx+fJliAiaN2+OVq1aIUOGDPprCOBZr3Vxqbi9jEvlPrYZsw1Qu49txmwD1Om7du0a7ty5gxo1aqBz584uExdTpUqFBg0a4NChQzhx4oThJhuo3PYoEUFERARSp06N2rVrY8WKFZg4cSIA40+mcI59z549iIiIQJo0aWCz2fT3jQcOHMCVK1cwatQoHD16FP3798eYMWOQK1cut3//+iQZM2ZEunTpcPbsWVy8eBF58+bV2ywWC44cOYJTp07hyy+/RKZMmfDPP/9g+PDhKFKkCEqXLu3u4T81VbaXCVG5DVC7j23GbHM3TqQgj3D8+HEAQI4cOVC0aFG0a9cOZrNZ30EBJLySm0wmfUdMp06d0LRp0wR/FepuKvep2Ob8FfS2bdtw6tQp5M6dG4DrDs2CBQuiU6dOiIiIwJo1a5AjRw707t0bwcHBCA4ORtu2bREdHY20adPCarXqXyq7k/ML7WLFimHVqlU4ffo0gH+7nF+Ep06dGgAQFRUV7zFq1KiBSpUq4cyZM4iMjESWLFmQKlUqpEyZMmkiEqFy26NUXOfiepY+Z0/27NnRrl07NGnSBHa7HX5+fnA4HC4Tatyhfv36iIiIQOnSpfUvb0wmE0wmE4KCglCzZk1s3LgR586dQ6ZMmRKdWOBUsGBB9OjRA7ly5UKGDBmSKiNBzi8vgoKCsHnzZty8eRPAv+sa8LD15s2bsNlsaNSoEaKiovDrr79C0zR9uRqJj48PfHx88Oeff6J79+4YNWoUevTogXXr1sHLywtvvfUWJkyYgMyZM2Pw4MHw8/PT1z9/f390794dadKkcesRUhKi2jbF19cXDRs2xJYtW7B06VIEBQWhePHiWLRoEUqXLo1OnTrhtddeAwBs374dkyZNwo4dOzB16lSMHTsWhQsXRuHChfHee+/BZDIhRYoUT1w3k9rzvE/JlSsXevbsGa+jW7du6Ny5s0ccdQlQuw1Qs8+5zS9ZsiR+//13nDp1CuXKlYPFYsHNmzcxbNgwREZGIjAwEOnSpUPBggWxbt06zJ49GxERERg0aBC8vLz09cyTXusepdr28lEq97HNmG2A2n1sM2YboEZfQEAAunbtCi8vLwQEBOiXO99vON+jzJs3DzVq1PC4zzCPo3Lbo06ePImTJ0+ibNmy6Nq1K1KlSoXQ0FAlJlNUq1YNBQsWxOHDhzFu3Dj06NFD/9FFWFgYpk+fjrRp08LX1xdvv/02tm/fjtWrVyMsLAw5cuRI9BfXnqBAgQIoUaIE1q5di65du2LGjBn68/Do0aMYN24czGYzMmbMiODgYLRp0wajRo3CH3/8gdKlS3t026NU2F4mRuU2QO0+thmzzZ04kYI8gre3NwoXLoxChQoBAIoXL442bdoAwBNX8rhfKnrqyq1yn2ptIoK8efPizTffxC+//IJNmzbpH7IeVbhwYXTr1g1t27bFzJkzUa5cOVSoUAEAPOLQ33HF3bmZJ08eWK1WzJgxA6+//rr+y3inhJZP3J1IVqsV+fPnT6KRP5nKbQlRbZ171LP0OY984Ozy9fVNsDUpPfqBtlmzZvFu4xxzcHAw1qxZg4ULF6JUqVKPHbPzcStWrPhSxv004rY51zvnL6EnTJiAcuXKoXDhwgAermthYWGYOXMm8uXLhxo1asDHxwdbtmzBzJkzUbVqVY9b1+L2Of877mXZs2dHnjx5sGfPHjgcDhQqVAhjx45F9+7dsXr1auzevRvXrl1DzZo1E/wVdWBgYJL2PC2jb1MePdWP2WxGrly58Mknn6B58+ZYuHAh9u3bh8jISHTp0kX/BY+IoFy5cvD19UW/fv3w+++/44cffsC7774LAEiRIoXLB0pP8bzvU6ZPn45y5cqhfPnyLrfxpNcCldsAdfuc62CGDBmQOnVqpEmTRr8uKioK+/fvR9GiRTFlyhT4+PggWbJk2LJlCz777DMsW7YMQUFBaN68uctRfNz5Wvc4Rt9ePonKfWwzZhugdh/bjNkGGL/P+VnucRMIKleujOrVq2P79u04ffo0MmXK5PLdi6dSuQ2I/33DjRs38ODBA+TMmRNp0qRB27ZtYTKZMG/ePENNpjh37hyuXLmCiIgI5M+fX58g0bFjRwwZMgQLFizAkSNHUKVKFURHR+Onn37CuXPn0K5dO/0IstWqVcOKFSuwbNkyVKtWzb1BcTx48AA3b97Uf5jhfJ4NGjQIV69exb59+9C4cWMUL14cKVKkwMaNG3Hz5k20bNkSpUqVAgA0bNgQoaGh2LhxIzp37uwRP9Z7WkbfXj6Oym2A2n1sM2abO3EiBSUp56/AkyVL5nJ5xYoVkT59epfDrT3LSu4pVO57Vdqc46pcuTJWr16NkSNHInfu3KhcuXKC4y5btiw+/fRTfPXVVzh69CgqVKjgUX07d+5E6dKlYbFY9A+GlSpVQpcuXZAsWTKXiQbON/P37t0DAH1HYNzzEO7duxdBQUHxngfuoHIboPY6B7y4vrg7XuI2uqv37t27uHv3LgAgZcqU+nMt7hczzjemdevWxaxZs7B9+3bs27cPxYoVS3RZecLyi9uWIkUKpEqVCu+++y7+/PNPrFixAh9++CEGDBgAf39/REVFYcqUKdi/fz+6desG4OHy+uijjzB16lRcvHjR4yZSxO3z8fHRJ+Y4d847dxSGhYXp9wkMDET79u3x+eef48aNGyhQoADatWsHIP6XXO6m6jYloeUGAKVLl8Ynn3yC4cOHIywsDFWrVnWZROFUpEgRdOjQAT169MCePXvw7rvvxutzd+uLep9y+PBhlC9f3qOWn8ptgNp9MTExAP49nVq9evVQpkwZl19+ZsqUCePHj0dQUBB8fX3105NUqFAB/fr1Q6dOnbBt2zY0a9ZMP22JJ1B1e+mkch/bHjJaG6B2H9seMloboF7fmTNnkDNnTlgslseOx/n5p2zZsli3bh2mT5+OsmXLevREA5XbgH/7EnouVahQQf9Rl6+vr/48NMpkihkzZmDJkiU4c+YMgIen9Khbty6aN2+OGjVqICYmBlOmTMGePXuwZ88eAA/XyTZt2qBHjx764wQHByNdunS4c+cOAPd/hgOAuXPnYvXq1dizZw/SpEmDHDly4IMPPkDhwoWRM2dOTJo0CZ9++in++usv/Pbbb7BYLPD390fHjh3RuXNnAA+/S0qVKhXSpUuHiIgI3L9/3yMnUqi2vYxL5TZA7T62PWS0Nk/EiRSUJFatWoU9e/bgwIEDsFqtaNy4MUqVKqUfijZDhgz6oVudX26bTCYUL15cn027du1a/TpPW8lV7nvV2kqWLImMGTOiXr16CAsLw6RJk9CxY0fMmjULZcqUifeLZeDh6SQA4M8//0TLli095kvgMWPGYMqUKejWrRvat28Pi8WC6OhoWK1W/cUTQLxDlt+/fx/Av5MNnBMNFi9ejK+++gotWrRAnz59krAkPpXbVF7nALX7vv/+e6xevRpHjhyB2WxGoUKFULduXTRs2DDeFzoOhwMZM2ZEkyZNMHbsWKxbtw7FihVze0NiEmqrVasW3n77bQwZMgQigl9//RW9e/d2uV+LFi3Qvn17/f81TQMAHDx4EJUqVUrShsd53LIzm82IiYmBt7c3ChYsiE2bNuH48ePQNA27d+/Gzz//jAcPHsDf3x/Hjh1Dx44d8cUXX7j9cPtOr+o6Bzw8ZdOBAwfw66+/6rPpE3odL1y4MJIlS4ZTp07BZrPpE2fcTeX3KSq3AWr3rVy5En///TcOHz4MHx8ftGjRAiVLlkTy5MldJlE4BQcHu7wGOidTFClSBL6+vrhw4YK+jXU3lbeXgNp9bDNmG6B2H9uM2Qao2Td+/Hj8/vvv6N27NypVqvTY8TjfbzRs2BBLlizB1q1b8csvv+CNN95I6mE/FZXbgIT7nN91lS5dGsmSJdOPfuhwOODr66s/D7/77jtMmDABIoLXX3/d4yZTDB8+HLNmzUKGDBnQrFkzXLlyBUePHsX333+P27dvo1+/fqhbty5CQkKwYsUK3L59G+nTp0e2bNn00zU6vwv08vKCzWbzmB9Hffvtt5g+fToCAgJQsWJFREREYP/+/ejbty/KlCmDNm3aoFy5cpg2bRp27dqFq1ev6hMmnMvT2WYymfTvHDzt9MMqbi+dVG4D1O5jmzHbPBknUtBL9+2332LmzJlwOBxInTo1bt26hb1796Jdu3bo1KlTvB2dcb9ANJlMKFasmL5jdO3atbBYLIiJidHfPLqbyn2valuHDh1gsVjQpUsX3Lx5EwsXLkTr1q0xbdo0l8Mqx8TEwGq1ImfOnLBarUidOrVHzWJ3/kJ3wYIFMJlM+Oijj2C1Wl2OwgD8+0EyJiYGIoLz588DcP3F7pIlSzBu3Dh4e3ujQYMGSViRMFXbVF7nALX7RowYgZkzZ8LLywv58+fHrVu3sGnTJmzatAlRUVF4//33Xcbo7AwJCcGcOXMwffp0lClTRv8liSd5XFt0dDSaNGmCb7/9FiVKlMDJkyfxzz//oHTp0ihYsKD+hdSDBw+QPHly5MyZE4D7Tr2SkKdZds4dfAUKFIDdbkdUVBROnDiBMWPGYOfOnfjiiy9QvXp1NG/eHOvWrUNkZCTGjBnj9kPhvarr3IMHD9C4cWNkz54dISEhMJlMCR4pxG63w8vLC/7+/vo5aOO+hriTyu9TVG4D1O5zfiEMPDwdWnR0NP78808MHjwY9erVS/DLF+e4404ktFgsSJYsGex2O7JmzeoRv6pTeXsJqN3HNmO2AWr3sc2YbYC6fSaTCcePH8eCBQsgIqhcufJjd5zY7Xb4+fmha9eu6NGjB1auXImQkBD4+/vH+ztwN5XbgIT7zGaz/v1X0aJFAUA/upfD4YCPj4/+PPzuu+/0I1N40mSKxYsXY9asWQgJCUGvXr1QsGBB2O12bNu2DaNHj8amTZvQqFEjpE+fHqlSpULTpk1d7u9cvs73kd9//z1u3bqlT7Bw507BNWvWYMaMGahUqRJ69eqlf4+wevVqLF++HJs2bcLJkycxYMAAVK9eXT+FB/Dw+RkdHQ2z2ay3fffddzhz5gxatGjhEZ8JnFTdXgJqtwFq97HNmG0eT4heohkzZoimadK6dWvZvn27REREyLx586RMmTJSqFAhOXPmzGPv73A49P/++++/pUuXLqJpmnz88cdy7969lz38J1K571Vvizv+zz//XDRNk0KFCsmSJUvk+vXrLo83depU0TRNpk6dKiKu7e7k7NQ0TYKDg2XatGn6dXa7PcH7xMTESK1ataRw4cJy69YtERFZvHixlCtXToKDg+XYsWNJMvYnUbFN5XVORO2+2bNn62179+4VEZHz58/L9OnTRdM0qVOnjhw+fPiJ9+/QoYOEh4cn0aifztO0HThwwOU+0dHRLv8fFRWl//fgwYNF0zTZsmXLSx/703jWZff3339LUFCQdOnSRZo1ayaapsmMGTP06w8fPixly5YVTdPk4sWLSZ3j4lVf5w4dOpTgfW02W7zLJk2a5FGv4yq/T1G5TUTtvilTpoimadK8eXPZuHGjnDhxQkaOHCmapknNmjXl5s2b8e7jfE/mHHfc92ijR48WTdNk8uTJLrdxB5W3lyJq97EtcZ7cJqJ2H9sS58ltImr3Od8/a5omLVu2lM2bN+vXPe41+OLFi9KpUyeX12yRxL93cQeV20Ser8/ZcOfOHRkzZoyULFlSqlSpIrNnz5bbt28nybgfJyYmRtq2bSslSpSQ/fv3i8i/n9NsNpsMHz5cNE2Tzp07i8PhiLdMDh8+LAMGDJDNmzfLgQMHZPz48VKmTBmpU6eO2z+Hi4h8++23EhgYKHv27BGRf78jsdlscvbsWfnss89E0zQpWbKkrFu3Tr+fw+GQI0eOSPfu3WXEiBGycuVKGTJkiAQHB0v16tXl7NmzbulJiMrbS5XbRNTuY1viPLnNCDiRgl6a8PBwqVOnjoSEhMTbgdSrVy/RNC3BHUuPvjmKu5Lv3LlTevfuLWFhYS9n0M9A5T62Pbw8bs+IESNE0zQpWLCgdOnSRb7//nv5+++/ZcSIEVKmTBmpVq2anD9/PklbEuP8e9+wYYOULFlSWrRoIUWKFJFChQq5TDhIaGeSw+GQWrVqSaVKleTevXuydOlSKVu2rAQHB8vRo0eTrCExqrapvM6JqNvncDgkPDxcXn/9dSlXrly8Hbc3b96UNm3aSKFChWTNmjUJ3l9EJDIyUjp27CiFCxeWyZMnx9uR5g7/pc1ut4vD4ZB//vlHNmzYIKdOnRKRh18AlSlTRt577z25du1aUqUk6Hn7oqKi5PXXX5ciRYqIpmkyc+ZM/TrnlyPHjx9/4geYl43rXMLr3K1bt2TQoEGyatUqOXTokNy9e1cmTpwor732mtSoUUMuXLiQVCmJUvl9isptImr3HTt2TCpVqiTVqlWL956pUaNGUq9ePbl//36C942MjJRRo0bJtm3bJDw8XGw2m0ydOlWCg4M9Yr1TdXvppHIf24zZJqJ2H9uM2Saibp9zPM6Jw1WqVJHAwEBp1qzZU0842LZtmxQtWlQ0TZPQ0NCXPuanpXKbyH/vizuZYty4cfqEc+cPi9zp0qVLUrhwYXnnnXdE5OHECpF/W44dOyaFCxeWDz74IN59HQ6HPtk4KChIAgMDRdM0qVq1qhw/fjzJGhJit9slJiZGWrRoIYUKFZLw8PAEl8/9+/dlyJAhommalCtXTrZt26Zft2rVKilVqpQ+eUbTNKldu7bb2+JSdXsponabiNp9bDNmm1F4xrFrSUmRkZE4d+4c6tSpg8DAQIgIoqKi9EN7+/j4YNmyZUiePDkAIH/+/Khfv368Q6jFPRxbqVKlUKRIEY8435nKfWz7ty137txo2LAhevXqhVy5cmHVqlVYvXo1Vq9erT9mzpw5MXHiRGTJksVdWS6ch2LKmzcvHA4HihcvjkaNGmHgwIEYP348AKBt27bxDgcXHR2N6OhoPHjwAJcvX8bw4cOxatUq2Gw2LFiwAAUKFEjylkep2qbyOgeo22cymXDjxg2cOHECb775JgoWLOhy/rk0adKgQIEC2Lx5M7Zu3YoaNWq4HF7S+W9fX180bNgQx48fx5gxY5AxY0Y0bNjQ7Yd+fd42s9mMu3fvIjQ0FD/99BN8fX2RKlUqXLhwAf7+/hgyZAjSpUvntrbn7bPb7RAR5MyZE2FhYejbty9atmwJ4OGh6r29vSEiyJcvnxvLHuI6l/A6t2vXLoSGhiI0NBRmsxl+fn64desWMmXKhMmTJyNz5sxua3NS+X2Kym2A2n3Xrl3D5cuX0bJlSxQoUAAiggcPHsDLywu+vr5ImzYtvv/+e0RHRyNHjhzInj07goKCAAC7d+/GtGnTMHXqVPj4+CB58uS4fv06MmTI4BHrnarbSyeV+9hmzDZA7T62GbMNULfP+T743r17+unu9u7di19++UW/TYUKFR57Koxy5crh888/R79+/TBy5Eg0aNAAvr6+bj8MuMptwH/vi3uaj1atWsHb2xu1atXyiFN7WK1W+Pn54fbt27h58ybSpk0L4N9m56l7T58+jfv378NqtbqcLq5t27bw9fXF2bNnceHCBf20olmzZnVbE/Dw79xsNiMoKAh//vknLl26hOzZs8c73H7y5MnRo0cP2O12hIaG6qcF1TQNderUQfbs2XHkyBFcvHgR2bJlQ7ly5ZAxY0Y3lrlSdXsJqN0GqN3HNmO2GQUnUtBLI7HnZrt7967+/8mTJ8eNGzfw22+/4e7du1i+fDkiIiL0+xw8eBCffPIJALi8yYi7knvKyq1yH9tc2w4dOoR+/frhnXfeQeXKlXH48GHs378f9+/fR2BgIMqUKeNRb2iBh10+Pj5ImzYtrl27hk6dOiEiIgLffvstxo8fD5PJhDZt2uDIkSM4ffo06tSpA6vVCqvVisKFC+P69etYunQprFarR0w0iEvFNpXXOUDtvvv37+s7coF/z/3uHHP+/PkBALdv3waARL+QqVGjBo4cOYLvv/8exYoV84gvbv5Lm4+PDypVqgQfHx9s2bIF6dKlw2uvvYYOHTogR44cSRuSiGftM5vNSJYsGQYMGID33nsPlStX1u8X9/npCbjOJfy8rFatGjp37owDBw7g8OHDyJUrFwIDA9GsWTNky5YtaUMSofL7FJXbALX77t27BwCw2WwAHq5XKVKkwKFDh7B//348ePAAW7du1W+fLVs2tG/fXu/68MMPcfDgQRw/fhy5c+dG3bp18cEHHyB79uxu6YlL5e0loHYf24zZBqjdxzZjtgFq992+fRs7duyA1WpFtWrVUKhQIURFReH333/Xb5PYDnnn/7/11luIjIxEuXLl4Ofn546MBKncBvy3PuDfyRS+vr746KOPPObzatq0aZE/f378+eefmDdvHtq3bw+r1Qrg4XKxWq3w8vKCxWJx+aGUPDy6OsxmM5o2bequ4SfKuQxy584NABg5ciQmT54Mf3//eMsnWbJkaN++Pa5cuYI1a9Zg06ZNyJ8/P8xmMwoVKoRChQq5K+OJVN5eqtwGqN3HNmO2GQUnUtBLkyZNGpjNZmzfvh3Xrl1DQEAAbDYbZs+ejbCwMLz22mto1KgRUqZMicuXL2Pw4MGYPXs2UqRIgS5duiQ4Y8qTqNzHNte27777DilTpkS3bt2QPn16pE+fHpUqVXJ3ymOZTCb4+/sjd+7cOHDgAOx2O9555x2YzWaMGDEC48aNw4ULFxAWFoa//voLS5YsQeHChWEymVCgQAGsW7cOqVOnRmhoqL5TylOo2KbyOgeo3Zc6dWokS5YMv/32G1q2bInAwEAA0Mfs6+sLs9msfymQELvdDovFgk6dOqFp06bw9/dPkrE/yfO2Od+g16tXD/Xq1cPt27eRKlUqREdHP/bvIak9a5/JZILD4UDmzJn1X1A7l52n4ToX/3lps9ng5eWFjh07wuFwICIiAv7+/vrlnkLl9ykqtwFq9jm3cenSpYO3tzfmz5+PUqVKwc/PD3fv3sXAgQNhs9nw3nvvoVatWjhz5gwOHjyIH3/8EV988QW8vLz0o27YbDbcunUL6dKlQ0xMDLy9vd2dB0Dt7SWgdh/bjNkGqN3HNmO2AWr1JTRhIDo6GkWLFkXGjBmRMWNGfPjhhwDwxB3ycf//gw8+SNqQBKjcBrzYPidPm/Tv1KZNG9y5cwc5c+Z0+ezm3LlntVqRNm1a/Trn9wwm08OjFHrK9yZxOf+O3333Xfz000/Ys2cPpk2bhk6dOsHX1zfe8gkICEDjxo2xe/duLFmyBE2bNo13u8SOpuJOKm0vH6VyG6B2H9uM2WYYQvQSTZw4UTZs2KCfj8fhcMiPP/4oHTp0kKioKJfbbty4UTRNk9KlS8uxY8fcMdxnpnIf2x56tO1x51ZMag6HQx9PYue8+uyzzyQkJEQ/B2JkZKR8//33UqhQIQkKChJN02T27Nki8u85CcPDw+XDDz9067JUuS0xKq9zImr3ffbZZxIYGCi//vprvOt+/fVX0TRNPv30U/2yR5/TIhLv78BTvIi26OhoEXn8uWnd5UX0JXSZJ+A6Z9znpSrvUxKicpuIOn0TJkyQQ4cOuVw2bNgwl3M1FypUyOW9ltPt27dlwoQJommafPzxx3L37l39urh/L55E5e2liNp9bHvIaG0iavex7SGjtYkYv2/16tVy/vx5EXF9rY2MjJRp06bJDz/8IDabTb98//790qVLFwkMDJRmzZrJ5s2b9es87bVa5TYRtfv++OMPmT17tvTv31+GDBkihw8flqioKImKipKwsLAE73Ps2DHRNE3effddEfn3s5uIyNKlS6VXr15y4MCBJBn/4xw+fFg2btwooaGhsmPHDrlz545+3bZt26Rq1apSpkwZmTFjhn5dQsuna9euomma7Ny5M8nG/l8ZfXv5OCq3iajdx7aHjNZmBJxIQS/ElStXXP4/sR2fIiK3bt2S27dvi8jDnZt2u12/vmfPnlKoUCHZu3fvyx3wM1K5j20PGa3NKTIyUkT+nSjgbHR+wPrpp59E0zTZtm2bfp9t27ZJrVq19C/BFyxYoF9nt9slOjra5c2/u6japvrzUuW+xNoiIiJkx44dLtc5O0JDQ0XTNBkwYICIuH4BsGTJEpk/f/7LHPJTexltcdc/d3sVl50T1znPXG4ir+ayczJym4jafUOHDhVN0+Tzzz+XqKgo/X2YiMjkyZOlbdu20qhRI6lWrZo0adJEvy7uunbmzBmpW7euFCtWTM6ePZuk438clZebiNp9bHvIaG0iavex7SGjtYmo2bdmzRrRNE369+8vly5dEhHXjtu3b8f7bkXEGDvkVW4TUbtv7NixUqRIEZfJuCEhITJ79mz9Oz+R+OPduXOnaJomrVu3drlu8eLFUqpUKSlfvrw+8cRdpkyZIpUrV5aCBQuKpmlSvHhx+fjjjyUiIkJEHn6nOWfOHHnttdekYsWKLs3O5ejc+TljxgzRNE1+//1398Q8horbSyeV20TU7mPbQ0ZrMzLzk49ZQfR4o0ePRo8ePXDkyBH9sscdLiZVqlT6Oee8vLz087UBQMqUKWGz2XDt2rUkGPnTUbmPbcZsA4AffvgB3bp1w5tvvokWLVpg8ODBOHr0qN7oPNS887x8znNk7du3D5MnT8aZM2dQvXp1mM1mDB06FDNmzADw8O/I29sbPj4+SR8VS+U21Z+XKvc9ri116tR47bXXAEAfv7MzJiYGAPRz3DsPZb5kyRKMHDkSY8aMwc2bN5MmIhEvq2306NFubwNe3WXnxHXO85Yb8OouOyejtgFq9w0dOhRz5sxBrVq10KxZM/381Ha7HQDQvn17TJ06FYsXL0adOnUQEBAAAIiOjnY5VUeOHDmQOXNmPHjwAHfu3HFLy6NUXm6A2n1sM2YboHYf24zZBqjblzt3blgsFmzYsAETJkzA5cuX9VMTAoCfn59+Ojuz2QwRAQAUKVIEbdq0Qa1atbBr1y5MnToV27ZtA+A5h/5WuQ1Qt2/MmDGYNGkS8uTJg8GDB2Pq1Kl47733cP/+fUyZMgW7d+8G8PDz3KPjdX6mCwgI0K9bsmQJRo8eDZPJhFmzZiFLlixJGxTH6NGjMXr0aKRIkQKdO3dGmzZtkDVrVqxbtw4zZsyAzWaDr68v3njjDbRo0QJRUVGYPn06Zs2ahYiICJjNZpdTnx46dAgpUqRAzpw53daUEFW3l4DabYDafWwzZpvRcSIF/WenT5/G3r17MWnSJJeVPDHON3xONptN3zF67tw5BAQEQNO0lzLW56FyH9v+ZaS2kSNHYsCAAVi7di3sdjuOHz+OhQsXonHjxpg5cybOnj2r3zYgIABWqxUnT57E33//jREjRuCvv/7CJ598gokTJ6JPnz6wWCwYOXIkvvvuOzdWPaRyG6D28xJQuy+xtkcbHn2D69yBlCJFCv0y5xcADocDoaGhSJs27Usc+ZOp3Aao3cd1zpjLDXg1l11ijNQGqNs3ZMgQzJ07FzVr1kTXrl2RL18+AA/Hb7FY9C9kHA4H7t+/j/Xr1+PQoUO4ceOG/iWwzWbTHy8iIgJZs2ZFhgwZkj4mAaouNyeV+9j2LyO1AWr3se1fRmoD1O2zWq2wWq24fv06fv31V0ycOBGXL1922fEel8lkSnCHvPO7lR07diR1QqJUbgPU7Pvjjz8wY8YMFCtWDN988w3efvttVK5cGV999RXeffddREREYNy4cYiKinL5POfsunXrFgDoOwgXL16M0aNHIzo6GqGhoW5d5zZt2oTp06cjODgYY8aMQYcOHdCrVy8MGTIEFosFZ86c0Se++Pv7491330WHDh1gsVgwefJkfPrppy7fbc6fPx8bNmxAoUKFkD59endlJUjV7SWgdhugdh/b/mWkNqPjRAp6bs5fJvn7+8Nms2Hz5s2YMGECjh49+tj7mUwm3Lt3D5cuXQIA/c3F3LlzsW3bNhQtWtQjvuBWuY9t8RmhDQCWLl2KWbNmoUqVKvj++++xZs0arF69Gm3btoW3tzdGjx6NqVOn4tChQwCALFmyIE+ePFi1ahXGjRuHXbt2oU+fPmjVqhUA4M0338THH3+MdOnSISQkxJ1pSrep/rxUue9JbU/6pceVK1cAQD8SyqJFi1y+AChQoMBLHP3jqdwGqN3HdS5xnrzcgFd72SXGCG2A2n0jRozAd999h1q1aqFr167Imzevfp3zy/q4Rwbz8/NDYGAgzp07h59//lk/youzbc6cOTh48CCKFy/u1iOBAWovN0DtPrbFZ4Q2QO0+tsVnhDZA7T4RwdWrV3H//n3kz58fmTNnxpIlS/Qd8nF3vMf16A75tm3bokyZMrhw4QKyZ8+e1BkJUrkNULfv4MGDsNlsaNOmjb4Tz3mUiXbt2iFHjhy4fPlyvF9KP/pZ7+7du/qRBaOjo7FgwQK3f547cuQIHA4HWrVqBU3TICJwOBzInDkzAgICYLPZcPDgQezduxdXrlxBQEAAmjVrhi+//BKBgYHYsGEDGjRogEaNGqF+/foYNGgQUqRIga+++srt2xInlbeXKrcBavexLT4jtKnCy90DIONyfpnmfJNjtVqxdu1aiAi6dOmS6Gyn6OhoTJkyBb///jveeust+Pj44ODBg/j111/h7++PPn36wNfXN8k6EqNyH9viM0IbAGzduhU+Pj7o1KkTChUqBBFBqlSp0LNnT+TPnx/z5s3D0qVLERkZidatW6NYsWLInTs3Vq1ahbCwMPTq1QutW7cG8PADm5+fH5o2bYr33nsPqVOnZttLovrzUuW+52179P42mw0rV67EmDFjYLPZPOILAJXbALX7uM49+f6euNwALruEGKENULfv22+/xcyZMxEcHIyePXu6HFJ406ZNOH78OP755x+kSJECNWrUQL58+ZArVy7Ur18f27dvx8yZM3H69GnUqVMHfn5+WLFiBZYsWYKAgAB07doVyZIlc1sboO5yc1K5j23xGaENULuPbfEZoQ1Qu89kMumTGhs1aoR8+fJh2LBhWLp0KQCgY8eOyJgxI0Qk3o5q5w55k8mEwoULo2fPnkiXLp1+mjx3U7kNULPP4XDg+PHjAICsWbPql3l7e0NEYLVa4evri/DwcISHh+u3ics5EXfz5s3YsmULHA6Hx3yeu3r1KgDoR12z2Wzw9vZGVFQUIiIisGXLFpw6dQqnTp1CQEAA6tSpg8aNG6Nq1aooU6YMpkyZgoMHD2Lfvn3IkSMH6tevj86dO3vUaT1U3l6q3Aao3ce2+IzQpgpOpKDn5lyxb9++jZQpU6Jnz574/fffsW7dOgBIdCWPiorC/fv3cfr0aYwaNQrAw9lSBQoUwIgRI5ArV64ka3gclfvYZsy2u3fvYs+ePcicOTMKFSqkn0fQ4XDAbDajQYMGSJs2LaZPn47ff/8dXl5e6NOnDz744AP89ttv6NmzJ9q0aQMA+n0AuP2XgoDabYDaz0tA7b7nbXNyfomxatUqhIeHe9QOXZXbALX7uM4Zc7kBXHZGbQPU7XOO2dvbG8mTJ9cvnzFjBkaNGqWf0gMAVqxYgQoVKqB169aoUqUKmjVrhh9++AGLFi3C4sWLISIQEeTOnRvjx49Hjhw5krznUaouNyeV+9hmzDZA7T62GbMNULvP4XBg586dAIDAwECUKlUKXbt2xbhx4555h3xQUFCSj/9xVG4D1Owzm836j5n++usv5M+fH97e3gAejtfX1xfZsmXDsWPH9F9PP6pw4cLIlSsXTp8+jdSpU3vU5zln2/r161GoUCF4e3vj9u3bGDVqFKKiopA/f36kS5cO6dOnx9GjRzF//nxcunQJ3bp1Q758+dCzZ0+ICM6fP48MGTLA4XC4vAf3BCpvL1VuA9TuY5sx21TBiRT0n1y9ehV///030qZNi5CQEGTPnh1RUVGPXcn9/PzQv39/FC5cGBEREbh69SpKliyJokWLIiAgwB0ZiVK5j23Ga0uZMiUyZcqEsLAwHD9+XP8QEfeDU8WKFWEymRATE4PffvsN2bJlQ/fu3bF+/XpkzpwZgOtEA0+hcpuTqs9LJ5X7nqfNyfkhe/fu3UiVKpVHfQEAqN0GqN3Hdc6Yyw3gsjNqG6BmX+XKldG4cWMsWrQIv/zyC9q0aYPvv/8eI0eORI4cOdCqVStkzpwZhw8fxtq1a7Fx40ZERETgyy+/RNu2bREcHIyVK1fi4sWLSJkyJUqXLo2aNWsiU6ZM7k7Tqbjc4lK5j23GbAPU7mObMdsAdfvMZjOsViuyZs2KwoULw2q1onLlygDwTDvkPZHKbYAafc6JtGazGTabDV5eXqhZsyZWrFiBtWvXom7duvGOknHv3j3YbDZERUXplzm/zxMRJE+eHOnTp8e1a9ewYMECl9POJaW4bXa7HRaLBVWqVMGqVatQokQJ/fvH8+fPY9u2bShbtizGjRsHPz8/mEwmbNiwAbNmzcLmzZtRvXp15MuXT+/MkiWL3uuJVN1eAmq3AWr3sc2YbSrgRAr6T6Kjo3H79m2UKVMGWbNmRZYsWWAymTBx4sQEV/K4b0DefPNNdw79qajcxzbjtZlMJhQvXhx///03li9fjvbt28PPz0+/zvmBqkKFCrh79y6GDh2KWbNmoXz58njttdcAeO5EA5XbnFR9Xjqp3PesbXHVqlULM2bMwM2bN7Fw4UK3fQGQGJXbALX7uM4Zc7kBXHZGbQPU6nO+t/Lz89NP0zFy5EjkypULf/31F4oUKYL+/fujePHiAIAqVaqgXLlymDt3Ln777TcsX74cffr0Qbly5VCuXDn9i3NPpNJyS4jKfWwzZhugdh/bjNkGqNnn3Ln7+uuvIygoSP9le/LkyVGlShUAT79D3tOo3Aao0+f8wZPVatXfC+bNmxfvvPMOcuXKFW8Shd1u1ydQOI8iG/d95NGjR5EvXz588cUX8PLycuspL+K2WSwWAEDOnDkRGhoKf39//XYFCxbEl19+idKlSyNVqlSIiYmBt7c3KlasiCtXrmDnzp1YvHgx3nzzTf17y0cP5e9pVNxeOqncBqjdxzZjtqnAM7/pIMNImTIl/ve//yFTpkz6G7kyZcoAQIIruclk8tg3CAlRuY9txmx74403sHbtWqxcuRJly5ZFxYoV9eviTjioXbs2Tp8+jdGjR2PChAkoVKgQfH19PXqigcptgNrPS0Dtvmdti8vX1xeNGzdGrVq1PPKQaiq3AWr3cZ0z5nIDuOwAY7YBavXFHVepUqXQokULDBo0CEOHDsX58+cxZMgQfRKF88vg4sWL4+7du9i8eTOWLFmCxo0b66fv8NRJFIBayy0hKvexzZhtgNp9bDNmG2Dsvv379+PkyZM4deoUfHx8ULx4cRQsWFD/AYqmacifPz/MZrP+I5NkyZIZYoe8ym2A2n2rVq3C7t27sW/fPuTLlw/ly5dH1apVkTFjRnTs2BG+vr4A4DJeEYHdbkeKFCmQKlUqAP++j1y4cCHmzJmD999/H61bt3ZPVKzE2pxjdnIus9q1a+uXeXt7Q0Tg5eWFKlWqIH369IiOjvaIZfa0jLy9fBKV2wC1+9hmzDYVmMRTjx9EHuPw4cO4evUqzp49i+DgYGTJksXlTcONGzfg5+cHb29v/c2Dw+HAX3/9hYkTJ2Lnzp2oXr36E89p7S4q97HNmG1A/L7MmTMjderUuHfvHiZMmIBZs2ZB0zSMGDEi3iHL4x6Z4d1338XNmzexePFil9nS7vQqtan+vFSp72W0OX9h4u6jpajcBqjdx3XOmMsN4LIzahugdt+mTZsQHh6OM2fOoFixYihWrJg+EQIA+vfvr39R/9NPPyEwMFD/EifuF+A9evTAypUrERoailKlSrml5VEqLzdA7T62GbMNULuPbcZsA9TsmzhxIkJDQ3Hz5k2XywsVKoThw4c/8QhsUVFR2LhxI8aNG4fTp0/j3XffxUcffaSfItWdVG4D1O4bOXIkZsyY4XKZv78/WrdujVatWsFsNsd7Dwk8XAebNGmCy5cvY9myZcidOzcAYMmSJRg7dixu3LiBX375xa1HFnxcW+vWrZ/qs6Zz+3L58mW88cYbKFKkCGbOnPmyhvxcVNxeOqncBqjdxzZjtqnOc382Qh5hypQpmD9/Pq5duwYRQYYMGVC7dm20a9cO6dOnBwCXHZhxD0+V0Iyprl27etR5qlXuY5sx24DE+9q0aYOMGTOiXbt2OHnyJDZu3IhPP/003ocvs9mM6OhoWK1WZMyYEf/88w9OnjzpEZMNXsU21Z+XKvS9rDbnoR/duUNX5TZA7T6uc8ZcbgCXnVHbALX7xowZg+nTp8PhcEBEEBoairp162L48OEwmUzw8vLCxx9/jH379iFZsmT6YZidX3ybTCaX92AAcP36dbf1xKXycgPU7mObMdsAtfvYZsw2QM2+iRMnYvz48ShdujTat2+PHDlyICIiAqNHj8a2bdvw4Ycf4vPPP0dISAisVmuCjxH36AaTJk3CokWLYLVa8cknn+jvn91B5TZA7b4pU6ZgxowZKFu2LD766COkTZsWmzZtwqxZs7B27Vq0bNkywUkUTna7Hb6+vsiaNSuAh5MoRo8ejZiYGCxfvtytkyieps1sNru0OXd4xj3UvnP7smjRIty+fVuffOwpRxNRcXvppHIboHYf24zZ9koQokSMGDFCNE2T6tWry9ixY6VHjx5So0YNCQ4OlnXr1j3VY9jtdtm+fbs0a9ZMgoKCpEWLFnL8+PGXPPKno3If2x7PU9tEnr7v2rVr0qxZM9E0TRo2bCi7du2SBw8eiIhIdHS0frsWLVpItWrV5NKlS0ne8ii2PZ4Kz8vH8dQ+tj2ep7aJqN3Htsfz1DYRtftUbhNRu2/ixImiaZo0atRIFi9eLN9//718/vnncvjwYZfbRUVFyfbt2+X27dvxHiMmJkb/748++khCQkIkPDz8pY/9SVRebiJq97Ht8Ty1TUTtPrY9nqe2iajZt2vXLilTpoxUrlxZDh48GO/6Nm3aiKZpUqFCBVm5cqXYbLbHPt6DBw/k559/lvfee0/CwsJe1rCfisptImr3HT58WCpWrCg1atRweS9548YNadCggZQrV07u3Lnjch+73a7/94ULF6Rs2bJSvXp1uX37tixZskTKli0rwcHBcvTo0STrSMjztDmXXWRkpMybN09+/vlnOXfunNy+fVumTp0qwcHBUqNGDblw4UKStjyOittLJ5XbRNTuY9vjeWrbq4ITKShBq1atEk3T5IMPPtDfONy9e1fmz58vmqbJ0KFDn/qx7Ha7/Pnnn9KgQQMJDg6WixcvvqxhPzWV+9j2dDytTeTZ+65cuSJt27YVTdOkZs2aEhoa6jKpYP78+RIUFCQdOnSQyMjIJG15FNuejgrPy8fxtD62PR1PaxNRu49tT8fT2kTU7lO5TUTtvkOHDkn58uWlQYMGCX457Zys6pwo4XA49OsiIyPl2LFjLu+1nO/BWrduLbdu3XrJo388lZebiNp9bHs6ntYmonYf256Op7WJqNv3yy+/SMGCBWXUqFEi8u9rtPMHJidOnJBSpUqJpmlSpkwZ2bRpk4i47rR+1IMHD9z+HYqI2m0iavdt3bpVAgMDZfz48SLy8D2ks6tDhw7yv//9T2bPni2zZ8+W0NBQuXz5ssv9L1y4IBUqVJDSpUvL+PHjJSQkREqVKuX2SRQi/61t/fr1Ur16ddE0TV577TUJCQkRTdOkYsWKHrWzU9XtpYjabSJq97Ht6Xha26uEp/agBP39998AgA4dOiAwMBA2mw0pU6ZEvnz54Ovri9y5c+Ps2bOwWq3w9vZ+7GH1zWYzgoODMXDgQGTOnBmZMmVKoorEqdzHNmO2Ac/WBwAZM2bEsGHDMGnSJPz6668YNGgQQkNDUaJECVy+fBnbt29H2rRp0bt3b/j6+rqxjG2vyvPSaH1sM2YboHYf24zZBqjdp3IboHbf+fPncf36dbRv397l0J8nTpzAgQMHsG7dOlitVqRPnx5vvPEGgoKCADw8J/eYMWOwa9cuZMmSBcWLF8eBAwewfv16pEqVCgMGDHA5n6s7qLzcALX72GbMNkDtPrYZsw1Qt+/QoUNwOByw2WwAHp4OwcvLC97e3hAR+Pj4wGKxoEqVKvjzzz/Rv39/LFiwANmzZ9dPNfCoZMmSIVmyZEmdEo/KbYDafRcuXICIIDw8HPfu3UPKlCkBAP/88w927NiBe/fuYdeuXfrtp02bhq+++goVKlSAxWJBunTpkCVLFuzbtw8TJ06Ej48PFixY4BGHqH/etsqVK6NgwYJ499138ccff+DixYtInz49Xn/9dTRr1gzZs2d3V1I8qm4vAbXbALX72GbMtlcJJ1KQC4fDAbvdjoMHDwL497w8IgIAuHnzJkQEY8aMwdixY2GxWODr64vevXujYsWK8Pb2jveYIgKLxaKfC8ydVO5jmzHbgOfv69GjB2rWrImePXuiYsWK+Pnnn/H777/j1KlTSJs2LcqWLYsBAwYgd+7cbPOgNtWfl0boY5sx2wC1+9hmzDZA7T6V2wD1+wDg8uXLAOAy8XTPnj0YO3Ys/vzzT5fbhoaGol+/fmjYsCFMJhMsFguuXLmCI0eOYP369bBYLChSpAgGDx7M92Avkcp9bDNmG6B2H9uM2Qao3+d8rT19+jQAwMvLCzabDRaLBSaTCeHh4YiJiUGbNm2QN29ezJw5EwMHDsSECRPc/oOTJ1G5DVC7r1SpUsicOTP27t2LJUuWoECBArh58ya+/PJLREdH4/3330fVqlVx//59/Pzzz9iwYQP69++PiRMnolixYgCAwMBA7Nu3D/7+/pgzZw7y58/v5qqHnrdt/PjxKFGiBNq1a4e2bdvi+vXrSJ06NSwWCywWi7uzAKi9vVS5DVC7j23GbHsVxZ/eSK80s9kMb29v5M2bFwCwYcMG3LhxA2azGfv378dnn32Gu3fvImfOnNA0DZkyZcKpU6fQtWtX/PDDDwD+3Rg4mUymJO9IjMp9bDNmG/D8fT169MD8+fORIkUKVK5cGd9++y1WrlyJn376Cb/88gvGjRuHPHnysM3D2lR/Xhqhj23GbAPU7mObMdsAtftUbgPU7wP+nUBx4sQJAMCdO3cwf/58/Pnnn+jQoQOmT5+OuXPn4v3330dMTAwGDx6MlStXInny5OjUqRPmzp2LIUOG4Msvv8TcuXMxefJk/e/LXVRfbir3sc2YbYDafWwzZhugXt+jYylZsiRSpEiBdevWYcKECQAe7pA3mUwICwvDlClTkC5dOuTPnx9du3ZF0aJFsXv3bvzzzz8JPp47qdwGqN336FgCAgJQp04d3LhxA0OHDkXr1q3Rp08f3Lp1Cz179sSXX36JKlWqoG7dupg4cSLeeecdXLt2DUOGDMGdO3dgtVpRt25dpEmTBrNnz3brJIoX1TZ06FDcuXMHJpMJZrMZ6dOnh9Vq9ZhJFIB628u4VG4D1O5jmzHbXklCFIfzXGxr1qyR8uXLS5kyZaRRo0bSpk0bKV26tBQpUkSmT5/ucp9x48aJpmlSsGBB2b9/vzuG/dRU7mObMdtE/nvfvn379MvjntfaE7Dt1X1eenIf24zZJqJ2H9uM2Saidp/KbSLq9sV9z3Tp0iWpUaOG1K1bV+x2u5w/f140TZNJkybFu9/06dNF0zQpXry4R5ynOjGqLjcnlfvYZsw2EbX72GbMNhE1+2w2m4j8+1q+bNkyKVq0qGiaJp9++qksXLhQQkNDpW7duqJpmowZM0a/788//yyapsk333zjlrE/icptImr3OducLl++LKGhodKuXTtp0qSJ1KpVS1q2bOly+5iYGBERiYqKkjp16kilSpXk0qVL+m3u3LmTNIN/gpfR5olU3F46qdwmonYf24zZ9iriESnIhfMcbOXLl0ezZs2QIUMGHDp0CFu3bsXt27fRsGFDtGnTBgAQExMDAOjcuTM++OADOBwO/PTTTwAeHrrGE6ncxzZjtgH/vW/58uUAHvZ52sxEtr26z0tP7mObMdsAtfvYZsw2QO0+ldsA9fqcp/EwmUz6r1fSpEmDUqVK4eTJk+jbty/+/vtvZM6cGTVq1ADwcOzOtjZt2uC9997D/fv3sXHjRgDQz+3tSVRbbo9SuY9txmwD1O5jmzHbAHX6xo8fj0GDBgEALBYL7Ha7/v1HzZo1MXDgQFitVvz444/44osv8PXXX+PUqVNo3bo1unbtqj9Onjx5YDab9fcDnkDlNkDtvoTagIe/kM6QIQOaNm2KKVOmYPr06ahUqZJ+GPro6GhYLBZ4eXkhOjoaVqsV/v7+uHz5Mi5duqSvbz4+Pu4Jw8trEw86ksijVNleJkTlNkDtPrYZs+1V5OXuAZB7nTx5EpcuXcLFixeRIkUKlC1bFsmTJ0fKlCnRunVr1K9fH+fOnUNkZCTmzJmDWrVqAQDsdju8vb1ht9thsVhQv359zJs3D5cuXQLw74bC3VTuY5sx2wC1+9hmzDZA7T62GbMNULuPbcZsA9TuU7kNULvvm2++wZ49e9CvXz8UK1YMJpMJdrsdyZIlQ+/evbFr1y78/PPPOHToEK5du6YfathsNsNsNru0LV68GMeOHQPw8DDU7qbycgPU7mObMdsAtfvYZsw2QL0+EYHD4cCyZctw48YN+Pr6onv37vqOXYvFAh8fH7zzzjsoUqQI/vjjD5w5cwYFChRA9uzZUbVqVQBAVFQUkiVLBn9/f5jNZo84nYDKbYDafU/T5uQ8lcXu3bv1HYFWq1WfUGK1WgEA9+/fR968efUJI+7ysts86Qdgqm0v41K5DVC7j23GbCNOpHilTZ06FUuXLkV4eLh+WdasWVG3bl28/fbbyJ07N7Jnz47s2bPjwIED2LNnD+7cuZPgYyVLlgwOhwMpUqRIquE/kcp9bDNmG6B2H9uM2Qao3cc2Y7YBavexzZhtgNp9KrcBavdFRkZiw4YNuHDhAqZOnYr27dujaNGisFgsiImJgb+/P7755ht06dIFYWFhSJ06NU6ePIk8efLA4XDAbDbrv6Dz9/cHAP2Xd+6m8nID1O5jmzHbALX72GbMNkDNPpvNBm9vb+TIkQMXLlzAokWL4HA40LNnT5cduw6HA5qmQdO0BB8nWbJkAIDFixfDZrOhdOnSSZmRIJXbALX7nrbNOWnAYrEgXbp02Lx5MyZMmIBOnTq5TEiYNWsWDh06hPr167t9oojKbXGpuL10UrkNULuPbcZso4c4neUVNW7cOIwePRqpU6fGsGHDMGfOHHTr1g0pU6bEjBkz0L9/fxw+fBjAw9mazkO6Ll++HDabDRaLRT+cFQAsW7YMAFCsWDH9Pu6kch/bjNkGqN3HNmO2AWr3sc2YbYDafWwzZhugdp/KbYD6fX5+fsiUKROio6OxZcsWTJgwAf/88w+AhxMiRAQlS5bEl19+ifTp0+PWrVuYOnUqrly5ov/CxXnkiV9++QUAULBgQQB8Xr5MKvexzZhtgNp9bDNmG6Bun3PSovPfd+/exYwZMzBmzBgA/55yIKFfo+7btw+TJk3Cnj17cOnSJUybNg0LFixAgQIFUKlSpSRrSIzKbYDafU/b5mS1WtG6dWtYrVZMmDABX3/9Nfbv34/jx49jxIgRmDJlCgICAtClSxekTJkyyXviUrnNSdXtJaB2G6B2H9uM2UZxCL1yNm7cKEFBQVKnTh05fPiwfrnNZpPdu3fL+++/L5qmSa1atWT//v0iIhIRESE1atQQTdNkwIAB8uDBA/1+c+bMkZIlS0qdOnXk0qVLSd7zKJX72GbMNhG1+9hmzDYRtfvYZsw2EbX72GbMNhG1+1RuE1G/z263i4jI0KFDpXjx4lKvXj3RNE0+/PBDvccpKipKNmzYIFWrVhVN06RRo0aybNkyOXz4sFy5ckXGjx8vpUqVklq1asnFixfdkaNTfbmp3Mc2Y7aJqN3HNmO2iajd53wNb9SokVStWlVmz54tpUuXlsDAQBk1apR+O5vN5nK/mJgYGT58uGiaJkFBQVK8eHHRNE0qVqwox48fT9KGxKjcJqJ237O2ORwOcTgcsmDBAilUqJBomqb/W9M0qVOnDtuSiMrbS5XbRNTuY5sx28gVJ1K8gmbOnCmapsncuXP1y5xvJGw2m8yZM0d/Q1CvXj05ePCgiIhs2bJFQkJC9JW/cePG0rBhQ9E0TUJCQjzmjYPKfWwzZpuI2n1sM2abiNp9bDNmm4jafWwzZpuI2n0qt4mo3+f0ww8/SMGCBWXOnDnSuXPnRCdTiIgcOnRI3njjDb27UKFC+hf51apV84g21Zebyn1sM2abiNp9bDNmm4j6fWfPnpVixYpJ8+bN5fr167J48WIJDg5+4g7506dPy8yZM6VLly7Svn17GT58uJw9ezaph/9YKreJqN33vG179+6VXr16SevWraVz584ya9Ysj9sZqHKbyttLldtE1O5jmzHbyBVP7fEKOnr0KAAgderUAB6eH8xsNsPhcMBisaBEiRIICAhA0aJFceLECYwcORLnzp1DSEgIxowZg6CgINy6dQt79+7FgwcPUK9ePSxYsAD58uVzZ5ZO5T62GbMNULuPbcZsA9TuY5sx2wC1+9hmzDZA7T6V2wD1+5wyZ84Ms9mM9OnTo23btihfvjy2bNmCsWPH6qf5OHHiBMLDw1GwYEGEhoZi4MCBeP3116FpGsqUKYOuXbviu+++84g21Zebyn1sM2YboHYf24zZBqjfd+nSJTx48AAlSpSAv78/3njjDfTr1w8+Pj6YNm0aRo8eDcD1lAMigpw5c6J169YYO3YsJk+ejN69eyNbtmzuTIlH5TZA7b5naXMepl5EULx4cQwfPhwzZ87EuHHj0KpVK2TMmNGdKfGo3Kby9lLlNkDtPrYZs41cebl7AJT0rFYrAGDv3r1488034eXl5XLetrNnzyImJgbffPMNxowZg9WrV+Pnn39Gu3btUKpUKXz33Xe4ceMGLl++jPz588Pb2xs+Pj7uTHKhch/bjNkGqN3HNmO2AWr3sc2YbYDafWwzZhugdp/KbYD6fU5FixaFl5cXjh49inr16uHjjz+GyWTCli1b4OXlhZo1a2LlypW4e/cuxo8fj/Tp0+N///sf/ve//yEyMhJ+fn7uTnCh+nJTuY9txmwD1O5jmzHbAHX69u/fj6JFi8a7PEuWLAgJCUH+/PkBAMmTJ0e9evUAAEOGDMG0adMAAN27d9d3yDvPo+5wOPS/BxGByWRKipR4VG4D1O57EW3OddLZFreHbUlLle1lQlRuA9TuY5sx2+gR7jkQBrmDw+EQEZE///xTypQpI7Vq1ZKNGze63CYsLEyaNWsm77//vty7d0/27t0r1atXlzp16sitW7cSfDxPoXIf24zZJqJ2H9uM2Saidh/bjNkmonYf24zZJqJ2n8ptIur3xeVwOCQqKkqqV68uXbp0EZGH5+DetWuXtGvXTjRNk9dee000TZP58+fr90nocdxN9eWmch/bjNkmonYf24zZJqJW37fffiuapsmSJUsSvP7SpUsSFRXlctn9+/dl6dKlT3XKAXdSuU1E7T62GbMtISptLx+lcpuI2n1sM2YbJYyn9lDctm3bcPnyZQDQZ0nmzJkTVapUwZkzZzBu3DhMnjwZu3btwqpVq/DJJ59g586dqFKlClKkSIECBQqgRIkSOHXqFFauXOny2J4w61LlPrYZsw1Qu49txmwD1O5jmzHbALX72GbMNkDtPpXbALX74rY9SkRgtVpRpEgRHDhwANevX4eXlxeCg4NRrVo1pE6dGrdu3ULu3LlRuXJll/vF5a5GlZcboHYf24zZBqjdxzZjtgHq9t2/fx8A8Pnnn2PJkiXxrs+YMSOsVqvL67LzV/KJnXLA4XAkzeCfQOU2QO0+thmzzUnV7SWgdhugdh/bjNlGT8GNkzjoJVu7dq1omiaTJ0+WS5cuuVx3+PBh6dGjh5QsWVI0TXP5Z8iQIS633bx5swQGBsqUKVOScvhPpHIf24zZJqJ2H9uM2Saidh/bjNkmonYf24zZJqJ2n8ptImr3Pa4trvHjx0tgYKBcv35dRET2798vzZo1E03TpHbt2qJpmrRp00b27duXVEN/IpWXm4jafWwzZpuI2n1sM2abiJp9drtdRETmzp3rMubFixc/9WPE/ZV84cKF4/W6i8ptImr3se3xPLUtLhW3l04qt4mo3cc2Y7bR0/Fy90QOennSpk0LAJg3bx5MJhMaNWqEDBkyAAACAwPRo0cPvPHGG/jpp5/w4MED5M2bF0FBQXj99dcBANHR0bBarbBYLBAR3L17120tCVG5j23GbAPU7mObMdsAtfvYZsw2QO0+thmzDVC7T+U2QO2+x7UB/56juUCBAhARnDt3DmfPnsXIkSOxc+dO9O7dG1WrVsXgwYOxdetWREZG4vPPP0fBggXdlaRTebkBavexzZhtgNp9bDNmG6Bmn/Mc6cWLF4fFYkGBAgVw+PBhDBw4EGazGW+//fYTH8P5K3mz2Yy+ffvip59+wkcffQR/f/+XPfzHUrkNULuPbY/nqW1xqbi9dFK5DVC7j23GbKOnw4kUCkuXLh1SpEiB69evY/r06XA4HHjnnXeQPn16AEDWrFmRNWtWVKpUCXa7Hd7e3i73t1qtAIANGzbAy8sLJUuWTPKGx1G5j23GbAPU7mObMdsAtfvYZsw2QO0+thmzDVC7T+U2QO2+J7U5DwVasGBBmM1mLF68GBcvXtQnUXz44YcAgA4dOuDOnTsICwtDmjRp3JXjQuXlBqjdxzZjtgFq97HNmG2Aun0iglSpUsFqteJ///sf7t69i6FDh6J///4A4LJj1+Fw6DuC40qePDlq164Ni8WCQoUKecwOXZXbALX72PaQ0dqcVN1eAmq3AWr3sc2YbfR04r9SkDIuXryI+/fvI3fu3EiRIgWmT5+OpUuX4urVq/ptnG8YvL29ISI4ceIElixZgps3b+LevXuYO3culi9fjsDAQBQpUsSNNfGp3Mc2Y7YBavexzZhtgNp9bDNmG6B2H9uM2Qao3adyG6B239O2eXt7I126dPjhhx+wdetWl0kUABAcHIx+/frh559/RubMmd2REo/Kyw1Qu49txmwD1O5jmzHbAHX7TCYTcuXKhYCAAGzduhUffPABOnfuDADo378/li5dCgD4559/sGXLFty7dy/Bx0mRIgXeeOMN5MmTJ8nG/iQqtwFq97HNmG1Oqm4vAbXbALX72GbMNno6PCKFwq5duwYAaNOmDaKiojBp0iRMmzYNwMPZl+nTp3eZdXnv3j3MnDkTP/74IyZPngyTyYTz588jICAA33zzDdKlS+eWjsSo3Mc2Y7YBavexzZhtgNp9bDNmG6B2H9uM2Qao3adyG6B239O2ZcqUCXXq1MG8efPQt29ftGzZEsDDL3VMJhNMJhOKFSvmrowEqbzcALX72GbMNkDtPrYZsw1Qvy9Pnjw4fvw4AKBjx44wm80YO3Ys+vfvj7Nnz2Lr1q04dOgQfvrpJ+TPn9/No302KrcBavexzZhtKm8vVW4D1O5jmzHb6OlwIoWCnOfI3b17NwAgd+7cKFGiBKKjozFjxox4K7mTj48PmjZtiuvXr+Pq1auIiYlBo0aN0L59e+TMmdMtLQlRuY9txmwD1O5jmzHbALX72GbMNkDtPrYZsw1Qu0/lNkDtvudp+/DDD1G7dm2UKlUKQOKHLHY3lZcboHYf24zZBqjdxzZjtgHq9zlfh4sVK4atW7fizJkzyJkzJzp06IBkyZJh+PDhmDJlCgCge/fuhtqhq3IboHYf24zZpvL2UuU2QO0+thmzjZ4NJ1IoyGQyQURgs9lQuHBh5MuXDyaTCc2bN4fJZML06dMTXMlFBIULF8bUqVNht9sRExMDLy8veHl51tNE5T62GbMNULuPbcZsA9TuY5sx2wC1+9hmzDZA7T6V2wC1+56lrVGjRsiYMSMyZcqETJkyAfDcSRSA2ssNULuPbcZsA9TuY5sx2wD1+5yvw/nz54fNZsP169f1nSgVKlTAggULcP78eYgIcufODcCzX7/jUrkNULuPbcZsU3l7qXIboHYf24zZRs9ISFnh4eGydOlSiYmJEbvdLiIidrtd5syZIyEhIVKiRAmZPHmyXLlyRb+P83ZGoHIf24zZJqJ2H9uM2Saidh/bjNkmonYf24zZJqJ2n8ptImr3PW3b5cuX3TzSZ6fychNRu49txmwTUbuPbcZsE1G7z+FwyN69e0XTNFm4cKGIiPz111/SokUL0TRN2rRpI5qmiaZp8sMPP7h5tM9G5TYRtfvYZsw2EbW3lyq3iajdxzZjttHT4UQKxTkcDv3fT7uSG4nKfWwzZpuI2n1sM2abiNp9bDNmm4jafWwzZpuI2n0qt4mo3cc2Y7aJqN3HNmO2iajdxzZjtomo3RcZGSmVKlWSr7/+Wo4cOSLNmjUTTdNkzpw5IiIyZcoUfcfu8uXL3TzaZ6Nym4jafWwzZpuI2ttLldtE1O5jmzHb6Ml4LBHFmUwm/d8mk0k/VFXz5s0BQD/8jNlsxptvvomMGTO6c7jPTOU+thmzDVC7j23GbAPU7mObMdsAtfvYZsw2QO0+ldsAtfvYZsw2QO0+thmzDVC7j23GbAPU7jOZTEiTJg3WrFmDQ4cOYffu3ejTpw8++OADAMBHH32Ee/fuYdasWQgKCnLzaJ+Nym2A2n1sM2YboP720vlv1doAtfvYZsw2egrunslBSe/RGVNVqlQRTdNk1qxZYrPZ3Dy6/07lPrYZl8p9bDMulfvYZlwq97HNuFTuU7lNRO0+thmXyn1sMy6V+9hmXCr1DRs2TP8F/MyZM/XLo6Ki9P++ceOGO4b2n6ncJqJ2H9uM2ZYQlbaXj1K5TUTtPraRCnhEileQ2Wx2mTEVFRWFZcuWoXLlyrBYLO4e3n+mch/bjEvlPrYZl8p9bDMulfvYZlwq96ncBqjdxzbjUrmPbcalch/bjEulvoYNG+LHH39Ep06d9F+wOhwOWK1WvTFt2rRuHuXzUbkNULuPbcZsS4hK28tHqdwGqN3HNlKBSUTE3YMg93Cu5A6HA5GRkUidOrW7h/RCqdzHNuNSuY9txqVyH9uMS+U+thmXyn0qtwFq97HNuFTuY5txqdzHNuNSpe/mzZv6jltnkypUbgPU7mObWlTZXiZE5TZA7T62kZFxIsUrTvU3ECr3sc24VO5jm3Gp3Mc241K5j23GpXKfym2A2n1sMy6V+9hmXCr3sc24VOpTqeVRKrcBavexTR0q96rcBqjdxzYyKk6kICIiIiIiIiIiIiIiIiIiIorFKTJEREREREREREREREREREREsTiRgoiIiIiIiIiIiIiIiIiIiCgWJ1IQERERERERERERERERERERxeJECiIiIiIiIiIiIiIiIiIiIqJYyk2k+OuvvxAYGIglS5Y80/2io6Mxa9YsvPHGGyhevDjKly+PXr16ITw8/CWNlIiIiIiIiIiIiIiIiIiI6Ok9aX/4sWPH0KNHD5QvXx5FixZF3bp1MXbsWNy6dSvB2zscDsyfPx+NGjVCsWLFUKxYMTRq1Ajz5s2D3W5/4nhu3Lih71tPzJ07dzBq1CjUqlULhQsXRrly5dCxY0fs2bPnqZpPnTqFYsWKYfTo0fGu+/HHH6Fp2hP/qVat2lP9WU5ez3RrD3fy5En06NEDIvJM97PZbOjUqRM2bdqEDBkyoFKlSjh79ix++eUXrF+/HgsWLEBgYOBLGjUREREREREREREREREREdHjPWl/+MaNG9GlSxdERUUhY8aMKF++PC5duoRJkybh119/xYwZM5AjRw6X+/Tu3RsrVqxAypQpUbp0aYgIdu/eja+//hrbt2/HxIkTYTKZEvzz7t27h86dO+P69euJjvnBgwdo2bIl/vnnH6RPnx4VK1bErVu3sG7dOqxfvx5ff/013n777UTvf+3aNXTs2BEPHjxI8PocOXLgjTfeSPT+27Ztw/Xr1xEUFJTobRKizESK7du3o2fPno9dSIlZsGABNm3ahPLly2PSpElIkSIFAGDOnDkYOnQo+vbti2XLliX6BCEiIiIiIiIiIiIiIiIiInpZnrQ//NatW+jVqxeioqLQvHlz9OnTB1arFQCwcuVK9OzZE7169cL333+v7/feuHEjVqxYgezZs2P+/PnImDEjAODChQto2rQp1q1bh1WrVqFevXrx/rxz586ha9euOHDgwGPHPX78ePzzzz+oXLkyxo4dq++L37ZtG9q2bYtBgwahWrVqSJs2bbz7HjlyBF26dMGZM2cSffxSpUqhVKlSCV63fft2/Prrr8iZMyeGDRv22HE+yvCn9rh+/Tq++OILtG7dGrdu3UKWLFme6f4igtmzZwMABg4cqC84AGjZsiVKly6Nw4cPY8eOHS903ERERERERERERERERERERI/ztPvDV65cicjISBQqVAj9+vXTJ1EAQL169fDuu+9i3759WLdunX75li1bAADvv/++PokCALJkyYImTZoAAHbu3Ony50RHR2PWrFlo2LAhDhw4gOzZsyc6dhHBL7/8AgD47LPPXPbFly9fHuXLl8f9+/fj/Rl37tzB6NGj8d577+HMmTPIli3bY/+OEhIREYHevXvDZDJh5MiR8PX1fab7G34ixZQpU7Bw4ULkyJEDc+fOxWuvvfZM9z927BguXLiAPHnyIE+ePPGur1GjBoCHs3GIiIiIiIiIiIiIiIiIiIiSytPuDz927BgAoEqVKjCb408DKFu2LADgjz/+0C9zHpni8uXL8W5/48YNAEDq1KldLv/jjz/wzTffAAC++uorfPzxx4mO3WQyYdWqVVi+fHmCkyHu3bsHALBYLC6X//DDD5gyZQpSpUqFiRMnomHDhon+GYkZN24crl69ihYtWqBo0aLPfH/DT6TInj07Pv/8c6xYsSLRQ3Y8TlhYGAAgf/78CV6fL18+AP8+8YiIiIiIiIiIiIiIiIiIiJLC0+4PdzgcAJDokReckxVOnjypX1apUiUAwMKFCxEaGoqbN2/i1q1bWLhwIebPn4/UqVPjnXfecXkcX19ftG3bFmvWrMH777//xPH7+PggMDDQ5TIRwaJFi7Bnzx5kyJAB5cuXd7k+ICAAPXr0wO+//64f+OBZHD9+HIsWLYK/vz86der0zPcHAK/nupcHadGixX+6/5UrVwAAGTJkSPD69OnTAwCuXbv2n/4cIiIiIiIiIiIiIiIiIiKiZ/G0+8OdZ1/466+/0Lp163jX7969G8C/R5oAgIoVK6JLly6YPHkyBg0ahEGDBunXBQcHY/DgwfGOJFG2bFn96BbP6vTp0xg+fDiOHDmC8+fPI3/+/Pj2229dTvkBAPXr13+ux3eaNGkS7HY7WrVq9cyn9HAy/BEp/ivn4UKSJ0+e4PXOy523IyIiIiIiIiIiIiIiIiIi8iT16tVD8uTJsWHDBsydOxciol+3adMmLFiwAAAQHR3tcr/KlSujRIkS8PX1RUhICMqUKYOUKVNi3759WLhwocvj/FdHjx7FunXrcP78eQCA3W7H8ePHX9jjA8DFixfx+++/w8/PD02bNn3uxzH8ESn+K+chTJznf0nM8zxBnLN6iIiIiIiIiIiIiIiIiIiIgoODX8rjpk+fHoMGDULfvn0xZMgQzJ8/H/nz58fFixdx8OBBNGvWDKGhofDy+neKwPr169GlSxcULlwYv/32m362hgsXLqBjx46YO3cufHx80LVr1xcyxrJly2LPnj24f/8+1qxZgxEjRqBnz56w2+148803X8ifsXDhQthsNjRu3Pi5j0YBcCIFUqZMCQB48OBBgtc7L3fe7lkUKFAg3oyeJzGbzVi4cCEuXrz4zH/es8qcOTOaNGminy/nZUvKNiBp+1RuA9R/Xq5ZsybJTt8TEBCAmjVrJtnzUtU2QP3nparbFJXbALX7VG4DuE15kfi8fHH4vHxxkrrveURFReHixYvInDkzkiVL5u7hJIrPyxeH28sXh8/LF4fPyxdH5T6V256XUV7Hn5fKfWwzLiP0qb695HuwF4ev4y+Oys/Lp9WgQQNky5YNkydPxp49e3D16lUEBgZi3LhxKF68OEJDQ5EqVSoAgM1mw1dffQWHw4ERI0bokygAIEuWLBg5ciTq16+POXPmoF27dvFOv/E8UqdODQDw8fFBkyZNkCpVKvTo0QNjx459YRMpfvvtNwBAo0aN/tPjvPITKTJmzAgAie5ovHr1KgC4PHGelp+f33ONKTIyEtevX3+u+z4LX19fpE2b9qX/OXElVRuQ9H0qtwFqPy9PnTqFM2fOJMmflTNnziTtU7kNUPt5qfI2ReU2QO0+ldsAblNeFD4vXyw+L18Md/Q9q3v37uHixYtIkybNc03kT0p8Xr4Y3F6+WHxevhh8Xr5YKvep3PY8jPQ6/jxU7mObcRmlT/XtJd+DvRh8HX+xVH5ePq2SJUti+vTp8S7fvn07gIeTJADgzJkzuHjxInLnzo3s2bPHu33evHmRLVs2hIeH48yZMwgMDHzhY61Tpw4+/fRTnD9/HhEREUiTJs1/erxjx47hzJkz0DQNefPm/U+P9cpPpMifPz8AICwsLMHrnZcXKFAgycZERERERERERERERERERET0tO7cuYNDhw7Bz88PBQsWjHf9jh07AABFihQB8HDSCQCXU308ymKxAABiYmKea0yXLl3CjBkzYDab0a9fv3jXm81meHl5ISoqCjab7bn+jLj++OMPAECtWrX+82OZ//MjGFyePHmQPXt2HD9+HOHh4fGuX7NmDQCgcuXKST00IiIiIiIiIiIiIiIiIiKiJ7p69SqaN2+OAQMGxLsuKioKy5Ytg5eXlz7JIFeuXLBYLDhx4gTOnTsX7z7nzp1DeHg4rFbrcx/dwWq1YsGCBZg3b16Cp1zZvXs37t69i0yZMiEgIOC5/oy49u/fDwAoUaLEf36sV2oixY0bN3DixAlcuHDB5fJmzZpBRNC/f3/cuXNHv3zu3LnYtWsXgoKCUL58+aQeLhERERERERERERERERER0RPlzp0befLkwYEDB/Dzzz/rl0dHR6Nv3764fPkyGjVqpJ/GI02aNKhTpw4cDgd69+6Nmzdv6ve5du0aevfuDbvdjnfeeee5T6Pk7++P2rVrw+Fw4JNPPtGPggEAJ06c0I9S0bp16+d6/EcdOHAAABAUFPSfH+uVOrXH/PnzMWHCBJQpUwbz5s3TL2/WrBk2bNiAHTt2oFatWihVqhTOnTuHgwcPInXq1BgxYoQbR01ERERERERERERERERERPR4w4YNw//+9z/07t0bCxYsQEBAAPbu3Ytr164hODgYn376qcvtP/vsM4SFhWHPnj2oVq0aypQpg5iYGOzbtw937txByZIl0adPn/80poEDB+Lo0aP4888/UaNGDZQoUQK3b9/G/v37ERMTg7fffhstWrT4T38GANjtdly4cAFeXl5Imzbtf368V2oiRWK8vLwwbdo0zJgxAz///DM2bNiAdOnSoUGDBujcuTNy5Mjh7iESERERERERERERERERERElqlixYggNDcWECROwZ88eHD16FLly5ULbtm3RpEkTJEuWzOX2adKkwffff4+5c+di5cqV2LFjBwAgT548eOONN9CsWTNYrdb/NCZ/f38sWbIE06dPx2+//YYtW7YgWbJkKFGiBJo0aYJ69er9p8d3ioiIgIjA19f3hTyechMphg0bhmHDhiV4XefOndG5c+cEr0uWLBk6duyIjh07vszhERERERERERERERERERERPZfH7Q8HgOLFi2PGjBlP/XgpUqRA+/bt0b59++caz1tvvYW33nrrsbfx8fFBt27d0K1bt+f6Mx63n98pXbp0OHr06HM9fkLML+yRiIiIiIiIiIiIiIiIiIiIiAyOEymIiIiIiIiIiIiIiIiIiIiIYnEiBREREREREREREREREREREVEsTqQgIiIiIiIiIiIiIiIiIiIiisWJFERERERERERERERERERERESxOJGCiIiIiIiIiIiIiIiIiIiIKBYnUhARERERERERERERERERERHF4kQKIiIiIiIiIiIiIiIiIiIiolicSEFEREREREREREREREREREQUixMpiIiIiIiIiIiIiIiIiIiIiGJxIgURERERERERERERERERERFRLE6kICIiIiIiIiIiIiIiIiIiIorFiRREREREREREREREREREREREsTiRgoiIiIiIiIiIiIiIiIiIiCgWJ1IQERERERERERERERERERERxeJECiIiIiIiIiIiIiIiIiIiIqJYnEhBREREREREREREREREREREFIsTKYiIiIiIiIiIiIiIiIiIiIhicSIFERERERERERERERERERERUSxOpCAiIiIiIiIiIiIiIiIiIiKKxYkURERERERERERERERERERERLE4kYKIiIiIiIiIiIiIiIiIiIgoFidSEBEREREREREREREREREREcXiRAoiIiIiIiIiIiIiIiIiIiKiWJxIQURERERERERERERERERERBSLEymIiIiIiIiIiIiIiIiIiIiIYnEiBREREREREREREREREREREVEsTqQgIiIiIiIiIiIiIiIiIiIiisWJFERERERERERERERERERERESxOJGCiIiIiIiIiIiIiIiIiIiIKBYnUhARERERERERERERERERERHF4kQKIiIiIiIiIiIiIiIiIiIiolicSEFEREREREREREREREREREQUixMpiIiIiIiIiIiIiIiIiIiIiGJxIgURERERERERERERERERERFRLE6kICIiIiIiIiIiIiIiIiIiIorFiRREREREREREREREREREREREsTiRgoiIiIiIiIiIiIiIiIiIiCgWJ1IQERERERERERERERERERERxeJECiIiIiIiIiIiIiIiIiIiIqJYnEhBREREREREREREREREREREFIsTKYiIiIiIiIiIiIiIiIiIiIhicSIFERERERERERERERERERERUSxOpCAiIiIiIiIiIiIiIiIiIiKKxYkURERERERERERERERERERERLE4kYKIiIiIiIiIiIiIiIiIiIgoFidSEBEREREREREREREREREREcXiRAoiIiIiIiIiIiIiIiIiIiKiWJxIQURERERERERERERERERERBSLEymIiIiIiIiIiIiIiIiIiIiIYnEiBREREREREREREREREREREVEsTqQgIiIiIiIiIiIiIiIiIiIiisWJFERERERERERERERERERERESxOJGCiIiIiIiIiIiIiIiIiIiIKBYnUhARERERERERERERERERERHF4kQKIiIiIiIiIiIiIiIiIiIiolicSEFEREREREREREREREREREQUixMpiIiIiIiIiIiIiIiIiIiIiGJxIgURERERERERERERERERERFRLE6kICIiIiIiIiIiIiIiIiIiIorFiRREREREREREREREREREREREsTiRgoiIiIiIiIiIiIiIiIiIiCgWJ1IQERERERERERERERERERERxeJECiIiIiIiIiIiIiIiIiIiIqJYnEhBREREREREREREREREREREFIsTKYiIiIiIiIiIiIiIiIiIiIhicSIFERERERERERERERERERERUSxOpCAiIiIiIiIiIiIiIiIiIiKKxYkURERERERERERERERERERERLE4kYKIiIiIiIiIiIiIiIiIiIgoFidSEBEREREREREREREREREREcXiRAoiIiIiIiIiIiIiIiIiIiKiWJxIQURERERERERERERERERERBSLEymIiIiIiIiIiIiIiIiIiIiIYnEiBREREREREREREREREREREVEsTqQgIiIiIiIiIiIiIiIiIiIiisWJFERERERERERERERERERERESxOJGCiIiIiIiIiIiIiIiIiIiIKBYnUhARERERERERERERERERERHF4kQKIiIiIiIiIiIiIiIiIiIiolicSEFEREREREREREREREREREQUixMpiIiIiIiIiIiIiIiIiIiIiGJxIgURERERERERERERERERERFRLE6kICIiIiIiIiIiIiIiIiIiIorFiRREREREREREREREREREREREsTiRgoiIiIiIiIiIiIiIiIiIiCgWJ1IQERERERERERERERERERERxeJECiIiIiIiIiIiIiIiIiIiIqJYnEhBREREREREREREREREREREFIsTKYiIiIiIiIiIiIiIiIiIiIhicSIFERERERERERERERERERERUSxOpCAiIiIiIiIiIiIiIiIiIiKKxYkURERERERERERERERERERERLE4kYKIiIiIiIiIiIiIiIiIiIgoFidSEBEREREREREREREREREREcXiRAoiIiIiIiIiIiIiIiIiIiKiWJxIQURERERERERERERERERERBSLEymIiIiIiIiIiIiIiIiIiIiIYnEiBREREREREREREREREREREVEsTqQgIiIiIiIiIiIiIiIiIiIiisWJFERERERERERERERERERERESxOJGCiIiIiIiIiIiIiIiIiIiIKBYnUhARERERERERERERERERERHF4kQKIiIiIiIiIiIiIiIiIiIiolicSEFEREREREREREREREREREQUixMpiIiIiIiIiIiIiIiIiIiIiGJxIgURERERERERERERERERERFRLE6kICIiIiIiIiIiIiIiIiIiIorFiRREREREREREREREREREREREsTiRgoiIiIiIiIiIiIiIiIiIiCgWJ1IQERERERERERERERERERER/Z+9Ow/Xqqzbh39uJgVkcELQNNMQDWcw/JVAEjlkapCoFBLhSDk8CmI+ampOZGqmlko54YBgYqk5JFYoRoig4YCYOKSGICgzG1T2+weL+2m3N6O3FPv9fI7Do/Za17rW+t58j3uT6+y6CoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgEKD//QDlMvTTz+dG264IVOmTEllZWXatWuXvn375utf//oazzFz5sxcd911eeKJJzJr1qw0bdo0e++9d0488cTsueeen97DAwAAAAAAAMBqPP300+nbt28uuuii9OrVq8b5ww47LFOnTl3p9Q899FB23HHH0s9Lly7NTTfdlAcffDD/+Mc/0qBBg7Rr1y7f/va3c9hhh9W4/qWXXkqPHj1WOv8ee+yRkSNHVjs2a9as/PKXv8yTTz6ZGTNmZMstt8xBBx2UAQMGZJNNNqkxx8UXX5zbb799pfe44IIL0rt379LPVVVVueeee3L33Xfn1VdfTcOGDdOuXbsceeSR+eY3v7nSeValTgQp7r///gwePDgNGjRIp06dUr9+/YwbNy6nn356Xn311Zx66qmrnePtt9/O0Ucfnffeey+f+cxn8pWvfCX//Oc/88c//jFjxozJlVdemYMPPng9VAMAAAAAAAAA1b322ms544wzUlVVVev5pUuXZtq0aWnRokW6dOlS65hmzZpVG9+/f/9MmDAhLVu2zJe//OUsXrw4zzzzTM4888xMnjw55557brXrX3zxxSTJrrvums997nM15v/sZz9b7eeZM2fm6KOPzjvvvJOddtopX/nKV/L888/n17/+dZ544okMHz68RpjipZdeSpIcfPDBadCgZqRh++23r/bzRRddlDvvvDONGzfOPvvsk4qKikycODFnnXVWxo8fn8suu6zWz2JVNvggxaxZs3LeeeelcePGueOOO9K+ffskybRp09K3b9/88pe/zFe/+tXS8ZW5/PLL89577+Xb3/52zj333NSvXz9J8pvf/CbnnHNOzj///HTr1i0bbbTRp14TAAAAAAAAAKwwbty4DBw4MLNnz17pmKlTp+ajjz7KvvvumyuuuGK1c95zzz2ZMGFC9thjj/z6179O8+bNkyRTpkxJnz59cvvtt+fQQw/NHnvsUbpmypQpSZJTTjklX/nKV1Z7jx//+Md55513ctJJJ+X0009PsjzAMXjw4Dz88MO5+uqrq4U1qqqq8vLLL2fTTTfN1Vdfvdr5x4wZkzvvvDNt2rTJ8OHD06ZNmyTJ9OnT07t374waNSoHHXRQunbtutq5/lW9tRr9X+jOO+9MZWVl+vTpUy0sseOOO5bSOLfddttq5xk7dmyS5OSTTy6FKJLkiCOOyPbbb5+5c+eucgkUAAAAAAAAACin2bNn54ILLkj//v0zd+7cbL311isdu2Ilh1133XWN5n7yySeTJP369SuFKJJkl112ySGHHJIkmTBhQq33WN1CBkny5ptvZvTo0WnTpk1OOeWU0vFGjRrloosuyiabbJJ77rknCxcuLJ174403snDhwjWaP1m+e0WSnHrqqaUQRZK0adMm3/nOd6rVuTY2+CDFmDFjkiTdu3evca579+6pqKjIn//859XOU6/e8o/i3XffrXb8ww8/zIIFC5IkLVu2/GQPCwAAAAAAAABr6IYbbsjw4cOz3Xbb5bbbbkunTp1WOnZtQg7J/70jnzFjRo1z77//fpKkRYsWpWPLli3L1KlT06pVq2y55Zarnf+JJ55IVVVVunbtWmOLjmbNmqVTp06prKzMX//619LxFSterGkYZMiQIfn973+fgw46qMa5RYsWJUm1hRTW1AYdpKiqqsqrr76aJGnbtm2N8y1atMgWW2yRuXPn1vqH/69W7BEzePDgPPPMM1m8eHHeeOONDBw4MLNmzUr37t2z3Xbblb8IAAAAAAAAAKjFtttum/PPPz8PPvhgOnbsuMqxK4IUM2fOTL9+/dKpU6fstdde6du3b62rMqx4R37ttdfmgQceyPz58zN79uxcd911eeyxx7L11lvn4IMPLo1//fXXs2jRonz2s5/N9ddfX9r2o3PnzjnvvPNqvJP/+9//niTZaaedan3ez3/+80mSV155pXTsxRdfTJI0aNAgAwcOTNeuXbP77rvn8MMPz+23355ly5ZVm6Nhw4b5/Oc/nyZNmlQ7/uyzz+auu+5K/fr1c+ihh67yc6tNg9UP+e81d+7cLFmyJE2bNq3xwazQqlWrvPfee5k1a1a22mqrlc517rnn5t13383EiRNLS3wkSUVFRU466aT84Ac/KPvzAwAAAAAAAMDK9O3bd43Gffzxx6VAwg9/+MPsvPPO2WefffL6669n/PjxGT9+fM4666z079+/dE2vXr3y8ssv5+67786gQYOqzffVr341559/fjbZZJPSsRUhhwkTJuRvf/tbvvjFL2arrbbKCy+8kJEjR+aPf/xjbrvttlJAYubMmUmy0tUrVhyfPXt26diKMMh1112XNm3aZLfddsuMGTMyZcqUXHzxxRk3blyuvfbala4yMXDgwEybNi1TpkxJy5Ytc+WVV67x6hb/aoMOUixevDhJ0rhx45WO2WijjZL837IdK9OyZcv06NEjr776apo3b56ddtopb7/9dqZOnZpRo0alY8eO6dy581o93/z587N06dK1uqZevXpp1qxZNt9887W6bl00a9YsH3zwQY3UzqdlfdaWrN/66nJtib4sJ31ZPvqyfPRl+dTl+upybYnvlHLSl+WjL8tnfde3LpYsWZIkmTNnTul/6/430pfl4/uyfPRl+ejL8qnL9dXl2tbVhvJ7fF3V5frUtuHaEOqr69+X/g5WPn6Pl8+G0pef1vNNmzYtlZWV2WijjXL11VenW7dupXMPPfRQzjzzzPz0pz9Nx44ds/vuuydZvuXFgQcemEmTJmXmzJnZfffdM2/evLzwwgt56qmncv/99+f4448vzbNi243ddtstv/jFL0oLGSxatCjnnXdeHnzwwZxxxhn53e9+l4qKitJ39MYbb1zrM684vnDhwtKxFUGK73//+zn55JNLgYkpU6bk+9//fh5//PHcdttt1QIhK3zwwQd58MEHSz9XVFTklVdeyQEHHLDW23tUVFVVVa3VFf9FZsyYkS5dumTLLbfM2LFjax3Tu3fvTJo0KcOGDVvlfjFnnHFGfv/73+e0007LgAEDUlFRkST5wx/+kDPOOCP16tXLqFGjSumZNTFx4sS1KwgAAAAAAACAOqtDhw6f6Pof/vCHue+++3LxxRenV69e1c7NmjUrixcvzrbbblvjuksvvTS33XZbevbsmcsuuyxJMnz48Fx44YXp1q1bLr/88tLqE1OnTs2AAQPyzjvv5JJLLskRRxyRJPnwww/zz3/+My1btkyLFi2qzb948eIceOCBmTFjRm6//fZ88YtfTP/+/fPUU0/l17/+da2LFtxzzz0599xz06NHjwwZMiRJsmDBgkyfPj1t27atMX706NH5wQ9+kG222SZ//OMfa5xfunRp5s+fn4022igTJ07MJZdckjfffDO9evXKxRdfvCYfb8kGvSJF06ZNkySVlZUrHbMilbiyrT+SZOzYsfn973+fTp065fvf/361cwcccED69++fG2+8MTfffHMuvfTSNX6+nXbaaZ1WpBg+fHimT5++VtetizZt2qR3797rNQW2vmpL1m99dbm2RF+Wk74sH31ZPvqyfOpyfXW5tsR3Sjnpy/LRl+WzvutbF0uWLMn06dPTpk2b0sqK/430Zfn4viwffVk++rJ86nJ9dbm2dbWh/B5fV3W5PrVtuDaE+ur696W/g5WP3+PlU5f7ck1tscUWKz23//7757bbbssLL7yQZPmqPpdffnmaN2+eIUOGVNvCo127dvnxj3+cY489NjfccEMpSNGwYcN89rOfrXX+xo0bZ999983vfve7vPDCC/niF79Yeke/4p39v1vxnv9f3+VvsskmtYYokqRr166pX79+3nnnnXzwwQfZdNNNq51v1KhRacWPrl275nOf+1wOO+yw3HvvvTnxxBNrDZiszAYfpGjatGnmz5+fysrKWpcEWbHvSqtWrVY6z1//+tckyX777Vfr+S5duuTGG28sLVWyppo1a7ZW41eYP39+tX1gPi2bbLJJjeb6tK2v2pL1X19dri3Rl+WiL8tLX5aHviyvulxfXa4t8Z1SLvqyvPRlefwn6ltbixYtyvTp09OyZctV/h8B/hvoy/LwfVle+rI89GV51eX66nJt62JD+j2+LupyfWrbcG0o9dX170t/BysPv8fLqy735Se15ZZbJklpu43Jkydn0aJF2W+//dK8efMa4/fdd980atQob731VhYsWFAtaLEyK4IcK+6xYuuP9957r9bxK46veLbVadiwYVq0aJH3339/lYstrLDddttlr732yl/+8pdMmTJlrYIU9dZ45H+hioqKUhpl2rRpNc7PmTMns2bNSosWLUp/SLWZN29ekqx0X5QGDZbnTT788MNP+sgAAAAAAAAAUFaPPvpoBg4cmHvuuafW82+99VaSpHXr1kmWh06S/3sX/u/q1auXevWWxwlW7MJw2WWX5eSTT84777xT6zVvv/12tXus6l1+kvz9739Psnynh2T5liJnn312aeuRf7dw4cK8//77adiwYWnliauuuiqnnXZaFi1aVOs1jRo1SpJ89NFHtZ5fmQ06SJGktJfK6NGja5wbPXp0qqqq0qVLl1XOseOOOyZJxowZU+v5p556Kkmy8847f5JHBQAAAAAAAICymzt3bh588MHceeedqaqqqnH+vvvuS/J/uzSseEf+zDPPZMGCBTXGT5o0KZWVlWndunU222yzJMnf/va3PPbYY3nsscdqjH/vvfcyduzY1K9fP1/60peSLH+XX1FRkT/96U/5+OOPq42fP39+xo8fn8aNG+eLX/xikmSjjTbKqFGjcuedd+b999+vcY/f/va3SZIvfvGLpYDEmDFj8sgjj9SaF5g3b16ee+65JEn79u1rnF+VDT5IccQRR6Rx48a59dZbM2nSpNLx1157LVdffXWS5LjjjisdnzlzZqZNm1ba8iNJvvGNb6Rp06YZP358fvWrX1VrrLFjx2bo0KGpqKjIMccc8+kXBAAAAAAAAABr4aCDDkrLli0zZcqU/OIXv6j2znvkyJF59NFHs/nmm+foo49OsnwRgT333DMLFizI2WefXdqOI0nefPPNnHvuuUlS7R35imuvu+66vPTSS6XjK+ZYuHBhevbsmTZt2iRJttlmm+y///55++2389Of/rT0TEuXLs2PfvSjLFy4MEcddVSaNWuWJNl+++2z77775sMPP8wPf/jDLFy4sHSPyZMn5+c//3kqKiry/e9/v8YzXX755XnjjTdKx+fOnZszzzwzc+bMSffu3fPZz352rT7P2tfp2IC0bt0655xzTs4777z06dMnnTp1SqNGjTJu3LgsWbIkAwcOrLaSxFVXXZX77rsvPXr0yJAhQ5Ikm2++ea688sqcdtppueKKKzJy5MjsvPPOeeedd/Liiy+moqIiP/zhD7PHHnv8p8oEAAAAAAAAgFo1b948l19+eU4++eRce+21eeCBB9KuXbu88cYbmTp1apo0aZJrr702LVu2LF1zxRVX5Jhjjskf/vCHPP300+nQoUPmz5+fyZMnp7KyMgceeGD69+9fGn/44Ydn3Lhx+e1vf5tevXpl7733TosWLTJhwoTMmTMnHTp0yP/+7/9We64f/ehHefHFF3PLLbdkzJgxadu2bZ5//vn885//TPv27XPqqadWG3/ppZfmO9/5TsaMGZOvfe1r2WOPPbJgwYJMnDgxy5Yty9lnn52OHTuWxh911FEZP358Hn744Rx66KHp0KFDGjRokMmTJ2fu3Llp3759Lr300rX+PDf4IEWS9OrVK61bt87QoUPz3HPPpX79+vnCF76Q/v3754ADDlijOfbff/+MGjUqv/rVrzJu3Lj86U9/StOmTbP//vvne9/7Xjp16vQpVwEAAAAAAAAA66Zr16659957c8MNN2T8+PH54x//mM022yzf+ta3MmDAgGy77bbVxm+77ba57777ctNNN2X06NF58skn06BBg+y888454ogjcsQRR6SioqI0vqKiIj/5yU+y7777ZsSIEXnhhReybNmybL/99jnxxBNzzDHHpGHDhtXu0aZNm9xzzz259tpr8+c//zl/+tOfsvXWW+ekk07K8ccfn6ZNm1Ybv80222TUqFG58cYb8/jjj+fJJ59M06ZN07lz5xx77LGlbUBWqFevXn72s5/ly1/+ckaOHJlnn302yfLVLY477rh897vfzUYbbbTWn2WdCFIky/dX6dy582rHDRkypLQSxb/7/Oc/n5/85CflfjQAAAAAAAAA+MRW9b47SXbaaadcddVVazzfpptumkGDBmXQoEFrfE2PHj3So0ePNR6/1VZb5eKLL17j8ZtttlnOPvvsnH322Ws0vqKiIr169UqvXr3W+B6rU69sMwEAAAAAAAAAbOAEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAA/sPq1auXJk2apF49/zMdAAAA4D+twX/6AQAAAKAuGTZsWKZPn75e7tWmTZv07dt3vdwLAAAA4P8vBCkAAACgjKZPn54333zzP/0YAAAAAKwja4YCAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABTqTJDi6aefTv/+/fP//t//y1577ZWjjz46Dz300FrPc//99+fb3/52OnTokN133z09evTIiBEjUlVV9Sk8NQAAAAAAAACsmaeffjo777xz7rnnnlWO++1vf5t27drlL3/5y0rHvP766xk0aFD222+/tG/fPvvtt18GDhyYadOm1Tp+6dKluf7663PIIYdkt912K72Xv//++2uM7datW9q1a7faf0aNGlXtuokTJ+akk05Kp06dsueee+awww7LzTffnMrKylqfadmyZfnNb36Tnj17Zu+9906nTp3y/e9/Py+88MIqP5/VafCJrv4vcf/992fw4MFp0KBBOnXqlPr162fcuHE5/fTT8+qrr+bUU09do3nOPvvsjBo1KhtttFH23XffLFmyJBMnTsyPfvSjvPnmmxk8ePCnXAkAAAAAAAAA1PTaa6/ljDPOWO0iAJMmTcqFF164yjHPP/98+vbtm0WLFuXzn/989txzz7z22mt58MEHM3r06Nx0003p2LFjafzSpUvTv3//TJgwIS1btsyXv/zlLF68OM8880zOPPPMTJ48Oeeee25pfPfu3fP+++/Xeu/Zs2fnL3/5Sxo2bJi2bduWjo8cOTLnn39+li1blm233TZ77713Xn/99fzkJz/JQw89lKFDh2azzTarNtcFF1yQESNGpEWLFvnSl76UWbNm5fHHH88TTzyRG264Ifvtt98qP4eV2eCDFLNmzcp5552Xxo0b54477kj79u2TJNOmTUvfvn3zy1/+Ml/96ldLx1fmt7/9bUaNGpXPfe5zuemmm7LNNtskSf7+97+nT58+uemmm3LooYdml112+dRrAgAAAAAAAIAVxo0bl4EDB2b27NmrHPfQQw/lnHPOyaJFi1Y57oILLsiiRYsycODAnHDCCUmSqqqqXHfddbnuuuvyox/9qNoOEPfcc08mTJiQPfbYI7/+9a/TvHnzJMmUKVPSp0+f3H777Tn00EOzxx57JEn+93//t9b7Llu2LN/97neTJOecc0522223JMtXx7jwwguzbNmyDBw4MMcff3wqKiqSJLfcckuGDBmSCy+8MD//+c9Lcz3++OMZMWJEdtpppwwbNiybbrppkuTRRx/N6aefnh/+8Id57LHH0rhx41V+FrXZ4Lf2uPPOO1NZWZk+ffpUC0vsuOOOpTTObbfdttp5fvnLX6Z+/fq5+uqrSyGKJGnbtm369++fNm3afOLlPwAAAAAAAABgTc2ePTsXXHBB+vfvn7lz52brrbeuddxbb72V008/PaeffnqqqqqyxRZbrHTOOXPm5IUXXkiTJk1y3HHHlY5XVFTk+9//fho3bpxp06ZVW1HiySefTJL069evFKJIkl122SWHHHJIkmTChAmrrWfo0KF5+umn061bt/Tu3bt0/N57781HH32U7t2754QTTiiFKJLke9/7Xvbbb7888sgjmTJlSun4zTffnCQZPHhwKUSRJAceeGAOPfTQvPfee/n973+/2meqzQYfpBgzZkyS5UuD/Lvu3bunoqIif/7zn1c5x8svv5w333wz++67b3beeeca50888cT8+c9/Tq9evcryzAAAAAAAAACwOjfccEOGDx+e7bbbLrfddls6depU67jLLrssDz30UHbbbbeMHDkyO+yww0rnrFdveUygsrIyc+bMqXZu/vz5Wbp0aRo0aJCmTZvWuGbGjBk15lsRuGjRosUqa5k+fXquv/76NGnSJBdccEG1c6+88kqSpFu3brVeu6LuFYGO+fPnZ9KkSWnSpEn+3//7fzXGf+1rX0uS1WYFVmaDDlJUVVXl1VdfTZJqe6es0KJFi2yxxRaZO3durX+gK6xYaWK33XZLVVVVnnjiiQwZMiTnnntuhg0blrlz5346BQAAAAAAAADASmy77bY5//zz8+CDD6Zjx44rHdeuXbtceeWVueeee7LTTjutcs7mzZtnzz33zLJly/KDH/wgL774YiorK/Pyyy/nlFNOyccff5zvfOc72WijjUrXdOnSJUly7bXX5oEHHsj8+fMze/bsXHfddXnsscey9dZb5+CDD17lfX/605+msrIyJ510Urbaaqtq55YtW5Yk2WSTTWq9tkGDBkmSadOmlf5z2bJl2WGHHUrn/tWOO+6Y5P8CGmur5owbkLlz52bJkiVp2rRpmjRpUuuYVq1a5b333susWbNq/GGs8I9//CPJ8j+U4447LmPHjq12/vrrr88vfvGL7L333uUtAAAAAAAAAABWom/fvms07rTTTlurea+44oqcdNJJmTRpUnr27Fk63rBhw5xzzjk55phjqo3v1atXXn755dx9990ZNGhQtXNf/epXc/755680BJEsDz489NBDadmyZY25k2SHHXbIk08+mQkTJuTAAw+scX7ixIlJ/m/1i5kzZyZJttxyy1rvt+L4rFmzVvpMq7JBBykWL16cJGncuPFKx6xIySxatGilY+bPn59k+X4s9erVyxVXXJHOnTtn3rx5+fWvf50RI0ZkwIABeeCBB9KqVas1fr4Vy56sjXr16qVZs2bZfPPN1+q6ddGsWbN88MEHpXTPp2191pas3/rqcm2JviwnfVk++rJ89GX51OX66nJtie+UctKX5aMvy0dflo++LB99WT76snz0ZfnU5frqcm3rasmSJUmW7zW+4t9Z1yV1uT61bbg2hPrq+velv4OVj9/j5bOh9OX6+vxXp1WrVunZs2d+/vOfZ7vttst2222XadOm5Y033sidd96ZDh06pH379qXx9evXz4EHHphJkyZl5syZ2X333TNv3ry88MILeeqpp3L//ffn+OOPX+n9brvttlRVVeWYY46pdZGEHj16ZNiwYbnrrrvSsWPHHHTQQaVzI0eOzOjRo5Ok9P59xfv/lWUFNt544yRZ598TFVVVVVXrdOV/gRkzZqRLly7Zcssta6wisULv3r0zadKkDBs2bKX7xZxzzjn5zW9+kyS1jjvppJPypz/9KSeccEIGDhy4xs+3IhUDAAAAAAAAAB06dPhE1//whz/Mfffdl4svvji9evVa6bhjjjkmTz/9dG655ZZ86Utfqnbuww8/zPe+970888wzufTSS6utSHHHHXfkoosuSsuWLfPwww9ns802S5IMHz48F154Ybp165bLL7+8tPrE1KlTM2DAgLzzzju55JJLcsQRR9R4lnnz5qVz585JkjFjxqRly5a1PvPQoUNz5ZVXJkl22mmnbLfddnnjjTcybdq09O7dO3fddVf222+/3HTTTXnggQcyaNCgHHLIIbnqqqtqzPXRRx+VgiAvv/xyKioqVvpZ1WaDXpGiadOmSZLKysqVjlmRSlzZ1h/J/6VU2rZtW2vYonfv3vnTn/6Uv/71r2v1fDvttNM6rUgxfPjwTJ8+fa2uWxdt2rRJ796912sKbH3Vlqzf+upybYm+LCd9WT76snz0ZfnU5frqcm2J75Ry0pfloy/LR1+Wj74sH31ZPvqyfPRl+dTl+upybetqyZIlmT59etq0aVNtH/G6oi7Xp7YN14ZQX13/vvR3sPLxe7x86nJfltuoUaMyYcKE9OzZs1qIIkn69OmTZ599Ng8++GCGDx+eH/zgB5kzZ04uv/zyNG/ePEOGDKm2hUe7du3y4x//OMcee2xuuOGGWoMUf/rTn1JZWZmDDz54pSGKJDnhhBPStm3b3HTTTXnxxRfzz3/+M3vssUfOO++8NGrUKHfddVeaN2+e5P/e/68sK7DieJMmTdY6RJHUgSBF06ZNM3/+/FRWVpaW5/hXK/ZGWdWWHJtuummS5DOf+Uyt51cc/+CDD9bq+Zo1a7ZW41eYP39+Zs+evU7Xro1NNtmkVPv6sr5qS9Z/fXW5tkRflou+LC99WR76srzqcn11ubbEd0q56Mvy0pfloS/LS1+Wh74sL31ZHvqyvOpyfXW5tnWxaNGiTJ8+PS1btlzl/6FvQ1WX61PbhmtDqa+uf1/6O1h5+D1eXnW5L8tpxQIC++23X63nu3TpkgcffDBTpkxJkkyePDmLFi3KfvvtVwoy/Kt99903jRo1yltvvZUFCxZUC1okyeOPP54kOeSQQ1b7bPvvv3/233//GsdX7DCx9dZbJ0m22mqrJMmsWbNqnee9995Lkmy55ZarvWdt6q3TVf8lKioq0rZt2yTJtGnTapyfM2dOZs2alRYtWpQ+yNq0a9cuyfKtQmqz4kP+b9mvBgAAAAAAAADWxbx585Ik9evXr/V8gwbL12P48MMPkywPqPzr8X9Xr1691Ku3PHrw7zs2LFu2LGPHjs3GG2+crl27rvSZ3n///YwbNy6vvfZaredXhD922223JMmOO+6YevXq5bXXXqt1ZZBXX301yfJdJNbFBh2kSFLaS2X06NE1zo0ePTpVVVXp0qXLKufYd999s9FGG2XKlCm1BjKeeOKJJEnHjh3L8MQAAAAAAAAA8J+x4447JknGjBlT6/mnnnoqSbLzzjtXG//MM89kwYIFNcZPmjQplZWVad26dTbbbLNq51599dUsXLgwu+yySxo1arTSZ3rppZfSr1+/XH311TXOzZo1K6NHj07z5s1Lq2g0btw4++yzT+bPn5/x48fXuOaxxx5LklWGN1Zlgw9SHHHEEWncuHFuvfXWTJo0qXT8tddeK33Ixx13XOn4zJkzM23atNKWH8nypVeOPPLIVFVV5cwzz6y23MvYsWNz++23Z+ONN85RRx316RcEAAAAAAAAAJ+SXr16pX79+vntb3+bBx98sNq53/72txk1alS19+M777xz9txzzyxYsCBnn312Fi9eXBr/5ptv5txzz02SHHPMMTXu9cILLyRJ2rdvv8pn6tixY1q0aJHRo0fn6aefLh2fN29eTj/99CxevDj9+/evtm3IivtddNFFpV0mkuQPf/hDHnzwwWy55ZY5/PDD1+gz+Xe1r72xAWndunXOOeecnHfeeenTp086deqURo0aZdy4cVmyZEkGDhxYSsokyVVXXZX77rsvPXr0yJAhQ0rHzzjjjLz88suZMGFCunfvnk6dOmXOnDn529/+loqKipx//vnZbrvt/hMlAgAAAAAAAEBZtG3bNueff34uuOCCDBw4MDfeeGO23377vP766/n73/+ehg0bZsiQIdl6661L11xxxRU55phj8oc//CFPP/10OnTokPnz52fy5MmprKzMgQcemP79+9e419tvv50k2XzzzVf5TBtvvHEuuuiinHrqqenXr1/22WefNGnSJM8880zmzZuXAw88MMcff3y1a772ta/l8MMPz+9+97scdNBB2XffffPBBx9k0qRJadiwYa688spVroKxKht8kCJZnphp3bp1hg4dmueeey7169fPF77whfTv3z8HHHDAGs3RpEmT3HLLLbnzzjvz29/+NuPGjcvGG2+c/fbbLyeccEL22WefT7kKAAAAAAAAAPj0HXXUUWnXrl1uuummTJw4Ma+99lpatmyZgw8+OCeeeGJ22WWXauO33Xbb3HfffbnpppsyevToPPnkk2nQoEF23nnnHHHEETniiCNSUVFR4z7vv/9+kqR58+arfaYDDzwwQ4cOzdChQ/O3v/0tDRs2zA477JCjjjoqhx9+eOrXr1/jmiFDhmT33XfPyJEj8+STT6Z58+bp1q1bTjnllBo1rI06EaRIks6dO6dz586rHTdkyJBqK1H8q4YNG6Zfv37p169fmZ8OAAAAAAAAAD6ZVb3v/le33377asfsueeeufbaa9f43ptuumkGDRqUQYMGrfE1F1xwQS644II1Ht+1a9d07dp1jcfXq1cvffr0SZ8+fdb4mjWat6yzAQAAAAAAAABswAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFBr8px+gXJ5++unccMMNmTJlSiorK9OuXbv07ds3X//619d5zgceeCCDBg3KoYcemiuuuKKMTwsAAAAAAAAAK9euXbs1Gjds2LB06tSp9POf/vSn3HrrrXnhhRdSVVWVHXfcMUceeWR69uyZ+vXr17h+5syZufHGGzNmzJi8++67adSoUXbZZZf07t073/jGN2qMv/XWW3PZZZet9HmOP/74DBo0qNqxhx9+OMOHD8+UKVOyePHitGrVKl26dMmAAQOy1VZb1Zjjo48+yt13351Ro0bl9ddfT1VVVXbeeef069cvBx100Bp9Lp9EnQhS3H///Rk8eHAaNGiQTp06pX79+hk3blxOP/30vPrqqzn11FPXes7p06fnxz/+8afwtAAAAAAAAACwaoceeuhKz7311lt57rnn0qxZs2y77bal41dffXWuv/76JEnbtm2z7bbbZurUqTn33HMzevTo/PznP8/GG29cGv/666/nO9/5TmbPnp02bdqkc+fOmTt3bp599tk888wzefbZZ3PeeedVu/eLL76YJPnKV76SZs2a1Xi2L3zhC9V+vuSSSzJs2LA0bNgwu+22W1q2bJmXXnopw4cPz6OPPpo77rgjO+64Y2n80qVLM2DAgIwdOzbNmjXLvvvum3nz5mXSpEk57bTT8qMf/Sjf+c531uKTXHsbfJBi1qxZOe+889K4cePccccdad++fZJk2rRp6du3b375y1/mq1/9aun4mqiqqspZZ52VefPmfVqPDQAAAAAAAAArtbJdExYvXpyePXumoqIiV155Zbbeeuskybhx43L99denfv36ueyyy3L44YcnWb66w+WXX57bbrst11xzTQYPHlya65xzzsns2bPzne98J2effXYaNmyYJHnppZfSr1+/3HHHHenSpUu6du1aumbKlCmle2+yySarrGHcuHEZNmxYNttss9xyyy3ZeeedkywPS1x88cUZMWJEzjrrrPzmN78pXTN06NCMHTs2e+21V2644Ya0bNkySTJ+/Pgce+yxueyyy3LwwQdns802W8tPdM3V+9RmXk/uvPPOVFZWpk+fPtXCEjvuuGPOOOOMVFVV5bbbblurOW+55ZaMHz8+++yzT7kfFwAAAAAAAADW2aWXXprXXnstffr0qRZwGDlyZJKkT58+pRBFkjRo0CBnnXVWPv/5z2fYsGF5//33kyRvvvlmJk6cmFatWuWHP/xhKUSRLF9V4qSTTkqSPPjgg6XjlZWVee2117L99tuvNkSRJPfee2+S5OSTTy6FKJKkUaNGOffcc9OyZcs8//zz+cc//lGa/+abb07Tpk1zzTXXlEIUSdKpU6ccccQRadWqVZ5//vk1/rzWxQYfpBgzZkySpHv37jXOde/ePRUVFfnzn/+8xvNNnTo1P/vZz7L//vunZ8+e5XpMAAAAAAAAAPhEJk+enHvuuSdt2rTJGWecUe3cK6+8kiTp1q1bjevq16+fjh075sMPP8y4ceOSJO+//3723HPPdOnSJY0aNapxzfbbb58kmTlzZunY1KlT8/HHH6/xjhCNGzdO27Zt06FDhxrnGjVqlG222abaPcaOHZuFCxfmkEMOSatWrWpcc8EFF+SPf/xjtQDJp2GD3tqjqqoqr776apLl+7v8uxYtWmSLLbbIe++9lxkzZmSrrbZa5XxLly7NoEGD0rRp01x88cV54oknPpXnBgAAAAAAAIC1demll6aqqiqDBg1KkyZNqp1btmxZkqx0pYgGDZbHA6ZNm5Yk2WuvvTJixIiV3mvy5MlJktatW5eOvfjii0mWv4v/0Y9+lKeeeiozZ87MNttsk0MPPTTHHXdcNtpoo9L4iy66aKXzL1iwoPQsK97lr5h/9913z0cffZTRo0fnmWeeydKlS7PrrrvmsMMOy8Ybb7zSOctlgw5SzJ07N0uWLEnTpk1rNMkKrVq1ynvvvZdZs2atNkhx1VVX5ZVXXsk111yTLbbY4tN4ZAAAAAAAAABYa0888USeffbZ7LjjjjnkkENqnN9hhx3y2muvZcKECdl1112rnauqqsqkSZOSpLS1x6q89957uf3225MkBx54YOn4Sy+9lCS58847s9lmm2WvvfbKVlttlRdffDHXXHNNnnjiidxyyy0rfX//r66//vpUVlamffv22XbbbZOktMVHRUVFjjzyyFKwIklGjBiRoUOHZujQodlhhx1WO/8nsUEHKRYvXpxk+XIgK7Mi7bJo0aJVzjVu3LjceuutOeyww6o1wicxf/78LF26dK2uqVevXpo1a5bNN9+8LM+wKs2aNcsHH3xQSiZ92tZnbcn6ra8u15boy3LSl+WjL8tHX5ZPXa6vLteW+E4pJ31ZPvqyfPRl+ejL8tGX5aMvy0dflk9drq8u17aulixZkiSZM2dO6d9Z1yV1uT61bbg2hPrq+velv4OVj9/j5bOh9OUneb7bbrstSXLCCSekoqKixvkePXpk9OjRufbaa7Prrrtmn332SbJ8pYprr722FIJY3TvsRYsW5ZRTTsmCBQuy7777VtsqZMUc3/rWt3LBBReUtgR56623cvLJJ+e5557LlVdemfPOO2+V93j00Udz8803p169ehk8eHDp+Pz585MkP/nJT9KsWbPceOON6dixY959991cddVVefzxx3PiiSfmgQce+FRXpqioqqqq+tRm/5TNmDEjXbp0yZZbbpmxY8fWOqZ3796ZNGlShg0blk6dOtU6Zt68eTnssMNSVVWVBx54IM2bN0+SjBo1KmeffXYOPfTQXHHFFWv9fBMnTlzrawAAAAAAAAComzp06LBO17322mv5+te/nq222iqPP/54aZuOf3f++efn7rvvTkVFRXbddddstdVWmTp1at5999307NkzI0aMyFFHHZUf//jHtV6/YMGCnHTSSZkwYUI+85nPZMSIEdV2c1i8eHHefvvt7LDDDqlfv361a6dMmZIePXqkUaNGmTBhQrUtPv7Vww8/nDPPPDMffvhhBg0alOOPP750rl+/fhk3blwaNmyY+++/v9rKE8uWLUvPnj0zZcqUXHjhhTn66KPX+PNbWxv0ihRNmzZNklRWVq50zIpU4qqWDrnwwgvz7rvv5uabby6FKMphp512WqcVKYYPH57p06eX7TlWpk2bNundu/d6TYGtr9qS9VtfXa4t0ZflpC/LR1+Wj74sn7pcX12uLfGdUk76snz0Zfnoy/LRl+WjL8tHX5aPviyfulxfXa5tXS1ZsiTTp09PmzZtVvqyYENWl+tT24ZrQ6ivrn9f+jtY+fg9Xj51uS+T5JFHHklVVVW+8Y1vrDREkSx/973nnnvmjjvuyCuvvJK33norHTt2zDXXXJNp06ZlxIgRK30n/u677+bEE0/Myy+/nO222y633nprtRBFsny3iLZt29Z6/S677JLWrVtn+vTp+fvf/15je5Fk+aoaQ4YMybJly3LaaadVC1EkKa0y8eUvf7nG9h316tXLkUcemQsvvDB//etfBSlWpmnTpmnatGnmz5+fysrKWpfumDlzZpKkVatWtc7x/PPP58EHH0zLli0zatSojBo1qnTu7bffTpI8++yzGTRoUHbccccMGDBgjZ+vWbNma1NOyfz58zN79ux1unZtbLLJJtl0000/9fv8q/VVW7L+66vLtSX6slz0ZXnpy/LQl+VVl+ury7UlvlPKRV+Wl74sD31ZXvqyPPRleenL8tCX5VWX66vLta2LRYsWZfr06WnZsuUa7QW+oanL9altw7Wh1FfXvy/9Haw8/B4vr7rcl6NHj06SHHLIIasd26NHj/To0aPG8cceeyxJsvXWW9c499JLL+XEE0/MzJkz0759+wwdOrRGiGJNbLHFFpk+fXqNxRA+/vjjXHrppbnjjjtSv379XHDBBendu3eN6zfbbLMkyWc+85la599mm22SJB988MFaP9va2KCDFBUVFWnbtm2ee+65TJs2Le3bt692fs6cOZk1a1ZatGiRrbbaqtY5Fi1aVBr7wAMP1Drm7bffzttvv50vfvGLaxWkAAAAAAAAAIBPYvbs2XnxxRfzmc98Jl/4whdWOu7dd9/NtGnTssMOO6RNmzY1zv/1r39Nkuy22241jg8YMCCLFi1K165d87Of/ay0O8S/mjFjRq655posXbo0P/3pT2t9hrfeeitJqr2f//DDD/M///M/GT16dJo0aZIrr7wy3bp1q/X6nXbaqXSv2syaNStJsvnmm9d6vlzqfaqzrwedO3dO8n8JnH81evToVFVVpUuXLiu9vlOnTpk6dWqt/1x22WVJkkMPPTRTp07N7bff/ukUAQAAAAAAAAC1mDx5cpJkzz33XOW4MWPGpH///hk2bFiNcy+//HKeffbZfPazn6225cbzzz9fClH06tUr119/fa0himT5Shz3339/7r///kyZMqXG+T//+c+ZM2dOtt9++2y77bal44MHD87o0aOz2WabZdiwYSsNUSRJ165dkyRPPfVUratOPPHEE0mSjh07rnSOctjggxRHHHFEGjdunFtvvTWTJk0qHX/ttddy9dVXJ0mOO+640vGZM2dm2rRppS0/AAAAAAAAAOC/1QsvvJAkNXZo+HedO3dOw4YNM2LEiEybNq10fMaMGRk0aFCqqqpy8sknp6KiIkmyZMmSnHHGGVm0aFEOP/zwXHzxxalfv/5K52/atGkOPfTQJMm5556b999/v3TujTfeyI9//OMkyQ9+8IPS8ZEjR+ahhx5K48aNc8stt9RYDePffe5zn0u3bt2yaNGiDB48OAsXLiydu++++/LII49k8803zze+8Y1VzvNJbdBbeyRJ69atc8455+S8885Lnz590qlTpzRq1Cjjxo3LkiVLMnDgwOy8886l8VdddVXuu+++9OjRI0OGDPkPPjkAAAAAAAAArNrbb7+dZPXbWWy99dYZOHBghgwZkh49euSLX/xi6tWrl/Hjx6eysjLf/e53c9hhh5XG33vvvfnHP/6RJFm8eHEGDRpU67yf+9znSuGIs846Ky+88EJeeOGFHHDAAdl7773z8ccf5+mnn87SpUvTr1+/0j0++uijXHvttUmSVq1a5de//vVKn/3EE09M27ZtkyQXXXRR/vGPf+SJJ55It27d0qFDh0yfPj0vvfRSNt5441x22WVp3rz5mnx062yDD1IkSa9evdK6desMHTo0zz33XOrXr58vfOEL6d+/fw444ID/9OMBAAAAAAAAwDpZsfLDmoQHvve972XTTTfNsGHDMn78+DRt2jR77LFH+vbtm+7du1cbu2KbjCT5wx/+sNI5995771KQokWLFrn77rtz880356GHHsq4ceOy0UYbZc8990zfvn3zta99rXTd1KlTSztFvPnmm3nzzTdXeo+ePXuWghRbbLFFRo4cWbrHk08+mWbNmuWggw7KgAEDqi2k8GmpE0GKZPkyJZ07d17tuCFDhqzxShQ9e/ZMz549P+mjAQAAAAAAAMA6+dWvfrVW47/5zW/mm9/85mrH3XDDDev0PE2aNMnJJ5+ck08+eZXj2rdvn6lTp67TPZo2bZpTTjklp5xyyjpd/0nV+4/cFQAAAAAAAADgv5AgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAAAAAAAFQQoAAAAAAAAAgIIgBQAAAAAAAABAQZACAAAAAAAAAKAgSAEAAAAAAAAAUBCkAAAAAAAAAAAoCFIAAAAAAAAAABQEKQAAAAAAAAAACoIUAAAAAACwjurVq5cmTZqkXr26+a/b63J9dbk2AOCTafCffgAAAAAAAPhvMWzYsEyfPv1Tv0+bNm3St2/fT/0+/2p91Zas//rqcm3roq6HROp6fQD859WZIMXTTz+dG264IVOmTEllZWXatWuXvn375utf//oaz/H6669n6NChGTduXGbNmpUmTZpkt912S79+/dK5c+dP8ekBAAAAAPhvMH369Lz55pv/6cf4VKhtwyXgUx4bQggGoDZVVVW55557cvfdd+fVV19Nw4YN065duxx55JH55je/WWP8xIkT86tf/SrPPvtslixZku222y7f/OY38+1vfzsbb7xxrfd47rnnMnTo0EyaNCkLFixI69atc9BBB2XAgAFp2rRpjfEPP/xwhg8fnilTpmTx4sVp1apVunTpkgEDBmSrrbaqMf6jjz7K3XffnVGjRuX1119PVVVVdt555/Tr1y8HHXRQjfELFizI0KFD88gjj+Sf//xnmjVrlr333jvHHnts9t5777X/ENdSnQhS3H///Rk8eHAaNGiQTp06pX79+hk3blxOP/30vPrqqzn11FNXO8fEiRNz3HHHZdGiRdl+++3TtWvXzJgxI2PHjs3YsWMzePDgHHvsseuhGgAAAAAAgP9Tl4Midbk2gHK56KKLcuedd6Zx48bZZ599UlFRkYkTJ+ass87K+PHjc9lll5XGjhw5Mueff36WLVuWbbfdNnvvvXdef/31/OQnP8lDDz2UoUOHZrPNNqs2/7333ptzzz03VVVV2WuvvdKiRYs8++yz+dWvfpWJEydm2LBhadiwYWn8JZdcUjq22267pWXLlnnppZcyfPjwPProo7njjjuy4447lsYvXbo0AwYMyNixY9OsWbPsu+++mTdvXiZNmpTTTjstP/rRj/Kd73ynNL6ysjL9+vXL888/ny233DKdO3fO3Llz8/jjj+ePf/xjLr744nzrW9/6FD/xOhCkmDVrVs4777w0btw4d9xxR9q3b58kmTZtWvr27Ztf/vKX+epXv1o6XpuPPvoogwcPzqJFizJw4MAcf/zxqaioSJI89dRTOfHEE3PFFVekc+fO2WmnndZLXQAAAAAAAAD8/9uYMWNy5513pk2bNhk+fHjatGmTZHkQrXfv3hk1alQOOuigdO3aNa+//nouvPDCLFu2rMZ771tuuSVDhgzJhRdemJ///Oel+V9//fVccMEF2XjjjfOLX/wiX/rSl5Ikc+bMyYknnphJkyblzjvvTL9+/ZIk48aNy7Bhw7LZZpvllltuyc4775xkeVji4osvzogRI3LWWWflN7/5TekeQ4cOzdixY7PXXnvlhhtuSMuWLZMk48ePz7HHHpvLLrssBx98cCngce211+b5559P165d8/Of/zyNGzdOkvzlL3/J8ccfn4suuijdunXLpptu+ql97hv85lF33nlnKisr06dPn2phiR133DFnnHFGqqqqctttt61yjqeffjpvv/12dtttt5xwwgmlZkqSL3/5yznqqKOybNmyPPTQQ59aHQAAAAAAAADwr+6///4kyamnnloKUSTLtytasYrDk08+mWT5yhIfffRRunfvXuO99/e+973st99+eeSRRzJlypTS8ZtuuilLly7NwIEDSyGKJGnZsmXOPPPMbLnllnn11VdLx++9994kycknn1wKUSRJo0aNcu6556Zly5Z5/vnn849//CPJ8tUlbr755jRt2jTXXHNNKUSRJJ06dcoRRxyRVq1a5fnnn0+yfBuTBx54IEnyox/9qBSiSJIvfelL+dKXvpTFixdnwoQJ6/JxrrENfkWKMWPGJEm6d+9e41z37t1zzjnn5M9//vMq51i4cGF22223dOnSpdbz22+/fZJk5syZn+hZAQAAAAAAAGBNDRkyJAMGDMjWW29d49yiRYuSJPXr10+SvPLKK0mSbt261TpXp06dMnbs2Dz55JPZZZddUlVVlT/84Q9p0qRJjjzyyBrjO3bsmLFjx1Y71rhx47Rt2zYdOnSoMb5Ro0bZZpttMmfOnMycOTPbbbddxo4dm4ULF+bII49Mq1atalxzwQUXVPu5oqIiDz/8cN5666185jOfWW3Nn5YNOkhRVVVVSr+0bdu2xvkWLVpkiy22yHvvvZcZM2Zkq622qnWer33ta/na17620vtMnjw5SdK6desyPDUAAAAAAAAArF7Dhg3z+c9/vsbxZ599NnfddVfq16+fQw89NEmybNmyJMkmm2xS61wNGiyPB0ybNi1J8vbbb2fu3LnZe++906hRo7z88st59NFHM2PGjGy99db5xje+UVp0YIWLLrpopc+6YMGC0twr3s2/+OKLSZLdd989H330UUaPHp1nnnkmS5cuza677prDDjssG2+8cbV5mjZtWm21i2R5NmDEiBGZNGlSWrVqVW31jE/DBh2kmDt3bpYsWZKmTZumSZMmtY5p1apV3nvvvcyaNWulQYpVmTp1an7/+9+noqIiBxxwwCd9ZAAAAAAAAABYJwMHDsy0adMyZcqUtGzZMldeeWV23XXXJMkOO+yQJ598MhMmTMiBBx5Y49qJEycmSd5///0kyZtvvpkk2XLLLfOzn/0sN954Y6qqqkrjb7jhhpx99tmlLURW5/rrr09lZWXat2+fbbfdNklKW3xUVFTkyCOPLAUrkmTEiBEZOnRohg4dmh122KHWOd94441cfvnlefnll/POO++kbdu2ufLKK6tt+fFp2KCDFIsXL06SVX5IG220UZL/W+JjbcyePTunnnpqPv744/Ts2bNG6mV15s+fn6VLl67VNfXq1UuzZs2y+eabr9V166JZs2b54IMPSsmkT9v6rC1Zv/XV5doSfVlO+rJ89GX56Mvyqcv11eXaEt8p5aQvy0dflo++LB99WT76snz0Zfnoy/Kpy/XV5doS3ynlpC/LR1+Wj74sH31ZPvqyfDaUvvwkz/fBBx/kwQcfLP1cUVGRV155JQcccEDq16+fHj16ZNiwYbnrrrvSsWPHHHTQQaWxI0eOzOjRo5Ok9A57/vz5SZJx48bl8ccfz8knn5xvfetbadSoUX7/+9/npz/9aS666KJst9126dy58yqf7dFHH83NN9+cevXqZfDgwaXjK+7xk5/8JM2aNcuNN96Yjh075t13381VV12Vxx9/PCeeeGIeeOCBGitTJMsXPnj88cdLP3/88cf5+9//nnbt2q3tx7dWKqr+NVKygZkxY0a6dOmSLbfcssbeLCv07t07kyZNyrBhw9KpU6e1mrt///559dVXs+uuu+aOO+5Y61TLikQPAAAAAAAAAHTo0GGdr126dGnmz5+fjTbaKBMnTswll1ySN998M7169crFF1+cJBk6dGiuvPLKJMlOO+2U7bbbLm+88UamTZuW3r1756677sp+++2Xm266Kb/73e9KoYeTTz45p5xySrX73XLLLRkyZEj23HPPjBgxYqXP9fDDD+fMM8/Mhx9+mEGDBuX4448vnevXr1/GjRuXhg0b5v7776+28sSyZcvSs2fPTJkyJRdeeGGOPvroGnPPnTs3DRo0yOLFi/PYY4/lpz/9aRYuXJjLL788hx9++Dp/lquzQa9I0bRp0yRJZWXlSscsWbIkSVa69UdtXnnllZx00kl55513sttuu+Wmm25ap6VBdtppp3VakWL48OGZPn36Wt9vbbVp0ya9e/derymw9VVbsn7rq8u1JfqynPRl+ejL8tGX5VOX66vLtSW+U8pJX5aPviwffVk++rJ89GX56Mvy0ZflU5frq8u1Jb5Tyklflo++LB99WT76snz0ZfnU5b5coVGjRqUVLbp27ZrPfe5zOeyww3LvvffmxBNPzLbbbpsTTjghbdu2zU033ZQXX3wx//znP7PHHnvkvPPOS6NGjXLXXXelefPmSarv/FDb9h1HHXVUfvKTn2Ty5MlZuHBh6f38v7rtttsyZMiQLFu2LKeddlq1EEWS0ioTX/7yl2ts31GvXr0ceeSRufDCC/PXv/611iBFixYtkizPBvTu3TvNmzfPGWeckZ///OeCFCvTtGnTNG3aNPPnz09lZWWtS33MnDkzSdKqVas1mvOpp57KqaeemgULFmS//fbLNddcU2tDrIlmzZqt03Xz58/P7Nmz1+natbHJJptk0003/dTv86/WV23J+q+vLteW6Mty0ZflpS/LQ1+WV12ury7XlvhOKRd9WV76sjz0ZXnpy/LQl+WlL8tDX5ZXXa6vLteW+E4pF31ZXvqyPPRleenL8tCX5VWX+7I22223Xfbaa6/85S9/yZQpU7LtttsmSfbff//sv//+Ncb/5je/SZJsvfXWSZLNNtssyfJFCVb893+14vjs2bMzb968au/NP/7441x66aW54447Ur9+/VxwwQXp3bt3jTlWzPuZz3ym1hq22WabJMu3LVkTBx10UM4+++y88847mTNnTlq2bLlG162tep/KrOtJRUVF2rZtmySZNm1ajfNz5szJrFmz0qJFi2y11Varne+BBx7ICSeckAULFuSII47IjTfeuM4hCgAAAAAAAAD4JK666qqcdtppWbRoUa3nGzVqlCT56KOP8v7772fcuHF57bXXah3717/+NUmy2267JUnpXfuiRYsyf/78GuM//PDDzJkzJ0mqBS0+/PDDnHrqqbnjjjvSpEmTXHfddbWGKJLluzgkyYwZM2o9P2vWrCQprbTx7rvv5uKLL86ll15a6/h69eqlQYMGpZo/LRt0kCJJOnfunCQZPXp0jXOjR49OVVVVunTpstp5/vjHP+ass87KRx99lFNOOSWXXHJJ6Q8AAAAAAAAAANa3MWPG5JFHHqn1ffi8efPy3HPPJUnat2+fl156Kf369cvVV19dY+ysWbMyevToNG/ePPvtt1+S5dtm7LXXXkmShx56qMY1f/nLX/Lxxx9n9913z0YbbVQ6Pnjw4IwePTqbbbZZhg0blm7duq30+bt27Zpk+c4Qta068cQTTyRJOnbsmCSl7Uduv/32WrdpmThxYhYuXJjWrVtniy22WOl9P6kNPkhxxBFHpHHjxrn11lszadKk0vHXXnut1CDHHXdc6fjMmTMzbdq00pYfyfKmOfvss/Pxxx9nwIABOfnkk9fb8wMAAAAAAABAbY4++ugkyeWXX5433nijdHzu3Lk588wzM2fOnHTv3j2f/exn07Fjx7Ro0SKjR4/O008/XRo7b968nH766Vm8eHH69++fTTbZpHTu2GOPTbJ85YvJkyeXjr/11lu55JJLkiTf/e53S8dHjhyZhx56KI0bN84tt9xSWt1iZT73uc+lW7duWbRoUQYPHpyFCxeWzt1333155JFHsvnmm+cb3/hGkuUrXxx44IFZtmxZzjrrrGorZUybNi3/+7//myTp37//mn2A62iDX3KhdevWOeecc3LeeeelT58+6dSpUxo1apRx48ZlyZIlGThwYHbeeefS+Kuuuir33XdfevTokSFDhiRJbrnllsyZMycNGjTIW2+9lUGDBtV6r7333jvf/va310tdAAAAAAAAAPz/21FHHZXx48fn4YcfzqGHHpoOHTqkQYMGmTx5cubOnZv27duXtsHYeOONc9FFF+XUU09Nv379ss8++6RJkyZ55plnMm/evBx44IE5/vjjq83/ta99Lf3798/NN9+co446Kh07dkyjRo0yadKkLFq0KD179iyFHD766KNce+21SZJWrVrl17/+9Uqf+8QTTyxtHXLRRRflH//4R5544ol069YtHTp0yPTp0/PSSy9l4403zmWXXZbmzZuXrj3vvPMyderUjB8/Pt27d89ee+2VefPmZfLkyfnwww/zrW99K3379i3r5/zvNvggRZL06tUrrVu3ztChQ/Pcc8+lfv36+cIXvpD+/fvngAMOWO31K5YL+eijj/Lggw+ucqwgBQAAAAAAAADrQ7169fKzn/0sX/7ylzNy5Mg8++yzSZLtt98+xx13XL773e9W23bjwAMPzNChQzN06ND87W9/S8OGDbPDDjvkqKOOyuGHH5769evXuMdZZ52VDh065Pbbb88LL7yQZcuWZccdd8zRRx+dI444ojRu6tSppZ0f3nzzzbz55psrfe6ePXuWghRbbLFFRo4cmZtvvjkPPfRQnnzyyTRr1iwHHXRQBgwYUG1hhGT5qhT33HNPfvWrX+WRRx7J2LFjs9FGG2WvvfZK79698/Wvf33dP9A1VCeCFEnSuXPndO7cebXjhgwZUlqJYoUHHnjg03osAAAAAAAAAFhnFRUV6dWrV3r16rVG47t27ZquXbuu1T26d++e7t27r3JM+/btM3Xq1LWad4WmTZvmlFNOySmnnLLG4//nf/4n//M//7NO9/uk6v1H7goAAAAAAAAA8F9IkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAAAAAAAEBBkAIAAAAAAAAAoCBIAQAAAAAAAABQEKQAAAAAAAAAACgIUgAAAAAAAAAAFAQpAAAAAAAAAAAKghQAAAAAAAAAAAVBCgAAAAAAAACAgiAFAAAA/x979x0W1dG2AfwG7BqjicbERKNJzKGpYMGCDWsssfcuKlbsPSExsfeCDRE1isaosZuixt4QNWKPBRErKqj0suzz/QFnsguY6usu+e7fdb3Xm+wum5k950x9ZoaIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjSMZCCiIiIiIiIiIiIiIiIiIiIKB0DKYiIiIiIiIiIiIiIiIiIiIjS5bB0Al6WU6dOYdmyZbhy5QoSExOhaRq6d++OJk2a/OXviI2Nhb+/P37++Wfcv38fhQoVgoeHB4YMGYI333zzf5h6IiIiIiIiIiIiIiIiIiKirL2M+XD66/4TO1Ls2LED3bt3x6lTp+Do6IjKlSvj0qVLGD58OBYuXPiXviM2Nhbdu3fHsmXLkJqaijp16iBfvnzYsGEDWrVqhYcPH/6Pc0FERERERERERERERERERGTuZcyH09+T7XekePLkCXx8fJA3b14EBgbCyckJAHDz5k10794dS5YsQb169dTrL+Lr64tLly6hZcuWmDJlCnLkyAGj0YgZM2Zg9erV+Oqrr7B06dJXkSUiIiIiIiIiIiIiIiIiIqKXNh9Of0+235Fi3bp1SExMRNeuXc1ujg8//BAjRoyAiOCbb775w++IjY3Fxo0bkTdvXkyYMAE5cqTFl9ja2mLMmDEoUaIE9u/fj/Dw8P9pXoiIiIiIiIiIiIiIiIiIiHQvYz6c/r5sH0hx6NAhAED9+vUzvVe/fn3Y2Njg4MGDf/gdp06dQnx8PCpVqoTXX3/d7D07Ozt4eHgAwJ9+DxERERERERERERERERER0cvyMubD6e/L1oEUIoIbN24AAMqUKZPp/ddffx1FihTB8+fPERER8cLv+aPvAICPPvoIAHDt2rV/m2QiIiIiIiIiIiIiIiIiIqI/9bLmw+nvy9aBFM+fP0dSUhLy58+PfPnyZfmZt956C0Da2TEv8ujRI7PPZlS0aNE//Q4iIiIiIiIiIiIiIiIiIqKX5WXNh9Pfl8PSCfg3EhISAAB58+Z94Wdy584NAIiPj3/hZ/T38uTJk+X7+ut/9B1ZiYmJQXJy8t/6G1tbW5QuXRqvvfba3/q7f6JIkSJ4+vQpjEbj//y/BbzavAGvNn//5bwBvC9fJt6XLw/vy5eH9+XL81/O3385bwDLlJeJ9+XLw/vy5eF9+fLwvnx5eF++PLwvXx7ely/Pfzl//+W8ASxTXibely8P78uXh/fly8P78uXhffnyZJf78s033/zLn31Z8+H099mIiFg6Ef9UREQEatWqhaJFi+Lo0aNZfqZTp044e/Ys1qxZgypVqmT5GR8fH2zcuBFfffUVOnbsmOn948ePo1evXnBzc8PatWtfah6IiIiIiIiIiIiIiIiIiIgyelnz4fT3ZeujPfLnzw8ASExMfOFnkpKSAOCFW52Yvvei79Ff/6PvICIiIiIiIiIiIiIiIiIielle1nw4/X3ZPpAif/78iImJeeHN8+jRIwC/nw2TlWLFigF48bkxjx8/BgAULVr03ySXiIiIiIiIiIiIiIiIiIjoL3lZ8+H092XrQAobGxuUKVMGAHDz5s1M7z979gxPnjzB66+/roIlsqJ/x40bN7J8//r16wCAjz/++N8mmYiIiIiIiIiIiIiIiIiI6E+9rPlw+vuydSAFANSsWRMAsG/fvkzv7du3DyKCWrVq/eF3VKpUCfny5cOpU6cQExNj9l5qaioOHDgAGxsb9d8iIiIiIiIiIiIiIiIiIiL6X3sZ8+H092X7QIq2bdsib968WL16Nc6ePateDw0Nxfz58wEAffr0Ua8/evQIN2/eVFucAEDevHnRpk0bxMXF4YsvvkBycjIAQEQwa9Ys3L17F/Xr10fp0qVfTaaIiIiIiIiIiIiIiIiIiOj/vb87H04vh42IiKUT8W9t2rQJPj4+sLW1RZUqVZArVy6cOHECSUlJGDlyJLy8vNRnx40bh61bt6JVq1aYPn26ej02NhadOnXCtWvX8O6778LZ2RnXr19HaGgo3n33XWzYsIHnyhARERERERERERERERER0Sv1d+bD6eXIYekEvAzt2rXD22+/jeXLl+PcuXOws7ODo6MjPD090bBhw7/0HQUKFMC6deuwdOlS/Pzzzzhw4ACKFSuGzp07Y+DAgShatOj/OBdERERERERERERERERERETmXsZ8OP09/4kdKYiIiIiIiIiIiIiIiIiIiIheBltLJ4CIiIiIiIiIiIiIiIiIiIjIWjCQgoiIiIiIiIiIiIiIiIiIiCgdAymI6E89e/bM0kkgIiIiIiIiIiIiypLRaLR0EoiIrF5cXJylk0CUrTCQgoj+0KxZszB9+nSkpqZaOilEREREREREREREypUrV/D06VPY2toymIKIXqqMZYqIWCglL8fJkycxcuRInD9/3tJJIco2GEhBRC+0aNEiBAQEICwsDBEREZZOzv+EwWAw+/fs3hgiIiIiIiIiIiL6/+DChQto1aoVevfuzWAKsnrR0dGWTgL9DampqbC1TZtCvXLlCgDAxsbGkkn6V2JiYrB8+XIcPHgQy5Ytw8WLFy2dpP8p1gX0sjCQwkpw8paszaVLlxAYGIjixYvjiy++QPHixTMFHWR3qampyJEjB+Lj4zF16lRERERk68YQERH9O2yPERERERH9sazazJysIEspUKAA3n//fVy+fBlDhgxhMAVZrdWrV2PNmjVISEiwdFLoL7KzswMAzJs3Dz169MD+/fstnKJ/p0CBAujbty9q1qyJ/fv3Y9GiRf/ZYAqDwQBbW1skJyfj5s2blk4OZXMMpLACqampavI2JiYGT58+RVRUlIVT9fL8UcP1vzBhYZoH03/O7kdh5MyZE0lJSciRIwcKFy6M+Ph4zJs37z+17ZOdnR0SEhLQpUsXrFmzBhs2bLB0kl6a7H7//R3/hXLkz/wX8vhfH8T4o2uU3a/ff7kez5g3vT32X7lf/z9dO112zxfw37n//oyIqOv1X82znq//wn2Z0X/1mv0XZbxWRqNRvZbdr+OL0p/d8/X/xf/Hdsp/gT6GaTQakZiYiMjISABQK3b/S/T7MLvfj38mu9+vpUuXhp+fHxwdHREcHPyfDKb4L/d9dFmNr/+X8jd//nxMnz4d+/fvR2JioqWT81K8aE7kv8D0mfv+++/h5+eH0qVLo2jRohZM1b8jIrCxsUGVKlXg5eWFqlWr4uDBg/D19f1PzfcAadcvR44ciIuLQ58+fTB58mQ8ePDA0smibCyHpRPw/11qaqqKbNu0aRP27duHmzdv4q233sLAgQNRo0YNC6fw3zHN3+nTp3Hnzh3Ex8ejYsWKsLe3h42NjSrEsyM9f0ajEampqQgPD0eBAgVQrFgxle/sKl++fKhcuTKOHDmCGTNm4Ny5c4iKioKrq6vZdc2ODAYDcuTIARHB4cOHceXKFXh6eqJXr16WTtpLoecvKSkJ+/btQ1RUFEqVKoUqVaogV65clk7ev2Y0Gs0GaUzLj+xcngCZ85acnIxcuXJl6zwB5nXBw4cP8fjxYxgMBuTJkwcODg7qc9n1+pnmLyoqCk+fPkVMTAxKliyJ/PnzI3fu3JmubXZhmrebN2/i4cOHKFCgAIoXL46iRYtm63o8Yxvl7t27iIqKQqNGjfDuu+9aOHX/nmn+jh07hitXriA6OhpOTk5o1KjRf+bahYeH49GjR8iVKxc++OADFChQIFu3U0zTfvfuXTx+/BhFihTB66+/joIFC1o4df+eaVloY2Oj6jnTgIrsWFZmpD9bUVFRKFKkCAwGA3LmzPmfyR9gvs3tf8F/6dqYMi1T9u3bp/rkr732Gnx8fJA/f34Lp/CfM83bjRs3cP/+fSQnJ6NMmTJ4//33LZy6lyM712d/xjRv586dw/379/H06VNomoaKFStmy/aJTu+PJycn4+LFi7h27RreffddvPfeeyhdurSlk/ev6HlLTEzEsmXLcObMGdy9excff/wx5s2bh3z58lk6if9KxmcuOjoar7/+ulmwdXatKzKmPSEhATlz5kSOHDnU69m1bwAApUqVwpw5czBy5EgVTLFw4UIULlw4W1834M/7Ptk9f7rIyEjExcVBRFCwYEG88cYb2fZ+zOjJkyfYsWMHqlatiqFDh6Jw4cKWTtI/YlpG6PdlamoqjEYjoqKiUKxYMQun8OXRn6nLly/j/PnzKFWqFKZOnYoPP/zQwin752xsbJCSkoKcOXOicuXKGDZsGBYuXIhjx47Bzs4O/fv3R7ly5SydzH9NLxMNBgNGjhyJU6dOoW3btihUqJClk0bZGAMpLMhoNKqG0Ny5c7F8+XLY2toif/78uHv3Lvr06YPFixejXr16Fk7pP2Pa0Fu+fDlWrFihzgFzdHREx44d0b59+2w7kK93IBMSEjBnzhyEhITgwoULyJcvHz7++GN07NgR1apVy7aNiPfeew+DBw/G06dP8dNPPyFHjhxo3bo1atWqBTs7u2x5zXR6ROLcuXMRERGBd955B0OHDkXu3Lmz/WCVHnEZHx8Pb29vHDt2TL3Xq1cvtG3bNls3+kyvT1hYGB4/fozbt2+jdOnSePfdd/H2229bOIX/nGnefv75Z/z666+4evUq8uTJg4oVK6Jq1aooW7ashVP595nWdQEBAdi0aRPCwsLU+02bNkXDhg2z7cSuaf5Wr16NnTt34tq1a0hJScH7778PZ2dnjBo1Cu+88062y5vpPblq1SoEBgbi3r17yJEjB+rWrYvu3bujUqVK2fK6mebNz88PK1euxPPnzwEA/v7+WLBgAdzc3CyZxH/F9L5cvHgx/P39zVa9jBgxAl5eXtn+2q1atQrfffcdwsLC8Prrr8PFxQWTJ09G0aJFs2V9bprmb775Bhs3bsTNmzdRuHBhNGjQAD169PjP1OH79u3DmTNn8Ouvv+KNN95A8eLF0bFjR3z00UcWTuW/l5iYiG+++QYXLlzA5cuXUbJkSRQvXhwDBw7Ee++9Z+nk/Ws//fQTQkJCcPLkSdSqVQstWrTABx98YOlk/Sumk56nTp3C1atX8cYbb+Cjjz7K1oOJpnXBvHnzEBAQoI5qfP3119GhQwe4urpmu3oAMM+bv78/1q9fr1aZubu7Y8GCBShQoIAlk/ivmd6Xu3btUudyV65cGQ0aNMh218yU6fXz8/PDmjVr1K4G9erVw3vvvZdt+3Wmx4eOGzcOQUFBeP78OWxsbDB+/Hi8++672XZxg2neevXqhZCQEBQsWBBGoxHx8fF48uQJSpYsCSB7TsibBomsWbMGFy5cwJ07d/D222/D1dUVzZs3z5Z9OsC8DbZjxw4EBwfj1KlTyJ8/PypUqIAGDRrA1dVVBbdmt/zpSpcu/Z8LpvirfZ/smj/d6tWrsXv3bty6dQsigg8++AANGjRAnz59snW+AGDmzJkqALlNmzbZeoGijY0NUlNTISJqTmTmzJm4ceMGbt++jaZNm6Jz58547733sm05YmrhwoVYt24dChUqhBIlSuDDDz9UdUV2ZDQakTNnTgC/B1jrC91OnDgBEcGgQYPg7Oxs4ZT+c3pZmJycjPPnz+P69eto06YNvvzyS+TMmTNb13FkWTbyX9t3JxtatGgRFi1aBBcXFwwYMAClSpXChg0bsGrVKgCAr68vGjRoYOFU/j2mhdLs2bOxYsUKFCpUCI0aNcLt27dx9uxZFC1aFD179kS3bt0y/Y210wvl+Ph4dOvWDZcuXULJkiVRrFgxREZGIjQ0FPnz50ejRo3Qv39/1ZnMLkwbBVWqVEFcXBwMBgMaN26MoUOHolSpUgCy1zXLyNfXF4sXL0bRokVRtGhR+Pv74/XXX4ednV22zZMuJSUFAwcOxNGjR1G9enWULFkSO3fuRGxsLFq1aoXevXtny4kK047hihUrsHHjRoSHhwNIC45544038Pnnn6NOnTrZbnDKNG96YB0AvPbaa4iJiQEA5M2bF5MmTUKzZs0sls5/Q8/Xu+++i8aNG8NgMOD06dP47bffUKhQIXTr1g1eXl6WTubfYloGzpkzB/7+/ihSpAg8PDzw5MkT3L17F9evX0fhwoWxbNkylC9f3sIp/utM78lZs2YhICAAuXPnhqurK+7du4c7d+7Azc0NAwYMQLVq1QBknzrBNJ163ooUKYK6devi3r17OHbsGAoVKoTZs2dny53BsmqDFS9eHM2aNUNqaioCAgIAAIMHD8bgwYMz/Y01y+q+zJ8/P8qUKYOoqCiEh4fDxcUFCxYsQLFixbLVAFVWecuRIwfKlCmDR48eISoqCg0aNMDAgQNhb29v4dT+fRnrOX9/f4iIancZDAbky5cPX3zxBRo1aoS8efNaOMX/TFxcHDw9PRESEoL8+fMjR44cMBqNiImJQbFixeDj44NatWplu3aKbt68eVixYgWAtIH9MmXKYMqUKShbtmy2KEOyopcT8fHxGDFiBIKCgtS51f3790f//v2RJ08eC6fy31m2bBnmz58PNzc39OzZE8WLF8eDBw/g5uaWLXekMK2z9Pbl+++/jwYNGiAmJgaOjo5o0aKFum7ZpY4zZXpfent74/jx42rnnu7du2PQoEF4/fXXLZzKf2/+/PlYtmwZSpUqhWbNmiEuLg7Ozs6oW7euWT2QXa6hXtclJCSge/fuuHDhAqpXr6524Ktfvz5cXV0tnMp/JykpCQMHDsTJkyfRvXt39O3bFykpKYiPj8/Wu22YjvHpQSIFChRAnjx5EBMTg6SkJLzzzjuYN28eXFxcLJ3cvyWrvkHu3Lnxzjvv4NGjR4iPj0fx4sXxySefYNCgQdmyXtDpeb116xZGjhyJy5cvo3Llytk2mOLv9n2yW/50el1eoEABODo6IjY2FpcvXwYANG7cGMOGDcu2O03dvHkTvXr1wqNHjwCktS2HDRsGIPvUbQAwfvx4pKamYubMmeo10zmRvHnzqvZzjRo10K9fP1SsWDFb3o+61NRUbNmyBTNnzkRMTAw++OADfP/998ibN2+2GmfIij5++cYbb8DV1VWVJ5GRkahduzaGDBkCJycnSyfzH0tOTkaHDh1gY2ODBw8ewMfHB02aNEFSUhJy585t6eRRdiVkUUePHpWKFStKt27d5MqVK+r1K1euSPXq1UXTNNE0Tfbt22fBVP5z69atE03TpF+/fnLx4kUREbl9+7YMGTJENE2Thg0bSmBgoPq80Wi0VFL/tqSkJOnXr584ODjInDlzJCkpSb0eGBgoLVu2FAcHB/niiy/k4cOHFk7tX5eSkiIiIomJifLDDz+Ih4eHeHt7S7t27UTTNBkxYoTZvZqdrpmpxMRE8fHxUc/Y1q1b1XvZNU+6Bw8eiLu7u0yePFkSExNFRGTPnj3SsmVL0TRNxo0bJ9evX7dwKv+5WbNmiaZpUrduXZkzZ45MnjxZ+vfvL5qmiYODgwQEBEhCQoKlk/mPrFixQjRNE09PTzl16pQ8e/ZMrl27JgsWLFD36sGDBy2dzL9t9+7domma9OzZU3777Tf1enR0tHTq1EmcnJxkxIgREhUVZcFU/nPbtm1T1+3SpUsiIpKamioiIg0aNFBlZ3x8vNl72cGqVatE0zTp27evnD9/XkRETp06JW3atBEnJyfp3r27nDx5Un3e2srPP0pPYGCgaqOY1mu9e/cWTdPEzc1NDh8+/CqS+T+xefNmcXBwkH79+snVq1fV67Nnz1blyZIlS9Tr1nbt/khAQIBomia9e/dWz9yVK1ekUaNGommatG3bViIiIkRExGAwWDKpf5teD/Tu3VtCQkJERGTfvn3SsGFDKV++vAwePNjsemY3y5YtE03TpFevXnL8+HF58OCB/Pbbb/L555+LpmlSrlw52bRpk4hkr3tSJK0P0KNHD3FycpJp06ZJVFSUPHnyRMLDw2Xo0KGiaZpUq1ZNDhw4ICLZqy4QEVm8eLFomiZdu3aVEydOyJEjRyQ4OFjVbaayy7XT0xkfHy9t27YVTdOkf//+smrVKvHz85Pg4GALp/DfCwoKkipVqkiHDh3M6joRkfDwcNm/f79MmTJFDh8+nK36rCIi33//vZQtW1b69OmTZbn48OFDuX37dpb3qDXT78uEhATp2LGjODo6yogRI+SXX36RjRs3qvaYqexWnoik9Q+cnJzEy8tL3Zt6nZ2SkiKhoaFy+vRpSU5Oz15lHgABAABJREFUFpHsU66kpKTIuHHjxMnJSebPn5/p2iQnJ8vly5fl2rVrFkrhv/Ptt9+Ko6OjfP7555merUOHDsns2bPFy8tL1qxZI3fu3LFQKv+ZpKQk8fT0FHt7e5k2bZo8e/ZMIiIiJCIiQvr16yeapkmDBg3k1q1blk7qP6L36zw9PeXMmTOSkJAgN27ckNWrV4uTk5O4u7vLmjVr1LNm7c9cxmcrY3pDQ0OlVatWqu2ijzVkx/Ly7/R9slv+zp49KxUrVpQ+ffqouiA5OVmOHz+u8jd48GCJiYkREeu/L7OyZ88e6dKli9jb20vz5s3l1KlTlk7SX2Y0GiU8PFyNH0yaNEm9N27cOClfvrxMnjxZbt++LQcPHpTOnTub9Rey2/2YUWJiouzevVvc3d1F0zQZNWqUaqtkt3EG3c6dO0XTNLP2c2pqqhw6dEi8vb1F0zTx8vJS83jZ0f3792XkyJHi7OwsmqbJhAkT1HvZ/Z4ky2EghYX5+vqKpmmZBmmGDh0q7u7u8uWXX6rKas+ePRZK5T/z5MkTadeunXh4eJgNbERFRUmbNm2kYsWKUq5cOalZs6ZVB1Po6clY0J4+fVrKli0r/fv3V5O2+qR1UlKSHDx4UFq0aCGurq5qQDi7VLJxcXHStWtXmTJliurgnzhxQtq3b68mBC9fvqw+b23XLKMX/e6pqalmz9iRI0fUe9aeJ1MZ8xcSEiJOTk5y48YNs9f379+vOpLZNZhi165dqsGXcTDYz89PKlSoIA4ODrJ582YRyV7X8datW9KoUSOpU6eO2fMlIuLv7y8ODg7So0cPuX37tgrcsjYvapB+/fXX4uTkZDbhLiKyaNEiNZgTHh4uDx48kDNnzryKpL4U+v01cuRIcXJyUpOeOj8/PzVRr+cvMjLSEkn9Qy+6n27cuCGNGjWSpk2bmtXj4eHh8sknn0jZsmVF0zTp3Lmz1QZT6HnLWE4+ePBA2rRpI3Xq1FEdRIPBIM+ePZPWrVtL5cqVVTCFad2QHRiNRnn27Jl4enqKm5ubWQf46dOn0qVLF6lWrVqWwRTWJqt76fLly1K/fn1p2rSpWVkZFhamgg00TZMOHTpYbTCFHrSa0aVLl8TDw0NatGhh9sxdv35d6tWrJ46OjqJpmgwcODBbBlNcunRJatasKQ0bNswy/X5+fuLo6Cjly5eXoKAgEbGu8uTPbN26VTRNk9GjR2cZODd9+nTRNE1q1aolt2/fFpHsk7+goCCpXLmytG/fPlMb5c6dO3LgwAFZvny5bN++Xb2eXfJmMBjEx8dHnJycZO7cuZnKi+joaAkKCsq2QRUbN24UTdPk+++/F5G062I0GuXbb7+VFi1aqAHGypUry8yZM+X58+cWTvFf5+3tLRUrVpSzZ8+KyO/P2549e2TMmDFSrVo1qVixovj4+GS6b61damqqzJgxQzRNk9mzZ2easI6IiJA9e/bIjh07JC4uTkSyzzMnkpa/zz77TDRNk9OnT5u998MPP4i3t7e6N9u0aSP37t0TkeyRx1u3bkmNGjWkY8eOZtfNaDTK4sWLpXv37qJpmjg6OsrixYstmNJ/5osvvhAnJye5cOGCiIjExsbKgwcPZNiwYeLq6qraly4uLjJp0iSJjo62cIqzZtoW0++rn376Sezt7WX48OHqudJ16NBB3NzcZOrUqWIwGNS1tdZ7MmO6Hj9+LM2aNZNatWplGkfRF6r07t1bwsPDJSYm5oVtVWthWlfv2bNHZs2aJaNGjZKVK1eaLarJTsEUep4ypu2f9H2sMX9ZCQsLk1OnTommaXLo0CH1up7+CxcuSJs2bUTTNJk6daqlkvm3ZVUu7NmzRwXtjhw5MtsE0+l5CQoKEicnJ9E0Tb7++msREWnVqpX069fP7Jm7cuWKDBw4MNsFU/xRAFlMTIzs3r1bLXiePn26ypO1jTP8FV9++aU4Ojqq9rNe3huNRrl//76MGTNGjWNmFbxrjfQ8mN5rN2/elOnTp6sxlA0bNqj3ssM9SdYn++6vk80ZjUYYDAYEBQXBxsYGhQsXVu8tWrQIP/30E4YOHYqJEyeiY8eOAABvb2/s3LnT7IxraxYREYHz58/Dw8MDmqap1xcvXozr169j8uTJGDhwIKKiorB69WqsXLkSANR53dZA3wZVP8vM1LVr15CcnIyWLVsiT548MBgManugXLlyoVq1aujYsSPi4+Oxfv16JCcnZ5ttn3766ScEBwdjx44dOHbsGACgatWqGDhwIFxcXLB7926sWLECV69eBWBd18yUiMBgMMDOzg7JycnYv38/Fi9ejJUrV2L79u2wtbXF+PHj0bNnTwBpZ8br+bXWPGWk5y8xMRGLFy/GmDFjsHr1arz11lvqfkxJSQEAeHh4wNvbG46Ojti6dSsCAgJw48YNSyY/S9HR0er8Zp1+LU6cOAEbGxu1tbl+jQHAy8sLQ4YMgdFoxMSJE3H58mWr2yYvNTX1he/pR0F8+umnavtXIO0YmtmzZ6NatWqYOHEi4uPjMWjQIHVOsjUICwtDTEwMbG1t1f0GpF23xMREnDp1Cu+8847ZOeOLFi2Cr68v3N3dMXr0aOTLlw99+/bFtm3bLJCDPxYZGQmj0Zjle8+fP0dQUBBKly4NJycnda8uWrQIc+fOhbu7O4YNG4acOXOidevWmD9/PgwGwwu/71U7ffo0fH191TE5pu7du4ewsDC0bNnSrB739/dHREQEJk2ahCZNmuDMmTNYtGgRjh49CgBW89wdO3YM7dq1w82bNzPVv5GRkbh69SqaNm2qtiy0s7ODr68vQkNDsXz5crRv3x7Pnz/HqFGjsH///mxRJwBpv//z589x/vx5uLm5mW3J6Ofnh3PnzmH16tVYuHAhAGDBggVYsGCBpZKbpSdPngD4/QxWU3fv3sW9e/fQpUsXs7JyxYoViIqKwoIFC+Dq6opz585h4MCBePjwIezs7Kzm+v36668IDAzE48ePM713+/Zt3L9/Hx06dDB75latWoVnz55h4sSJqFKlCvbv34+FCxfi0qVLrzLpf8nVq1fx8OHDLN+LiIhAZGQkWrVqZZY/03q8Z8+eSExMxNSpUxEVFWU15clfobc7unTpgrx586rtlfV7eMyYMWjSpAkiIiIwb948JCcnZ5v8hYWFITo6Gp6ennBwcEBycjKMRiM2bNigjr+YM2cOxowZg8mTJwOwnrrgz0RFReH48eP4+OOP4eXlpeqL1NRUzJs3D3379kX37t3RtWtXTJo0CdHR0RZO8d+jHxFna2uLqKgo/Pjjjxg2bBgmTpyIq1evon79+mjatCkKFiyIzZs34/79+xZO8V8TGxuLX3/9Fe+++y7KlSuH1NRUXLp0CdOnT4e3t7fq6wHA5s2b8dNPPwGA1dQFfyYhIQGnT5/Gu+++i8GDB6tjLhISEjBlyhT07t0b3t7eGD16NDp16oSHDx9mm/4rkPZ83b17F2+++SYqVqwIADhz5gwmTZqE4cOHY8+ePShevDjeffddXLx4EZ999hmA7FGuREdH48mTJ3B2dkbevHkRERGBw4cPo0uXLli4cCEuXLiAUqVKITU1FQsXLrTKvs8f0Y+sunTpEk6ePAl/f3/06NEDP/74IwoWLIgRI0ZgyJAhKFasGPbv36/KIGvxww8/AEjLh97+0O+rK1euQETQpUsX5MuXD0Da2G2HDh1w7tw5tG/fHmPGjMGaNWswevRoPHv2zKruyYsXL+LQoUMAMo9nPX78GNevX0ezZs3MjohbtGgRVqxYgRo1auCzzz6DiGDChAkIDQ195en/q4xGo6qrfX19MWTIEKxYsQI7d+7EjBkzMHr0aMTHxwMASpcujTlz5sDR0RHBwcEYMmQInj59CltbW6vpjwNpffJZs2YhKioq09jzP+n72NraWn194Ovri0aNGmHJkiV47733VN6MRqN6rpydneHj44O8efNiw4YNCAkJsWSS/7JHjx7hzp07uHfvnnqtQYMG6NevHxwdHbFr1y6sWrUK169ft2Aq/5rU1FSkpqbCzc0Nq1atgo2NDdatW4e+ffviwYMH6NSpE/LkyYPk5GQAgL29PYYMGYJ69eohODgYixcvxqlTp6zqecsoNTVV3XMRERG4evUqTp8+jZs3bwIAChQogHr16mH8+PEoXLgwVq1ahVmzZqmy6I/Gea2J0WhEYmIigoODkStXLrzzzjtmR5TY2NjgnXfewYABA1ClShUcPXoUS5cuxYULFyyc8j+XI0cOxMfHY9q0aThx4gQA4IMPPkDbtm3RtWtX2NraYuXKldi9ezcAWF0dQNkDAyksxNbWFjly5ICmaciRIwciIiIAALt27YK/vz8aNWqEKlWqAACaNGmCggULAgBGjx6NIUOG4MGDBxZL+1+lV0KmgR9r165FYGAgOnXqhDp16qB+/fr44IMPcOfOHaxfvx5fffWV1Qyanj59GvPnz8fatWvNJiz1CvL58+cA0hq1AMwauyKCXLlyoU2bNnBwcMDly5dx7dq1V5j6f6d+/foYPXo0YmJisGDBAqxevRoAULt2bQwYMEAFU/j7+1tdMMX69euxZ88eAGlp0ivTfv36YciQIfD19cXMmTMxduxY9O7dG0ePHoWXlxd69eqF6OhoDB8+PNsEU4iIyl/Pnj3h6+uLHTt24IcffsD9+/exbt06AEDOnDlfGEyxevVqdQ2tQVBQEDp37oxDhw6ZNWpEBLGxsThy5AjeeOMNlClTBgaDQV1j/bM9evRA165dkZKSgi1btiA1NdVqGkdhYWFYvnw5EhISskzTo0ePYDAYzM5NX7RoERYvXgx3d3eMGDECpUqVwnfffYcjR46oRr2lGQwGrFmzBhMnTsTz58+RM2dO9Z7+DOXMmRNPnjzBnTt3AKTla9GiRXB3d8fIkSNhb2+P27dv4/r16zh58iSePXtmodyY0wNBxo8fj02bNmUqD2xsbFCwYEEUKlRIBTXZ2Nhkmb9Hjx7h6dOnePDgAXLkyGEVZ0U+fvwY48aNg7+/P86dO5fp/adPnwKA2RmCK1euxMaNG9GrVy80bNgQn376KQoXLoxff/0Vvr6+VjMYnJKSgoCAAPz2229m105/9p48eQKDwYA7d+6oszzXrFmDwMBAtGnTBi4uLpg4cSJcXFzw7NkzfPbZZ1ixYkW2GbwxGAxITEzE48ePVXtl/fr1WLVqFVq3bo0iRYqgYcOGaN++PQBg6dKlGDNmjFUEjDx//hzz58/H9u3bASBTEMy1a9dgNBrVYA2QNpC4adMmdO3aFTVr1sSwYcPwzjvv4OLFi+jduzcuXryI2NjYV5qPrDx48AADBw7E9OnTcfHixUzv6xOYpufe+/v74/vvv0ffvn3RtGlTtGjRAnny5MHx48cxd+5cNVBgDS5cuICWLVti6NChqm9j6vr160hNTUWePHkA/B7oaVqPjx49Gq6urrh37546Szi7ePDgAWxsbDL1Y/QBNhsbG4wbNw5FihRBWFiYWeChtdLLA30g+N69ezAYDPjpp58wYsQITJw4ETdu3ICHhwe6dOkCW1tbBAYGIigoyJLJ/ltiY2Px6NEjlC5dGvnz58eDBw+wd+9edO7cGX5+frh58ybKli2LAgUKYN26ddiyZYulk/ynjEajunZlypRB/vz5MWnSJHTs2BEjRozA/v374eDggAULFmDevHmYOXMmGjRooIJDAesPOChQoAAqVaqEGzduYM6cORg/frwKKM+bNy98fHzw7bffYubMmciXLx927txpNeMMf0VsbCwiIyORL18+5M6dGxEREdi6dSu6dOmCtWvX4tGjR6hVqxY0TcNvv/2W7QKY7OzsUKRIEURGRqJnz54YMmQIhg0bhnXr1qFQoUKYNGkS1q9fj02bNqFUqVK4dOnSC4P0rE3+/PmRM2dOrF27FlOmTMHw4cMxatQonD17Fs7OzlixYgU2b96szpk/e/as1T5verpMFy+4ubnhjTfewKRJk9CzZ08sW7YMycnJaNWqFTZt2gQvLy8MHDgQZcuWxf37960q+H/+/PkYMWIERo8eDcA8mAJApgn25ORkdO7cGSEhIejbty/69esHOzs7bN68GefPn7eqIJGbN2+ibdu2mDFjBg4fPgwgrTzQ86KPyyYlJam/Me2zDhs2DKVLl8bq1auxZ88eq63HRUT1o+fNm4fFixejZMmSGDduHMaNG4cSJUpg7969GDp06AuDKYYPH55lwIKlREZGYuzYsVi9erUKhAF+77feuHEj2/Z9/kjRokVRpEgRBAcH4+7du6qfbWtrq+oyEUH58uXRr18/JCUlmQUmWKt169Zh8ODBaN++Pdq2bYtt27apsqJ+/foYOHAgHB0dsWXLFqxatcoqF7cBwMaNG3Hu3DlVTooIKleujJUrV8LGxgZHjhzB8+fPcevWLQAwGwfUNC1TMMXp06etZmzWlGlg1urVq9G/f3907NgRXbt2RefOnTFkyBCEhoYiR44caNKkCT777LNsG0xha2uLPHnywN7eHikpKXj27FmWC05KlSoFd3d3GAwGnDp1CgsWLMD58+ctlOq/xmg0YtasWVi7di3WrVuHM2fOAAA+/PBDdOzYEZ07d8adO3ewaNEiBlPQP2YdrYb/uD96KBs0aIBu3brhww8/REREBNatW4d8+fKhW7duKFmyJACgcOHCiI6OhqOjIzRNg7u7O955551Xlfy/Rc+riCBPnjz4+OOPERkZCYPBgKNHj8Lf3x/ly5dXuzh8+OGHakXagwcP8O233+LkyZOWzIJSsmRJjBgxAiNHjoSzs7NaQaxXsHq69Ulo0wLYxsYGSUlJyJkzJ95//30AyBY7ieiVZ8GCBdG+fXsMGzYMSUlJWLhwIb755hsAfx5MYUm7d+/G119/jZUrV5p1QPr27YuTJ0+ibt26mDx5Mnx8fPDxxx/j+PHjmDp1Kvbs2QMvLy9079492wRTiIjqGPv4+ODy5cto2bIlvvjiCzRs2BB58+bF5s2bERgYCCDrYIqyZcti8+bN2LRpk1mnzFIMBgM2btyIGzduICgoyOx3t7W1RYECBVCsWDF1TUwnXmxtbdUgSOvWrWFra4s7d+7Azs7OajrIsbGxWLBgAebMmaNeCw4OVv+s70ykv7Z8+XKzyXh9Vflrr70GAGqViKXvTzs7O9y7dw+7d+/GjBkzAABLlixR/5w3b15UrVoVSUlJOHv2LObOnavyNWLECDg6OgJIK1P1nVRMO2GWZGNjg1y5cuHWrVv48ssvsXXrVgBpgzb6AFViYiIKFSqEW7duYfPmzZmCKPT86dc3MjISycnJFr9uQNrgRZcuXVRwIwCzlbaFChVCwYIF1QDovn37sHz5clSpUgWNGzdG3rx54eHhgffeew8GgwEhISEYN26c6rRYUs6cOTFlyhQMHjwY/fv3h42NDZ49e6bKg3LlyqFy5cp48803kTt3bpw8eRLLly+Hq6srOnToACDt+r/11lsA0gYd58yZY3WBrC9qY7777rv49NNP8d577yFv3rw4c+YMVqxYAUdHR3Tq1AlvvPEGAKgJ7SJFimDHjh2Ijo62eF2enJyMoKAgLF26FHv37gWQNpCj18uurq54++23VcDV/v37sXz5clSvXh2ffvopbG1t8eGHH6oAIH1gWR/ksaQ333wTrVq1QqtWrVCpUiUAUIE8QNozmStXLjVQsWfPHvj7+8Pd3R0NGjRAvnz50KhRI7z11ltISEjAsWPH0KtXL6sJiHz99ddRokQJVRbok156eac/T2fPngWQ9pzq7+n1uNFoxNtvv42YmJgsd8qxRvrAWZEiRWA0GnHgwAEAMFsNqA+w5c2bF3nz5sXt27cRERFhFXXBH9HLg3r16iFHjhyYNWsWWrVqhTFjxuCXX37Bxx9/jIULF2Lp0qXw8fHBqFGjAKTVddlFvnz5ULRoUezevRtjx47FgAEDMGbMGISEhMDV1RWrV69GYGCgar8dP37c6vp1GesC00mImjVronfv3njvvffw5MkTfPTRR/Dx8cHs2bPRqFEj1b99/fXXYWtri48//hiA5ft1OtOJ3Iyv1a5dG8WKFcPKlSuxY8cOPHv2DJ07d0ZAQAC6dOmCEiVKoG7duihZsiRy5MiBHDlyWCQP/0SRIkVQqlQpXL9+HW3atEHXrl3x+eef49q1a6hevTq+/fZbLF++HOvWrcNbb72F8PBwxMXFWTrZmbyojLO1tcWYMWPw8ccf4+TJk9izZw9sbW3Ru3dvLF++HO3atUOhQoXwxhtvoESJEkhMTLSae1L3orx9+OGHmDBhAoC0hURnz57Fxx9/jNGjR2PFihWoUKECChQooHbisMbdifR6TU+XvngBAOrUqYMhQ4agRo0a+PDDD9G+fXvMmTMHPj4+KFq0qPqO6OhoFCtWzGwHKkurXbs2cufOjZ07d2Ls2LEA0oIp9LGQt956C0ajUQWp9ujRA+fOnVNBFAUKFEBycjJee+01PHr0yKomdQ0GAxo2bIjw8HD4+vqq8TC976Mvzrtx4wYSEhIy9VmdnZ0B/D7Oaa2T8Po9uXPnTgQEBKBWrVpYsGABevbsqf6XM2dOHDlyBMOHD1ftbD2YwtnZGSdPnkSfPn3MgkosKU+ePPD09ETHjh1Rr149AGljDPq1K1++fLbt+2RFLzs7duyI4cOHo1ixYgDSxsEytmf0z7799tsAoBYoWuvk56xZszBp0iRcuHABBQsWxNOnT/H5559j06ZNiIqKApA5mGLlypVWGUxx8uRJ9OrVC+fOnUPu3LkRGxuLqKgoVKtWDQEBAWo8+uzZs2qRm2kwgWkwxdmzZzFlyhTVB7Qm+nM2Z84cTJ8+HU+ePEGzZs3QqlUr5M+fH3v27IG3tzf279+P1NRUNGzYEJ9//rkKppg9e7bZXJG1yKr9rF+fUqVKwWAwYObMmXjy5InZfJY+ru7u7o5ChQqhaNGiOHr0qNUvcLC1tUXdunVRq1Yt/PLLL1ixYoUalyxdujS6dOmCzp074/bt21i8eDGDKegfyT49yWzKdIucmzdvqpVmRYsWhb29Pdzc3FCuXDnkzZsXJ0+exPnz59G3b181uApAPfgTJkyApmlqEk2fSLUk0/zp/64P3JQuXRrTp09H4cKFkSNHDhw/fhyPHz/GF198obbk17ef/vjjjzF16lSEhYWhSZMmFszR79566y10794defLkUTsaVK5cGYMGDYKdnR1KlCiBYsWKYefOnbC3t0fv3r1VAWw0GlUjNiIiAiVLllSDUtbEYDAgR44c6lroDSFbW1u89tpr6liZefPmYcGCBRAR9OzZE7Vr1waQtoL1p59+QlxcHEaMGGHxPFaqVAlNmzbFzz//jKVLlwJIG4S6ceMGunTpgjFjxqjV/rVr18Z3332HDRs2YMWKFShWrBhGjhyJxMREbNy4EaNHj8a0adNQu3Ztiz9nGenPXWpqKhITExEaGoqGDRti4sSJyJMnD2rWrIkyZcogICAAK1asUNtM68EUOXPmhIeHB1JSUrB27Vp06tTJbBcES8mRIwfGjx+PcuXKoWXLlrCzs8Pt27dRvHhx5MyZEwaDAYULF8b58+fh5+eHoUOHqkkYGxsbVRa9+eabAID4+PhMZZQl6Y2zwMBA5MuXD8+fP8d3332HpUuXwsPDA66urnB0dERQUBC6dOmCM2fOoFatWhgyZIiajAfSJmNy5MiB8uXLA7D8QLeNjQ3atm2LsLAwbNmyBefPn8eNGzfQpk0bREREoFixYqhcuTK+//57TJs2DUlJSWrVhOmRA8ePH8ejR49Qp04d5MuXzyrqOCCtYd24cWMsX74cEyZMwA8//ICjR4/i008/RcWKFZE/f3707dsXv/76K6ZMmYKEhATUqFEDo0ePNhs4vHr1KkQE1atXR65cuSw6eRYZGYm8efMiX7586NWrF5KTk5ErVy7MmzcPMTEx8PT0xHvvvYfatWvjq6++UgNre/fuRXx8PPr06YOPPvpI1RcGgwGVKlVCs2bNkJiYqAaGLe2dd97B4MGDAQBffPEFDhw4gHXr1qFkyZLInz8/Ro0ahbfffhu2trY4dOgQnjx5gqlTp6JMmTIA0u7tlJQUFC9eHMOHD8drr72mAk6sgWn59ttvvyEqKgpPnjxBmTJlUKZMGQwePBg5c+ZErly5EBwcjIcPH2L8+PFmW8ImJSWhWLFimDJlCnLlygU3NzdLZUexsbFBhQoVsGvXLvj6+uK7777D0aNHMWTIEFSuXBkODg4YNmyYus927dqFpKQkeHl54YMPPgCQVvfny5cPLi4ucHV1xfvvv292tJAlGI1G5MqVCyNGjFD/vGjRItjY2KjglqZNmyI+Ph7u7u4A0vKWkpKCPn364IMPPoDRaETOnDlhY2ODatWqoWzZssifP7/ZFs2WYjQaUbJkSQQEBGDIkCE4ceIExo8fj2nTpqnBz8qVK+ONN97AgQMHsHnzZrRt21a1PU0nafLkyYN8+fKp62nt9OewadOm+Pbbb7Fjxw6ULVsWdevWNVsNamdnh9deew0igo8//hglS5a0inruRfR+ApC2tfLkyZMxZcoU3Lt3DyVLlkSvXr3g5uaGDz/8UP2NvpORHjRjTV7UrihWrBjGjBmDkSNHYvfu3TAYDKhWrRrq1auHTz/9VO0QU6lSJeTOnRt58uRRQWjWwLQuOHPmDO7cuYOLFy/C3t4eJUqUQJUqVTBw4EC0a9cOUVFRKF68uNl4gq2tLU6fPo2NGzeiRIkSVnXtTPP27NkzxMTEIDU1Fblz50bx4sXRsmVLFCtWDHfv3sXDhw/RrFkzvPXWW8ifP7/6juPHj+Pq1ato2LAhcuXKpdou1uJF6bGzs8PUqVMxYMAAdYxTnTp18Mknn6BevXrqGubKlQu2trbIlSuXVfTnTJlev6dPn+Lx48eIiYlB8eLFkS9fPrz11ltYt24d9u7dC6PRiOrVq6NIkSLqOtnZ2SEoKAjnzp1D1apVzXZrsjTT/nhsbCzu3r0LW1tblChRAvny5UPHjh3x7rvvqoC5Zs2aIUeOHGbB4hs3bgQAq2k36/SyPzk5Gdu3b8fly5cRHh6OokWLon79+qhbty7at2+P9u3bq75eRuvWrcORI0dQu3Ztq7luRqMRrq6uCAwMRPfu3dXOZzNmzFDPTqNGjbBixQqsW7cOO3fuxJ07d9CvXz94enqiQIECANKeuZSUFHzwwQcWH/8ypWkaBg0ahDx58mDHjh3w9fUFADV2995776F+/fr45Zdf4OXlheDg4EyB/8Dv9bi1tsFEBKmpqfjll1+QK1cuDB8+XLWDExIS8Msvv6BgwYLImzcvDh06hGHDhmHevHnIly8fSpcujZkzZ8LLywv16tUz23XRkvLnz4+2bdvC1tYWOXPmxLx58xAZGYlRo0ahUKFCsLe3z5Z9nxcxbYu1adMGRqMRCxYswDfffIMyZcqgbdu2mf5GDxTUxyWsqR7XLVmyBAEBAahatSoGDBgANzc3LF++HPPmzcPcuXMhImjdujUKFy6M+vXrq7/ZsWMHUlNT0atXL6vo0+ny5s2LhIQE9O7dG0uXLsXo0aORJ08efP/996hevTq++eYb9OjRA3v27MHkyZMxceJEVS+aLkAdOnQoYmNjcenSJatdFKzvDl+9enWMGTNGXQeDwYBmzZrh5s2bOHLkCKpWrYrXXnsNDRo0AABMmzYNK1euRFxcHL766itLZsFMxvZXSkoK8ufPr+qx/v374/Dhwzh+/DiWLl2KQYMG4Y033lDzBUDaTpOxsbGYOXMm8ubNi8qVK1ssP39G7+PVrFkTuXLlQmpqKg4ePKjer1ixogqmANJ2al28eDFsbGzQpEkTqyxPyEoJ/c+kpqaqf/b39xcPDw/RNE00TZN+/frJ06dPzT6/YsUK0TRN1q9fr147ffq0NGnSROrWrSs3btxQrxuNxv95+v+MwWBQ/7xjxw6ZNGmS9OzZU1atWiUPHz40++zTp0+lQYMG0qRJE7PXjx07Jk5OTjJ69OgXfrel6GkwGo2ybds20TRN3N3dxd/fX1JSUkREZPfu3eqaBgQEZPqOwMBA0TRNhg8fLnFxca80/X9VXFyczJw5U86dO6deM713o6OjZfny5eLg4CAVKlSQVatWqfcOHTokjRs3lkqVKmW65pby6NEjGTlypDg4OEiPHj3k888/FwcHB/X8pKSkqOfn8ePHMnfuXHF0dJRevXqJSNr1/vrrr0XTNKlbt67Ex8dbxfOWUXx8vPTp00e++uorcXR0lMOHD4vI72XDw4cPZeHChVK+fHmpVauWBAYGqr9NTk5W/xwbG/tqE/4HMj7306ZNk2rVqsnRo0clKSlJRERCQkLEzc1NPDw8ZMuWLepv9PdFRH766SfRNE0WLVokItZRXuqOHDkilStXVuVG7969zcr2PXv2SI0aNUTTNGnWrJkEBweLyO95OHXqlNSqVUvq1q0r165ds0geXuTXX3+VihUripOTk9SpU0dCQkLM3p82bZo4ODiIs7Oz+Pn5mZUzp06dkrZt24qLi4v88ssvrzrpL2SaRr08t7e3l7Zt25r9/k+fPpXJkyeLs7OzuLi4yLJly8y+JygoSNq2bSvly5eXo0ePvrL0Z+XEiRNSpUoV+f777yU+Pl69HhISIi4uLuLk5CRz586V8PBws7+7d++euLq6ipeXl4j8fk8eOHBANE2TuXPnmn3e9LezBNPyJDY2Vlq0aCGapkmLFi3k1q1bIiKqLn/27JlUr15dGjRooF4TETl8+LA4OTnJl19+afbdls5bxjT4+fmJh4eHODk5iaZpUqNGDdm0aZN6PyEhQRo2bChubm4SGRmprt3x48elYsWK0q9fvxd+t6XcvXtXZsyYIfb29mJvby+tWrUyuyf1NN65c0fs7e1l4MCBIvL7dd+7d69ommbWZjH9O0sxvb9OnTolmqZJpUqVZNWqVRIREWH22Vu3bomjo6OMHDlSRH5Pu7XmzTQNt2/fVs9cz5495cGDB+oz33zzjZQvX15atGghu3fvVvejaT3n5uYmHTt2lMePH7/6TPwFSUlJsm/fPtmyZYts27ZNvZ6amqrquq5du8qBAwcy/e3KlStF0zSZOnWqJCcnW1UbRffDDz/Il19+Kf369ctUtj9+/FjCw8Oz7NecPn1aPDw8pGHDhhIWFvaqkvuX6GVDSkqK3L17V44fPy7Hjx+X27dvq+fyzJkzcvDgQdmxY4ekpKSYPa8iIr6+vqJpmqxcufKVp/9FTJ97X19fqVixompj6v/Lqk4/c+aM3Lx5U54/fy4HDx6Utm3biqZpsnHjxledhRcyrccDAwOlU6dOUrFiRXF1dZW6devKtGnTJCoqSn1G/y2io6PVa8HBwdKlSxdxcHCQPXv2vLrE/0X6PZacnCzXrl2T3bt3y5YtW+TixYty//59EUkrb86dO6f6BBn5+/uLpmkyc+ZMqypPTK/f6tWrpVWrVuqerFKlivTr10+OHTsmImnlv/75u3fvqvbpyZMnpWPHjqJpmuzevfvVZ+IF9OsWHx8vn3/+uTRs2FDlrW3btjJ16lTVNzV9Rk2fw7Vr10rlypWladOmVjOOIvL7dYuNjZWuXbuKpmni6Oio2piapsnYsWMz3Y+rV6+WgwcPysWLF2XmzJlSsWJFqV69umpzWwv9uly4cEFcXV1F0zQZMWKEet9gMMiSJUukfPnyommaDBo0SGJiYsy+Y/Xq1ervrGWMz/TZv3r1qnzxxReiaZp06NDBrG+9bds2qVSpUqaxBl1wcLDUq1dPatWqJVeuXHll6f+7Hj9+LG5ubtKmTRuz12fNmiX29vZy8OBBuX79uhqD7969uyQmJqrPPX/+XP2zNZWbIiKXLl1SffLp06dLZGSkiPw+hpfd+j6mHj58KCEhIXLixAm5cuWKWX29adMmqVKlimiaJoGBgSrfImltlmbNmkn16tWt9r48c+aMVK1aVTp16iS//fabev369etSq1YtVZb6+/ub5W3fvn3SrFkz0TRNjh8/bomk/6Evv/xSpb1s2bKyaNEiiY2NVffVqVOnxN7eXjRNk6+++kr9Xcax3evXr5v1B62F/vxPmDBB7O3tM5WJehurb9++cufOHYmIiFD5iIuLk+3bt4ujo2Om586STJ/5NWvWSIcOHaRmzZrSrFkz+eGHH+TRo0cikjaf06BBA3F0dJRRo0aZtUWCg4OlXbt20rhxY7l7926W321JGftoIuZpO3nypHh6eoqmadK/f385ffq0ei80NFQmTZokZcuWlerVq8vPP//8StJM/w3ckeJ/REzObZs7dy6WL1+OkiVLomvXrkhNTUWZMmUyRb/qqz9Wr16NEiVK4O7du/juu+9w8+ZNTJo0yWy1j6VXL4mIim6bM2cO/P391XsnTpxASEgIevbsqVZMJyUlISkpCU+ePMGRI0dQs2ZNnDp1CkuWLIGNjQ0++eQTs++39ApyPQo/MTERhw4dQtWqVTF69GgsW7YMAQEBMBqN8PT0RJMmTfD06VNMmjQJM2fORGhoKFxdXVGqVCn8+OOP2LFjB4oUKYKhQ4ciX758Fs1TVkQEkyZNwtatW/Hw4UN4enrCyclJ7ayh70zRrl07xMfHY9myZfDz81P5r1WrFuzs7PD+++9nuRLBEooWLYqxY8dCRPDjjz/iwoULeOONN1RUpZ2dnXp+ihQpgnbt2uHo0aM4fvw4tm3bhpYtW2L06NHIlSsXWrRogbx581oyO2ZMVwaeO3cOR44cQeHChZE3b161HaXBYEDOnDlRrFgxtG/fHjY2NlixYgWWL18OAGpnCv27TFdsWZrpc5+QkIDQ0FBERUVh+vTpGD9+PCpXrowyZcqgc+fOWLlyJZYvX47Y2Fh069ZNrSI5ffo0Vq1ahXz58qmdfSxdXgK/R8jWqFFDbSdpNBrx5ptvmpXtFSpUQIcOHbBu3TpERERg//79KF68OGxtbRESEoKlS5ciIiICkydPVqvmrcXp06cRGxuLPHny4MGDB9i8eTNKly6tVsyNGzcOsbGx2Lx5M/z9/XHx4kWUKVMGsbGx2LZtG54/f47PPvsMdevWtXBOfmdaFj5//hxA2rW8cOECrly5oq5BoUKF0Lp1a8TFxWHnzp1YuXIl7t27h48++ghRUVHYtGkTIiMj4ePjo1aaW8rhw4fx7NkzzJ07Fzlz5kSjRo2QK1culCtXDuPHj0dAQABWrlwJo9GoVtMBQFRUFOLj4/H48WPcv38fxYsXx+nTp7Fs2TLky5cPVatWNfvvWDqqWy9Pfv31V7i6usLf3x/jxo3DsWPH4O3tjUWLFuH999+HiCBv3rx44403EBcXh9u3b+PDDz/EmTNnsHTpUtjZ2aFmzZpm323pvJmmwdfXF4sXL8b777+PPn364OnTp7h//z4qVKigPpsnTx689957uHfvHi5fvowaNWrg5MmT8PX1RUJCAlq2bJnld1vSu+++i9jYWFV2xsbG4ubNmyhRogSA37eozHjes52dHc6ePQs/Pz+89tprmbaTtmTejEajqsMjIiLg6uqKYcOGYe3atVi6dClEBC1atFDHrjx8+BCpqal4/vw5YmNjUaBAAZw+fRp+fn4oWLCg2Y4+gHVcN73MLFmyJBYuXJjlzhQeHh64desWNm3ahAULFuD69evw9PQEAHXtnj9/jk6dOqFIkSIWzlFm8fHxGDx4MI4fP65e279/P4YMGYIPP/xQ7cb0888/48mTJ7h69SqaNm0KGxsbbNmyBRs2bEDx4sXRrVs3qznGytT8+fOxbNky9e8HDx5EWFgYvvzyS7zxxhtm1+Tq1auIiIhAzZo1ceDAASxfvhz379/H5MmT1dGG1kBv8yYkJGDixIkICgpSx84UL14cZcuWxeTJk83KTQC4fv26qufXrFmDtWvX4v3338/Ub7Uk/bnXt2h3dnZWOyrevn0bAQEB8PPzw+PHj9GvXz+8//77CA8PR48ePWBjY4PChQvj8ePHMBqNGDduHNq1awfA8jtfmo41zJ49GytWrEDBggVRpUoVGAwGHD9+HKtXr8aVK1cwcOBAVKpUCXZ2djh16hSGDBmCunXrQkSwb98+xMTEYNy4cWr1oLVITU1Fjhw5EB8fj7Fjx+LEiRNqK/3cuXPj448/xqBBg1CnTh2UK1dOXY+goCDY29vjtddew+rVq+Hv74/ixYujU6dOVtHnAcyv36xZsxAQEICiRYuiVatWiI6Oxv3793Hw4EEcPnwYixYtQt26dWFnZ4cLFy5g9OjRKFq0KN555x0cOHBAXT9911JL35t6XR4fH4+uXbvi8uXL0DQNzZs3x507dxAWFqb6CStWrFB91ICAAKxbtw41atTA/fv3cezYMRQqVAjz58+3mnEUIK0dlZiYiN69e+PixYto3bo1unfvDiBtm/ctW7Zg+/btePr0qeo/HD58GNOmTVM706ampqJUqVJYtGgRSpUqZdkMmTAYDMiVKxcSExNRokQJeHp6YunSpWp77zlz5sDOzg716tXDgwcPsG3bNly+fBnffvutOm5h8+bN2LhxI95++22rGuMz3VY+T5480DQNJUuWxIULF7By5UoAQN26ddGiRQvcuXMHS5cuxfXr13H06FHEx8ejdOnSOHPmDAICAnD37l1MnjzZqlbGZ1SwYEEUKlQI8fHxeP78OV5//XVs2rQJK1asQJs2beDo6IiiRYuiWbNmCAgIQFBQEJo3b44BAwagcePG6pgTS5cnWSlZsiS+/PJLLF68GOvWrYPRaMSAAQNQqFAhiEi26vuYWrlyJb7//nvcvHlTvebu7o5GjRqhffv2ape62bNnY9KkSTh48CDef/992Nra4pdffsG9e/fU7tbW6MqVK3j69CmmTZtmtlONn58fkpKS0LdvXwQGBmL+/PmwtbVVO2jVq1cPSUlJEBFUq1bNgjkwp+9M8MUXX2DLli1ITU2FiKBKlSrInz8/kpOTYWdnh8qVK2PNmjXo3r071q9fDyBtJ9CMO1N89NFHlswOIiIikJqaiuLFi5u9ro8zhISE4L333lNzWADMjj8aPny46qu7u7tj9uzZ6sjNsmXLonTp0q86Sy9kelSJPlf35ptv4vr16/jyyy/Ru3dvtGvXTu2iu2DBAuzcuRPHjh1D+fLlkTNnThw/fhyxsbHw8fFR44Gm321Jkn7Md3x8PA4fPqz6ZaZjt1WqVFGfP3jwoDoevFKlSihdujS6du2KuLg47N+/32rLFLJSrzJq4/+jLVu2SNmyZcXLy8ssKlH38OFDuX79uoikRZj26NHDbAWJg4ODrFmzRn3e2qJl169fL5qmSZcuXWT37t2yYcMGadq0qdp14+zZs+qz06dPF03TpHLlytKnTx8pV66caJomq1evtmAOXiwuLk6tprt06ZJERUXJihUrpFKlSlK1alVZvny5ioLbsWOHuLm5ZVoB1Lx5c7PV5tbo1q1barXB0KFD5eLFi+o904i+GzduSPPmzdVq14wrrq1NRESEjBkzRsqWLSuapomfn1+mz+jP0/fff291K8wy0q9FTEyMbN68WaKjo+Xnn3+W2rVri6Zp0q1bN7XCwjQ68+HDh+Lr6yvly5eXunXrWnUedfp1iYyMlFGjRommadKkSRO1aun+/fsybdo0qVChgmiaJl27dpVZs2bJ119/LdWqVRNN02Tt2rWWzEKWjEajhIeHyyeffCLNmjVTq2AmT55stjPAw4cPZfny5SpyvXLlyiqvZcuWtdo64bvvvpOpU6fKt99+q1ZmjR07NtMqHj8/P+nQoYNZWdmsWTP5/vvv1WesJdJZl5SUJFu3bpUJEybI5MmTVbq/++47s8/duHFD/Pz81PXSNE2cnZ2lQYMGZqs8LZk/o9Eos2bNUisCd+zYYbaaauPGjVKvXj1xdnaW2bNnqwh0g8GgVgW2b99eJk2apJ4303vSmujXSt9xKSIiQnr16qXuudu3b4tIWrnq7e2tdqzo1auXqtOtpY2S1T1z5MgRcXFxkW7dupmtzskqQt/Pz0+VIS1btlRtMNPVE9ZSnqSmpkp8fLz0799f+vXrJyNGjBB7e3tp0qRJphWpjx8/lurVq6vdv6ZPn65Wn1ljPSAiMmfOHBk0aJCIiCQmJsqyZcukSpUq4ubmJitXrlSrlJ49eyb169cXJycnGTBggMybN0/VC6a7TFkjfRVSWFiYakv36NFDrXS5efOmzJgxQ9WD7u7uahWag4OD2XNnLfelSNr16ty5s2p7TJkyRaW7V69ecvXqVRFJWw06adIktUKrQoUKqj3aoEED1fezNnPmzBFN0+TTTz+VDRs2yA8//CDu7u6iaZoMHDjQbAVdWFiYNGjQQDRNU/dlxjLTGq6dXnbGxcWpFfHt2rWTKVOmiLe3t2pHt2rVymwXjYCAAClXrpx4e3ura169enWrvHbBwcHi4uIirVq1yrRSc8SIEWpnGH0FWmpqqkyePFl69uwp7u7uMnz4cLPVWNbUBtP7aH369DHL2927d9UObjNnzlQ77G3YsEFcXFzU/diwYUPZvHmz+jtryZv+bMTHx6v70tPTUwIDA2XatGlqFZ2mabJ37171d/rYyyeffCKNGzdW/XJr26VOt3HjRtE0Tby8vNT1S0lJkYcPH6qxIU3T1M6KBw4ckLp166od4Jo3by5btmxR32ct1y85OVm1G+fOnavuv2fPnsmZM2dUP8jT01OSkpIkOTlZZs6cqfJbvnx56dWrl4SGhlo4J1nTd06aNGlSph0XgoKCpF+/fqJpmowbN06SkpIkLi5OVq1aJRMmTJDBgwfLkiVL1I4q1kJvl8TFxUnPnj2lfPnyUr16dbPdNkaNGqU+f+XKFZk1a5ZqL1eoUEGcnZ1F0zRp3LixVY3xmT4Xc+fOFQ8PD7G3t1f1t76bounOFAEBAWY7Jut5c3FxMWs/W7oez+q/n5KSIvHx8TJ37lzx9vaW58+fy/nz56VRo0byySefyPnz59Vn165dK5qmqbxu2LDhVSb/T72oTIuLi5OtW7eKh4eHlC1bVqZNm6Z2tH748GG26/vo5V+dOnVk0qRJ8sUXX0j79u1VO3nx4sXqs5s3b5aaNWuq9+rXry/e3t6ya9cu9RlL35dZmTRpkmiaJmfOnFGvLVmyxKzfNnfuXNE0TZycnGTx4sVy4cKFTN9jLfWcSFpdt2bNGnFxcZEmTZqoMkIfW0lJSVFl61/dmeJVMxqNEh8fLxMnTpQlS5aY7bqgj58nJiZKq1atpFatWmrnmoULF6p6/NKlSyKStqtG+fLlpX379ln+tyx97Uz/+/v27ZPy5ctL37595fTp0xIdHS1z586VqlWriqurqyxevFjt6hYWFibDhw9X/Tl9tx/T8Vlre+ZSUlLUPZmxXDf9HY4fPy6dOnUSBwcH8fb2llOnTqn3bt++bbW7X5L1YiDF/1Bqaqp4e3uLq6ur2kZGL3z27Nkj48aNE3d3dylXrpx8+eWXEhkZKUlJSTJjxgwZMWKEzJw5Uw4ePGj2fZaWsRL09vaW+vXrmw1sBAcHS8+ePVUwhd6QuHnzpowfP140TRNXV1f55JNPrG7izHTyQe/cjx8/Xm39FhkZqYIpqlSpYhZMcenSJdm6datMmjRJ5syZIz/99FO2KZTDw8OlU6dOfxpMMXz4cPnkk0/UYOKzZ88skdy/7OHDhzJu3DhxcnKSjh07mh01YDQa1bX78ccfRdM0mTVrlqWS+kKmDZakpCSpVauWeHp6ikhag2/Pnj2q4/T555+/MJhi0aJFommaNG3a1Oqu2x81yiIjI9UgcOPGjVUwxaNHj+T7779Xg/j6oFTjxo3NtrS3dLmSMW8pKSly48YNiY6OlpMnT6otmKdMmSIJCQnqc/Hx8XLz5k0ZN26ceHp6SvPmzWX27Nkq/yKWzdsfdfpFRA4ePCj169dXg2wZgymio6Pl2LFjsnfvXgkJCTEbbLP0NXtRGpKTk1XAiz4prWlZb4MdGhoq27Ztk2XLlsnRo0fNJl4smT/TI6v0Os7NzU127NhhdsxPxmAKfSti0+2/nZ2dxd3d3azjYg3XzpQeMLJu3Tr1WsZgCn3L4du3b0vnzp3VBEyDBg2sYvLl7t276rnKWJ4sW7ZMNE0zGxjN+JlLly7J9u3bRSRtkrRatWpSsWJFadOmjVVNTmRVDzx//lyePHkiDx8+lK+++uqFwRRHjx5VA936PW16TJ6l82bq/v37apJIbx8nJSWJn5+fWTCF3n7cunWrquccHR2tMm9/lobQ0FAVTNG9e3c1gBUdHS3BwcHSu3dvadu2rTRo0EC++OILs+MwrCF/pvbu3StOTk4yY8YMNdB25coVFWTWo0cPFUyRmJgox44dkyFDhki3bt1kwIAB4ufnZ5Xb2oqkHVfo7OycKfh/48aNaoLFNJji7t278s0330irVq2kVatWMnz4cLMJX2u6dsnJyTJy5EjRNE3mz5+vylSDwSD3799X169169bqvVWrVqnJNTc3N+nXr5/VbVGvW7dunWialunoCr3t37dvX7l3756EhoaatZFF0p5D02tlLdfNaDRKUlKSeHt7i7Ozs9niDJHfj0Tt06ePhIeHS2RkpLp2Fy9elBMnTkhwcLDZNbOWvOkMBoN8/vnnajLedNv52NhYNeFiGmhw+PBhad26tVStWlUaNmwoo0ePVkGhlqLX3xnHiYxGowwcOFBcXV3VZFHGz+jBWw0bNlTXKiYmRs6cOSOXLl2Se/fuqc9aU7/nypUr4ubmJj179lR9btO8RUREqGCKqVOnqt/oypUrEhQUJJcuXTI7WsASzp49a3Y0jqkRI0ZI+fLlVXlvMBjM2mlBQUEqAMiajlz5M0lJSdK9e3dxcnKSiRMnyp07dyQ4OFh27NihjrsYNmyY+nxMTIycO3dOxowZI15eXjJkyBD55ptvrOooFlMBAQFq4m/fvn0SHh4umzZtUkE/rVq1MuszBAcHy9q1a8XLy0v69u0rfn5+cuLECfW+pctM02cqKSlJBQPq9+KjR4/UPfrtt9+Kg4ODWd9GJK0vWL16dbl27ZrZGKc1MM3fr7/+Kjt37pR58+bJ4cOHJTIyUgwGg2zdulXq1q0rZcuWlalTp6r8Hjp0KNv0fbZs2aLaIqbty3v37qlAl4zB/Zs2bZJ69eqJpmkyY8YMs2fO0hPzL6KPD+nXYffu3eLq6ir9+/dXQXPR0dEqgNfaAyF14eHh6rgLfT7HxcVFBSy9KJhi7NixFktzVjp37ixly5YVf39/MRgMsnz5ctm4caPExsZKSkqKWsC3fft2WbVqVaYgCqPRKHfv3hVXV1epV6+eJCQkWFVwQcZnftOmTWZBLyJpeVi9erXUqFFDXFxcZPHixWZHi4aHh8uvv/4qYWFhZs+cNZUnpvQjF02fO51pmrdu3arGUry9veXkyZOvOqn0H8JAipfItEKPj4+XmJgY8fDwkBYtWohIWgVz4cIFs+j7GjVqqAbQnDlzRCTrwWRrK7jmzp0r/v7+8umnn8ry5ctFxHzi9uzZs2qiwsvLy2wC++TJk3Lp0iWzCHxryl9sbKxs2rRJpk2bJl26dFGv69fXNJiiatWq4ufnl+Xqz+zmj4IpdC1bthRfX18JCQmxqij8PxIREaEGT4cNG6YGuE3p0cM//vijBVKY2bp167JcGXbz5s1MjdKkpCT5+eefpWbNmqJpmnz99ddZBlPcv39f/Pz8rO66mZabkZGREhoaKlevXpXk5GSV/sePH2cZTKH/zf79+2Xjxo1y+vRpqypXTPOWkpIiMTExmcr3X3755YXBFKb0a6qzhsl4kbSyfv/+/bJ9+3Z5+vSpSldiYqIcOnTILJgiqwHDjL+HNXRGTPMXGhoqJ0+elKtXr5qtxBUxD6b47rvv/lLaLZ0/07ylpqbK+PHjxdHRUapWrSq7du16YTDFrFmzVCfr0aNHsmrVKvnll1/M6nZLP29ZOXv2rFStWlXq1q1rdv0yBlPcvHlTRESePHkiISEhcv78eas4C/LXX3+VChUqyJdffpmpDEhJSVEDo3q5nlU69Ym0b775RkTSVsqHh4ebBXpa+tqZ3pd37tyR8+fPy5UrV8yC/kJDQ/8wmCI0NFRWrVolu3btMluJZum8ZUUvO3x8fFSQWcZgioCAAImOjhaDwSA3btyQ2bNny/bt283O97SGvJleu5CQENm1a5csW7ZMVq1aJb/99puapLl586bZzhSmwQT6vZ2x/rOG/Oni4uJk//794uvrKx4eHiqoTj+v+urVq2p3tx49esiVK1fUb2OtA746PbB49OjR4uzsbFauJyQkSO/evaV69epqQNvb21uePHmiPhMTEyPx8fFmO2tZ07UTSVtpVbt2benQoYO6Hqmpqeqfo6KiVDDFkCFD1Ot37tyR69evS3h4eKaAUEvJ+NsajUYZN26caJpmtspKH1z09PSUkJAQSU5Olho1aoizs7PcuHHD4u2RvyIyMlI8PDykbdu2Zq+b5u3q1aty8+ZNqV27tsyePfuF32WN+Y2OjpbGjRtL8+bNVZ9HL1N0eqBBq1atVNBxcnKy3Lt3TyIjI1/Yb3hVDh8+LAMHDjQLTtI9fvxYypUrJ+3atRMR836pfh8bjUYZOnSoODg4yI4dO17437HE9Zs5c6bZjmumz56+U4o+8WfaRtN/g3Pnzkn16tWlWbNmVjfxvmDBArV63bStlZKSIk+fPpWGDRuKi4uL3L9/3yzfptdBD+Dq1auXJCUlmV1fa3veTBezaZomY8aMMQtcEklbaay3U0aMGGGJZP4r9+7dk6ZNm4q7u3umnZAfPHigArNat25tFkwhYj4RqrN0PW763w8MDBRPT09xdXWVgQMHyqZNm8yuX3JysvTu3VucnJzMAsuCgoKkWrVq0qFDB7M+rqXzljENixYtksqVK6txBUdHR/H19RWRtDaW6c4UpsEUv/32m6xatUp++OEHq+77jBw5UsqXL68CIjPea/rORfXr11cT9iJpO1PoO2/MmjXLrO1paaZlnP57P336VAICAuThw4cSGxsrvXr1kgoVKphN2j558kScnJykQ4cOMnbsWKvZ+VL3V/osY8aMUcEUep8hKSlJ/Q5BQUHqXtaDnyzNaDSKr6+v1KhRQypVqqR2mvv6669V+37v3r0q3fo81uXLl82+Z+/evWJvby+TJ09W32ttfHx85NNPP5XOnTtL//791et6+zI1NVW++eYbqVGjhri6usqSJUuyDPLX82aNeTS9T/UdtDIunhL5/dlMTU2Vli1bqp3chgwZYvH2M2VfDKR4SUwbK6tWrVJbN+nb3s2cOVPGjBmjVvO7urpKYGCg3LlzR/bt2ydly5aV2rVrmxVg1lhgGY1GuXz5smha2nbzLi4usmDBAvWeaZpNgyn69u0rv/766wu/01qkpqaq41VcXFzUyn8R83RmDKZYvny5Ksz1TqQ15euvMg2m8Pb2NrtmgYGB4uLiYvXbSWfl0aNHaiK+b9++snPnTklISJC4uDhZvXq1VKpUSRo3bmwWjWkpu3btEk3TpEOHDmY70oikDQTru6SY0oMp9C1uXxRMYW2D+abl5sqVK6Vly5bi4OCgOvlLly5VneGsgikyDoCYsvTzZ5q39evXq917xo0bJ/v37zf77IEDB9QqmEmTJql8PXz40Gzg3tJ5EjHP1+LFi6Vy5cpqxWbfvn3lp59+UvdZUlJSpmAKkbTVWHoAnrUxzd/SpUvNjlfp1q1bpoGprHamOHfunBw7duyFq7wsxTRv/v7+MmTIEKlWrZoK5nR3d5edO3e+8JiPWbNmmQUXmLKGe/NF9M6+n5+fGAwG9Tu8aGcKawnuMRgM8u2334qmpW01bDpIr+dBnzzbuXOn2eumQkJCRNM0+eyzz7L871j62pnWS2vWrJHmzZuLo6OjaFraNq+mZeCtW7eyDKa4evVqlsGfls5bRvr1efbsmbRp00bq1Klj1u4wDaaoXLmyBAQEZArg0llD3jLWB/oRF6bB4sOHD1c72pgGU/Ts2VP1eUwn1KwhXxkZDAZp3769Kis6depktquZnubffvtNBVP06tVLLl++bJY30/+3Ns+fP5eGDRtKs2bNzF6fPXu22NvbS1BQkISGhqrAz379+kl4eLhZuWTpvP3Rqnx957np06eLSNaTng8ePBAPDw+pVq2a2YSENTH9jU0Do5cuXSqapql+gz5JarqSTkSkW7du4ujoaJXHCWQMPhZJm3CoU6eO2X1pGkSh5+3YsWNq0Ds70evn0aNHi4hkWc8nJCSIl5eXODs7y/Hjxy2SzheJjY1Vuy54e3urYCr9Wj59+lQqVaokzZo1y3Q8hMjv97M+iTZhwoRXl/g/cfLkSVWXZbXT4bZt20TTNJk3b94LvyMmJka8vLxE0zSrunbJyckyffp0qVChgtSoUUMCAwPVkQG6/v37S7ly5dSus6bPp37dIiIi1Pbf2YXeb9Mna/WJJT1/N27cUMdhjBw5Uv2dNdV1L3Lp0iVxcnJSQSBGo9HsukVHR8uMGTPUOJNpMIW1Tbybmj17tmha2vEk+jiRu7u7LF68WI2ZJCcnq3Gi+fPni9FolAMHDqhxTb2fZI0WL16sxr6+//578ff3lylTppjV07GxsS8MpsjI2u7PZ8+eibu7uzRr1kxSU1OzbOvHxcXJF198IZqW+bjQrVu3qmdy/vz5VjMxHxsbK3fu3DHbFUXk97IkJCREypcvL5MmTTL7ux07dqidfLIKxrAkve2VkJAgc+fOlQEDBkjv3r1l9erVmYJYsgqmEBHVbz979qzVLOAz/Z03btwoLi4uYm9vL82aNcu0M4F+nIeTk5MsXbrU7L2TJ09K27ZtpXz58nLo0KFXkvZ/Qg8S0QNGnj9/rn4D036paTDF0qVLrS7g09SfPR/+/v5ZBlPof5eUlCRVq1aV+fPny5IlS6zymEbKPmxB/5qIwNY27aecM2cOpk+fDj8/P6SmpqJ58+YoWLAgAgICsH37dkRHR6Nz585YsWIFunTpgvfeew/16tXDRx99hNy5cyNfvnzqe21sbCyVpReysbGBg4MDZsyYgejoaCQkJCA8PBxGoxE2NjaQtOAcAICrqyu8vb3h7u6Oo0ePwtfXF8HBwVl+p7WwtbVFr1698PbbbyMhIQH3799HSEgIAPN0vvHGG2jVqhX69+8Pg8GANWvWYOnSpUhNTUWOHDkyfT67KFGiBGbMmIGKFStiz549+OqrrzBp0iSMGTMGM2bMQOHCheHh4WHpZP5tRYsWxbhx49C0aVMcO3YMo0aNQtu2bdGwYUPMnTsX+fPnx8KFC/HWW29ZOqmoVKkSmjZtiosXL2LZsmU4ePCgek9/tnRGoxEAkCtXLtSpUwc+Pj4oWrQo1q1bhxkzZiA5ORk5cuSAwWAAANjZ2b2yfPwVerk5b948zJgxA48fP0bdunXh5OSEe/fuYf78+RgzZgx+++03FClSBOPHj0fTpk0RGhqKKVOm4OzZsypvGVn6+dPzNnfuXHz11Vc4cOAA7ty5g61bt2L06NHYtWuX+mydOnUwa9YsvPbaawgMDMScOXOwdetWDBgwAL6+voiJiQFg+TwBv+dr0aJFWLhwIXLlyoWmTZvinXfewZEjR7Bo0SLs2rULqampyJUrF6pWrQofHx+ULFkSW7duRfv27TF06FDMmTMHR44csXBuMtPzt3DhQsyfPx/JyclwdXVF4cKFcerUKXh7e+Pq1avq815eXhgxYgQAwMfHB2PHjsWAAQMwZMgQPHz40CJ5eBE9b76+vpg9ezauXbuGTp06oV+/fqhWrRqePHmCr7/+Gnv37kVCQgIAoF27dujXrx+KFSuGdevWITAwEHfu3Mn03Za8N1NTU83+XS8X9dcHDhyIt956C0eOHIGtrS1sbW1hNBrx1ltvYcaMGahevTquX7+O/v3749atW5nyYqm82dnZoVGjRggICMAXX3yBXLly4ejRo4iLi1PXsmbNmrCzs8Pu3buRmpqq8gb8Xl8ULVoUuXPnxu3btyEimX4vS147EVH10pw5czBlyhRERESgdu3acHNzw/vvv49cuXKpz5cqVQpdunRBp06dEBoaigULFmDq1Kn44osv4O3tjdDQULN60pJ506+DKf265c+fH+7u7njw4AGWLFliVpf37NkTnp6esLW1xYoVK/D999/jyZMnmb7LmuqD+fPnqzaUj48PZs+eja5du6JgwYL44YcfMGLECNy5cwcffPAB5s6dC3t7e5w4cQKfffYZHjx4oL7HxsbGKvKVkZ2dHbp37458+fLh+vXriImJQVJSEnLkyIHU1FTV//n444/h4+ODypUr4/jx45g7dy4uX74MEVH5ssb8Ab+n69GjR7h+/ToAYNOmTVixYgVatGiBkiVLonTp0qq+O3jwIAYPHoylS5ciKSnJ7Dss4ebNmxgyZAgOHz6sXjMtCwoWLAgAiIuLAwCzcsXOzg6pqal4++23UaNGDURFReHu3buvKOV/j/4bz5s3DxMnTlSvlyhRAgAwffp0zJgxA0uWLIG7uztGjBgBR0dH9Tm9zLW2/gDwex/F399f9X1y586NDz74ANevX8f58+exevVqLFq0CO7u7hg5cqTKW6FChWBjY4PHjx+/sF9gjfT78P79+6rdrNPLxTx58qBcuXJISUnBhQsXLJLOF8mfPz9mzZqFDz/8EHv27MHIkSORkJCgnqlChQqhRIkSuHHjBvbs2YOUlBSzv9frPv06Go3GTH1dS3Fzc8Pw4cMBAJ9//jk2bdoE4Pfr8vbbbwMAjh49itDQ0Ex/n5qaigIFCuCDDz4AAKu6L3PmzIkhQ4bA09MTiYmJWLJkCXbv3o3nz58DSLsOpUqVQlJSEqZOnYr4+Hh1TYHfy1Y7OzskJyejaNGiFsvLHzG9l/TfX8+jXs/lzJkTQFpejEYjPvzwQ8ycORM5cuTArl27MHLkSADmdYY11ONZPSd6HvX/N21jA8Brr72G5s2b46OPPsKFCxewevVq7N+/H8Dv97U1MM3b3r17sXr1atSsWRPr16/Hli1b0K9fPxgMBqxatQr+/v5ITExEzpw58emnnwIAli5dirp166J///44e/Ysxo0bh2bNmmX6bmvw66+/YvXq1ShXrhymTp2K1q1bo0+fPhg7dixKly6tPpc/f340b94cQ4YMQZEiRbBp0yYsWbIEUVFRmb7TGu5PU/r9GBUVhfDw8Czb+vny5UOVKlUAAMeOHYPBYFD1Q8uWLTFq1CgUK1YMS5cuxaZNm7LsY71K69atg5eXFxo1aoRGjRqhRYsWWLFiBa5fv66euWvXriExMRGpqalITEwEAJw5cwYrV67Em2++idKlS5v9DpZ+Bo1GI3LkyIH4+Hj06NEDfn5+OHToEI4ePYq5c+diypQpePTokfr8jBkz0KJFCyQkJKBHjx44deoUFi1ahO7duyMoKAiurq748MMPLZij35neL7/99hsSEhKQK1cu3Lt3D1euXDHLV+fOndGjRw8YDAbMnz8fkydPxrJlyzBv3jwMHz4cFy5cwMiRI1GrVi1LZOUP6eXbunXrUL9+fcTExODq1au4dOkSbGxsYDQa1ViRjY0NunXrhr59++L111/HokWLsH79ejUGaE0MBgNsbW2RnJyM8+fPY9OmTVi/fj2OHTuG8PBwAECfPn1Uff31119j3bp16u8AYO3atUhMTIS7uzsGDBiAjz76yGL5of+AVxu38d9jGuWbmJgoHh4e0rVrV7MtqY4fPy7r16+XuXPnyo0bN1SUnh4VdvjwYXF0dJQRI0ZY3TlLGdNiGgm2e/duFfWl70qhf8b073799Ve1Qmvfvn3/+0T/Q6Z5O378uIp8HTFihNmWh6YiIyPVVkINGjTIFM2fXd25c0cGDx6sVirb29vLJ598YjVRpf9URESEjB07VsqWLSuVKlWScePGyQ8//GB10ZePHj2SkSNHioODg3Ts2FHtYPD8+XNxcHCQoUOHZtrmXSQtmvvnn39W9+64ceOy/JylmZabV69elbJly0qfPn3Uqpfnz5/L8ePHxdPTU21nq997T548USsOGjRoIAcPHrTaMnPXrl3i4OAg3bp1k+PHj8uBAwfUMTKOjo6ZtrA9ePCgVKtWzWxFr74dv6Xp18xoNEpYWJjUqlVLevbsqa7ZlStX5IsvvpCyZctK06ZNZdu2bWY7Uxw/flxtC25vb281+dKZlv/6aiRPT08VrXz9+nUZOHCgaJom9erVU/nWBQQEiLOzs1pJb3rGpzU5efKkuLi4SIcOHbLcXcPd3V0qVaok27ZtM1s9uHnzZrU7h+nRAtZk+/btmeoog8EgMTExqszIakelR48eqTbK9u3bX1Vy/7YlS5aIpqWdo65fm7CwMLXC38fHx+zzell04MABs5XY1lRe6jZs2CCapkn//v3Vs2W6uufZs2dm/37jxg2ZMmWKWVlpTVujmv7GoaGhZsca6WVNZGSk1KlTRz755BO1kl5fGZmUlCT+/v5SoUIF0TTN6s50NnXw4EEpW7asdO7c2axMSU5OlmvXrqlnq3fv3qqtFRYWJq1btxZN06Rt27ZZrlS2Rnv37hUXFxezHZZEzOtHkbSdKfTd7QYPHmyV7TAR8+1ORdKewwkTJkh0dLT89ttv8sknn0ijRo3MVppt2rRJNE2T5s2bS9myZa1ml7rQ0FC1+urXX3+VhIQE6d+/v6rDL1y4IJqmSbVq1bKsw/TfQF+tvHXr1leZ/L/lyZMn0qpVK9E0zWxFcZ8+fUTTNNXuvHPnjtnfBQUFibOzs3Tv3l2ePXtmlXWBvrOEvkODiKhzqvX2Vb9+/eTChQtmf7d7926xt7c3G4+wJi/6rZOSkqRx48ZSsWJF2bt3b6adA/U6QT+OwFrblufPn1c7UwwYMMDsmJ9169ap++7KlSvqtzA9wkTfkeK777575Wn/I6mpqWq3l4zpe/bsmfTv318cHR1lyZIlZmNFpmV+nz59pEqVKupYFmugl3fx8fHi6+srlSpVkurVq0tgYKBqaz179kyaNWum6rusjm7Rd4fx9/cXEetqX+orqzM+UwcPHlR5yrhjlP75sLAwqVq1qlStWvUPd3SzlIzjz7rQ0FBxd3cXJyenTGWkqaFDh6p7ukGDBi/cbdASMq469vX1FVdXV7N2cGRkpKxdu1aqVasmFStWlIULF6oy54cffpAGDRpIo0aNxNPTU3bt2vXC77YGetmnpzOr3WMvXLggS5YskZCQEDEYDLJ9+3a1C63pnIM1mTdvnvj5+al/HzRokDg5OcnGjRszXQc9z6GhoeLo6Cj9+/c3O4JNt2HDBmnUqJHFV5Lru7pUqVJFBg0aJL179xZnZ2ext7eXnj17qnHb8+fPS/Xq1cXDw0N8fX3Fz89PGjRoIJqmybfffmvRPLyIfmyos7OzTJgwQYKCgmTx4sWqLhg0aFCmHZzHjh2rdnDQd6gICwuzUA4yM72HNm3aJO3bt5cBAwbIzJkzxd3dXSpUqCDLli0z2+lE37Va7/PpbetGjRpluUOVpWS126Fp22rw4MGiaZq4ubmp480z9v2MRqOsWrVKypUrZzV9OlN6WRAXFycDBgwQV1dXs/GfNm3ayNq1a9Xnly9frt6bNGmSbNmyRWbMmKF2ILem44Eo+2IgxUsyZcoU+eyzz6RBgwby448/ikjmMy51ptt9BwcHS6dOncTR0VH27t37StL6V5k25OLi4iQyMlKio6PNPvPDDz+ogmrx4sXq9YzBFMHBwbJnz57/faL/howN1YzHBJw4cUKdyfbZZ5+Znatn6vHjx7JmzRp1xvp/xbNnz+TAgQMyY8YM2b59u9UFG/xTERERajv0SZMmmR19YU0yBlMcPHhQbY3XunVrFQQSHByc6Xyvn3/+WVxcXKRcuXJWs/2diGT6rYODg+XQoUPi5OSktjw1fS4fPXokffv2VQMe+iBVZGSkDBkyxOoGu00b00lJSTJt2jRxd3c3m3RPTk5WW8ZldR7wkSNHZNCgQeLp6WnWULcWZ86ckdDQUClXrpwcOHDA7L2wsDCZNGlSlsEUImnP3o8//ihnzpxRr1m6A5JRaGio2nIxKCjI7L2oqCjx9vZ+YTDFnj17ZOvWrWZHt1hb/vQJ6y1btohIWudJb6sYjUZZv369uLm5SZUqVWTnzp1m9V5gYKB8//33Fkn3n1m9erVomiYVKlSQxYsXZ9qmMTg4WOzt7aVHjx5ZTiA9fPjQ6toopmmMj4+XVatWSbVq1aRy5coyb948FZR7+vRpKV++vGiaJmPHjpXw8HBVJ5w6dUodSZDxqKhX7bfffpOrV6+a5ctoNEpUVJR07dpV3NzcMm2nf/jwYZk+fbrUrFlTWrRoIfPnz1fvPXr0SLZu3SpTp041u3bWNJCvB84NHTo0y99/2bJlZpMQImIWgLZw4ULZsGHDK0vvP6EH+Oh9GKPRaFbuhYeHS7t27cTBwUHWrl2r3gsLC5O6deua9R2sSVJSkoSEhMi2bdskJCREtT/27dunBtb083FFMgdTXL58Wfr165cpYM2aZOynRkVFqYDwzZs3i4ODg6ordIsWLRIXFxcJDg62qoFSEZFRo0apo7j0I8UCAwPVPffVV1+Jpmny+eefq6OcRMwnPb29vcXV1dXiA/V/ZuvWraps0ccVwsLCpE2bNqJpmrRo0UKePHmi8n7w4EHp0KGD2k7aWt2+fVtq1qwpTk5OZmWm3m+rUKFCprbxqVOnpGXLllKhQgWrOj5BZzohFB0dLQ8ePFBjDgaDQZYsWSIODg7So0cPs+OATO/LCRMmiJOTk1n72RqYtvFv3bqlJon69eunJt5v3rypjrfo06ePnDp1yqyePnPmjLRq1UqqVasm586de+V5eBHTMj0wMFA0TZOyZcuaTYBt3bpVqlevLk5OTrJs2bJMxwutW7dOypYtKwMHDnzhOJIlmG6tHxcXlymYQp9oCA4OVkHwvXr1krCwMBVo4e/vL9WrV5eGDRtmeba6NYiLixMvLy+zyaGbN2/Kp59+mmVwten58R4eHuLj4yMdOnSQy5cvv9J0/1Vz5syR+fPnmwXxTJs2TQWv6sHl+hEfetkycuRI6dChg0ydOtVqg7O++uor6devnwwfPlwFrqakpKj79unTpxIYGJhlMMX9+/fl2bNnWQYxWws9PfpxFqZt6Iz0o8n0Nufz589l48aNsnnz5leX4L8hKChINE2T8uXLq4UzW7duFXt7e2nYsKFZUIzpddHbNRnvSdPPZJyLeNX0wBcvLy+z9n1ISIjUrVtXypYtK9OnT5fExESJj4+XiRMnqoWJepDB+vXr1d9ZQ5/VdHz2/v37UrNmTRk7dqwaS0hISJCgoCBp2bKlaJomAwcOzBRMMXHiRGnfvr107drVaudE9ACYcePGqXmNlStXiru7u1SsWFH8/Pwy5evKlSty6NAhWbFihZw4ccKs72DpMsW0/WUwGOTx48cSHx+fKXj/rwZTZBzXtAb68xEfH68CyPv27SubN2+WJUuWSP/+/bOci1y1apVUrFhR7O3t1ft16tSx+r4dZR8MpHgJbt68aRYV5evrm2lATRccHCzu7u7i4+MjPj4+6jxha2vEmhbM3333nfTr109q1KghXl5eKlBE91eDKUxftzS9wZCYmCirVq2SUaNGyYgRIzJNyp48efIvBVNYQ57or3v48KFMmDDB6itT02CKTp06yfr1680iY/X/NW3aVLy8vGTRokVy5swZOXXqlBw7dswqBrmPHTuWaaW0yO/nQXbu3FkcHR1VWjOWGTdu3JDmzZuLh4eH2eq6J0+eWO3ZdHPnzpUZM2ZIhw4dZMyYMSKSNjhjGjWsr+LJKpgiKSkpy3OSLU1fldW6dWupXbu2CtIxrS9u3779h8EUpqwlX7ply5aJk5OTDBs2TJo3by5JSUmSmppqlv5nz579YTCFKWvLn8jv56brwR4ZV3ykpqbK9OnTRdM0qV69eqZgCp015c1gMEhISIh89tlnajeecuXKybRp0+Ty5ctqwsLHx0c0TfvTcsMa8maaBtMdDDZs2KBWT8ydO1cFUwQFBan2ZJMmTaRPnz4ycuRIcXNzs4o25pUrV0TTNPn000/l3r17Zu9FRERIjRo1xNPTU70WGhoqc+fOVXWcvspF0zQJCAgw+/usfitrYDAYxMfHR9q1a6fSPmbMGLNzmi9duiTly5cXd3d3uXbtmno94wBHxn+2FqmpqTJgwABxcHBQA/YZy3uj0Sg7d+4UZ2dn6dGjh9kEjn7/6p+zFvHx8eLt7a1Wo7q7u8uYMWNUnWe6M8UfBVNY604Ue/fulTlz5kirVq3E29tbpk+fLg8ePFCTSCkpKWpHMNPVjkFBQVKrVi1p06aNVU1QmN5z48ePV22rcePGmQ0QHz9+XFq1aiWOjo7i4+OTadL222+/FRcXF/H09LT4YP2L6PdWYmKieHp6SuXKlVXZkZKSIiEhISpgonLlytKpUyfp2rWrOlfetC6wpmdO5Pf06BMVs2fPVu+Fh4erAGpXV1fx9/eXjRs3ip+fn9otyxpX0pmeNz5jxgxp166dVKlSRdq2basmpMPCwqR3796iaZr07Nkz005769atk4oVK0rnzp3NFuNYmulz9/PPP4ufn58a6M4YTBESEqJ2J2rYsKGMGzdONm/eLIsWLRIPDw/RNPMzrS3N9Lpt3rxZ5s6dK5UqVRJN08TZ2dkswHHlypVSpUoVcXJykq5du4q/v7/s3LlTJkyYoOp308kXSzPdqUGvgzMGU6xdu1ZNzp84cUKaNGmiypSaNWtK7dq1RdM0qVWrllWPpXzzzTfqnjMNwNqyZYu6T7MqN1asWCGapsmFCxesrpzUnTlzRuXB399flSePHj1SO2J5e3uriTPd2bNnpUaNGjJhwgSzRWTWlM8nT55I8+bNVf46duyo+qKm6cwYTOHr62u2G05Wq7Wtjb7jkl43ZzVmcuvWLSlfvrx07NhRtStN2zaWboNl9Pz5c/Hz8xMnJydxcXGRjRs3itFoVAGRTZo0kePHj5v1AU6fPi2tWrUSNze3LHfZsIY8GgwGGThwoLi6uqr2o35v6eN6np6ecufOHYmIiJDnz59LTEyMbN++XSZPniyrV682C/a0hjzp4uLiZNy4cfLdd99J1apV1Y42+n2Wmpoqp0+f/sNgitjYWLPnz9JMf99r165J3bp1pWvXrmZt/5SUFFm1apUKpli2bJnZ4tEXjWFaukzJOFfn5eUlLi4uUrNmTWnVqpXs3LnTLLBTH7v8o2AKnTXdlyJp6dED4efOnWu2CEAfG9PrCtOdKfS+7qBBg2Tx4sWZdukj+jcYSPGS7N+/Xw3g9+7dW8LDw7P83ObNm80mQBs0aGAWTWoNBZdpGubMmaMGsfUVj+XKlcsUcPCiYIoXVT6WZLo9ULdu3TJNSpuuDBT568EUlL1Y604UGT169EhGjBgh9vb20qJFC3FxcZH+/fvLihUrZNKkSdKsWbNMx0HUqVPHKnaieP78uZrIy7iF3YkTJ9TEn4uLi1qJnPG6pKSkqAhifbAjY7liDeWmLiwsTD755BO1csl0+++MK7FNgylMJ9cyfs5aHDt2TG0Z6uLiYraltGk6TYMpmjdvLlu3brXKusCUwWCQwMBAs+NHMq6O15kGUzRs2FAuXbr0ilP7z+k7N/j4+GTq7OrP0dOnT9WWoeXLl5eNGzdmm/Ly/PnzsmLFClXuuLu7y/DhwyUsLEx27twpjo6O0rVrV3n8+LGlk/qXLFiwQBYsWKAGzqKiouTbb7/NMpji/PnzMmbMGDUpUa5cOenYsaNZe81SZeWVK1fEw8NDvLy8Mh3j8PDhQ6lZs6bY29uLn5+fTJs2TR1X4uDgIMuXL5ezZ8+q4+T69+9vVWV+VvTyMDU1VeLj42X9+vXStm1bcXZ2VgNtBw8eFIPBIOvXrxdN02Tbtm3qb7ILg8EggwYNEk37492h7t27J7Vq1RJ3d3c12G8aUGFN9VxcXJyaDPz000/F09NT1QuDBw9WA2wvCqaw9rJyzpw5aoWOo6OjClL69NNP5dtvv1UBBHq7a+HChXLr1i355ZdfVFCQtR2BZPqb6yuN9Um/jDtL7d69W1q0aCEODg5St25dmTlzpgQEBMiIESPE2dlZqlWrJqGhoa86C1nKqt1k+qzoR0sOHjzYrD5/9uyZ+Pj4SNOmTaV8+fJSo0YN6devn9lCCEuXM3/0zN+4cUMaN24s5cqVM5sEjIyMVLv8ZOzzmB65YOm86UzHG9q2bavaJHXq1Mm0avPy5cvSq1cv0bS042f69OkjM2bMULvyVatWzaqO1zS9frNmzVL9Ai8vL2nSpIkaN+nfv7+q8y9fvixffvmlmoTX/1erVi2zwARL1wf6/aNfNycnJ6ldu7Z06NBBGjVqpNJtmubNmzer66f/z9nZWdq0aWNV100XFxcnvXv3ljVr1qiACdNgimrVqsnatWtVwNyTJ0/k888/ly5duoibm5t06tRJpkyZ8sLxTmsRHh4uc+fOFWdnZ6lbt65s3LhRvacHWWiaJuPHj5fvvvtOQkJCZMaMGVKlShWpX7++1e/GumbNGnFzcxMHBwfx8/NT1zI4OFg6d+4smqZJ3bp1ZfPmzXLw4EHZsmWLat9kXBhnbfQj0pycnKR+/fpy6NChLBcq6sEUNWrUkHLlysmsWbOsbswhq7au3m45duyYODs7S5UqVVTAVVbHWnh4eEjr1q0z7aBsraKjo2X58uXi4OAg5cqVk23btklKSooKiHR3d5chQ4ZIQECAzJkzx6oDInUPHz4UNzc36dGjh9nr+k6z+i4w4eHh0qpVqz/Mi7W0U3QBAQFqot3Z2VkFs5jes38UTGHpevuPrF69WtUD+nirvkuPSNrzpgdTVKhQQfz8/CQmJkZ+++032bZtm9XVA6a/9cyZM0XTNKlYsaJ07dpVle/lypWT4cOHmwUlmQZT6LupWFtZmZX4+Hhp1qyZNG3aVLUnMy5U0MdSWrZsabYwheh/hYEU/5Jp4XPw4EE1eJ/VJIXu8uXLEhQUJKdOnTJbMW5tFao+ydehQwc5fPiwXLx4UQVWlC1bNtN2YqbBFAsXLrRQqv+Y6ZmQekUzaNAg2bVrl9l5SkuWLDH7O9NgCh8fH7MIWqL/NX1nCn3Ae/jw4eq9yMhIuXfvnvzwww8ya9Ys6devn1UN2uzbt0/Gjh2rtos2dfbsWdVx8vb2Vq/rz6levu7fv180TZMVK1a8kjT/W4cOHTIb/DSNPs/Y0dDLWU3TrHaLRpHf0x0UFCQ1a9ZU10yfFDP9jEhaMMWUKVNE0zSpWbOmVd2TGZmu8Ny4caOqG0aMGPHCQcJnz56pM2bd3NwkMjLSqjuRuocPH0qjRo3Ew8NDjhw5YtaG0Sc24+PjpWrVqtKhQwcpX768Va0SfJGMHcFr165JYGCgWkVXo0YNmT9/vnh4eEitWrXk9OnTImJ9HX/TduCDBw/EwcFB3N3dxc/PL8tgCldXV7NgitjYWHny5ImcPn1aQkNDzZ5PS7cx7969qwJRN23aJBcvXlTXbePGjeLo6KjKwqpVq8qYMWPMJkETEhKkRo0a8sknn2SLgcSM99bdu3fl4MGD0rp1a3FychInJyfp3LmzrFq1SmrWrCmffPKJVa02NvVHz8m2bdvEyclJRo0aZbZLgenfGQwGadq0qdSpU8dqV/qLpA3MeHl5ibOz8/+x995xUZ3Z//ixRFO/2U02yWY/m91Nsrt3hkEYujAWsIAoFmyIYgFURMWCNSpqxF6woYINC6CCShHUSOyKwtiwYcEGqIBiow5l3r8/+N0n984ManazztXM+/XilTgzXJ5nnnaec97nfbBw4UK2Zn7++Wc4OjrC3Nxc5DgUkinmzZtnzKa/FnglsEGDBiE9PR23b9/GxYsXERQUBFtbW7Ro0QIxMTGorq7GoUOH0KxZM3AcB1tbW6ZowMs0A9LYP4XB6g0bNiA0NBSrV6/G1KlT2fl87Ngx0e8cO3aMZUbyP9bW1vD29paMrSL8bq9cuSIi//Hzsrq6Gp6enmjRogVTx9LN1Lpx4wYKCwtF91ZjnwXC81roDBUSYnj1rMWLF0Oj0Yi+j1OnTiEhIQGLFi1CWlqaSF7b2H3ThUajwZAhQ2BmZoaFCxfi2bNnePHihci25Mfs/v37WLJkCStLw5+Ffn5+kiH36IKXYg8ICGAZrEVFRVCr1XB3d9cjUxQXFyM3NxdRUVGIjIzEwYMHRcpuxho/3b2surqa2fhLly5lZ9udO3ewbt06g2SKhw8f4ueff0ZkZCRWrlyJEydOSJa0y5MI2rRpg/j4+JeSKXg7sqamBhqNBnl5eaitra23hLEUIBzP+/fvM7JPmzZtRKSrnTt3snkq/HF3d5e00oZwD926dStsbGwgl8sRERGB0tJSJtMeHBys1zelUoktW7YYsfVivMyOyM7OZklv/fv3x5UrVwySEp49e4bNmzdDoVBI7s6qW7JW924G1JVa4TgOHh4e7GwQ7oUnTpwAx3EYP368SOFUitAtw8GTKZo1a4akpCTU1NQgLCyMJR3xP+3atRMRnaTUx7i4ODx+/BjFxcWwtbWFh4cH2zN5P56/vz9LrNm1axd7Tcr7pBCPHz/GvHnzmI9vzpw5opJBPHTJFAMGDJBE8l59uHr1KrtzW1lZMVVSodIGICZT2NjYYOjQoWjVqhUcHR2Zz0hqiImJAcfVlZkR2lFpaWmikkdC+58nUyiVSsmWrNJFTk4OOI7DqFGjABhOWHjy5Amz2aRcvtCEdwcmIsWvgO7ljpf9FuLw4cOMTDFr1ixWW8rQ7wshJWMBqAsE2trawsfHR7TJqtVqlkUuk8n0MtD279/PDKKsrKw33OrXQ3V1NUJCQmBmZoalS5eKDBzhhSMyMlL0e6dOnWJB39DQUMmNmQnvNgoLCzF58mRWMuH48eMGPyfcc6QC/hI5c+ZMjB49WvSekEyxfPlyg7WBt2zZAo7jcODAgTfW5l8DQ3vBkSNHmDT2iBEjROoGup9funSpXoDCmHiVI/P06dNMsWDy5Mn1yrPfvXsXU6ZMEcmsSQEv27srKiqwY8cOdOjQAZaWlpg/fz4ePHhg8LNPnz6Fn5+fSIVJ6igvL2eEyJ49e0KtVrOgNP+9nDx5EhYWFti7d69kgkr/KcrKyhAVFcUccVZWVuzSKTUInW1nzpxBXFwc+vXrx5RP6iNTWFtbY+nSpS9Vy5KSvcJnDfj5+bFsYz5wGxISgpkzZ+LSpUsi5QKgjlCnUChE2f9vA3S/+2fPnuHIkSMsK0sul0Mmk8HS0hLbtm2TXCBQOC8fP36M27dvi/aFy5cvw8PDAxxXp0gnPLv5vh87dgzm5uaszJeU5qMQCQkJMDc3x5QpU0Traf/+/VAoFOx+p6tMwUu+C0sRSA1XrlyBk5MTPDw82Lrjx/bRo0dYs2YN7O3t0a5dO5bBtHPnTvj7+6NNmzYYMWKEyDklpXlaUVHBZMCFRH6+nJMhMoVGo0FmZiZ27dqFjRs34vz583rBDSlg7ty54DgOvr6+jGwrXD/btm0Dx3GYO3cue+1lShZSWnvz58/HhAkTDDo9nzx5Ag8PD7i5ubEg9qsCElLqG4+EhAQoFApMnjxZdD/TarVIS0vD3Llz4eXlhVWrVrEA2osXL3D8+HEcOnQIubm5kiSf8d/1pEmTwHEczp8/r/eZ58+fs3UpJFO86plvEkLpayFycnKgUqng7e1t8F6dmJhokEzxtuDp06dYsmQJlEolWrduXS+ZwsnJCdHR0QZJuVJab4YCK/WRKdq2bSsiU9y8eRNHjhzB4sWLsWzZMqSmpkoqA7m+/Vx4BuuSKYT38v379yMqKgoTJ07Etm3bREFBY5/jun179uwZS7zhxy87O5uVBRo0aBCuXr1q8Dx78uTJS8ttGgPC/sXExKBXr16wsrKCs7Mz4uLi2DyrqalhijYuLi44e/YsW3Pp6enw9vaWtB8sKSlJVC6hPjKFhYUFdu3aBaBurJOTk7Fz506cOnVKVP7I2PNSiOjoaHAch7S0NADAgAED0Lx5c1y7ds0giQKom7MymQx+fn5vRcY/v44eP36M2bNnw8bGBq1atcKuXbvqLe969uxZtGnTBgqFAg8fPnzTTX5tlJaWIjExkd1TPT09GcFRqB4J1K3DLVu2wNXVlZ3vxi6NWh8qKiowcOBA2NvbMx8z3x+eiOzr64vc3Fw8ffpUdKYNHz4cHKevFi1V8ESKzp07v5S0w8cLlixZ8gZbZ8LvFSYixWtCeAimpKQgNDQUXbt2Rb9+/bBy5UqcO3eObcKHDh2ql0zxtmD58uUwMzPTq1E2atQoODk5MceOTCZjBhGPhIQEvfIYUkJOTg5cXFzg4+MjcvhevnwZtra26NOnT71kimPHjsHV1VVyhroJvw8UFhZi/PjxkMlk8PLywuHDh9l7UpeTvnv3LltXfDCFx7lz56BSqSCXyzFv3jzRBers2bPo2rUrnJycJMWc1b3klZWV6e31R44cgY+PD2QyGUaPHl0vmUKr1erV6jYWhGfdnTt3kJGRgYMHDyI9PR0ajYa9/7pkivpeNxaE/Xv48CEuXbqEo0ePsosGUEc2iIuLQ/v27aFUKrFgwYJ6yRTCM0RKF/+X4eHDh4yR3qVLF0RFReH+/fvQaDRIT09H3759YW5ujsuXL7PfeVv6JoSu82Lnzp2sDMGaNWuM1CrDEK6NJUuWMMJHu3btmFKDSqXC2rVr6yVTrFixggUqpLDW6sPFixfh5eXFnE/Xrl0T1enk/1+4d6jVavTu3Rvm5uaic+9tg+46OnDgABYuXMhKLUgti0K4hqKjo9G7d282H4XKbSkpKUy9YNGiRSIbOTMzE97e3lAoFKJyUFLEzJkzYW5uLspEff78OVxdXdGvXz9kZmay0hEBAQHMebh37160atVKrxa5lMAT3flsTV6BiN8rnjx5gnnz5rG+8SgpKcGLFy9EKjBSOA+ENm9ycjI4jsP8+fP1gs7Tp0+vl0zxNmD58uXw9/dn9vOQIUMQHR3NVC9zc3PRqlUrWFhY4Ny5c0Zu7esjKysLSqWS7X3Dhw/H0aNHWfCosrISixYteilBSQrz8FWYP38+OI5DZmYmgLr1lJubi5EjR7JSTxxXV0J1woQJkgrgvgplZWXo0qUL7O3t9WwPfn0WFhayoERAQACbt1K4sy5ZsgStW7c2uC8cPXqU+fEAw+QBPhtUoVCIAhJSn5d8+54/f45Fixa9FpkiJiZGsopZPEpLSxEeHi5SexGORX5+PiNTtGvXTkSmkBoMzaGVK1eybGrAMJnCysqKlcR7/Pjxr/4bbxJC+3L37t0IDg6GSqWCSqXCiBEjsHXrVnaevy6Zgoex+6bbBv4s44kSvArW6tWrkZ+fD6DOBuPVTJVKJdzc3ODl5cXUaKUa0OX3wVGjRonIBML+P3v2DCtXroRMJoOtrS02bdrE3tMdP6ndX48cOQKFQoHg4GAAYArWfEl3oRoTj927dzNyudSg6xvR9V0WFRVhzpw5sLCwQNu2bZGQkFAvmeLChQuSKu9UX2nkkpISJCUlMZXS6dOn6yVrCP0Qp06dQnJyssg2kMKeIkR+fj7Mzc0xZswY0es8ucfPzw85OTm4d+8e+vbtiyNHjog+p/tvKeBlpCMfHx/Y2Nhg//79eu/xJOsjR46A4zisXLnyf9ZGE0zgYSJSvAaEG+fixYvBcXV1D52dnZlDsUOHDli5ciW7GB49evStJFPU1taiuroavXr1gkKhEGWd8Rsz74SbNWsWI1PUx8aXAgtTtw0HDhzQ22Srq6vRpUsXdOjQAcAvBhCfJQ+Iy4KYYIKxwJf5kMvl8PHxkSw73RBOnjwJR0dHcByHqVOnit47e/YsC8z36tULM2bMQEhICLuoSKlmonBPSU5OxtSpU+Hp6YmePXti/vz5+Pnnn9n7R48eZWSKUaNGiS5bUrv8C/92ZGQku+zzPwMHDkRsbCzbAzMzM5kE4KRJk15KmpDCxVjYv/Xr14vkXB0cHDBo0CAWiKioqEBcXBxcXV1fSaYApNG/1wHfzvv37+OHH35gdopKpYKbmxsLhgqdHO8Snj59KiqpJrVx4+WW/f39kZmZiadPn+LcuXNYtGgRmjdvDnt7e4NkitatW8Pc3Bzz5s2TRIDCEIRB2+zsbFa/2c/PD9nZ2aKxOHfuHEaMGIGVK1ciIiKC7UXvyrzUtUuzsrJYtpNUYKgGKy8z3717dyQkJIjuNvHx8Ww/adeuHQICAjBy5EjY2NhI2gksRFBQEMzNzUWkx0GDBsHGxgaHDx+GRqPBli1bYGlpCYVCgX79+rHszldlWxsL/LkXFhYGjuOYQpRuWSegjmSnUqkgk8nqVRWU0p5ZWlqKqVOnYuHChWjbti0bg+rqalH/hGQKvtTanj17kJ6eLtlxE6KqqgppaWnw9/dn6iedO3dGQkICXrx4gYMHD4LjOGzYsAGAtMboZbh27Rr2798PNzc3phjVp08fnDx5EhqNBsXFxWjRogU6d+6sl0H4toD3nURGRuLo0aNsrvIBta1bt2LTpk3o3r07HBwc3rrazgMHDoRCoWDkOeG64/9/48aNrDSQv7+/JNZcUVERBg4cCI7jkJKSovf++fPnIZPJEBAQgNraWoMBmhcvXrDgrrW1taTKJfDQVXHRlTMXkimcnZ0NkimaN28OMzMzxMXFSSKYdOHCBUZiFAbBhg0bxpI17t+/zz4vbHNubi5mzpwJuVyOTp06icgU/Hw15h6Tk5NjUJlz37594DgOdnZ2OHXqFHtdl0yxYcMGcBwHMzMzrF+/XkR+kcLY8RB+xzzJwNLSEh4eHlCpVIxkFhgYyPogLPMxaNAgvXuDVLFmzRrW5szMTGi1WkRGRkKpVMLW1hbh4eEi/8LKlSvh6+sLOzs7uLi4IDAwEHv37mXvS2kcgbqkGy8vL5Y0VF8SRm5uLiOFtmrVChs3bjRGc381CgoKmJpBRkYGysrKmEKIvb09Dh06JPq8Wq1G586dYWdnJyrtKwXw+39lZSViY2MxefJkDBgwAGPHjsW2bdsYqed1yRRSgm7JOL4Eo/C8TkpKQrt27WBubo7Zs2ezvUWXTKELqa05ALh9+zZkMhn69+/PzgohiYL3N0dGRjI/rVar1bMJpBCr02q1bG5WVFRgy5YtCAkJwYoVK5hvhPePdejQQUTYEvq8pk+fDoVCgZMnT77ZDpjwu4SJSPErwBunfn5+yMzMxPPnz5GVlcUYlq1btxYZ5EeOHGFOxenTp79VAfjQ0FAolUpWn3rPnj1o1qwZgoKC2CF76NAhWFpaskDUsGHDJJdJITQYfvrpJwB1Emn8gQLUbd6jR4+GhYUFYmJiUFVVxWqa8n3z8/NjmXRvg9FuwruNoqIiTJw4UVJOKSFeZnAeP36c7Yu6ZIpz587B2dmZZfi4urpixowZTM4YMP76E/59nljHO0T5LHJdFvrx48frJVNIEXzAxc3NDWvWrEFkZCT8/f1hZ2cHGxsbzJkzhwXQ1Go1I1P88MMPkpQg1gVf2qJNmzZYvHgxpkyZwpwzHMdh3759AOouYkIyxaJFi0TOubcVwgzkI0eOICgoCJ06dYKLiwvGjx8vYntL8fL4n0BqpCVDKC4uRvfu3eHk5MRqvgsdAKmpqXBycoK9vb1emY/o6Gg0a9ZMUoSzV32/V65cYWQKf39/5hTVaDRsjfI/KpXqrcr4/DXQnZtS6xsvazt06FDmIH3x4gVzxggJdKdOncKkSZPY3UAul6Nv375ISkpin5Fa/4BfxiAiIgJOTk7sDFiyZAkUCgUWLVrE7KysrCwoFAqWSe/i4vJWnHt79uxhiiE8hHOPvy+NGTMGHMfpKRJKEfy9XC6Xw9raWkSSA2CQTGFra8vud15eXqL5a0zorouSkhK9thUWFuLixYsICAhghAp3d3esXLkSrq6ucHR0FMljSwWGSqMKUVBQgMTERBZkUSgU8PPzQ2pqKpPqjY2NfZNN/o8gzJDm597x48fRrl070XnWpk0bhISEMHIIUOd34TgOycnJRmn7y2DIfqqqqoJWq8WqVavAcRxmzJjB3tMNSB8+fBgKhQLW1tbgOE4v8GQsXL16lWVlVlRUiMqTPHz4EN27d4e5uTnbC4XzmO9bUFAQIwva2NhI8iwoKSnBuHHjWIkcfnyEZIqFCxfC0tISLi4uiI+PZwp9ZWVlWLBgAVxcXHD79m2jtF+IBw8eYPDgwdiwYQMLlPFn15EjRxhBfurUqfWSKbKyslhJUQ8PD5YgJgWMGzcOgwYN0tsjAeCHH37QIwQC+mSKyZMng+M4NGvWDCtXrmTEGCkiKioKHMdh8ODBOHfuHGpqanDv3j0kJCSw7PGAgADWh2vXrrH7uo+Pj4j0KkWcO3cOjo6O6NOnD7vXAXV7Dz8HlUolVq5ciby8PPZ+eXk57t27h+LiYlEAW4r2M1BHkuDJBS8jU8ydO1fkK3tbFJ737dsHmUzGSqgVFxczckWnTp2wZs0apKSkYO3atcyPKaV9Bfhl3y8tLWV3b7lczpKC+f2QV+QzRKaQmr+Zh9DWj4uLw9ChQ6FUKhEYGIh9+/YxnyWvTNG2bdt6yRRvCyorK+Hh4QFXV1c8evRIRKIQEg1Onz4NjuMwYcIEI7ZWHzExMSw2x6OsrIzNTf7Hzs6OkVTHjx/P7j2HDh0SEQWjo6NhY2OD3r17v1KNyQQTfguYiBSviYcPH8LV1RUuLi56hz4v2+jv74+8vDyUlJSI6gIrlUpwHIfTp08bo+n14mXG2OHDhzF79mzcv38fBQUF8PLygqOjI86ePcs+c/36dTRr1gw9evRA27ZtJZttVlZWBk9PTyiVSty9exeFhYWYNm0aKz+ybds2KBQKTJo0iV2AtVotc7S1a9cODg4O9dbRNMEEY6CgoAAhISEi1RgpQGjMZmRkYOfOnVi0aBFSUlJYBsnJkyfrJVOcP3+eBeYDAgJELGEpXSA3bdrE9v2zZ8+iuroa9+/fx65du5jxFx8fzz7Pl/kwNzfHiBEjJFHKw9ClIS0tDQqFAj4+PqJLv1arhb+/PyODFBUVsfFQq9UsYzwoKOiVdayNidTUVJYZolsqpkePHixgyDtteGUKd3d3KBQK/Pjjj5Jn5b8OdMdeo9HoBW2ktN5+D+BrQI4YMQJAnXNYOAaVlZWIi4uDlZUVXFxcEBERwZytxcXFovVqbAjPgczMTMTFxWHlypXYsWMHcnNzWbsvXbokIlPwpRFKSkpw6tQpbNiwAampqW99mZm3FU+fPoWnpydcXFz09stTp04hLCwMHTp0gJ+fH3bt2sX2/gcPHuDq1au4d+8eC94A0h+7x48fs+yX4uJitG/fHt26dRMFx27dugVbW1ssXboU4eHhb00GuVqtZuQPYSBTN9s6MDAQtra2kpLrrQ+PHz/GvHnzmM04f/58vaCRcC+aPXs2I8G4uLhIZuyEbUxISMCkSZPQrl07uLu7Y+bMmdi3b5/oM1VVVThz5gxmz56NZs2asWx/hUKBiIgISakSCdu9b98+zJo1C8OGDWNqj7pITEzEpEmTRCRlc3NzdOnSRbL3cN2MPl37KjU1FePHj0evXr0wa9YsXLp0SS+5JigoCI6OjqKyQlKAsG/V1dUi8gdQR4jk/VxCpU/hvhIbG4s2bdrg+vXrkihfZcj+7dy5M9q1ayfKZOTrjKtUKnb+6d5F+/Xrh9mzZyMtLU1SNpgQo0aNAsdx6N27N7PzdckUz549w4wZMxiJPi4uTqRMwcugGxtFRUVo0aIFrKyssG3bNuTm5iIgIIDNq6NHj7JSMi8jU4wcOZIFQjt27CgZAkxAQAA4rq6EEwDMmDEDEydOBFA3T0NCQuolU/B29fbt29GyZUt0794dHMfVqy71pqG7TxYWFsLDwwOtW7dmtr9wbV6/fp2NkTCB4+bNm6wMs24gTmrYvn07OI7TU4+dMGEC7O3tERYWhtatW8Pa2lqvLA2Pl5UwkRJeRqbgx37WrFkICgrCwYMHJangIwR/hmm1WuTm5qJLly5QKpWsX8+ePcOoUaOYP5P/adWqlSixVkrjVllZib59+8LMzAwzZszA7du3cfPmTezfvx+DBg0Cx3EiO+TRo0eYO3cuLCws4Orqiu3bt0uOTCHc14VJGLxd7OrqitjYWNbul5EppH5H5cErOPAEXP7MGzJkiB65jC+9I6X1lpKSwsjswhIjfn5+zM+8adMmTJ06lY3jpk2bUFtbi+DgYHYG9unTBzNnzoSvry+bu1KLi5jw7sJEpHhNXLhwARzHYcWKFaLXefaXv78/7ty5g7t372L8+PEi1vbBgwcll0mhG+xMSkrCoUOHcOvWLfY6f9k6deqUwb7zG/PFixdFgSUpGAxCJ9KUKVPYpsz3SXixCgwMRPPmzfUuiV27dkVAQADu3Lmj5zgwwQQpQErOUkBsgIaHh8Pe3p4ZQPxFmA+0pKenv1SZgmfqjx49mu1XxuivIQfpw4cP0a1bNzg5OYlYv0CdhC3PCL53754oOH38+HF4eXmB4zij1qYTBhp09+uwsDDIZDIcP35c9Dqfdebv74/c3Fzk5+fj8OHDIjKFpaUl1qxZ87/vwGtCOF/4fk6fPh1mZmZ6tcR56c2AgADk5eUhLy+PGeOVlZXYsWMHmjdvLqmM/98Cb8ul8feA3NxcWFpaIiAgoN7PFBQUMAeiq6srNm7cqEdcMvaYCveU5cuXs3Ix/E+HDh0wZ84ctjcKyRR+fn64du1avX2Qgn35e8Lt27fBcRxz5AN18zQ8PJzVbeZ/WrZs+dJz7W0bO550J1RCAX7JHNc9Q6SEK1eu4ODBg0hNTRUpYIWHh4PjOLRv315U+5dHRkYGlEolfHx8UFxcLOkx49v2+PFjzJkzBzY2NmjZsiXi4+P1yI5CO+7QoUNIS0uTjLqUIYlzCwsLeHh4wM7ODgqFAjY2NggJCTFoA2dkZGDdunXMnk5ISHiDrX85hPs4H5QW/gQHBzMHtm7fTp8+jfHjx4tKzOmSuaQAofJlVFQUJk6ciF69eiE4OBi7d+8WjW99wejo6GgoFAr4+/tLKntcN8szMDAQNjY2GDNmDNLS0ljgNi0tjY3RokWLoNFo2Pdy9uxZdO/eHe3atRMFq40tJ83bTbW1tcjOzmZEcS8vLxw9epR9jlfoadGiBSvjxGPbtm0wNzfHggUL3mjbfy2ePHmCrl27guM49OjRg42DLpmipKQEbdq0AcdxaNu2LaKjoyVDMOCh0WiwdOlSqFQq2NnZsZKgsbGxbK0dOXJERKbglXSF6NChAxYsWIBjx45JKvBy69YttufxJYAGDRrESj9UVlYaJFMI19OKFSvQunVrpKamMnUtY+HUqVOIjIw0+N7FixfBcRxCQ0MB1PVB1+bIyMhAixYt0KlTJ9GZnZ2dLVJPlCoWLFgAjuNEajcRERHguLqStTU1NUzd1NbWFmFhYZJX2XgZhGSKUaNGiUg8arUarVq1wrRp00S/Y+w7qxBRUVFYvXq1XiID8Iv9HBYWxs6+iooKXL58GTExMYiMjMTPP/8sIkNKqW8AmMJXSEiIXtn5mpoaBAUFMWUKXiGluLiYzeOuXbtK7kzgwcfk+vbti1OnTiEnJwehoaFQKBTo1KkToqOjGYFVSKZQKpX48ccfJUMW/DW4d+8eOydat26tpySoVqvRsWNHNG/eXFIqgwUFBQgODoZCoWBkipycHKhUKsycOZOtr8rKSuzevVtEpgDqypXwyW8cV1d2lI/ZmWDCm4KJSGEAhgJnfDmIsLAw9rqhOkS8nFp9jgwpHKhCI3XFihUiZ2ifPn1YCQugrr185rUwu/rMmTNwc3ND27ZtRQaDlJxupaWlOHXqFEJCQtC/f3+DDqi8vDzI5XL06tVL9PratWvBcRyWLVv2ppprggnvDPigdI8ePRAXF4edO3dixowZetl/LyNTnD9/Hq1btwbHcRg3btwbd7qdPXuWZb/p/u0rV67A3Nwc06dPF70uPBNycnJw7do1eHl5iQIWhw4dMurl/9SpU/Dy8sLhw4fZa/y+XV1dDR8fH9jb24tk0YT9ys7OxtOnT+Hh4YGAgADRd8PLrAqf+aZx8OBB+Pn5sX8L6wGXl5ejffv2cHZ2FilHCft39epVFBcXw8HBASNHjmQBmcrKSsllCv6WkNLZ/VtDaHdJLZNCiIKCAjg5OUGhUBisq8r3gy+9o1Ao4OLiIokMT0NYt24dy4KMi4tDYmIiQkJCGElu2LBhzCFz+fJlRqYYOnQorl69+k7PSUD/PiCF+4EuiouL4eLigs6dO+PgwYMICwtjRB6O47B69WocOnSIzUkh4eJtB68uNXPmTPZabGwsbG1t0adPH8k63SIjI9ka4+0nvuxiSUkJpk2bBo7jYG1tjV27dqGgoAC1tbU4dOgQW4MpKSlG7oUYujaYrgP4deo5Gztw+yrw9X95lbPa2lrcunULycnJMDc3h4WFhSggpUugy83NNUiOMRaE+ze/P3h4eCA2NhYpKSlo3rw5UzHj15Lunl9SUoLc3FyMHz8eW7dufaPtfx3wc6qsrIzti1ZWVqxvHMdhzJgxevWaIyMjkZiYiNOnTzOpc0dHR0mUTuBRX5Yn/+Ph4YG4uDhUVlYC+KV0EJ9lOGbMGISEhEClUrFAt1QgHLfQ0FBcuXIFubm5LNOxd+/ejBSo0WgwevRocBwHc3NzjBw5EvPnz8eIESNgYWEBlUolKaWU+uyIJ0+esOx+Q2QKfhwnTpzIVGhdXV0lETSLjo7WUwNeuXIlLCwsIJfLMXjwYD1inJBM8cMPP4jKPm3ZsgVKpRKJiYlvpP0vg1qt1ktAKC0thVKphLm5OVq0aMGUTvg9vz4yBVDnu2jXrh3Gjh0reqYx7MtHjx6xPYFX+hK2hU/UCwkJqfcZJSUlTFGFL3skxXJ49dkXq1evBsdx2LhxI4A6gq61tTUCAgLYfv/ixQumqsVxHCwtLXHr1q239g6Um5vLbMn+/fsjIiICcXFx8PDwgFwuF80FKUF4hvXs2RMrV64UncklJSXo2LEj3NzcXsuXIMXxGzduHJRKJSOP8fNWmLDGZ/evW7eOvV5cXIywsDBJkc6EOHr0KOzs7NC/f3/RWXHkyBFWBq9NmzaIiYkRkSmSk5PZfSkzM9NYzf+PwO97V69eZb50Pz8/xMXFIT09HdHR0Wjfvr3BhAApoKioCOPGjYNcLseAAQMQEhIChULByBDCfT0xMZGRKdavX89eV6vVUKvVKCgoeCfUgk14u9CYTNBDo0aNiIgoJSWFXF1dqUmTJvThhx8SEdHNmzeptraW1qxZQ+Hh4aRSqWjcuHFkZmZGRET//Oc/iYjo2bNnREQEgBo0aMCe3bBhwzfYE8Pg2xMTE0OrVq2ib7/9lqytren27dt0/vx5mjt3Lmm1WmrXrh01bNiQPv74YyIiWrVqFf31r3+l/Px8io2Npbt371JoaCjrs/DZxkZtbS0NGzaM1Go1ffDBB+Tm5kaNG+tP908++YS++eYbunbtGp06dYr+8Y9/UEpKCm3YsIH+9re/Uc+ePY3QehNMeDug1Wr19rRLly7R1q1byczMjObMmUMcxxERUY8ePUSfq6ioIEdHR1q8eDGNHz+edu7cSbW1tTRv3jwiIlIqlbRs2TIaN24cpaSkUJMmTWju3LlvpF/Xr1+nvn370vvvv0/Jycn0t7/9jWpra9nZUFpaStXV1fTee++x3wkPD2dnQnBwMH3//fcUHh5OFy5coNzcXPY5FxcX9v+Gvr//JcrLyyk+Pp4uXLhA69ato8aNG1OLFi2oQYMGpNVqiYioSZMm9Pz5c7pz5w59/vnntGrVKtFZJ5PJ6NatW/Tw4UMqKiqiR48e0Z///GciIvriiy+M0i+iurO2urqaZs+eTQ8ePKChQ4fS2rVrqXHjxlRTU0ONGzemDz74gL766iu6c+cONWrUiBo0aCAat3HjxpFcLqc7d+5QZWUlPXz4kD766CMiImratCk764zRv/8lhP3hbRZj9fFVf1fXpvo1z0tOTqa8vDzq3r07ff311/91W/8TvKx/X331FQ0cOJDCwsIoMTGRvvzyS/r++++JiET7zdOnT+mrr76ibt26UXR0NCUkJJCbmxvbn4wF4R5JRHTixAn6xz/+QbNnz6Z//etfRETk6upKXbt2pZCQEDp8+DDNnj2bZsyYQQqFgkJCQmjOnDl09OhRev78Oa1Zs4b++Mc/Gqs7IvDzTjj/fu1cFEI4DxITE8nMzIz+/e9//2bt/a3w4YcfkpOTE+3cuZOGDx9OREQff/wxubq6kre3Nzk6OhIRkYWFBUVFRVFBQQEBICLp3Af+U1hYWFDTpk1p27ZtVFhYSFVVVXTy5En64x//SLNnz6bPPvvM2E3Uw9KlSykyMpL++te/kq+vL5WVlZGdnR39v//3/4iobuzGjh1LTZs2pejoaJoyZQr94Q9/oCZNmlBRURE1aNCAfvjhB+rUqRMR/Xdz/LcCf35rNBravXs3Xbx4kR48eECff/452dvbU8uWLen//u//aMiQIUREtGPHDgoPDyciovbt27Mz3Nj7oxC6Z+7Tp08pMTGR/vSnP9H48eNJJpMREdF3331HycnJVF1dTa1ataIOHTpQSUkJffzxxyL7U6vV0jfffEPffPON3vONBX7e7N27lzZu3EitW7em4OBgdicoKyuj0NBQOnDgAAGgH3/8ka0pft599NFH9PHHH9OiRYvYc6XQNx6NGjUijUZDgYGBdPHiRfLx8aHBgwcTEVFOTg6tWLGC9u3bRwDoq6++ou+//57UajUtXbqU7ZNERN9//z0tX76cvv32W2N1RQ/8dxweHk5r164lKysrGjVqFH366ae0fft22rlzJ0VHRxMRUdeuXcnDw4O++uorWrt2LV29epUuXLhA7733Hv3jH/+g0aNHU69evYhIGntKo0aNqLKyknx8fOjq1asEgEJCQtjYpaam0urVq6lhw4bUsmVLWrZsGS1dupSOHTtGaWlpRET00UcfkYWFBf3444/0t7/9zZjdYeD3yqqqKrpy5Qrl5OTQp59+Sn/+85/JwsKCoqOjqX///nT58mXy9fWlqKgo+uSTT6iqqoqaNm1KRETZ2dn0j3/8g4YNG0bfffcdffLJJ0bt086dOyk0NJS+//57io2NpY8++ogaN25MCQkJpNFo6MMPP6SsrCw6dOgQubm5sXto69atiYho/vz5tHv3brp//z6ZmZnRixcvKDU1lT7//HNycHAwZteoqKiIBg8eTJWVlfTdd9+Rq6srEREdOnSIKioqqEmTJvTo0SMKCwuj8PBweu+996impoaaNm1KU6dOJSKiuLg48vX1pWHDhpFWq6WffvqJ8vLyaOTIkaK/ZYw9809/+hNNnjyZzpw5Q3Z2dnpt4e37q1evUl5eHju/eGi1Wvr444/JysqKfvrpJ9JoNESkb1tK4Tzg7Yvw8HCytLSkli1bEhGRn58fNW3alDp06EAajYZ27dpFDRo0oEGDBtG3335LWq2Wqqqq6MWLF2Rra0t///vf6e9//zt99913xuzOf4VvvvmG5s+fT6GhoXT69GnKzMxk702dOpXatWtnxNbVj5YtW1JUVBStX7+esrOzKTw8nLZs2UKDBw8mS0tLcnBwoLZt29LatWtp06ZN7E5UH4x9zulCq9VSfn4+NW3alN5//30i+mXtNGrUiGpra6lx48Y0cOBAyszMpPT0dPL19SUios8++4xGjx4tibVmCJcuXaKSkhIaPHgws5+J6u7XjRo1Ih8fH0pKSqLNmzeTVqslT09P+vjjj8nZ2Zk0Gg01bNhQtEe9DWjYsCFptVqSy+UUHR1NISEhpFar6eTJk+wzf/zjH2nGjBnUp08fIpKG/cXjiy++oEmTJhEA2rdvH126dIm+/PJLFncUomvXrkRE9MMPP9CiRYuorKyMRo0aRba2tm+62SaY8AveNHPjbUFkZKSIQVtaWgpfX1+Ym5vDz8+Psb50Zd156Uphtq/UwDO8AgMD0bVrV5Zlm5OTg5kzZzI5OSFjlK/Zx//IZDJs3ryZvS9F1mV0dDTatWsHuVwONzc3kayaEHPmzGHZnTxrsVWrVu909rEJJvw3uHbtGqvjqLv2ExISwHEc4uLi2Pu6n7lx4wZWrFjBsizS09NZjV3dzC21Wo1OnTrpZaP8r9G/f39wXF1tXD6ThVc3uHTpEhQKBdzd3VFVVcXOC90zISoqChzHYcaMGQCkkRF56dIlTJgwgWWN6Zbw2LRpE2QyGVasWMGk/Pz8/ES1Lmtra9GxY0c4Ozvj6dOnb7gHL8e9e/fQrl071m4eGo0GtbW1TKY3PDwcS5cuNThuz58/h7W1Ndzd3SWnYPCyzJv/5BzWZXyHhYUZrWSQcH08ePAA586dw88//4zLly+Lymu9bvaR8HPx8fGwsrJCmzZtUFRU9Ns1+ldA2L+7d+8iIyMDaWlpOHLkCMrKylBbW4t79+7B29sbcrkcISEhehKvZ8+ehYuLCyZNmoR79+6xzBEpZVKsWrUKq1evRuvWrVnNdGG9dKBOgaJ9+/awtrYW2ZpXr15Fly5dRBkHxoZw3MrLy/HgwQM8evSIZfnzeN31J5yXu3btgkKhgEqlQmVlpaRsab4txcXFiIyMxJQpUzBmzBicO3dOr9zdzz//LFLtk1I//hvs2rULNjY24DgOSqUSXl5eks3ISkxMhEKhwLBhw1i9caFqQW5ursiO2rVrF4YPH46OHTvCw8MDU6dOFd1dpZTlWVpayjIc5XI5zMzMRJnx/H3NkDKFVM7wc+fOieqkC9fIjRs3RLYiD12Vs7t378LPz49lIEt5nWm1Wmg0GkyYMAHm5uYiie+Kigr4+/vDycmJydiPHj1apIRW3zOlBj6LdeLEiXpzzcPDA5aWlli0aBFqa2uZXHF8fDwWLVqE8ePHIyoqCg8fPjRG018JPptzwIABor0jLS0N1tbW4Li6EmNCZYqnT58iPz8fx44dw7Vr10TnpLH3FKFty5cVWLJkCSsvA9SVDOCVKXr16iUqV1VQUIDz58/j8OHDuHnzpiTUGngIVTZGjBjB7tS8agG//p49e4bOnTuD4zh0795dpKy0ZcsWWFhYYPfu3UbpgyE8fvwY3t7ezJbk94A9e/Zg8eLFCA8PR/PmzWFvb4+NGzfqKUUdP34c3t7esLS0ZN+Hq6urJM7xsrIyREREIDg4GM+fP2evX7x4EevWrcOxY8fg7OzMlIr49cPP4+rqasydO1fkn1UoFNiyZYtR+lMf+H1vzpw5WLp0KXu9pKQEw4YNA8dx2LBhg2h/F9rdvCKOlKTpeQj3NLVazUp0ZGRksNd5O+zy5cuwsLBgpUx48GXkdNWsjb1f/rd4/Pgxdu7cCV9fXyxbtkxy9mV9eP78ObKysjBt2jR2zpmZmWH58uU4fvw4bGxs4Ofnx857Kdolhr7fqqoq+Pj4gOM47Nixo97fzcvLg729Pdzd3UVno1RRW1uLoUOHQqlUipRxeft5586dePjwIfP/dezYEevWrWP+S35/4p/1toFvc1FREU6ePImwsDDMmzcPCQkJotKOUu1bYWEhJk6cyMrAbtiwgb2n22ahMsX8+fPfdFNNMEEEE5GiHuzZswdyuRyzZs1ir8XGxsLCwgIcx6Fbt2569XHVajVcXFzg4uIiuSC8bgCvtLQU7dq10zO2Hz58KCJT8BL0NTU1mDdvHoKCgjB//nzRxVJqG7OwPTt37mSXkJkzZ4oOWOF3snDhQnTt2hWenp6YPHmypGQaTTBBSsjOzgbHcejUqZNeMAWoKxfEcRxzGBvaH3iyRWBgIDNgjx49Wq/0GO+gexMQ7gs8gUxIptB9r1u3buA4DkOGDBEZrACwfv16yGQySUjvCy96V69exdixY5mErbAecGZmJtq0aQOZTMacNzyJgn/G4cOHIZfLMXbsWFRVVUnmEsmPXW5uLtv3fX19RZ+5evUqHBwcWCDGz89P77w+dOiQSNJdav0D6vp44cIFHDhwADk5Ocyh+2vaKlybcXFxcHBwQKtWrUTn5JuCsC3r169nkrxCmU1hebFX2R26JAonJyfY2dm9cUKWofasW7eOkX34n759+yIqKgo1NTU4deoUk0D19PTExo0bcfr0acTHx6NLly4ihxtP4jpx4oRR+qWLy5cvg+M4tGzZEs2aNWMXXV1yTm1tLXbs2AGZTKYn6yt03Bh77QnX3I4dO+Dn58dk2Fu2bIl169bh1q1b7DOvaq/uvHR0dISDgwOuXr362zf+NwDfXmH5J12o1Wr07t0bSqXSYEmatx05OTnYtWsXMjMzJVnOgw9Y8/VmdUnj+/fvx/jx46FUKqFQKDBkyBCUlJQAqAvklJaWory8XDTXpXSvq6ysRN++fWFmZoYZM2bg9u3buHnzJvbv349BgwaB4zg4Ojqyc/zRo0eYO3cuLCws4Orqiu3btxudTHH37l1wHAcbGxsRcYxfV7xdLdwLDZUPXbZsmZ6jUUrQnTcvXryAq6srOnfuLHp9yZIlkMlkyMjIQE5ODqysrMBxHIYPH478/HyjkTnrQ3p6uh5xjsf8+fPBcZyoNnN1dTW8vLwYuUyr1WL37t1YsWLFG2rxb4Nly5ZBLpfr2RejR4+Gg4MDQkJCoFQq0bFjR2zfvv2ldzVjn+U8SktL4ePjg2nTpqFt27aszcJgysvIFFJGRUUFevXqBYVCgeHDh2PHjh2YPXu2XhnKZ8+eMVvS3d0doaGh7PxwcXFBfn6+kXoghm7ZkZKSEowePVrPjl+zZg0cHBzqJVNcu3YNKSkpCA0Nxa5du/DgwYM304HXgEajYXNvyZIlWL58OQDxfZaXnheSKYTzNSUlBStXrkRUVJSIVC2lc5wnLnEch7Vr17LXd+/ezQJoMTExzDbhcfbsWTg7O6Njx44sgUcqENpM165dw6FDhxhJycnJiZEp+L0vLi4OHFdXypa3Sc6cOQNPT084OjqKElbeJbwNZQzrQ1ZWFqKioliJKg8PD1a6iy81IzXw9lNVVRWuXr2KrKwsRtZMSkpCs2bNMHToUJHNAvwyLs+fP4e9vT18fX0lc27z0I1nabVa1NbWYtKkSaKyhHw/g4KCkJeXB6DursqXs1coFPDw8BCV+X2b8ao+SL2PBQUFmDx5MhQKBTp37oyff/6Zvae7XyQnJ7MxlOKd3ITfD0xEinpw+/Zt5ugWBpn4C3OzZs2wfv16qNVqFBUVISUlhV1Kdu7cacSW60N46OzZswdz587F4sWL4eTkhNWrV0Or1Yo+IyRTtGnTBj/99JPBZwHSMIZe1Yb4+HhmAIWFhYkyXoTZWo8ePUJ5ebnocmKCCSaIcenSJbi4uGDIkCEi5zS/DnkVhnnz5tW7ljQaDVq1agUvLy+D7xt7X+HbXVtbi6CgIOaoF146MjIy4O7uzrJbeGUH3lhVq9Vo164dWrRooadcZCwI97tr165h7NixkMlk8PHxETkK165di2bNmkEmk2HBggWiunOnT59G7969YWFhIcqulAKE51NxcTFzPvXv35+9/vz5c6xatQp2dnZQKBRYuHCh6BmnT5+Gl5cXzM3NJeU81Q3Eu7m5sQuhra0tevfurade8LrP44kG1tbWLJP5TUJ4wVu0aBGzPRYuXIgff/yRqYDxNTt51LdPGApW29jYGKVvuli8eDE4joOLiwvmz5+PpUuXwsvLC9bW1pDL5Zg7dy5qamqgVquZI1/4Y25ujqioKPa84cOHw9bWVlIOxp07d7K5OWrUKPa67njl5OTAzs4OHh4eKC0t1bvoG/viL/z7CxcuBMdxsLCwQM+ePVmQjOM4DB48WE9JyRCkPC9fBeF3cezYMYwcORI7d+7E2rVrWUa51LIgf08oLy9Hjx490L59e/ZaVlYWZs+ezeZp69atmQM4ODgYgHhcjW131YctW7YwkkFFRYXovZqaGmajeXh4MIdpcXExU9Tq2rWr0TPHi4qKMHXqVHAcB2dnZz1lipycHHAchy5duqCoqAjh4eEG1bJ2794NjuMwd+5c9rtSgdD+OnToEHJycljShoODAyO6xMXFQSaTYdKkScy5Hx0dzeapp6cnVq5cKZm7OD8Wq1atEqlZ1dTUoLq6mmU58gSm2tpadj4sXrwYz58/R1FREczNzaFQKPDgwQNJrjVd/05NTQ369+8PGxsbkbOaJ/js3r0beXl5GD58ODiOQ+fOnbF582Z2L5RiH4G6pCiO42BlZQVra2tR5rhwPemSKaRCVuVhyF7iExnmz58vuu8Jwa+rFy9eoF+/fiL7UipKDefPn2ftrKmpYX0NCQlhe4SwnRqNBpGRkYxMsWHDBtFare+7kBLOnz8PjqtT/I2MjBS9l5eXJyJTvA7RzNjrz5ACJ08k4DgOa9asYa/zhHD+nE9NTcXt27eRmJgIT09PSfrWhetv6dKljKgql8uZurGTkxNOnz7NPnft2jW0aNEC7dq1w4IFC7B+/XoWa4iNjTVGN94IpGSnvC5018/t27cRHx8PT09Pdr/19fXFs2fPjNRCwxAqE40cORL29vawsLCAi4sL1Go18vPzMWDAALbWDCWORkREgOM4rF69GoA0xy8lJUWkiHv16lXMmTMH9+/fx9OnT9G7d2+oVCqcOXOGfSY7OxuWlpYYOHAgOnbsyBQUpYDfimwkxbF6XRQWFmL8+PGQy+Xw8vJ6qYJNamqqJGwVE37fMBEpXoLt27eD4zjMmTNHtIBXrFgBR0dHZvTxsq+WlpaSLnfBS6MJf/iMYkCcaSYkU7i6ujJlCkBa/eLbrNFokJWVhbi4OMTExODEiROiwAIfKDJEppCC3L4JJrwt0Gq1yM3NZcH1uLg43L59m72fk5MDFxcXuLq66ik0CMt8uLq6wtnZ2WAAzZjQJZWdOXOGEbGEZIrnz59jy5YtaNu2LZo1a4YJEybg4sWLuH79OpKTk+Hh4QGO+6XEibEhPMOSkpIQEhLCsv7NzMzQt29fEXFg2bJlUCgUMDMzg4+PD2bMmIFp06ax827Tpk3G6Ea90O3fpk2b4OnpyZQ1Bg0axN6/desW5s+fD1tbWygUCgQEBGD58uUIDQ2Fvb29pIOCYWFhjGiwePFi/Pjjj0yqUalUIiUl5ZUBCGMGdF+21hMTE5mjUNiWkpISZo9xHIfo6Oh6nyGVYLWhfqakpLDgGF/WCKgLsiUmJqJ169bgOA4rV65kMuDHjx/H4sWLERwcjA0bNoiIvZs3b0azZs0wfPhwEdlJCkhOTmZrT7hX1NbWilQO3NzcJFlCRwg+yDds2DBRUPPnn3/G0KFDIZPJ4OfnV2/5OEA68/K/hUajwahRo0T3CHt7e5ET2NgO/N8jKisrGeFszJgxCA4OZgQXW1tbbNu2DQ8ePEBOTg4cHR3h7Oz81mTyjBs3DkqlkjnNeBuN/291dTUrcbRu3Tr2enFxMcLCwiTjbCsqKkJoaKhBMgUATJ48GXK5HL169WLnoK5SzcaNGyWdCQmAEVj4czo2NhY//PADXrx4gWvXrqFDhw5wc3MTlfrgA2ydO3eGQqHA1q1bjdV8PWzYsAEODg6ws7PDmjVr9EqD8SohPJmud+/ejEQhzK7u06cP5HK5pLLheQj37KioKDY3eeU6/t9JSUkwNzdHUFAQ7t+/DwA4ceIEO+ubNWuGnj176hGepIZly5axNi9btgzl5eXsPUNkCjMzM7i5ueHUqVPGaK4IvAKWIRvT19cXLVu2ZPNO9y5QU1OD9PR0FpQuKSlBamoqNmzYgP3799eruvIm8fz5c7i7u2P16tWs/byv7/Hjx+ycE5YmBn4hUzRv3hx2dnaIiopCYWEhUlNTsWnTpleWDXrT0PU9Pnv2DNHR0bCysoJCoRARDQB9MgUAnDp1CqtXr5bEuNWHCxcuiP4tJFOsWrWKvb5p0ya0b9+erUtevl2pVIru41LyFwF1iQ38ve7kyZPIzc3FxYsXmZ3s4ODAyBTPnj3D3Llz4eDgwEgzSqVSdKeVWv9MEKOmpga7du3CsGHDYGFhoafYKgVoNBp4e3uD4zi4ubmhe/fujDx4+vRppKenM//fyJEjkZSUhGfPnuHp06eIiIhA8+bN4erqys54qYEnXo0dO1ZEZOGVizIyMkRkLX5s4uPjRbYaD2OPnfAs+C3UKY3dn/8GRUVFGDduHORyOfr06fPWlAMy4feJ3zWRwlAAXejkLSgoQM+ePWFvb8+yXHicOHEC4eHh8PHxQb9+/bBixQrRxiyFxS7cSBMSEliwbP369ezizzPXeeiSKXinj52dnaQyHgEx6zIwMJBJg/I/PXr0EDljXkamMMEEE349YmJiWJCa3x9KSkpY1kivXr1w69YtPaPu1KlTMDMzw6hRo0TZJsaGboaBvb09rK2tYWFhwTIMVCoVI44UFxcjLi4OXbt2ZU5EvmSEUqkU7T9S6SNPqHN2dsbEiRMxYsQItG3blo2XkEwRFxeHoUOH6u2riYmJ7DNSOOuEWLx4MWQyGaytrREYGIjevXuzLNyBAweyccjNzcXOnTtFJRYsLCzg4eGBXbt2sedJqX/79u1j/dCVtuWzdoKCgl5a5sPYAV3+omvI/po8eTIsLCxY2TTdz/AlgXTrz/Iwdt8AiDIkdL//6dOnQy6XM6dabW0t+4xGo8G+ffugUqlY5sjLnr9y5UqoVCo4OTnpyXNKBUlJSZDJZHB0dDSYTXb8+HEoFApMmzbNCK0Tw1CGn1arRXFxMfr27QtbW1tGDBTOy0uXLrGM+B9//NHgs6UwL39LFBUVISEhAUuWLEF8fLwo40dK++XvDdevX2eBFo6rK68zZ84cUaY8UBfQtbOzeyuIFHx2v4ODA5ObN1RH/ciRI1AoFPD19ZVsiRLg5WSKAwcOMHvEzc1NRJoD6lTOXF1doVKpJCUBLvy+Dx48CI7j0L17d3bOFRcXM2d3fHw8ZDIZdu/eLXpGeHg4lEol1Gq1iJhtTMTGxjJ7ODo6GiqVClZWVnpkCt4uc3JyYqX+Fi5cKCJRaLVadOzYEa6urqKgvdTAk2CGDh0KoE5Wf9asWXjw4AEeP36MHj16oGXLljh79iz7nYsXL8LCwgL9+/eHi4uLpLI8dSFUJuD9YHK5XM8+Ee4x165dQ0BAAGxsbPR8gW8a/N1aKHvNo7i4GLa2tujatStqa2sN2jSFhYVMQaQ+G9PYKC4uhqWlJTiOQ1RUFJ4/fw4PDw9Wzu7JkycYOHBgvWSKtWvXwtHREdbW1uxu3rVrV8med8Iy0S9evEBMTAwsLCxeSabo3LkzS+qTknqicO3MnTvXYGJCfWQKtVqNmJgYDBs2DAEBAYiMjBSVi5PaeZ6fnw83Nze4uLgYLBvJJz40b96ckbCKioqQlpaGadOmISoqSqR0I7X+mSAGb+totVocOHAAHMdh/Pjxkhk3vh1HjhyBjY0NwsLC8PTpU9TW1mLGjBmMTKFWq5Geng5vb2+mrtG2bVuWNCbFEvVCnDlzhiWeBAcHM/uS7/+qVavAceISQmq1Gu7u7mjTpg1u3LjBXpeKbxao2y/s7Oxw8ODB//gZwrn4+PFjSfXvdWEiU5jwtuB3TaTgsX37dmag64I3gmbNmgWNRqO3gIW17XhIYZHrtmH+/Plo1aqV6PDg68BzHIdFixax14WXrwcPHmDSpEmSq8fKHwzl5eUsgDRkyBDs3LkTq1evxrBhw1jfeGkqoE5ymidTLFu2TC+zxAQTTHh9nDt3jknY+vn5sYBeQUEBk47z9PTE3r17mYTvyZMn0adPH3Ach3379hmx9fWDzzDw9/fHsWPHcPv2bZw4cQKDBw9myhR8VlBFRQUeP36MsLAwjBs3Dt7e3oiIiBDJORrrTNA1oPfv38/GSngWnD9/HrNmzQLHcejZs6fIKVNeXo7s7GxkZGTg+vXrIgKaMc86Q4F4vm5eQEAACzSUlJTgxo0brG6pj4+P6Ht58uQJDh8+jPj4eJw7dw53795l70nhLAd+GUd+jHQdoDw7PyAgAPn5+cjPz2drsT7pdmMEdI8ePYquXbuytSNsT0lJCZycnODq6oqKigoRwUrYB97RLyzxofssYwWrT506hR49eoic3HzbS0pK4OHhgVatWrFsTd31+fz5c8yZMwcc94t0Ow+NRsOcj3Z2duA4Dh07dpS0swOoI7/IZDJYWlpi5cqVePbsGaqrq3HixAn069cPZmZmSEtLM2ob1Wo1oqKiDDrab9++DWtrawwfPlz0unDsTp8+jZYtW4LjOFbiiYcU5uVviZftiW+jw+ZtxalTpxAXF4f58+cjPT2dkefy8/OxdetWREVFITc3lwVt+XE7efIklEolgoKCJCd3bmhuVVVVMcWlHTt21Pu7eXl5sLe3h7u7O548efK/bOZ/jcLCQhGZQqj4uHbtWqhUKigUCsyYMQMHDhzAjRs3sHv3blZKTioqZ7pIT09HXFwclEqlQaJHbW0tUw4Rkq8yMzPRqlUr9OjRQ1SCxZj21969e8FxHLp164bHjx+jtrYWmzZtgpOTEyNTCLPAR44cCY6rK701c+ZMvedt2LABHMdh0qRJkilZAojt6Js3b0KlUsHHx0ekrsSTX9PT00W2F7/f82phmZmZomcb+zzQvSNUV1frkQuEZApdP6Cw/Tdu3JBE1j9PpFi/fr3o9ZqaGrx48QJt2rQRZUkbAl+apT6/pxQQHR3NAny8bRUeHs7WzqvIFJs2bULHjh0hl8vh6uoquu8aG8J5xSc2CMv1vYpMkZ+fD1dXVzRr1gzNmjV7qULfm4bumuNLveomlgBiMoXQTwvUrVUplpLWxeXLl2FmZoZZs2YB+KWNwn2GV3d2cnJ6qaKNFPtnQv0oKSlB+/bt0bVrV6Of6bprZceOHWjevLkosQMA8y/wyhS3bt1CbGwsPDw80L59e3h5eWHhwoVGJwy+DPz+mZWVxVT3hGQKoI5IwnEcevfuje3btyMxMZH5AKVkPwvX/M6dO1mbf02Z3vqet23bNowbN05E1HubICRT+Pj4SK6UtAkmACYiBas3ysuhpqamirIIKioq0LFjR7i7u+PRo0cA3q5SELNmzcK4cePg6+vLlCdqampEWTyvIlMIJZelZOjV1tbixx9/ZAoTQqdgcXGxSApcaMDv3r2bMbpXrVolqT6ZYMLbhNraWly6dIkRI/z8/Fg22YMHD5jsuVKpRJs2bdC7d29YWFjoOQ6M4XDjLz666//Bgwdwd3dH69atDWYY8KxuYZkPIX6rOnf/Ke7duyciAwixePFiyOVydpkX7vNPnz5lTh0vL69XZrgYy0mam5srKg0g/C/vZBQSDfjPFhYWon///kxB5VUwthNYiNraWpSXl8PDwwNOTk6iYAPvFPXz88Ply5dRVFQEKysr+Pn5obKykvXD2AFdjUbDJHk9PT3Z2uHbVVFRgbZt28LR0fGl6lc8AdTf399gIHDnzp1GCVaXl5dj4sSJBtePVqtFSUkJOnXqBHNz85dektVqNRQKBfr27Stan9XV1dixYwecnJwQEBCAZcuWSVZ2Uxc8mUKY9cI7iY1dIuj+/fuwt7eHXC7Hli1b9MgUWVlZkMlkGDhwoJ5zV7hH8OtQmAFj7DVnwruJ8PBwNmf5vTAvL8/g3BQG/zIyMuDl5QW5XC4K3ksB/F5XVVWFq1evIisri5Fvk5KS0KxZMwwdOlTP5uLX2PPnz2Fvbw9fX19Jnd31QVeZQkgqjo6OZgkCHMcxlTMbGxvJSoDzQTGVSoV27drplV/hsXDhQhYUvXfvHg4fPsxKYQiVzoyNhw8fYtSoUaJ7ClCnwickU/BlOkpKStC3b19wHIcWLVrgwoULyMnJwdOnT1mWobOzs+SUPXnExsZi3rx5MDMzw6FDhwBAz3ZcunSpXna5Wq2Gm5sb2rdvLyqhY+y5ye8nFRUViIyMxMiRI9GtWzd4e3tj165dojP4dckUxoSwHfz9prS0VI9UPX/+fHAch5kzZ+oRP/g7L29DR0RE/I9b/eshtJliYmJgZmbGSqfx5et4u/9lZIrq6mpcu3YNp06dkgQBhofufrhlyxaWAf5ryBSPHj3C2bNnRXcJY/syhX9/8+bNGDBgAFME4bi6MhYxMTGi3xGSKYTz0ZCaihRx7NgxcByHUaNGoaqqSvQdCM9AvlxX8+bN61VcNOHtgFBFslOnTmjdurVRkzL5tVJZWYnU1FRER0cjKCgI/fv3Z58R+kp4MoVSqWR+isrKSlRUVIjKL0sBumtEqAgC1E+mKCgowLhx4xgZjye5bt68mT1LSv28cuUKpk+fDjc3t/+4FKEuKcPe3h5mZmZMye9tRFFREfOr+fv7S7oErAm/T/zuiRQ3b95EQkICOnfujGbNmkEmk6FXr16szhkArF69mgXr3yYUFhayzEW5XI6AgABRdhJ/iLwOmQKQ1qEDgAWWOnXqxDZXXVZobGwsyyoRMtK3b98ONzc3yWdzmmCCFPCyC3p1dTUuXrxokEzx+PFjbNmyBX5+frC3t4dKpcKoUaOwd+/e13r2/wqnTp3CsGHDDLKus7OzRXLz/F4p3A8nTZrEMgx4xz5/UTGmM+PixYswMzNDcHAwI/4BdZeP6upqph5y/fp1URkrHgUFBRg9ejRkMhl8fHxE0tJS2P/PnTsHjuMwYcIEvfZUVlaib9++sLa2xvPnzwH8Mhb85evRo0dMOnvAgAHsGVLLzK0Pfn5+cHBwYIoGK1asYGvuypUrAOrOfXt7e3Tt2tXgM4xFNADqghNDhgwBx3Ho0qWLHpmCL5Gwdu1aPflrfgwfPHgAhUKBoUOH6tkou3btgo2NDezs7IwSrM7OzsbkyZPBcXXS5rpkpDlz5kAulzPygCGJ+vv378Pa2hq9evUymOXCZ5i8bU64PXv2MDJFmzZtcOzYMTZnAePtm1VVVVi+fDmcnJxgb2+vp0yRn5+PNm3aQKVSsaCZsK383sGXneEz04SIjY1Fy5Yt3ykShfA70CW2mfC/Ax/M7Ny5M2JjY7Fs2TKkpaUZLBdw4cIFeHh4YOTIkZg+fTqry60bHDY2hGUaR44cCXt7e1hYWLASR/n5+UzhLCQkBPfu3dN7RkREhCiz9W2Yi7pkCqFdnJ2djW3btiEoKAgBAQHYuHGjqJyVlIJmQF1wd/DgwbC2tgbHcdi4caNBRamffvoJ5ubmTFmJJ4kICXVSGTver1BWVoYJEyaw81yXTMETfoqLi5lqHa/CpFQqwXEc2rdvL1l/w9mzZ8FxHOzt7dGyZUtcuHABgL5dzEuZ+/j4IDExESkpKSxQKqUsT+F+wpN0HBwc0KZNG3AcB4VCAR8fHyQnJ7Pf4YmQhsgUUoHQ5quqqoK9vT04jhPJXl+6dAmdO3eGjY0N1q5di8LCQgBif96sWbMgk8kkVQ5CCH79BwQEsDHhlVD4Ocl/Fy8jU0gNwj0zOjoafn5+6NSpE/O/Ojo6ikhKryJT1PdsY4NXcm7bti2WLFmC6dOnIzAwkPUzNjZW9PmXKVNIHY8ePUKbNm3QtWtXZoPprlMACA4OZvNYpVKx+IJUzjoTfh2qqqowduxYcBwHb29vRvJ60+DXfVlZGUsU4u0PKysrkUJifWQK3TJRUpyThhR+6yNT8P6/7OxsbNmyBX369MHChQslWx5i+fLlsLe3h6urK/z9/QH8eiKZoaQNY/nBfmsUFBQgJCTkPyaYmGDC/xK/KyLFyzbO3NxcpKWlMQlRCwsL9OnTB2lpacjJyYGtrS06d+4smdqdr4vLly+jQ4cO7JIhlPsTMg+FZApeuULqyMnJYUxgwPDB8+TJE4wePRocxyE1NVX0njCr1wQTTDAM4aVw//79WLVqFaZOnYq9e/eygFNVVVW9ZAp+3y0oKMCTJ09ExrwxjNmSkhL06NEDHMchMjKSvc7vhZmZmawPupcj/rvQaDTo2bMnuxTzpQqMjT179sDCwgKjRo1icrxCzJ49Gxz3Sz1VQxcmXl7OwsICPXr0EF0+jAmtVovU1FRwHIeRI0eK+ldbW4uamhr4+/uD4zhWc1TYP/58SEhIYCx1Ly8vyV0ahcEHYeaDRqNhQfply5YhPDxcj0QB/JKZ6+zsLFLXAuoCAA4ODrC3tzfaBaugoIDJe3fu3Fm0dvbv3w97e3t06NABJ0+eZHuF8GxPSUkRZf7z39GTJ08wa9YsODg4GFSSeVO4fv06Y9Drkin27NnDHMMnT54EYLj8jlwuR3h4uMHnGwpOvS1ITExkZAph9pmxZFH580ej0SAiIgL29vaws7PTI1MEBwczJRv+9erqatEYbNy4EWZmZnrykw8ePICdnR1kMhmuX7/+Bnqlj996rgjP7bS0NCQlJRld2vb3gJ9++gkKhQJ+fn5sjxMq+mRnZyMzM5MRRBMTE2FlZcXudh06dMCuXbvY86TkTNRoNPD29gbHcXBzc0P37t1FEsTp6elwdXVl539SUhKePXuGp0+fIiIiAs2bN4erq+tbo9LD42XKFDyMrXKmC+F+Isz2PnfuHAICAmBmZoaePXvWW9t+165d8PX1hYuLC4YNG4aUlBSDn5MCtFotCwz26dOHOfSFZIrVq1eLvoetW7fihx9+gKenJ4YPH461a9cyEp4UUVpaiq1btzISzNixY9l7wvG4f/8+goKCGPmFJyUYO8szMjJSrzyYRqOBr68vZDIZZs+ejYKCAhQXFyM9PR2jRo1i9n9WVhb7neXLl4PjODRr1kxSxJD6wJcM4DiO1VSvqqpCdHQ0VCoVbG1tMW/ePJHdsWXLFlhbW8PDw4ORLKSIqqoqHD16FPHx8di2bRuzG9esWaNHkBeSKXr06CH54JGQaDB79mxMnz4dPj4+kMvlsLa2FinoCskUlpaW9d4LpIJDhw5BJpNhwIABonlXUlIiIkzoKlPs2rWLvfefytr/r/CyM6mkpIT5mCdPnsxer66uFv3e5MmT0aVLF4wYMQIcx6F///4mH/RbjEePHqF///7o3r270QO8Go0GgwYNYnfU4OBgduccNWqUaB0aIlNwHCdKmpIa+P1S6DPQJVNcuHCBEQuDg4NFa0s3NiQlG7OmpgZxcXGwtbUFx3Fwd3c3SMh6GX4Pypdvi0KRCb8//G6IFMINKT09HQkJCdi8eTOuX78uOlj4YA2fccxxHKZOncqcOlJlqutCGIS5cuUKc0ANHTpUdHkSfu7o0aOsz8L6pVIFT6To3LnzS2W1eOm8JUuWAHj7MjlNMMFYEDrFeGOWd2jY29tj3LhxzIFoiExhKHPQ2KitrcWpU6ewcOFCJgMnDDi/ePECHh4eaNeuHbsgCfcMvs7uiBEjWLaZQqHAixcvJBHcVKvVrD8HDx4UBap5hZ7OnTvrXf54QzU3NxetWrViZ57QyW1sVFZW4syZM+ySdPjwYdHYbNu2DRzHMTURQP/CdfLkScjlcjg7O4PjOElJnAsvRHw5CCGuXbsmCoz5+/vrZWDxpEj+O+D7XVJSgmnTpoHjOKNcsITj9Pz5c5Zh7OHhweZocXExa6OHhwf2798vuhCfPXsWvXr1gq2trSg7l8fVq1dZdqgxoUumEJKReOlluVyOn376STTGZ86cQc+ePaFUKkVBqHcJQjLFihUr2OvGsst4AkB1dTU2bNgAR0dH2NvbY9OmTcxWfvToEcu8DQoKwpMnT0TPOHv2LFxdXestB3X06FGjkXuE32tJSQnu3r2LK1eu4NmzZ4yM9mscS8LPxsXFwcrKCn369BHVpzXhfwM+4M7fz/ixSE5OZpnwfPY1T657+PAhTpw4gbNnz4rsMak4E/l2HDlyBDY2NggLC8PTp09RW1vLyqhZWVlBrVYjPT0d3t7ejAjJlwniuDppdClnI78MumQKIRlLyvfVxYsXw8fHRxSMPnfuHFOd8vPzE5UeEDpDX7x4gefPn+sRYqWIgoICTJgwAWZmZujduzcrjadLptAlS/BnuxTuBYagezZs376d3WmWL19u8HMXL17E2rVr0b17d8yZM4eVAQGMM378vXTmzJkoLS1l3/X+/fvBcRzGjRvHFNx4dO3aFRYWFggLC0NNTY3IJ8YTlO3t7fXsb6lA+D3z/ec4jpFJNBoNoqKiWBKVvb09hgwZwu50KpVKcnvlq+ZOfHy8QTKFsMwHXzqwX79+kiV2/vzzz4xoIFTJffjwIWJjY2FmZgYrKys9MsW2bduYooFwv5Ua1qxZA47jcOzYMQD6UvyJiYlsvgrLVAF1+6lU1bKAOt/IhQsXcODAAdy8eZNlvt+8eZMFQqdPn673DLVaDXt7e5ak6OnpCScnJ4OKqCa8PXj+/DmbA28aQlvq2rVrUKlUmDt3LrOnUlJS0K1bNxbDEu73wpjX1KlTwXGc0ckgQujavBEREVAoFFAqlaLkN13f3unTp9neMmbMGKbeKVX7iwdfkoW/y4wfP77e0ni6+D2QKEwwQcr4XRAphBvNypUrWQkPjqur1R0VFWXQ6D5y5AhCQkJEgYtWrVrh2bNnktqYdS8gpaWlepKMQjLF8OHDRdl2QjJFWloaNm7c+L9v9K/Ayw4SHx8f2NjYGAyG8d8BH1hauXLl/6yNJpjwLiMqKgocVyfHv3nzZlbLjeM4jBgxol4yxeDBgyVJptBqtWxfmTNnDry9vVkAtqKigjm1hwwZwvZX3VIYP/zwA7y8vDB69GisWrXqzXdCB7rnwIYNG8BxHEJDQ0V1xXmJzTFjxrDXhXss7+g4fPiwSOlAauAdnnPmzGHtv3TpEpP4093v+e9n9+7daNGiBdRqNX766ac33u76IByD5ORkjBs3Du3atYOvry8uX77MLs6bN29mzu7Zs2eLnpGeng4vLy8oFAqDSiL37t0zSmakcG4mJiZizpw5ohrwQmJPQUEBk0F1cHDAoEGDsGbNGsydO5ddNIVORqlCl0zBZwwCddLKHMfBzMwMgYGBWLRoEebNm4cWLVqA48Q1yN9FJCUlMRt8wYIFRmuHboAoNjYWHTt2ZMGGTZs2sRJJarUaHh4e4DgOnTp1QlJSEo4fP464uDj2+vbt20XPN3a9WWH/tm7dCm9vb+aMb9euHSZMmMCylV4nCKbrtFGpVLC2tjaqAszvAby94u/vD6VSyVQXTp48iZCQELaPdurUidXidnd3r5dgLoX7q+69bseOHWjevDlzfvLgs+Z4ZYpbt24hNjYWHh4eaN++Pby8vLBw4cK3PihRWFjI7M62bdtKiuDJQzhmT58+xbBhw1iwWhjcO3/+PCP26JIp6rvPS2FOvgxFRUUYN24c5HJ5vWSKNWvWiJQppFby6FV7/PPnzxEbGwtLS0u9UgK6PiUpZHkmJyczP57uGbR48WJwHIdLly6x14TlDXkSRUJCAubOnSsiW0RGRhpNPcoQXvXd8n3VJVOkp6cjJCQElpaW4DgOrq6uCA4OltydnJ9LGo0GR48exdq1a3HgwAE9BeBdu3Yxu3H16tVsTgpV6UaMGCFpe2TlypWQyWQsA1x3P9y9ezfMzMxgZ2cnUnp58eIFoqKi9MgHUgFfApVXhczKyoJWqzW4B/JldMzNzeu960iBVCdsw7p16+Dm5sZInLa2tujVqxfOnz8PoC6Iyyv6BAQEMDXr1NRUVl5o9+7dAOp8RxzHMVVCE0z4T1BWVoZ169Zh7dq1MDc3Z+VieKSlpbEEgJeRKYRlgKWEpKQkdofZsmULmjVrBnNzc4NkipqaGtTU1KBv375o3bo1OI6Dr6+vpAh1L1MTLSkpQWpqKpycnMBxdcrwuspLujCRKEwwwfj4XRApePBMWRcXF8yePRteXl6wsrKCg4MDIiIiRJlpPGpra3Hr1i388MMPcHV1xfr1643VfIMQbrAJCQmYPHkyXFxc4OHhgalTp+L48eOMVX/58uXXIlMYerYxoNVq2VhUVFRgy5YtCAkJwYoVK9hlcfPmzeC4OslaYdBPOIbTp0+HQqEwGa0mmPCaEK798vJyDBgwAJ6ensxAKysrw+nTpxnjefjw4XpkCr5MkpeXl2Qze+7fv4+2bduC4zgEBgayAEVubi4jigwbNkwv2/bMmTOwt7fXC2RL4fLPIz4+Hm5ubjA3N0doaCjL+s/KymLBFj8/P5GjUa1Ww8vLCy4uLiInlpT6BdTNz127dsHe3h4ymQxz585lc/ann35iTsUFCxbgwYMH7DzgM/5dXFxEY2rss074/S5ZsoS139zcHM7OzkhPTxeVyAkPD2dkCj4QHxoayrJihA44KYF3+DZv3hyTJ0+Gj48PqxEsJFMUFhYiPDxcRLbgHcFCyWUpzMuXXY6zs7NFZAphLdLIyEi2f/I/7u7uiI+PZ5+RQv9+Swj7s2/fPtZvoS36piAcryVLlsDMzAxKpRJt27Zljhh7e3ts3LiR7RXXrl1Dv379RGPGf07o4JZC4EzYhgULFrB1FxQUBB8fH7Rv356pF/BB0JfNN5PTxvjgz4Zu3bphyJAhcHBwYKSfhIQEto58fX1hZWUlWXIBfx7zmVjR0dEICgpC//792Wfqq+fMl0qqrKxERUWF0clKvyWKiopExBEpBTyF6z8lJQVLly6Fu7s7C9KOGzdOJMl+4cKFeskUb+t41UemiI6OZmSKyMhIEZlCKhDauCdPnsSGDRsQEBCABQsWiIipGo0G0dHRsLCw0CNTCP0qUhjDTZs2QSaTISwsDEBdsPnq1asAwNRs+FIsNTU1jESxePFivHjxAo8fP0br1q1hYWGhF4SSCvjvvKqqCnl5eThy5AguXLiAu3fvij63cOFCPTIFj4cPHyI3Nxfl5eUGSz4aE3z/ysrKEBAQwPYTS0tLPbUbQEymiIyMRHl5OaKjo7FlyxZoNBpJzEtD4EkFvAT/1atXRUQDHhqNBosWLYJMJkOrVq2wadMm0Xs8pHo3WLp0KTjul1Izhsbj1KlTbJyVSqWo3JgUwau+tGnTBosXL8aPP/7I/FuWlpZITk4GUHfmtWvXTu9+IJfLRQmKAwYMQNu2bY1y7zHh3YBWq8WiRYuYX8TV1ZWRkIW2c1pa2mspU0gBwj0tISGB+ZZ5osfmzZsNkimE+6K3tzf8/PwwZMgQScXrhPbXw4cPkZ2dDbVaLVICqaysxJ49e9C8efNXkilM93ETTJAG3mkihXDjKSwsRIcOHeDj48M2mvz8fKxfvx729vawt7fHmjVr9MgU/GZVWVkpuhxLwVgXtoE/UJVKJTw8PFitKJVKhR9//JEZbNnZ2SxAGBgYKDlDLiYmRi9LuKysDH379hUZpnZ2dozJPH78eBaEOHTokEh2OTo6GjY2NujduzceP378RvtigglvO7Zt24YTJ06w7FxAzAA+c+ZMvWSKrKwseHh4YN26dUZr/6tQU1ODc+fOMWJBQEAACz7k5OSw8g+9evXC2rVrkZ6ejri4OBbg3bNnD3uWMc+E+v52cnIyOnToADMzM4SGhrK+ZWZmon///uyS7+3tjf79+8PCwkKSGfGGgtVlZWVITU1ll445c+awczs1NZX1pXPnzhgxYgQmT57M2N5SzepZu3YtU3I5f/48CgoKcPHiRT1WfWVlJVJSUtC6dWuWYa5UKtGlSxeW9QIYz9lmaD7ygfMhQ4awjLHS0lLcuXOHZbYKyRTl5eUoKSnBvn37kJSUBLVaLXIeS8GRKLQxS0tL8fDhQxQWFopsxRs3bjAbRZdM8eDBA2RkZCAlJQXnzp1jRC7A+P17GUHkP9nrhP05efIkrl+/jqNHjxpdYpovdzR06FBcunQJ1dXVKC0txdKlS9G2bVtYW1tj48aNzK7UarWIj4/H8uXLMW7cOGzbtg0XLlxgzzP2uOli+/bt7GzjycZlZWW4ffs2goODWXYdH4AyNLYmp400UFxcjJEjR7J7UMuWLbF8+XJcvnwZwC/jNHz4cMhkMkkSKfg2lpWVMRuE4+pKxllZWeH48ePss/WRKYR7KCCN+/hvhcLCQkyZMgUbNmwwdlMYhN8vT4a0trZGz549WYkujuMwcuRIETFXSKYYOnSoiEzxtuJlyhQtW7YEx3HYuHGj0cm5Qgj372XLljHbWPjj5+eH9PR0VFdXv5RMIaV+8UqJfCk0Z2dnuLi44PHjx1i+fDk4jkNMTAxKS0tFJAqe2F9bW4uBAweC4zhJqhjw33VZWRmCg4PRqlUrNl5KpRJhYWE4e/Ys+7xQmUK4R0ppzIQQngX8nbp3796YMmUKvL29mdqIbqnhXbt2MWUA/vdcXFwk5880hHnz5oHjOFH5Jl3wKrocx6F169aSJxoAv5wRu3fvBsfVqZfyyRj8e/x4l5aWomXLlkzJzcXFRbJnA39vHThwoN4ewc+9oKAgFsS+f/8+9uzZg5CQEIwcORJhYWGszAnwy541evRolJaWvsmumPCOoaCgAJMmTWLn+dq1a9l7QtKjkEwREhIiyXubrtpZTk4OWrduDaVSiREjRhgkU0RERIiecfLkSZibm2P//v16ZXKNCWFboqKi0LVrV0Yks7e3R1BQEG7duoWamhrU1ta+NpkCqCuvqVKpTPdxE0wwEt4pIkV9TOsjR47g1q1bUCqVSE1NFb1XXFyMTZs2GSRT1CfLaOxNWRebNm0Cx9XVSz9//jw0Gg1yc3Oxbds2WFhYwNHREevWrWNOqezsbKZMMXToUMnIOqWkpLAMdj7rCAD8/Pwgk8kwatQobNq0CVOnTmXBo02bNqG2tpY5hO3t7dGnTx/MnDkTvr6+4DgOjo6Okqr/ZYIJbwMOHz4MjuPQokUL2NraYt++fQDExlxtbe1LyRRC8pKx982XBbguXLjALsUBAQEsO+n27dvw9/eHnZ2dyOmoW8PUmNCtc1xQUCDKZExNTRWRKfLz8wHUyaAuXLgQHTt2hLW1NZycnODj44PExET2u8YeM0Dcv6qqKuYs4y+KwkvH7Nmz2esnT57ExIkTGXlCLpejW7du2LlzJ3ueFPrH4+LFi3B2dkanTp30SqpkZ2cjISEB48aNw+rVq9n8LCgoQEZGBuLj43HhwgVRVp0xArp37tzRy8bkv+O5c+eC4zhkZmYabN+YMWMYmYJXT6kPUhg34byMi4vD4MGD4eDgAJVKBVdXV6xatYrN1Vu3bonIFMK64oZg7P4Jx6a2thbPnj3D06dP9dTa/pPn7dixA61bt8aYMWOMnh1ZXV3N9neeSCCUmU5NTUWnTp1gY2ODqKioV2YaS41EodFo4OfnBxsbG9Y/3TZOmjQJHMehZ8+eBvtnIlEYBw8fPkRWVhYyMjJw/fp1EZkuLS0NBw8eNHh3y8zMhL29PXx8fFBSUmL0vcQQNBoNy8wdNGgQgoODYWdnx+55Qll9Q2QKjuOYNPq7iPLycvb/UtpTdu3axWxkYVDp6NGjjAw5YsQIPWWKgIAAdrcvLCw0RtN/U9RHpti4cSNcXV1fab8YC7ykfp8+fZCWlobMzExRkKVr1644efIktFotysrKEBMTAwsLC1haWiI8PNzYzTeImTNnMvve3Nwc4eHhjMjv6OiI9u3bo0OHDnokCqDOzvL09JRkEJ7ft8vLy9ndtF+/fli2bBkmT56MVq1aQSaTwcfHR1Q2TkimMFTeTwoQnkkajQYjR46EQqHAkiVLmF2dlZXF7q2enp6shAKPpKQktGzZEvb29nB1dTU6IVeIl525PNHAy8tLr818358/f45WrVqhS5cukMvl6Ny5s54yh7Ggex7xylBC8Pv9uHHjmC9CeHfYv38/U63gkwf48qjGsFcMZebz7eDLMOoSPSIjI9lZmJ+fj7y8vHr9zBqNBpWVlYiIiICTkxNatWqlpyhjggn/CQoLCzF58mQoFAp07NhRRNDSJVP06NEDHFdX7ldKJS+Ee8qaNWvg5uYGlUrFyAY8SZf3J/NkCp5o8ODBAxw4cAB9+vSBtbU1MjIy2POkdP/hz2aVSoWpU6cyBXmO49jYVVVVQaPRICUlhfk1FyxYYPAekJqaCjs7O1hbW5vu4yaYYCS8M0SKU6dOISgoSG8z4bPNPDw8oFKpmOEqPGCePHnySjKFVFFcXIyuXbuiRYsW9daI9Pf3R25uLp4/f84cNNeuXUObNm3AcdwrnfpvCnxtdIVCwcgUOTk5UKlUmDlzJhuTyspK7N69W0SmAOoMW95Q4Lg6GeOAgADcuXPHiL0ywYS3A7p73aNHjzB79myWZTVx4kS9Ehf87wnJFEOHDsXDhw9FnzG2Masra7tlyxZMmDABO3bsYPvm+fPn2f4hJFMUFxcjKysLERERWLJkCeLj40VOHWOeEcJ+RUdHw8fHB/b29lAqlZg8eTLLSE1OToa7uzsjUwgv8UVFRSwALqxPLoWzT9i/nTt3YsSIEWjRogV69OiB+fPn4/Hjx6iqqkJqaiocHR0ZmUKYyVVUVIQLFy4gJydHROyRQv+EOHLkiEgCtLq6GuXl5di0aROcnZ1ZBhavkMITYgzBGOvt4sWLMDMzQ2BgoGifqK2tRVVVFXx8fCCTydi64tvI22JarZaRHz08PFg2kxSz6YTfLy+pbG5ujh49eqB79+5snIYNG8YcoDk5OZgwYQIjUwgDgVKai8LvOzExERMnToSbmxurr71t27Zf9TzdQLxKpYKFhYUkapA/evQIVlZW6NGjBwDxXATqvovdu3fD2tqaqTIJ9xApzk0h7t+/D7lcjgEDBgAQ33v4tpeXl2PgwIGwsLBggRfdDELARKJ4k1i7di0rvcL/BAYGIikpiX2GH7+8vDw8ePAAQJ1tw2fyCtWypADh3Lt27RpUKhXmzp3LyFQpKSmvJUE8depUcBz3zpLjjW0rvwzjxo1Ds2bNWIa4cP/Lzs7G2LFjwXEcxowZIwr8nTt3Dt7e3oiKinrTTf6fQUim8Pb2Zkoqz58/N3LLDEOtVsPa2hrdu3dn+zc/1+7du4fJkyczkgW/n7x48QLbtm1jvhapBHOBX+aeVquFra0t5HI5FAoFC6I8efKE9Ukul+OHH34QqZUCv2SHBwcH6wWDpYCamhq234WFhYnW2/Hjx6FUKuHo6IjY2FhRYExYHlAqhLOkpCSDyl2HDx+GlZWV3hgcO3YMFhYWLNDUrVs3nDt3TvTM7OxsZGdnS4qcJRyjgoICPHjwQK9szIgRI2BmZoYZM2Yw/6TQ1tq7dy84jkNiYiIjP0lBpl7Yt5SUFPz444/w8vKCt7c3Vq9ezRLgsrOzGfnH399f5JtWq9Xo27cvVCoVrly5gqysLMhkMvTv3x/V1dVv/PzjSYBFRUWi12tra1FeXg4PDw84OTnhxYsX7D1+TPz8/HD58mUUFRXBysoKvr6+qKioEJWkTkhIgKurK5vH7du3x40bN95oH014t1FYWIjx48dDLpfDy8tLRKAT2t2pqano16+fZG3nVatWMZJZTEwM9u3bh0WLFjHlmqCgIHb/jo6OZkQLvqStMB4kNezZswccx8HX11e0H1ZXVzOV+JCQELbP8GQKlUoFjuMwffp00fNqa2sRFRUFDw8PSappmWDC7wXvBJGipKSEOa/Xr18vMsSysrLg5eXFNtwtW7aILmA8hGQKR0dHhIeHS4qxVx+uX78OjuMwd+5c0eu8oefv74/bt2/j3r17CAwMFMluXrlyBQkJCW+4xS+H0DkxYMAAhISEQKFQGLxsJCYmsgu+8JKhVquhVqtRUFBgkk4zwYTXgHBdPXz4kDmuHz9+jNmzZ8POzg4qlQpxcXEG11RtbS3Onj3LDF6pkLMAcd9WrFgBpVIpClBMmTIFGo0GNTU1emSKlwWrdZ/9piE8v3jHmb29Pfz8/KBSqUQOUaDOkBcqUxjKmnuZnP+bhqH+WVhYoH379rC3t4eHhwdzoPHZ48IyHy8Lckqhf7qIj49nWTw5OTnYtm0bhg4dyubp1KlTsWrVKlabVRhUMza0Wi32798PpVKJ0aNHixyi/HfNO7VPnjwJwHBQNzk5mRFGOnToINkLP4/o6GhGmBDaVkePHkWfPn0YsYwnhWRnZ2PixImQy+Xo0qWLXi1rY8NQuTi5XI527dqxcnEcx2HmzJksAPqyPVDqgfiSkhK0bdvWYEaq8LsYPnw4U2dav3695LJX60NBQQHMzc3h5eVl8H1+fPiA0oIFC9h7wv7v3LlTcmP3roInwLu6umL58uWYN28eBg0aBLlcDktLS1Ft4CtXrrD6yAEBATA3NwfHcaKAtZTOurKyMqxbtw5r166Fubm5XoApLS0NXbt2fSWZQioqiob2vv/m+xY+T61WS6o0S0lJCdzc3NC+fXs2FrW1taL+nj9/nmX/jx8/XqRMIQxUSWlO/jcoKirCxIkTmYNcqCQiNezYsUNEsNJqtewHqCNkDRkyBBzHYcaMGez3SktLERUVJclyeNXV1Rg6dCgUCgUjYVlYWLDs8YKCAvTr149lfCYnJ+Py5cu4f/8+Fi1aBGtrazg7O+vtQ8YGPyZPnz5F+/bt0b17d5SVlYk+4+XlBYVCgcWLF0Or1eqVhgsNDQXHcZII2sbExIDjOIwaNUpPbW/ZsmXgOA7p6enstbKyMri7u6NXr164fv06s7+6du0qyjaWGnQl3D09PaFSqeDo6Ii4uDgWJDt58iS6desGhUKB4OBgEUFJrVbD29sbrVq1Qk5ODtRqNbtHaDQao+2dhso7cVxdeTEzMzNwXF25Gb68QHp6Olt7CoUCAwcOhL+/PytDwJeuqqiogIWFBYKDg994f8rLy+Hu7s6UlAyVf/bz84ODgwO7065YsYKRKPi5XFhYCHt7e3Tt2lX0u6WlpZg5cyYcHR3RtWtXzJ8/X1JnugnvDoSxkz59+tRLppBqTOTy5ctQqVRwd3cXJVnU1NQgLy+P+b2EZIrjx4+jR48e6NatGwICAkSK81JJTuH3zSlTpkAmk+kp26xbtw4cV1fyNi8vD4WFhSwRsaysDElJSTAzMzNIQtZoNG+NL8IEE95VvBNECqBOkWLhwoUsE7K4uJhtpFeuXEHfvn0Z2/7ixYsGA0ZPnjzB1q1bYWVlJTn2fX24dOkSOI7DtGnT2GtCtizv3OdltdetWwdA/5CRyqED1BkEwcHBkMvlsLKygouLC3Oc6bZTSKZYvny5MZprggnvDObOnQt3d3dcuHCBGd+PHz/GnDlzYGFhgbZt2yIhIaFeMsXp06eRnJz8ppv9WuClGL29vXHgwAHs27cPa9euFQVra2trcf78eVGZD96oldIeKURCQgJrK7/f37lzh102dDNdXF1dYWZmhrlz574Vaj1xcXGsf7w8/cOHD/Vk6Kurq7Fnzx44ODgwyT8+QC+lsavPEfb48WNWx5kvR6JUKtG3b18RMYnPVhIGPaWA8vJynD9/nkkn//TTT4zIo9VqmSN/0KBB7Hd0xyc7O5s5t6VGFhFCq9WiuLgYffr0ga2tLXOoCcf24sWL8PPz0wtMXL9+nZUiS0lJedNNfy3wgXW+rr1Wq0Vubi5SUlKYIzQkJIR93tD6kjqJggcvSb969Wq9YAVPpo6JiYGDgwNTvdENBkgVZWVlcHFxQbNmzZCWllav3X/69Gmm5qOLHTt2wN7eHnZ2dpIbu3cNqamp4Li6etzC71qj0cDHxwfNmjVDQEAAy6zOyMjAoEGDIJPJYG9vj169eonsL6mdezw5iyd/8ApYQpKEsNTAy8gUxoaQqJmfn4+srCzcvHnzleV/6oNu+aP27dtj+vTpkulzSUkJOnfuDAsLCxFBAhCfe3ypUZ4UypMIpUTS/S1RUFCAkJAQyZI++Xk1btw4cBzHyJvC+cuPSXZ2NhQKBVq1aiUK+P2nJb3eBK5fv47t27cDAEJCQhiZgldNKSgoQFBQECPQm5ubMxvGzc1NUiUhhN8zUJcIppsoVVtby+4JixYtQkVFBVJSUmBlZSWSdQcgmSDLnTt34OXlBZlMhtGjR+Py5cvsPT4oz7ddq9Vi8ODBsLKywv79+1FdXY20tDQ4ODjAzMwMLi4u2LFjh2QDggAQFhbGCAR8GWVeVeTFixfQarX46aef2DjK5XIEBARg6NChLOmPJxqUlpZCoVBg4sSJRu5VHfi7gZ+fHzIzM1FRUYGrV68yUrmQrFVUVITZs2fD1dUV5ubmsLOzg6enJ+Li4tjz+O9qy5YtAN78+ZCdnc0SgIYNG8b8JjU1NaiqqmIJAMuWLUN4eLgeiQKoUyGyt7eHs7OzqHQQUKegXFJSwpJ1TDDhf4XXJVNIEUeOHIGZmZmoxI+QqFtSUsLU9kaMGMHs7OLiYpSVlYnWnTFslIKCAhGRUYiSkhJ06tQJ7dq1E9nzwljd1atXkZubi+bNm2PcuHHsM5WVlcyGFkJqdpgJJvxe8c4QKYBfLoZz5szBgAEDkJOTY5BMwUuNGbrYFxcXIzIy8lfLFxsLubm5MDMzg7e3N168eCHamIWGHh9sW7ZsGQDpOzMKCwsxceJEVgeLv1QALydTzJ8//0031QQT3gk8ffoUvXr1Asdx8PHxQVZWFjP6iouLMXfu3FeSKYSQkqF3/fp1tGzZEl26dHlpMIjPsr5w4QIjU/j5+b1SmcIY4C8ao0ePhkKh0HNuC3Hnzh3G8k5JSWGOg6lTp4okK6WG8vJy+Pv7w87O7qXExvv37zOyXXJyMpPDEwZ7pQDhmqioqEBpaSlbRxqNBunp6RgyZAhcXFzg6emJ1NRUvVqqmzdvhlwuF7HvjYX6MoM3bNjA5hd/4S0qKhJlq/IQ2iIJCQmwt7dHbm6uKENNisjJyYGVlRUCAwNFrwv7o1armZKDsI51dnY2kwKXGvLz89GpUyeoVCqDe2VmZibr04oVKww+Q0okCmFbqqurodFoRE6ljIwMqFQqdOjQAfv372fkCaHDY8mSJbC3t8f69eslRxR8FSk6IiICcrkcQUFBIuKcsH98xqgucSkvLw8BAQFo1qyZiUTxBhAaGgpzc3NkZmaKXuezIAcPHoz8/Hzk5+ez8mI1NTW4dOkS7t27JwriS8n+4lFQUIBJkyaxQCafvQro13PmyRQhISGSm3vC73bTpk3MnuKDm7rBlF/zvPj4eDg5OaFZs2aSCvICdUEvmUyGdevW6QWF+PHTleSfN2+eMZr6RiH1IAXwC5E8JibG4Pv8ePIKrlIlhhiC8PvnA59CMsWTJ0+QlpaGqVOnYtCgQRg5ciQ2b96sV4JSCigtLcXy5ctRUlKCq1evMhI5UGdb8sH3xYsXs32Gl0TnydVSnI+5ubksEDZ69Gh2X01JSUHbtm2Z4snKlSuhUCgwZ84c1r+CggKoVCrIZDJwHAdLS0vJkEQA8f597NgxKJVK+Pv74+zZs6isrMSmTZuY/Dx/Pmi1Wty+fRsLFy6ESqWCQqGApaUlunXrJiIa8GqMvE/amL7bvLw8dOrUCS1bttSTk+dJBv7+/sjJyRGRkgsKCnD9+nXk5uaKyuts3boVzZs3h4eHx39MQPxvwI/b9evX2d1USKYA6u5qfHIl3z/dc/nIkSOihEap+9dNeHchJFP4+Pjokeukii1bttR7hvHr9Ny5c2jVqhXMzc0RGBho8L7zptcer2wzc+ZMrF69WtQmYTl6T09PtGrVivmYDSnb3Lx5E5aWlujdu7fBvyXFO50JJvze8U4RKYC6gAp/gR81apSITHH16tXXIlPwGx0gjY3rZW3QaDSsLikvJa1LogDqau5KOQPSEAoKCjB58mQoFAp07twZP//8M3tP9ztJTk5mDHApXbBMMOFtwt27d1k5AW9v71eSKXQzeKWKtLQ0cBzHnDWGMgOuXLmCOXPmsIytCxcuMJlp4d4jJRQXF8PR0RFubm4A9KWWAeDZs2eYO3cuPD09WQZoQkICVCoVNm/e/Kab/KuQn58Pa2tr+Pv7A/hFjliIBw8eYNSoUfD09ERBQQGqqqpYiQgp1eQWzrndu3dj1KhR6NatGwIDA/UCRY8fPxbZITzOnDkDNzc3qFQqoytmPX/+HFOnTsXu3bv13tuzZw/LQgoJCWFM/WvXrjGSy/Dhw1FYWMiIJGq1Gl5eXlCpVHjw4AEbZ6lm8Vy8eBEKhYI5uetrJx/A2Lp1q8H3pWBjCnH+/HkRCYnfU4Rr78SJE0zWV3fuSolEIRyTPXv24IcffsCAAQMwbdo05tB9/Pgx5s+fD4VCAQ8PD+zatUtUmubs2bNwdXXF0KFDReRBKYybsH9paWmYPXs2AgICMGHCBNaH7OxsFsAYN26cqAQNUNe/zp07o0WLFgaVNtLS0iQnff4uorKyEu7u7mjbtq3Betz+/v64du0anj59io4dO2Ly5Mn1qhVI2ZFfWFjI7nUdO3YUOXp1yRR8mbXQ0FDJlNo0VHLMyckJw4cPR0BAABITE/Xa+rLxkNJ++SqcOHECTk5OsLOzw9GjRw1+JjY2FtbW1ti6dSvLxpZSqb93GS8rNbNv3z5wHAeVSsVIWPz7wvnp7e0NlUolmfI5rwvhWWiITMFDKvuIIWi1WgQFBYHjONy6dQtarRZdunSBi4sLs491SRTAL/fbJUuWGLH1r4aQTDFq1CgWkD579iyAuv2/S5cucHd3Z+rCQJ2N5ujoiMWLF2P//v2SIvkI186zZ8+wZ88eWFlZ4cKFC6LP7d69mykNLl68WEQoyMvLQ05ODu7cuSN6PTo6Gg4ODkYnGvAwpJACiDOrb926hZycHPj7+7MyLMLv6PDhw4iLi8P48eNhbW0NJycnoxIG6yNTCPe/zZs3M0UbXdW29PR0VmZHqABgggnGgrDkmL+//1vhpz1z5gwsLCwwYMAA9pru/lNcXIxOnToxUtPIkSNZyThj38f79u2LZs2aMZLx2rVrWSns6upqjB8/niUrCFV9hGqm+fn5sLKyQtu2bVFRUSHpe5wJJphQh8b0juEvf/kLLVmyhObNm0c//fQTAaDRo0fTt99+S3K5nKZNm0azZ8+mEydOEBHR+PHjSSaTUYMGDQgANWjQgJo2bcqe17BhQ2N1hYiIamtrqVGjRkREdO7cObp9+zaVlpbSl19+SR07dqQmTZqQu7s7paen0/nz5+m7776jYcOGkZmZGXvGmTNnKD4+nr744gv69ttvjdWVX42vvvqKxo4dSzU1NZSamkrr1q2jRo0akbOzMzVs2JC0Wi0bn86dO1OjRo2I4zj67LPPjNxyE0yQNvi9Tvj/AOjvf/87TZ06lbRaLR0/fpzmzZtHkydPJjMzM/rss88oICCAiIi2b99Oa9asIa1WS66urvTxxx8bszv1gu/b3bt3iYhYn/k9VYiSkhLasmUL3bp1i5ydncnMzIymTZtGhYWF1LZt2zfZ7NfGhx9+SJ9++ilptVoiIrYv8v0kImrcuDFVVFTQ1atXKT09nTp27EjdunUjS0tLdh4I54OUUFFRQRqNhh49ekRPnjwxuLd//fXXVFVVRVevXqWioiL66quvqH379mRubi6J8w51hFU255YuXUqRkZFERPTee+9RdnY2qdVq2rRpEykUCgJAn3/+ORERXbx4ke7cuUMWFhZ048YNioyMpLt379LMmTPJwsLCaH0iIqqqqqLMzExSq9X0wQcfUIcOHWjHjh307bffkoeHBzVt2pSWLVtGu3btIgAUGBhIHMdReHg4BQUF0cGDB+n69ev01Vdf0R//+EfKyMigkpISCgkJoa+//pr9HUNrVQr46KOP6KOPPqLMzEy6desWff/996J1xNtu/FjeuXNH9DoPY9uYuigrKyMioiZNmrDXhHuDVqsllUpFPXr0oF27dtH9+/eJ4zj2Ht+fnTt3UlhYGFVVVVFMTAz7zJuCVqtl3/OyZcsoIiKCvZeRkUHnzp2jqKgo+vLLL6lv376k0WgoMTGRZs2aRQkJCeTk5EQlJSW0f/9+evDgAfn7+9NHH33EnmHscRP2b+nSpbR+/Xqqra0lorq52adPH7KysiKZTEZBQUG0cOFCSklJoVu3blHz5s1JqVRSfn4+7dixg3Jzc2nGjBmiewM/l9u1a2eU/v2eAIDee+89+vDDD6mwsJCKiorok08+ofDwcAoPD6cWLVpQcHAwcRxHWVlZdPv2baqpqaGysjL6wx/+oPc8KZ7lPL788kvRvW7Dhg303nvvkbOzMzVu3JhqamqocePG1K5dO6qqqqLY2Fjy9vYW7UfGBP/dxsXF0YYNG6hNmzY0evRo4jiOSktLmS387Nkz0mg09NVXX9U7HlLaL18H/L6/du1amjBhAoWEhFDz5s3pT3/6ExERnT9/nrZt20bffPMN+fj4UIMGDSg0NJRycnLIxcXFyK1/tyG0K+7du0cajYaIiP79738TEVGHDh2oe/futHv3bgoPD6cxY8aQubk5Ef0yp0+fPk0XL16kVq1a0QcffCCan1JHo0aNWHvnzZtHREQJCQnk5+dHmzdvJqVSSVqtVjL7iCHU1taSmZkZHThwgGJiYigkJIRUKhVt3LiRhg4dSpWVlTR8+HAaNGiQ6M599OhRatCggdHvBK/CN998QwsWLKBJkyYx/+zIkSPJ2tqaiOrssuvXr9Pw4cPp008/Zb8XFRVFT548ITMzM3JzczNW8w2CXztz586lEydO0CeffEIcx5GlpSURETvPPD09qUGDBrRo0SJat24dNWjQgHx8fOjLL7+kv/71r+x5hw8fpsePH9OZM2fo0KFD1KRJE1q6dCl99dVXb7RfwrW/b98++u6776i6upqISOQj520UlUpFwcHB9N1339GiRYvoxIkT1KlTJyL65TtSq9U0bNgwIqq7+9rb29O0adOMek/nfSb//ve/afny5TR69Gg6fPgwTZs2jebMmUOff/45ubm5UUlJCa1fv562bt1K9+/fp++++44qKyspKSmJSkpKaMqUKeTs7Gy0fphgAo8vvviCgoODqWnTpjRw4ED68MMPjd2kV+Lbb7+lv//975SRkcHOiIYNG1JNTQ01bNiQGjZsSJ999hl99dVX1KRJEwJAaWlpREQ0ffp0+uKLL4xmrwAgR0dHys3NpcjISDp8+DCdPXuW+vXrR+7u7tS4cWNyc3OjPXv20MSJE4mIqHXr1jRmzBiSy+VEVLdHZmdnU0VFBbm4uND7779PAN54X0wwwYRfiTfN3PhfQsjeOn/+PLp37w6O4xAUFFSvMsXQoUNFyhRSgpBht2rVKianzHEcPD09RVliUVFRrAzGvHnzkJaWhtu3b2P37t3o2LEjOI4TycW9TXhZ3S9jsxBNMOFtg6HMaf41fh+8d+8eBg8ezJRuLly4IFKmmD9/PmQyGezs7CSVHVIf+GysGTNmoLq62uC+odFo4Orqivbt2xuUZpbaXlNdXY2SkhL069cPHMdh06ZN7D1dmTu+9npsbKzec6TWLyE0Gg369esHBwcHqNVqAOL5y2ew8tK2vOKIEMbo3/nz5w1mpyxfvhwcx6FHjx7Yv38/Ll68iFGjRoHjOCiVSqYyodVq8fDhQ1Zehi9dpVAoRCoixrRbHj9+jMmTJ8PMzAweHh7w8/MDx3FYuXIlNBoNKioqsH//fnTs2BFmZmYICQnBgwcPANQph02ZMgWdO3cGx3Gws7NDnz59kJCQwJ4vBZvsVXOHz/oYPnw4U92ora0V/V5UVBRkMhlTupE6Tp8+DY7j0LZtW4NKBPy48FK+vNKGcLy2bdsGlUolicxqvgZz165dkZCQgKNHj7ISVu3bt2fy3vfv38eOHTvYvYFfdy4uLqwGOyCNeSlEREQEOI7DoEGDcPToUdy4cQNHjhxBeXm56HNZWVkIDg5mEtP8j5OTk+hckFr/fk/48ccfWcmL+fPns4wyoVKIRqNBixYt0KVLl7ci06w+vG4951eVkDMGnj17hgEDBsDW1lZPGerIkSOYN28eXFxc4OLigsjISPaecG29TUoUgLi9/DxVKpUYPHgwwsPDERYWxtRAeSWwzMxMcByHsLAwI7X69wHh2Kxbtw6urq6wtbVFs2bNsHLlSnZHu3HjBnx9fcFxHHr27Iljx46xOXny5En4+PhAJpNh3759RunHbwHhd8ErU1hbW7P7g9Rx69Yt2NnZwcbGBjk5OSgpKWE2ScuWLfX2m61bt0KpVKJ3794sO1fq0C3zwatkpaenM1uGP9uio6Nhb28PT09PSffvhx9+AMdxsLGxQZ8+fZi/RPc+kJCQwJQpli5dKiojwavBcRwHc3NzDBo0CLdv337jfRGCt0NWr17NlOh69OgBjUaDNWvWGFRB5m1SXqZf2P/U1FQkJyfjwoULTCHzTcOQjcu/dv36dbi7u7P4AD8+xcXF2Lt3L1q3bs3u40qlEl26dBGpMkrZn2LC7wtSLPH0MmRmZkKpVEImkxksG3r69GlYWVkhPDwcly5dYv6j0aNHG0WxBxDvJXFxcaz9Hh4eOH36tOizfDkPhUKBNWvWiN47ffo0evbsCUtLy3rV3kwwwQTp4Z0iUujiVWQKHx8fcBwHLy8vkZya1LBs2TJwHIcuXbpgw4YN2Lx5M5MMEmL79u1o3749qyPI/5eX2eTxNjpKTWQKE0z4bTFz5kwEBQWxfxsiU/DBUR8fHz0yRUhIiMHAtbFgaB/gX7t16xZcXV2hVCqRnp4uek/4e56ennBxccHz58/fQItfD7r90t2/+fqczs7O2L9/P3tdKPsdHR0NjuOwd+/e/21j/wO8bP+uqalhlw9PT082LrrOqfDwcCgUCpw4ceJ/3t5XISMjA0qlEgMHDhRd7g4fPgylUglfX19RfdkzZ84wkqSVlRUuX74MACgpKcG+ffswZcoUDBgwAKGhoaILlhTOvfv372PRokWQyWSQy+V6BM/KykpGppDL5Zg2bRry8/MBABUVFXj+/DkuXryIvLw8UUkuKfRNSNjJy8uDWq3GoUOHkJ2dzZxr+fn5zMacOHGiHvHg7NmzcHNzg7Ozs6QCZK/6focNGwaZTIbVq1eLbGPhd8LbpboBimvXrqF9+/aQy+W4fv36b9vwX4kDBw7AwsKClfLjceTIEVhbW4PjOLi6urJ1WlNTg8rKShw+fBhJSUnIzMwUEQWlMC+FyMjIgKOjI3r27KlXs/revXs4ePAgQkND8fPPP+PFixeorKzExYsXsWLFCixatAipqamsVjkgvf69y7hx4wbS09Px888/M4ebWq1G27ZtGSnez89Pb1x5Gfdp06ahpqbmrbzP8XhdMoXU8OjRI7Rs2RJ+fn7stZycHEba4jgOlpaW7P917eS3jUTBQ7j/R0VFYcCAASJSlkqlEvV1+vTpMDc3l4Rd9nsAP/8sLCxYWRyO4zB58mR2jl28eBHDhw9n73l4eMDDw4P9W1gO723dW4Tra+rUqWxuVlZWSrpPfNt46e+IiAgAdfdtvsykq6srpkyZgoiICAwbNgwcx8HR0fGtSGgQQpdMcf36dZSXl7PgWMeOHdG7d++3pn81NTVYuHAhW0e6pHBdMkWrVq3AcRzmzJkjKuOYnJyMnTt3Qq1WG4VoINzjjx07BoVCgZ49ezICz9ChQ6FQKNjYDR48mN1ZefBECiF5XColGoXt0Gq1KC4uZnccfv1lZ2cbJFMAdaXJMjIyEB8fjwsXLojufCb72QQT/jvs3r2bldEZN24czp8/j7y8PBw4cICV0OF9uGq1mvlfJkyYYJQ9Rvg3Q0NDmf1lZWWFqKgoFBYWsvcfP36MuXPnsjMiNDQUa9asQVhYGBwdHcFxHLZs2fLG+2CCCSb853iniRTAy8kUV65cQZcuXbB+/Xojt7J+7Nu3D0qlEn5+fnpOlrKyMly7dg179+5lpIrs7GzExcUhKCgII0aMwMaNG1mdOuDtNvSETjcfHx9RbV0TTDDh9ZGbm8uMualTp7LXdckUt2/fZoo2/fv3R1ZWFgvQCy//xt5XhMZsbm4usrKycPv2bVFWLu9ktLa21svqAeoYwQqFAkOGDEFpaanR+wSI+5WWloZFixYhODhY5KzWaDRYvHgxcz4JHThAXTC3U6dOsLe3x9WrV99U018Lwv4dPnwYGzZswOTJk5GRkcGcG8+ePWNOm169erHMfx5nzpyBi4sLWrZsqRd0etPIycmBSqWCUqnEnj17RO/xmT269alHjhyJFi1aIDAwkJEpdOvr6gaVpDA3eYSEhDDiZrt27XDw4EHR+0IyhZmZGaZNm8ZUAAxBCo5u4fcbGRmJNm3aiAJG/fr1Q1JSEgDg+PHjzPnr4eGBvXv34sSJE4iPj2evCxUNjA3dvfLs2bPIyMjArVu32OsHDhyASqWCo6MjoqKi9LI9zp49izZt2sDd3R15eXmi9x49eoSoqCij1j3m59D06dNhZmYm2u81Gg0CAgKgUqmY4oubm9tL56TwmVLCrl27RGpz/LyNjY2Fh4cHC8jb2tpixowZIoewLqTYv3cVERERcHZ2Fu0pwcHBOHbsGEJDQ9GyZUsoFAqsWLECGo2G/V5GRgZ69eoFS0tLHDp0yIg9+O3wNt7rCgoKmPrCunXrMHv2bHTp0oVlm61btw5XrlxBQkICIyJXVFQAeHtJFDyE50dFRQUyMjKwf/9+HD9+HHfv3mXv8ZnyXl5eL913TPjPIZxLarUaNjY2GDx4MCPHJSUlsQD8+PHjWTC6pKQEERER8PDwgJOTE1q3bo2RI0fip59+MvjstxG6CirCjHljg7fnhW0UrqusrCyoVCo4ODjgxo0bAIDnz59j8uTJaNu2LTszrKys4OfnZ3TVgv8UQjLFyJEjkZ2dDbVajZ49ezLyixRUGYTg7SRD6kJVVVXsLs5xnGg96ZIpdu7cCXNzc0Zckpr9dezYMaxfvx7m5ubsPqrVarF37152H+rUqZMeSU6tVqNNmzZo1aqV5M404RrbsWMHAgMDYW9vD0dHRwQEBCA2NpYR+oVkioCAgFeqoUht/Eww4W1ETU0Nfv75Z0YsEP7IZDKR8m51dTXS09PRt29fdk6+Seja8r1790ZgYCAWLlwIlUoFa2trREREiPaOsrIybNq0iZFFeOVLNzc3xMfHG3y2CSaYIF00AN79IjwXLlyg0NBQunLlCrm6utLo0aPp22+/pYYNG4rqrkOCdeLnzp1LW7dupfXr15NKpWJt3LNnD6WmplJmZiaVl5eTtbU1BQUFkaOjIxHV1Vps0KCBqF7U21Tvsj48evSIFi9eTElJSdSiRQtasWLFW1H/ywQTjAlDe1t6ejpNmDCBiouLydPTk9WV5Wvt8vvFqVOnyNfXl4jq6u3OnDmTrKysJLNXCmsDb9myhXbs2EG3bt2iDz/8kLy8vKhr164kk8mIiGjy5MmUmJhIH3zwAYWEhJCFhQX985//pGPHjlFERASdO3eOwsLCqGPHjsbsEhGJ9+uVK1fSmjVrSKvVsvf79+9P48aNo/fff5/y8vIoKiqKYmNjiYjIy8uL/v3vf1NZWRnt3LmTcnNzKSQkhPr162eUvhiCsH+rVq1i/dNqtfT1119Tr169qHv37vTnP/+Znjx5QoMGDaIbN27Qd999R126dCEzMzN6+PAhxcTE0I0bN2jGjBnk7e1t1D4dPXqURo0aRS1btqTw8HAiIrp79y79/e9/px49etC9e/fowIED9PnnnxMR0YoVK2j16tVszg0fPpwOHTpEH3zwAUVERJCVlRWr6Sw1+wQAaTQaGj9+PNXU1NDHH39Me/fupb///e80YsQI8vDwYJ/VaDR05MgRWrFiBd29e5d69uxJw4cPf+N1f38tli5dSpGRkfTtt99St27dqKSkhHJzc+nAgQNEVLefDBo0iM6cOUMRERF04sQJ0e//v//3/2j06NFs3Rl7DIVrbt26dbRt2zZ68OABEdXVGO/Zsyd17NiRHBwcaMOGDRQZGUlERCqVinr37k1/+tOf6ObNm7Rhwwa6cuUKzZkzh3r06KH3d4R7srFQWlpKvXr1ovLyckpLS2PriB/TyMhIatGiBfXq1YuuXr1Kf/vb32jt2rX0j3/8w6jtrg+GvtNVq1bRypUradq0adS+fXtSq9V04MABNj/d3d2padOmdPHiRXr69CmtXbuWmjVrZozmm/D/g9/z//rXv1KrVq2oQYMG9PPPP1NhYSEpFArq1KkTXbt2jY4ePUparZaaNWtGFhYW9OLFC9qzZw+VlJTQtGnTyMfHx9hd+c0g1Xvdy/brnTt30rRp09i/P/vsM2rdujV5enqSvb09e93FxYU+/vhjio+Pp/fff5+9vmPHDlq5ciVVVlZSTEwMcRz3v+vIb4z6vpcrV65QSUkJHTp0iJKSkui9996jzZs30/fff2+EVr7bEI7Bo0eP6MaNGxQYGEgbN24kW1tb9rkjR47QqlWr6NKlS9S5c2caPHgwm2vFxcWk1WqpcePG9P7779MHH3xARO+Gr4hI2v0oLy+n+fPn0/fff08DBw7Ue3/BggUUFRVFkydPpoEDB1KDBg2oqqqKnjx5QtnZ2VRbW0vfffcdffnll/Txxx8boQe/DfLy8mjSpEl07tw56tixIwUEBNA333xD58+fpy+//JK+/PJL+vTTT43dTCIS22CVlZX07NkzatKkCdXU1NCXX35JRHXrctGiRbRx40Zq2rQpLV68mNq3b8/eA8Dm5K1btyS5NyYnJ9PEiRPpiy++oA8//JD27NnD7OeysjLasGEDxcfHU2VlJXl6elLnzp3p008/pUuXLlFERATdvHmTZs+eTT179jRyT36BcL9cvHgxrV+/nj7++GOSy+VUVFREjx49Io1GQyqVimbNmkV//vOf6caNGzRmzBi6ffs2OTs70+zZs+lPf/qTkXtiggnvPvLz82n//v1069Ytevr0KZmbm5O1tTU5OTkRkdhPXVVVJbKt3zQWLlxIGzduJE9PTxozZgx99dVXFBUVRRs2bKDKykoaOnQodevWjZ0RRETXrl2joqIiunnzJikUCvrzn//MfA9StltMMMEEMX4XRAoiMZnCxcWFxo4dS//+97+ZcWVsB7ch1NbWUmBgIJ07d46OHDlCH374IanVakpNTaW4uDgiIjIzM6OysjK6d+8eqVQqWrdu3Tu/ARcWFtKqVato4MCBkryEmGCClCC8/BcWFlLjxo2pQYMG9Nlnn1F6ejoFBwfTs2fP9MgUPBHr5s2b5OPjQ05OTrRv3z5avHixKEhqTAgNTv5y3KhRI+I4ju7cuUPV1dXk4eFBffv2JQsLCyIimj59OsXFxVGjRo3o/fffpz/96U907949atiwIU2cOJEGDRpERMYNegr/dlhYGK1du5a+++476t27N7333ns0b948qqmpod69e9OUKVPo/fffp8LCQtq/fz8tWrSIampqiIiocePG9Pnnn1NgYCD16dPH6P3iIWzDkiVLaN26dfSXv/yFvL296caNG3Tq1CmqqamhPn36UK9evej//u//6OnTpzR9+nTKyMigFy9esGf94Q9/oKCgIEkEq3/66ScaPXo0vf/++5SYmEgxMTGUnJxMycnJtGDBAjp06BBt3ryZLC0tKTExkUJCQqh9+/Y0duxY+uabb+jgwYM0YcIEKi8vpwYNGlDXrl1pwoQJjHhhbBj6bktKSkij0RAAioyMpNjYWPrHP/5BI0aMoE6dOrHP8WSK8PBwunnzJnl4eNCsWbMkETAzhAMHDtDo0aPJzs6OpkyZwshYRET+/v6UkZFBzZo1o3Xr1jFHdkxMDBUVFdHt27fJ0dGR5HI5WVlZEZG0LsfCYG7Lli2poqKC1Go1PXjwgP785z/ThAkTqGPHjhQbG0s7duyg69evU8OGDalhw4ZUU1NDTZs2pXHjxtGAAQOISBp7Cg9hW3x8fOjmzZsUHR1N//rXv2j37t00ffp0cnd3p9GjR9Nf//pXSkhIoJkzZ5JGo6FPP/2UevbsSQEBAfT//t//M3JPDGPFihX0/vvv09ChQ+nixYsUGBhI1dXV9Omnn1JeXh41adKE/vnPf9KwYcPI1dWVNBoNrVq1itauXUsTJ04kPz8/Y3fhd4vCwkIaOHAg/eEPf6DQ0FD617/+RUR1TsOYmBjavn07/eMf/yAfHx8qKSmhAwcO0Llz59jvcxxHvr6+1K1bNyKS1p7y30Jq9zqh3VxbW0vPnz+n6upq+uKLL6hBgwbUoEED2rdvH507d46qq6upV69e9PXXX9Nnn33G9qDDhw/TyJEjydvbW0S6uHjxIg0fPpyePXtGu3fvpn//+9/G6uZvgqqqKkpPT6fAwEACQI0bNyaZTEYLFiyQxFi+y5g1axbFx8eTpaUllZeX0+7du4mIqKamhho3bkxEdQTf8PBwunTpEnXp0oV8fX1JLpeLnsPvJVI6y99VAKDZs2dTTEwMERHZ2dlR9+7dycnJiZGLHzx4QD4+PvTRRx/pkbDeNeTl5dHEiRPp/Pnz1KFDBxo2bJjI3pYChOdBfHw8O5s/+OADeu+992jgwIHUunVr+vbbb4noFyJMkyZNaMmSJfWSKYikd44XFhbSokWL6ODBg1RRUUGjRo2i4cOHs/dLS0tp27ZtlJiYSLdu3aKmTZtSw4YNqaKigj744AMaN24cI3oacz8x9L3GxsbSrFmzqGXLlhQUFEQWFhZUXFxMN2/epNGjR1NZWRn179+fxo4dS02aNKHr16/T2LFj6fbt29SiRQuaO3euKCBqggkmvFkYe78U/v2bN2/SsGHD6C9/+QuNHz+eLC0tiajO/oqOjqb169dTZWUlDRkyhLp168bO9/qSTUz2lwkmvF1obOwGvCkolUqaPn06hYSE0OHDh1nGLr9hSXHjatiwIXEcR8eOHaPAwEBq2rQpXb16lZ48eUJffvkljRkzhtzc3Ojx48c0atQoOnnyJF2+fJkFDN9VfPXVVzR9+nTmJDDBBBMMQ6vVMmNt8+bNlJSURCUlJfTJJ5/Q9OnTycnJiZYuXUrBwcGUkJBARETz5s2jRo0aUW1tLRERXb9+naqrq6lfv37k5+cnqYxWYXb1+vXrqUWLFhQUFESWlpaUmppKYWFhtG/fPkYKMTc3p1mzZpGlpSVduHCBMjIyqEmTJtS9e3dq06YNtWvXjoiMb6jz51FycjJt3LiRnJ2daezYsSyTrKKighYvXswIdVOmTKGvvvqKBg4cSA4ODnT79m26ceMGWVhY0F/+8hfmlDJ2v3jw/YuPj6eNGzdS69atafTo0WRmZkalpaW0ePFi2r59O+3cuZOI6hQ2vv76a5o/fz5dv36dLly4QE+ePKF//etf9P3335O5uTkRGb9/bm5u1Lt3b4qLiyMPDw+qqamhFi1aUGlpKXXp0oX+8Ic/0F/+8hfKy8ujrVu30h/+8Afy8fGhb775hoiIPv30UyovLyeZTEYvXrwgMzMzyZAohBe/Bw8eUFVVFTVq1Ii++eYb+uSTT4iIqG/fvkRU56xatWoVEREjUzRt2pTatGlDRHXOfwsLC8mSKIiIzp07R40aNaKRI0eKnLrh4eF08uRJat26NYWEhNDTp08pKyuLVCpVvYovuo7T/zV0L+PCsSsuLqa4uDiys7OjGTNm0D//+U8iqssm3rdvH61fv57mzp1LH374IfXt25ecnJwoKSmJbty4QcXFxdSyZTnAZksAAKMNSURBVEuysrJimSHGXnO6f1/Y7549e1JGRgZ9/vnndOvWLdq0aRN9+eWX5OPjQ3/961+JiOiTTz4hjUZD//rXv+jBgwf09ddfS5ZEoVarafXq1fT+++/Tp59+Sl5eXjR8+HDatWsX3blzh77//nsaNGgQWVtb0/fff08AqGnTpvTBBx+w4KYJxsHPP/9M1dXVlJubS/7+/vSvf/2LAJBWq6W//vWv5O/vT02bNqX169fT/v37adWqVTRo0CA6fvw41dbW0p/+9Cf6/PPP6euvvyYi46+73xpSutcJ98udO3fSsWPH6Ny5c1RTU0NKpZIsLCxo8ODB5O7uTm5uboxYUVJSQkR1e9CZM2coMjKSGjVqRC1atBA9/49//CP16NGDOnXq9NaTKABQkyZN6LPPPqNhw4ZRYWEhNW/enBwdHemLL74wdvPeeXz66adUXV1Nly9fpq+//pru379P//d//0eNGjVidkDr1q2JqM52SU5OpkaNGtHAgQNF5wG/l0jRF/auoUGDBhQUFETt27enFStW0OXLl0mtVtPf/vY3CgoKIqVSSd988w05OztTbGwsbd26lYYMGWLsZv/P8M0339DChQtpypQptH//fvrggw9o5syZ1LRpU2M3jYjq9jj+PFi0aBFt2LCBmjRpQv/617+oUaNGdPHiRZo/fz6dOnWKBg0aRI6O/x979x0W1dE+fPy79CYCItgQsbCAqCh2VOwx9hJ7771rROzdxIpiQcESa6yxYO8ae+8NxYYNFAVBgd15/+Ddk10xz+95UlzU+VxXLsmWwwxnd86cmXvuKcewYcNQqVQsWbKEwYMHM3PmTKpXr65cK/Rltuu4q6srP/74I+bm5mzbto0tW7ZQqFAhJRjEzs6O9u3bU6VKFTZt2sTdu3eJj4+nUqVK+Pv7K5mRjdVHuXv3Lg4ODjg7OxsEiCUnJ7Njxw6yZs3KoEGDlGCybNmysWrVKt68eUOlSpVo1aoV796948OHD6jVakJCQujVqxfHjh3jypUrVKtW7bPXSZK+NfoLneGPvomx20vd71++fDmvXr3ixYsXDBw4kGLFiin3dGZmZrRt2xaA8PBwFi1ahEqlolWrVsTExHDjxg3Kli2bISur7H9J0hfmX9guJFM7c+ZMhr3LM7OHDx+K5s2bK3sdV65cWcyePVtcuXJFCPHHvmz9+/cXpUqVEg8fPjRmcSVJyoRmz56t7N9cqVIloVarhZ+fn9i3b58QQojjx4+LsmXLCrVaLYYOHarsJXnhwgXxww8/iDp16oi3b98qx8tM+7ddv35dVK5cWTRq1EjcunVLeTwqKkpUq1ZNeHl5CW9vbzFkyBCl3dSJi4sTCQkJyp61QmSOumm1WpGcnCz69u0r/Pz8xKVLl5TnEhMTRYcOHUS5cuVE6dKlhVqtFmPHjhUJCQn/5zEzk+TkZNGxY0dRoUIFcf36deXxpKQk0bp1a1G6dGlRuXJlUaJECTFz5kwRExPzH49n7Prp779ap04dUbhwYeHj4yO2b98uhEivl+4cHTx4UBQuXFiEhYUZHGP58uXCx8dHXLp0Sbx8+VJ53Nh10/9OLFmyRNStW1eULl1alC5dWixfvlw8evRIef7+/ftiwoQJwtvbW3z//fdK/W/fvq3stfvgwQPl9caumxAZv/PJycmibt26IiAgwGB/97lz5wq1Wi06deokbty4IRISEkT16tVFr169REJCgvIZMHYbkpSUJITI+LfdvXu32LJliyhcuLCyf7N+2xcXFydCQkKEl5eX6NSpk8FnUAjDz7gQxq+nfnmOHz8uVq5cKYKCgsSGDRtEVFSU0Gq1ynUrMjJS+Pj4iBUrVhgcIywsTBQtWlRcvnw50/ef37x5I8LCwkThwoVFsWLFlD1VExMTxY0bNwyu0TpnzpwRVapUETVq1DDKPrKSEFu3bhVqtVr88MMPoly5cuLgwYNCCMPvnhBCxMTEiE6dOgm1Wm2wF/DHMkOb+bXS/9v+/PPPQq1Wi2LFiolGjRqJwMBAUbx4caFWq0WHDh3Ehw8flNdeuHBBdOjQQYSGhorQ0FBRpUqVDOdR/9gpKSmfp0J6PtVe/53Pkv7xzpw5I169evWXjyX93z6+/uosXrxY2W976dKlyuNardbg/B46dEg0b95cqNVq0adPH3m+MoHXr1+LEydOiIEDByr7wNerV0+sW7dOREVFiRIlSoiuXbsqbc3X3PZHR0eLLl26iDt37hi7KEKIjH/rlStXCrVaLXr06CGuXbumPL5nzx7RuXNn4eXlJbp06aLcr6empopp06Yp5zUyMvKzlv/vev78uRgyZIjw9vYWzZs3V/otn/JxX8ZY9waXLl0SarVadOvWTbl/0ZXl4cOHwsvLSwwaNEgI8cf51b+vu3v3rnj06JFo0aKF2LFjh/KaGzduiM2bN3/+CkmSlOlcv35dFC5cWBQuXFgUL15cHD58WAjxRzuoa3PS0tLE0qVLRUBAgPD39xfdunUTlSpVEuXKlRNnz541WvklSfpnGH/px2ci/n9km/7ekZlhH+f/RKvV4ubmRkhICHfu3CE2NpayZcvi5OSEhYWFUv6TJ09y7NgxChcu/EXvlShJ0j8vKiqKLVu2UKlSJbp3706BAgWYPn06GzZsYMiQIUyfPp1q1aoxa9YsBg4cyNatW7l37x7Ozs5cunSJ169fM27cOGXVORg/Iljf/fv3efr0Kb179zZY3RcREUF8fLyyymXnzp2YmJjQunVrJWuPg4ODslpBJzPUTaVS8e7dOy5fvoyPj49BlqH58+dz+vRpVq5cSVpaGm3btmXNmjWkpaUxYMAAnJycUKlU/3Gldmbw8uVLjh8/Tt26dQ3SDIeEhHDp0iWmT5/O27dvmTJlChs3biQ1NZU2bdqQK1euDBHqH/9sDLq+xOLFi7l79y45c+bk6dOnjBgxgpw5c1KiRAm0Wi0Ap0+fJi0tTclEAXD27FlWrVqFm5sb2bJlU/ZiFZkg1Z/uczRr1izCwsIwMzPD3d2dqKgofvrpJx49ekSrVq3w8PBQUtNDemaKOXPmcOHCBS5fvsyLFy/45ZdfyJs3L5A5VlXr9wPPnTtH4cKFsbKyInv27Dx58oS3b9+SLVs2QkNDCQ0NJSAggMGDB+Pl5UVUVBSxsbEIIQyOY8w6nTx5kpCQEIYPH27QbmzevJnhw4dTunRpAGU/dP2yOjk50bhxY27cuMGhQ4e4evUqlStXVp7/uL9s7EwUuvLMnTuXJUuWkJycDKTXtVmzZowcOZIsWbIghODw4cNoNBqDVPNnz55l7dq1uLu7kzNnTuU7lxk+l59ib29Py5YtUalUzJo1i4kTJ/L+/XvatGmjrC6+fPkyNjY2yvV73rx5xMTEGGwlIX1epUuXpk6dOuzfv5/3799z9uxZKleunCH7Qs6cOenatSvHjx/n8ePHf3o8Y18Pvma6v+3y5cuJiIggMDCQHj16ULx4cR4/fkxUVBSTJ0/mxIkTdOvWjUWLFmFhYcGRI0c4ceIEJ06cAMDZ2Zlx48bRvHlzIGObYm5u/lnrpX99evLkCXFxcdjY2JAlS5YMq+L+G/r1+fXXX1m8eDEBAQGMGDECc3Nz+Rn9F+jO39SpU/Hx8aF+/foAdOnSBVNTU3766SemTp1KlixZaNKkibKaU+hlptBqtfz000/4+/vj6OhozOpIpN+Dli1blrJly9KgQQNOnjzJ8uXLGTVqFKVKlcLe3p4jR46wY8cOGjZs+FV/r9zd3Zk/f/5nbxv13b17lw8fPlC4cGGD1dCvX79m+/bt2Nvb07t3b3x8fJQ2tUaNGri6umJpacmBAwdwd3enaNGimJmZMWDAADQaDUuXLuXVq1dGq9df4eLiwo8//ogQgh07dhAWFgag3BPobx/0cV/GWP3n5ORkcufOzeHDh5k4cSIjR47E2dnZIKuI7j5BpVIZ3NcNGjSIAgUKsGzZMi5cuECxYsX4/vvv0Wq1eHl5ZbrMnpIkGUfevHmZNGkS4eHh3Llzh9mzZ+Pj46O0NSYmJsoYRdu2bTE1NWXlypUcPnwYgKCgIPz9/Y1cC0mS/q5vJpBCpVJlCJzQT32YGekm+FxdXQ0GOh48eIC7uzumpqacPn2aOXPmkJiYSNOmTeWNsSR94z6+yXv9+jUxMTFMnDhR6bhNnDgRGxsbfvnlF4NgikWLFjF48GCuXr0KpKd4HDNmjDIYnBnbyxcvXgAYBHosXryYjRs3MmjQIJo3b465uTnnzp1j9+7dpKamUr9+fSpXrpyp09mmpaXx4cMHnj59yqNHj3Bzc+PXX38lIiKCRo0akSdPHrJnz06fPn0IDQ1l/fr1PHr0iCpVqtCmTZtMHSQI6QMv5ubmxMTEKJ/ZNWvWsGzZMpo3b05gYCAmJiZERkZy8uRJtm3bxt27d+nSpYsyGZwZVaxYkefPn9OxY0fCwsJYt24dHTt25JdfflH2T9SlZ1+zZg158uTh7t27rFy5kgcPHjB58mRy586tHM+Yn039tuT48eOsWLGCSpUq0adPH4oWLcqqVav45ZdfWLVqFVqtltatW5M/f34lmMLMzIxly5bx4MEDAIYPH64EUUDmCFrSfU9mzZpFZGQkwcHBVK1alYIFC3L8+HFl+5ylS5cqg20+Pj4A5M+fn6xZs2Jubp4p6pKcnExERAQXLlxg//79+Pr6KuUqWrQolStX5tChQwDExsYCGc9Bnjx5qFKlCgcPHuTYsWMGgRSZia7cixYtYt68eRQvXpwOHTqg0Wh4+PAhNWrUwMLCAkj/DumClrZs2UKePHm4evUqS5cuJSYmhqlTpypBFPrHNpZPBXnrvotZsmShRYsWQPpndubMmQghaNu2LU+fPqVdu3ZoNBrs7e2Ji4sD0gdtmjZtCmTOa/jXztXVlWHDhmFhYcH27ds5duwYNWvWNAh00p1fR0dHhBDcv38fkOfrc/i4zxwXF8dvv/1GtmzZGDRokLKlWp48eciTJw/e3t60a9eOkydPMn78eMaPH0+/fv0oXbo0jx49IkuWLOTLly/TTLx8vMXfhg0buHPnDpA+Cd+jR4//aRGGfn02bNjAnDlzSEhIoG3btkqbK/1z9P/et27dYtmyZQBYWVlRs2ZNADp27IhKpWLq1KmMGDEC4JPBFFWqVKFQoULK1layfTE+/UCXwMBA6tSpw65du9izZw9Pnz4FYNeuXdSuXfur/34ZM4ji+vXrNG7c2GCrSd134/Xr19y6dYsKFSooW0nqjyEXLVqUNm3acOnSJVauXEm1atUoV64cZmZmDBkyhOrVq3+RE2fZs2dn2LBhABmCKczMzDJd++Hv78+UKVOYOHEiu3btAmDEiBFkz54dc3Nz7O3tuX79Onfu3GH37t0GwfG6+zrddodv3rwBMt6DG/v+QJKkz0e/jdP9bGtrS7Vq1VCpVISFhXH9+nXmzp1L//79cXJyyhBM0bp1awoVKsTLly9xcHCgYsWKgPHvDSRJ+nsybSDFf2pc/krHTX8gYd26dbi6uhIYGJipOoCf8nH5rl69SuvWrfH29iZr1qycPn2a5ORkgoKCqFu3LiBvjCXpW6U/AXPlyhVSU1M5efIkRYoUoUyZMgB8+PABS0tLgoODAZRgimnTplG9enXWrl3LlStXsLCwwMXFRVnBm1k7fA4ODgBcvHiR7777jt27dyur46pVq4aFhQXNmjXjl19+4d69e+zYsYMdO3awadMm5cY5M3J1daV9+/Y8ePAAR0dHrl+/TkREBAULFqR169bK/tO6leX58+fn5MmTfPfdd5k+iALS9yUNCAjAzMyMpKQkbt26xeLFi/Hx8aFFixZYWVkBUKxYMU6ePIkQgiNHjlCxYsVMHUjh5eXFkCFDsLKyYvz48aSmprJ582batWvH8uXL8fPzo0aNGqxYsYLTp0/TrFkzZdXTiBEjaNy4MWD867juRhAgMTGR5ORk0tLS6NGjhzIB2Lp1axwcHJg/fz5r1qxRHtMFU3Tp0gVvb29u3ryJv78/1atXBzJHW6Jfhs2bNxMWFka+fPmUCfVmzZpx9OhRVqxYQVpaGuXKlSMoKMhgVf+hQ4d49uwZDRs2xNLS0uj1srCwYMCAAZQoUYJWrVphYmLCixcvcHFxwcPDg2HDhmFra0tkZCQLFy7E29vbYJ903Qoz3fl9+/atsaryX7l58yYrVqzAy8uLsWPHKhOen1KvXj2WLFnCb7/9xu7du5VVacOHD6dhw4aA8b9zOrr2e9u2beTNm5dixYopgzKfCqaYPXu2km2pc+fOnD9/ngcPHlCnTh1q1aql7G1t7M/nt8zFxYWBAweSmprK9u3b+eWXX+jZs6dBhhSA6OhoAMqWLQtkziDPr8GpU6d4+PAhTZs2NfhuQXq2rBs3btCwYUPUajVpaWmYmpoqCzJcXFwICQmha9euHD16lDt37qBWq5WV5fr0r6PGoP/7Z86cyaJFi8iWLRvVqlVDo9FQqFChDJOz/6kd/DiIYubMmaSkpLB+/XplAkr65+jf0128eJH3799TqFAh7ty5Q79+/QgNDVX6VR06dABQgilUKhWNGzdWgil0504GUWQuH58DX19fvLy86Ny5MwsWLODOnTsMHTr0qw+iMLYsWbKgVqs5duwY1tbWdO3alcKFCwMQHx9PUlIS7969Q6PRACjXBN33qFy5cjRt2pQFCxZw48YNypUrp2RC0AVRfIl9sI+DKSIiItBqtVStWjXTtR9mZmaULFmS4cOHM336dHbt2oVWqyU4OJgcOXLwww8/EBERwcCBA7l79y4VKlRg8ODBBlkxddnAihQpYqxqSJKUCej3v1JSUoiPj8fFxQUhBHZ2dlSpUgVIz4q5adMmLCws6NWrlxIQr39v8fG9wZd4LZAkyVCmDKTQb7iuXr3K48ePMTMzI1++fBQsWNCg4/rf0G+sNm3axOTJk9FqtRw/fhxbW9tM1xH8T4QQ5MiRg1u3bpGcnIyvry9t27alQYMGgGyYJelbpZ+6cP78+SxbtozU1FRSUlLQaDTs3buXWrVqYWlpSWpqKubm5hmCKaZPn0716tUJDAzMcOzM2q40bNiQtLQ0pZO6fft2UlNT6dy5MwUKFECr1ZKWlkZycjKlSpVSVolk5iAKnUaNGmFubo6dnR0XLlzg4cOHzJgxQ1kRA+kDPFmyZGHMmDE4ODj8x8nEzEKr1WJhYcHkyZPRaDRK/WJiYvjxxx8NBjWePn2Ks7Mza9eu5cWLF5QoUcKIJf/vWFlZKdfiKVOmAOmT9e3btyciIoKSJUuydOlS5s6dS1xcHLlz56ZixYpUq1YNyBzXcV2/aOzYsezevZsCBQrg6+ur/P11k+516tRBpVIxb968DMEUzs7ONGjQgHr16in1yQx1gz9WFT148IDLly+TPXt2Zs+erQQW5MmTh0aNGrFq1SpiY2Px9PTE3d1def+pU6cICwvDwsLCIPuBMURFRQFQoEABPD098fb2xsTEhIkTJ3Ls2DFmzZqFt7c37u7u9O7dm7S0NHbv3k1ERAS9evXCw8MDjUajpOe9desWgJJBJbN68OABL1++pGfPnqjV6k9mcrh27Rpbtmyhbt26rF+/np9++on4+Hjc3NyoUaMGtWrVAjLP51Jn9erVjB8/nu+++47u3bvj4+OTIZiiWbNmfPjwgXnz5jFr1iwsLCzo27cvkB4EY2dnl+m+d98yFxcXhg0bhlarZfv27aSkpNC4cWMlO9bZs2eJiIjA0tJSDuL/i2JiYmjfvj2Qfh1o0qSJwXcrJSUF+OMaqJ+23NTUFK1WS4ECBahatSpr167l+vXrf9rvMvb4gu73r1u3joiICKpWrUr//v1Rq9UkJiYqmSji4+P58OEDrq6u/3MQxapVq76IfueXRv+ebtasWSxfvpyUlBRsbW3JmjUrb968UTLSfSqYIjg4GJVKRaNGjVCpVBnOq7E/m9Kf02UnGj58OO/fv1cC5qV/j5ubG6GhoQwbNkzJZtClSxd8fX3JnTs3OXLk4O7du8TGxuLq6qq0hyqVShlT0WU+02US+diX2gfTBVOYmpqyZcsW4uPj8fT0VIKyMgNd/9/ExAQPDw/atGnD5MmTOXr0KJMnT2bcuHE0btyYY8eOcfv2bXLlykXz5s0Nxht0W2xmzZpVeVy2k5L07dEfT1i/fj379u3j9OnTlCtXTslqnCVLFqpWrQrAnDlzWLt2LUCGYIpP+VKvBZIk/SHTBVLoN1yLFy9m+fLlSgpiHx8fevXqRfXq1T+5B/ynfOrG38rKimXLlv1PqSwziyJFirBu3TrevXtHcnIyjo6OODk5AXKgVJK+ZbqbvQULFjBnzhwcHR0pVKgQly5dQqVSsW3bNiXdsLm5+SeDKfr378/MmTP57rvvPnnszEbX5v3www8A3Lt3j2PHjlGtWjXKly+vdGKPHDnC06dPadOmDZ07d87w/sxKt6VTamoqW7ZswcTExGDV36lTp9i0aRNqtZqiRYsqg22ZvV66sumuXUlJSezbt48sWbJQrlw55XUnTpxg7969lC1bluzZs5MrVy7g06nvMxv9iRn9YIrOnTsrwRRjx47FzMwMlUql1Ceznbu0tDRev37NjRs3yJMnj5LhwMzMTClr7dq1AZRgChMTE1q2bEn+/PkBwxvGzFS3hQsXMmfOHAoUKICfnx9eXl4IIdBoNFhZWdGsWTMSExPZtGkTa9eu5fz585QsWZK3b9+ya9cu3r17x4gRI5QAGGMQQrB161bOnz/P2LFjlRXujx49Ijo6mujoaCZOnMioUaPw8vIiX758DBgwgJSUFLZt20ZycjLdunVTslCcPXuWFStWYGtra5B9IzPRBVLrtozRTXR+qk148+aNko0oPDycadOmYWVlhUqlUoJfMtt3DqB8+fL4+fmxZ88eVCqVsjpSv13JmjUrDRo04OLFixw7dow5c+bw9u1bOnfujL29vcHxMlv9vlXZs2dn+PDhqFQqdu7cye+//46Pjw+mpqbcuHGDN2/eMGLECCWDmPTPy5kzpzIBPWLECLRarZKZQgih9KNOnTrFzZs3DbL2QHp/2MzMDE9PTwASEhI+ex3+F2/evCEyMhIbGxt69OihBD3Y2dlx+PBhTpw4wZ49ewBo0aIF3bp1AwwzFsggis9Hf4IW0rcVCwsLo2LFinTs2BEvLy8ePHjAnj17WLp0KX369GHu3LlK5iH9YArdRHzLli2NVR3pL9C1RSqVSgZRfEZubm789NNPBsEU3bt3x9vbG39/fyIjIxk5ciTTpk3DwcFByVak25Lk1atXmJmZfZUZpbJnz87gwYNJSEigXLlymSqIQj/r9IIFC9izZw/R0dFK5rk9e/ZgZmbGxIkTGTx4MOPHj+fx48ds2bKFly9f4unpya1bt1i+fDkPHz5k3LhxFC9e3JhVkiTJSPTbE10mN0i/Lh84cICoqChev35NgwYNsLOz+4/BFJlxfEGSpH9Gpgqk0I++nzZtGhEREWTJkoXvvvuOuLg4zp49y+TJk0lJSaF27doZ0nF+7Gu88RdCkDVrVrJmzZrhcdlQS9K3R39S+dWrV+zdu5fSpUsTHByMl5cXv/32G8uXL2f//v04OjrSoUMHChYsmCGYQqPRKKuvvxQft3mPHz8mOTmZ+Ph44uLiyJYtG2fPnmXRokXY29tnWGH9pbSZ5ubmqNVqrly5wvnz53F0dOT27duEhoYSFxfH8OHDDQbbvpR66VhaWmJhYUFCQgI7d+6kRYsWnDp1ivnz55OSkkKDBg2wtLRUXp/Zgyh0/iyYolu3bixevFhJ96rb2kP3nsxAN4g7ceJE7O3tWbJkCbdu3eLYsWPK9iP6Aa26YIqwsDBWrFhBYmIiwcHBZMmSxZjV+FMajQZ7e3vy5cvHnTt3lMwURYsWxczMDI1Gg4ODA127diV//vxs2bKFEydOcOXKFWUSrV27dsq2EMa6WVapVMTGxnLmzBkmT57MjBkzWLlyJbdv36ZPnz6Ym5tz8OBBxo4dy9ixY5VgimHDhmFiYsK+ffs4cOAAVatWJSEhgevXr5OQkMCIESMy7RY6usHp3LlzA+kZNDQaDSqVKsM5KFGiBG5ubjx+/JikpCSl75wZv3P68uXLx7Rp0wwG9D8VTOHm5kaBAgU4f/48L1++ZNq0aQQEBGSY/JUyj+zZsxMUFISZmRk7d+7kzJkzuLu706pVK0qVKkX58uWBzBng8zVQqVT06tULCwsLZs6cyahRowBo2rQpKpWKQoUK0bBhQ7Zu3cqePXvIkSOHsoWcLhMTwOvXrwEybcCZTmpqKvfv36do0aJKwFxUVBRbt25V9ry3srLi/fv3zJw5E1tbW1q3bi2DKD6jmzdv8uzZMyU7jc7Tp0/ZvHkz2bJlY9CgQcoqaScnJ4oXL46rqytTp06lb9++GTJTaDQapk2blukDfaRP+5om4b8kHwdTaLVahgwZwrBhw7h37x5Hjx5l/PjxjB071iBg9dy5c6xfvx4XFxelb/q1cXFxYcaMGcp4Q2bZHkjXZs6fP585c+bg7+9PcHAwzs7OnD9/ngMHDrBjxw5UKhWTJk1iypQpzJs3j0OHDrFv3z7lOI6OjowePZrmzZsDmad+kiR9Prr2JDQ0lEWLFuHv70/fvn3Jnj07a9asYe3ataxatQqtVkvjxo0zBFNs2LABjUZDnz59lAVjkiR9hUQmtGLFCqFWq0X37t3FtWvXhBBCPHjwQHTr1k2o1WpRrVo1sXPnTuX1Go0mwzH0H1u/fr0oV66c8Pf3Fzdv3vz3K/AZaLVaYxdBkqRM5ODBg2Lfvn3Cy8tLREZGGjy3d+9e0bBhQ6FWq0VwcLC4c+eO8lxKSory8+nTpz9bef8NcXFxom7dusLf31/06dNHzJo1S1SqVEmo1WqxcuVKYxfvb1m7dq1Qq9WicOHColKlSqJw4cJCrVaLpUuXKq/5Eq8Lumv1r7/+Kvz8/ETRokVF7dq1hbe391dRPyEM+yMjRowQarVaqNVq8fLlSyOWypDub6v/N05NTVV+njZtmlLu/fv3G7xPv37btm0TFStWFMuXL/8Mpf5rdHX88OGD2LBhg2jUqJFQq9ViwIAB4uHDh8rrdPXSvf7EiRPi8OHD4saNG+Lp06cZXmcs586dE23bthVqtVpUqVJFqNVqERQUJGJiYsTt27dFly5dhFqtFs2bNxc3btwQQqTX6d69e2LQoEFKu1KlShURHh4udu/erRzb2HX71O9PS0sTQghx9+5dUaNGDVGiRAlx5swZg9frv69BgwaiSpUq4u3bt5+hxP+shw8fipYtWwq1Wi369+8vrl69qjyn+zuMHz9e9O3bV+zfv1/88ssvxiqq9D96/vy5CAoKEoULFxZt2rQRhw8fVp7Tb3ulf5bue6PVasXKlSuFWq0Wvr6+Ys2aNcprdu7cqfSzwsLCRExMjMExzp07J2rUqCFq1qwpoqOjP2v5/1fPnj1TrguLFy8WEydOFPXr11fa/cWLF4tr166JzZs3C7VaLdq0aSOSk5OFEF//WEpmcPfuXeHj4yMqV65s0L4LIcStW7eEn5+fGDp0qPKYVqtVPsNCCLFgwQKlb6Z/7RZCKGNokiT9b/T7Xn379hXXr18XZ86cEXXr1hVqtVrUr19fREZGiuPHj4t169Ypj//666/GLvpnkdnux69evSrKly8v6tSpI27duqU8npKSIu7cuSNatGgh1Gq1GDRokEhISBAJCQniwIEDYty4cSI4OFisX79eXLhwQXmfse99JEkynsOHD4tSpUqJtm3bKuMmQghx6NAhUbJkSaFWq0XVqlXFqlWrRFJSkhBCiISEBLF161Zl3PlLH1OXJOk/M2pGCvGJSM9Hjx6xYcMG8uTJw4ABA5RVVRYWFjx79gwbGxseP36srO6sVatWhswUX8LqiU/V/b+lX7/Y2FiyZcsmI2Yl6Ru2detWfvzxR4oUKULOnDmV7R9SUlKwsLBQVinNmzePjRs3olKpPpmZolSpUsCXuxIyS5YstGjRgpUrV7J3714OHjyIra0tY8eOpUWLFsCXW7fmzZvz9u1b9u7dy4MHDyhVqhSNGjWifv36wJdbL12ZK1SoQMeOHdmwYQNPnjyhcOHCtGvXjnr16gFfbv3AMDPFxIkTSUxMJH/+/Dg7Oxu7aIBhVpukpCRev36NpaUlpqamSjT9kCFDEEIQERFBr169mD9/PlWrVs2warVu3br4+vqSL18+IHOs6Pm4DLqfLSwsqFu3LlqtlmXLlnHgwAFcXV1p164duXLlUtIr6+jS9X58bGN/LkuUKMGkSZNo2bIlL168wMnJiVq1apEzZ07S0tIYOnQoAEePHjXITOHu7k7v3r1JS0tj9+7dZMmShVKlSimrlvVXXxuD/ufy8ePHvHr1CgcHB1xcXDA1NaVAgQLUrFmT8PBwevXqxZIlS/D19QX+aFdOnTrFnTt3qFChAqampl9cO/Lx6kghBJ07d6Zo0aKYmppy9uxZ9u3bR6VKlZQVMfBlt5ffChcXFwYOHEhKSgo7duwgNTUVrVZL5cqVMTMzyxRt59dGv025ePEi2bNnx8PDg/v37zNlyhRUKhXNmzenVq1aPHv2TNkq7+rVq1StWpVChQpx+/Ztli1bxsOHD5k4cSLu7u5GrlW6P/u8uLq60qtXL0aOHMn06dOB9IwGjRo1olGjRkrmIR8fH0JCQoiPj1feq2tDfv31V+bOnZvpxlK+Bvb29pQqVYqsWbMq/Sad+Ph4kpOTefv2LfDHNVn/WtajRw8uXbrEwYMH6devn0FmCt15ktcDSfrf6Pe9dFtDdOjQgWnTpjFx4kTOnj3LoEGDlNfb29szatQomjVrBmSOe59/U2arW2xsLPHx8bRr1w5PT0+EEAghMDc3p2DBgoSEhNC3b18iIyPRaDQMHz6cKlWqEBgYmKFtzAz3dZIkGc+VK1dISEigS5cuBhkef/vtN0xNTWnTpg1btmxh+fLlaLVaGjVqhJ2dHZUrV+bDhw+YmJgoY+qSJH2djDJC+uzZMywtLT+5d9CLFy+4efNmhoZr0aJFPHjwgJCQEE6dOsWSJUv46aefSElJoX79+pk2iOLj+ulugvVTYv/V461du5YzZ87QqlUrJUW4JEnfnjJlylCnTh0OHDhAcnIykZGReHp6YmFhobQ5+sEUGzZswMTEhLZt21KoUCFlf0+dL/UG0tzcnIYNG1KiRAn27NlDvnz5yJ07NyVLlgS+3MFE3eB/165dadKkCVqtFmtra2xtbYEvt176cuXKRc+ePWnbti2JiYnY2NiQLVs24Ouon34wxezZs5XHjV03/YmlVatWsWPHDi5duoSFhQW2trZ07NiRcuXK4e3tzdChQ1GpVMrE9Z8FU2SmIAr9+j19+pTY2FhevXpF/vz5sbOzw9HRkXr16qFSqVi8eDG//vorKpWKtm3bkitXrv+z/Maun86RI0eIjY3F3t6eV69esWLFCtzc3MifPz8FChT402CKfPny0b9/f7RaLXv37mX06NGMHTsWPz+/TBNE8csvv7Bu3TqioqKws7OjefPm1K9fH09PT4YMGcKzZ8/Yvn07HTp0YOTIkRQpUoQCBQpw5MgRFi5ciEajoUGDBtjY2BitPn+HbkA/KCiI3bt38/r1awICAnBycuKXX37h5cuXBAYGGrznS28vvxUuLi4EBQWhUqnYsWMHERERpKamUqNGjUzTtnwthN72oTNnzmTNmjW8e/eOXLlykSVLFhISEhgzZgyQHrjaoUMHrKys2LRpE3v27GHPnj3KsWxsbBg5ciQ//PCDcmxjni/99lKj0fDmzRtSU1PJnj07KpWKH374AVtbW86fP09qaipNmzYlZ86cODk5KWU/ePAgL168oFq1alhZWSnHvnz5MnPnziU+Pp5Nmzbh6elprGp+lbJnz86cOXMwNzfH2tqa1atXky1bNr777jsKFSqEu7s7N27c4MWLF7i4uCj9LF2fElC2ngEYOHAgixcvpmzZsspnQl4PJOl/p+t7/fjjj+zYsQOtVku/fv345Zdf2Lx5M9HR0cTExODv70/hwoWV7UONfV/3LXr48CEajYZ3794B6ePt+mNbLi4uDBgwgKFDh3Lo0CEAgoODcXFxyXAs2feSpG+XVqvl8uXLWFlZGcxFhoaGsnPnTiZNmkRAQABxcXHs3LmTNWvW8P79e3744QccHByoX78+FhYWyrHktUCSvk6ffZT03bt3REREkC1bNpo3b46joyPwR0OTmJgIpA8E6CxbtozVq1fTuXNn/P39cXFx4fjx49y+fZuffvqJ2NhYmjRpQpYsWTJVEIX+wMbOnTs5deoUT58+xdPTk8GDB/+tIIqNGzcye/ZsEhISGDhw4D9edkmSvhyurq4MGzYMCwsLtm/fTmRkJN7e3tSqVQszM7MMwRQLFixg3bp1JCYmMmbMGGXf+K+Bra0t3t7eyl7COl/yCgNTU1NlsPvj/fa+pk66qakpjo6OODo6KpkAvuTz9jFdhgPdII2x66Y/sTR9+nTCw8PJmjUrgYGBfPjwgVOnTjFt2jTKli1L69atqV69OkOGDMHExIRFixbRq1cvFixYQJUqVZT66TP2YJRWq1XqFx4ezsaNG7l//z4AWbNmxcvLi759+1KyZEklmGLRokWsWbMGQAmm+BIEBATQq1cv1Go1q1ev5tixY4wfP54xY8bg4eHxfwZTDB48GJVKxZ49e5g4cSJBQUFKANrnpn/edJ9LExMT1Go10dHRLFu2jFevXtGqVSt8fX2ZPn06FhYWbNq0iZEjRypBWNHR0QAEBQVRu3ZtwPgTnn+Vm5sbU6dOZcKECZw8eZLTp08rz40YMUK5tktfnuzZszNs2DBMTU3ZsmULlpaWBAQEfLGBP5mJ/vdd9++KFStYtGgRlStXpmPHjhQvXpyrV69y/PhxQkNDDYIpWrRogZ+fH1euXOH48eO8f/+e0qVL4+PjQ5kyZQDj98H0xxo2bNjAkSNHOH/+PGlpafj5+VG0aFG6dOnC999/z3fffYdKpUKlUpGQkACk/13Onj1LWFgYpqamVKhQweD4jo6ONGnShDp16sggin+Jvb09AJGRkYwfPx4/Pz+sra2pVKkSZcqUYd26dYwePZqpU6fi4OBAWloapqamyufO1taWIkWKUKxYMVauXMm8efPw8PDA1dXVmNWSpC+em5sbP//8Mz/++CO7du0iLS2NgQMH0qRJk0++3tj3dd8qX19frK2tuXLlCpC+sObja3OhQoWwtrYmNjaWXbt2kZSUxJw5cwwCByVJ+nbo958BZezR0dGR5ORkzpw5Q506ddi6dSuLFi2iZs2alClThhw5ctC6dWv27t1LVFQUs2fPZsuWLaxZs0ZZ5AYyiFWSvmafPZDCysqKS5cuER0djZWVFR06dGDlypVYWlrSqFEjXFxcKFSoEM+fPwfg8OHDLF68GH9/fxo0aICdnR3e3t5ky5YNrVZLXFwcP//8M56ensrN/6pVqwgLCzN6JgpdwxwSEsKCBQuU5w4fPsyDBw8YO3Zshkmx/3S8j4NEtFotmzZtIk+ePP98BSRJ+qLo0kSnpaURGRnJ0qVLsbKyUtJE6wdTaLVafv75Z/z8/L6qIAp9+m3mlzp5pu/jVf86X1MnXT9rg3599W9yvnSf2mLCWHS/f/369YSHh1OpUiUGDx6s9JkuX77MwIEDOXfuHF5eXkr66UGDBiGEYPHixfTs2ZOQkBC+++47Y1blk3TfjZkzZ7Jo0SJy585Nt27diI+P59GjR5w4cYJTp04xc+ZMateurWwjowumUKlUtG7dmty5cxuzGv+VfPny0atXL8zMzMidOzdTp07l5MmTjB8/ntGjR+Ph4YGHhwdDhw5FCMGxY8cYOXIkY8eOVbZiGTRoEKampuzcuZOQkBDCw8OxtLT87HXRnbfFixcTHh5OhQoV6NOnD35+fmzdupXZs2ezY8cOANq0aYOPjw+TJ0+mePHiXLhwgVOnTmFiYkLDhg2pVq2aEmRg7AnPv8vNzY0pU6Zw6NAhIiMjKVasGMWKFaNy5crAl1+/b1n27NkZNGgQlpaWtG/fXgZR/E1v377F3t7e4BorhCA+Pp6dO3diZ2dHv3798PHxAdK3RypRogR58uQhKCjIIJjCy8sLLy8vmjZtmuE7ZuzvnH4w5LRp04iIiMDKyor8+fPz6tUrTp8+zaFDhzhz5gxhYWHKKrmLFy8SEhKiBMtt3LiRmJgYhg8frrQnun6zm5sbffr0yZC5TvrnlSxZkjp16rB7927mzZuHjY0NgwYN4urVqxw6dIjx48czduxYJfAC4Ny5c+zatYsKFSowcuRI7t27x7Vr10hMTJSBFJL0D9AFUwwbNox9+/ZhYWFBhw4dKFq06J9uHSh9Xu7u7uTLl48TJ04ogS8mJiakpaUpGXycnZ3JmzcvhQoV4tGjR5QqVUoGUUjSN0zXf46MjCQgIAAHBwdUKhXt27fH3t6e4sWLEx8fz6pVq7C3t6d9+/bKvJudnR1mZmaULFmSly9fUrVqVezs7IxZHUmSPifxmb17907MnDlTlC9fXlSoUEG0atVKqNVqMW3aNJGQkCCEEOL48ePi8ePHQgghxo4dKwoXLiwOHTokhBBCq9UKIYTo1KmTaN68udi+fbvYuHGjcvzXr1+LatWqCbVaLW7duvWZa5fRzJkzhVqtFvXq1RO//vqr2L17t6hYsaJQq9Wie/fu4uXLl//nMTQajfLz+vXrRbly5YS/v7+4efPmv1l0SZK+QC9evBCDBw8W3t7eokWLFuLgwYPKc6mpqcrPDx8+VH7WtatfC/02U1dn/bp/Lrq/66f+vn/lb65fr99//11cu3btrxfuH6TVag3K9lc/T/rHOH369N8ul/R/02g0onfv3sLPz09cuHBBCPHH+QsNDRVqtVp06tRJPHz4UMTFxYkHDx4o7/3555+FWq0WS5YsMUbR/yuRkZFCrVaLDh06iOvXrxs817RpU6FWq0XXrl1FfHy8EEKI9+/fi/Xr14vatWsLHx8fMXr0aJGYmGiMov9lGo1GXL16VbRu3Vqo1WrRvn17ERUVpTx/48YN0bt3b6FWq8WOHTsM3nv37l0xbNgwo/efr127JgIDA0Xjxo0NynL37l1RtWpV4eXlJXx8fMSwYcMytIOxsbEiISHBoM3Xb1u+Bh/X52ur37fKGP2Ur83p06dF8+bNxd69ezM89+TJE1G2bFnRvn175TGNRmPQZ1m3bp1Qq9XCx8dHrFmzRnk8LS3tXy3337Fs2TKhVqtFt27dxPnz54UQQjx69EgcOnRI1KxZU7kOfPjwQQghREhIiFCr1cp/AQEBYu3atcrxZHtiPPr3cM2aNRNHjx4V58+fF/Xr1xdqtVo0bNhQ7N27V1y4cEFs375dNG7cWKjVavHbb78JIdLHzdRq9Sc//5Ik/XUPHz4Ubdq0Ue4b4uLijF0kSc/Zs2eFn5+f8PLyEnPmzMnw/MmTJ0WRIkVEeHi4wTX/axsDkyTpvxcWFibUarUYOHCgMhYkRPp4kBBCnDp1SqjVarFgwQIhxB/txfr164VarRa///67wfFkeyJJ34bPvozCxsaGrl270rNnTxITE7lw4QKFChWievXqShRXuXLlyJ07N69evWLHjh34+voSGBioRP0ePXqU48eP4+XlRZ06dWjcuDGQvh+ag4MD8+fPZ9u2bZ81BaX+ViQ6O3fuZOnSpVSqVIlp06bRrFkzatasyYABA7CwsODQoUOMHDmS2NjYPz3upzJRGDPThiRJmZsuTXTt2rW5dOkSYWFhyn6QZmZmpKamAukrLMD42Rp0e/zqiP+fVu3jn/+X4+nazJUrV9KxY0cSExMxM/u8CZi0Wq3BFg5v3rwhPj6etLQ0IH3Vysd1/7+Op6vXunXrCA4OZvHixbx///6fL/x/WR4dlUpFSkqKwf//lePpX+vatm3L6NGj/35Bpf/o9evXnD17lmLFiuHn5wekn7/Q0FDmzp1LQEAAw4cPB6BLly4cP35cee/QoUNZvXo1HTt2NEbR/yNd23H69GlMTEzo27evwXY/Cxcu5PLly1SuXJnRo0eTkJDAnTt3sLS0pH79+nTo0AEHBwc8PT0N0jR+CUxMTPDx8WH48OGULFmSkydPMmHCBJ4/f87NmzfZtGkTLVu2JDw8nO+//97gvQUKFGDChAmfPYX7x219dHQ0z549o1WrVgZliYiI4M2bNwQFBVG0aFEiIyNZsWKFks4XwMnJCTs7O4NMNl9bpoaP29ivrX7fqs/dT/naJCcns3btWi5evMjly5cz3JunpqaSmprKy5cviY+PN8iApWuDdNtYaDQapk2bxurVqwEyTWasj/uNcXFx/Pbbb2TLlo1BgwZRvHhxAPLkyUNgYCArVqwgX758SoYirVZLv379WLZsGRMmTGD27NmEh4fTvHlz5fiyPTEe/Xu4y5cvs2DBAl69esX06dMpU6YMN27coE+fPrRo0YLBgwdz8+ZNgoKCaNCgAQAPHz4kR44cFC5c2Mg1kaSvi5ubG5MmTcLT05OAgID/Oquw9Hn4+/szZswYrKysmDdvHkOHDuXy5cvExMSwb98+Zs+ejVarxdPT02B8RmYRkaRvl7+/Pzly5GDHjh2MHz+eN2/eAChZ2M6ePQv8cQ+g2w5vyZIl5M6dm+zZsyvHku2JJH07PvuIjRACOzs7oqKiSE5OxsLCgtjYWK5evUr+/Pmxt7dXbuI/fPiARqPh2bNnREVFUaBAAc6ePcuCBQuwsLBQ9uVWKmNmhkaj+awDwA8fPiRv3ryYmpoq5RZCoNVqOXLkCFqtlt69eytBDx8+fGDnzp1kyZIFOzs7Dh06xOjRoxk7diwuLi4Gx5ZBFJIk/RW6gTiAHTt2EBERgVarpWrVqhnS8xqzw6e/N92uXbu4cuUK9+/fp1y5ctSsWRNXV9f/aVD34zYzLCyMly9fEhsb+1nTrenXa8uWLRw/fpxLly4hhMDX15dSpUrRokWLv1yvOXPmkJCQQM+ePY2SllK/fjt37uTs2bMcO3aMHDly4OzsTNeuXXFzc/uvJ6A/rt/s2bOxs7NTBvalf09aWpqyTdqbN2/ImjUroaGhhIaGEhAQwKBBgyhYsCCbN2/m+vXr7Nu3jx9++AFI73OVKFECyLjP5Ofy7NkzHBwcsLKyMriBFULw/v17Tpw4oWwZp3tev379+/fHxsaG2rVr4+/vz88//4ytrS0NGjSgePHiFCxY8LPX6Z+gUqmUYIopU6Zw4sQJWrZsiRCCp0+f4ufnR+3atYGME2efO4W7/mdHtwXV48ePAQy2nlq8eDGbNm1i8ODBtGzZEjMzMy5cuMCuXbtISUmhQYMGVKpUSfkMfM2DGV9z3STpr7K2tqZ9+/YUKVKERo0aYWpqSnR0NPny5QPS+8YlS5bk2LFjXLt2jYCAgAzBFLoU4ADv3r1j/PjxlC5d2qjXglOnTvHw4UOaNm1qsAUawMuXL7lx4wYNGzZErVaTlpaGqakpKpUKjUaDi4sLISEhdO3alaNHj3Lnzh3UajVly5albNmyBr9HV3/JuHT3cEIIduzYgVarZeDAgSxbtoydO3dy/fp17t+/j4+PD8WLF6d8+fIALFu2jBMnTlC9enWyZMli5FpI0tcnb968rF69WhlTkBNnmUu9evWwt7dn1KhRbNu2jW3bthlc24OCgqhYsaLyennuJOnbJYTA39+fOXPmMGDAACIjIwEYPXq0Mv6gC0rdt28f9vb2WFlZERERwb1795gwYQKFChVSjifbE0n6dnz2QApdA5MtWzaqVKlC3rx52bZtGwsXLuT9+/c0b95cufnLmTMn5cqVY+/evXTt2pXixYtz5MgREhISCA4OJjAwMMPxP+dA/rlz52jdujV16tRhxowZyuCDSqUiKSmJS5cu4e7uTrFixZT3zJs3j6NHj7Js2TJy5cpF06ZNOXDgAGlpaYwcOZKcOXNibm5uMLAsgygkSfpf6QbiTE1N2bJlC/Hx8Xh6eip7uxmbVqtV2ri5c+cyb9485Wb32LFjnDp1iuHDh5M7d+7/KpjiU4FnqampbN26VRlE/xyE3p7V06dPJzw8HBMTE3Lnzs3bt2+JjIwkMjKSW7duERQUhKWl5X+s358F1K1bt+6zrxrXlUdXvxkzZrB48WIAbG1tiYuLIzExkTNnztCyZUsaN278f+7R/Gf1W716tVHq97X6s8+Yq6srhQsX5ubNmzx79owVK1YoQQaDBw9W9pD38fFRAkVVKlWGvpYxgig0Gg0TJkygYMGC9OrVC0tLS+U5ExMTrK2tcXV15d69e8pEmX4QxeDBg/H29iY6OpoPHz7w9OlTJfjHwsJCmTj7Ulfo6oIpRo4cyZQpUzh16hTW1taMHDlSCaIA42Yz0G8vQ0NDuX//PlOnTlUCiy9cuED16tXZvXs3ixcvpkKFClStWhULCwuaN2/OihUriI6OVtrV3377DS8vL6PVR5Ik4ypatCi+vr6YmJjw888/s2/fPkaPHk2FChWwsbGhTJkyHDp0iEGDBrFs2TK8vb3RaDQG1zcrKytKly5NjRo1SE5ONmoQRUxMDO3btwfS2+omTZoYBFPosoHpxlf0s5roFnkUKFCAqlWrsnbtWq5fv/6n4whyEDjzyJ49O0FBQUB6wPK0adMYNGgQtWvXpnbt2spn9cOHDyQlJbFs2TJWrFhB9uzZGTx4sNyrW5L+JTKIIvMyNTWlatWqeHp6smPHDu7cucOrV6/w9fWldOnSBAQEAF/ufZ0kSX/Nxwt+dP8vhKBo0aLMnj37k8EUXl5e1K1bl127dnHp0iUgfcFJcHAwTZs2BeS1QJK+RZ8lkOJTnZU+ffqQmJiIlZUV2bJlY+nSpSxduhSAZs2aYW9vD8CAAQP48OEDR44c4dWrV2TLlo0ff/xRabiM2RF6/fq18u+HDx+UQXxdSnchBLGxsdy6dQu1Ws369etZvHgxDRo0wN3dnZw5czJ48GDGjBnDkSNH6N27N1WrVqVHjx5YW1sDsH79ekJCQmQQhSRJ/zPdgFpCQgLlypXLNEEU+qveZs2aRVhYGO7u7rRv357ExER27tzJvn37+PDhA2PGjCFPnjx/Kdhg1apVn30yXteRXrZsGeHh4QQGBtK1a1f8/f15/Pgxly9fJjg4mDVr1qDRaBg/fnyGFYb/Tb2MdS3QlWfRokXKxGaPHj3Ily8faWlpLFq0iP379xMeHo5Wq6Vly5Z/mv40M523r5n+zePVq1d5/Pgx79+/x9XVlXLlylG2bFlOnDhB586diY2NpXLlyvTu3VsJogC4ceMGWq2WEiVKZJoU56amprx8+ZL9+/djbm5Onz59mDx5Mvnz56dFixZotVqyZ8/O6dOnWbJkCRqNhoULF2YIEnF2dsbMzIzk5GSSkpKwsbEx+D1f8mCbSqXCy8uL8PBwzp07h4ODgxJoYOyBRP2Bh5CQEBYsWEDRokWJjY2lYcOGpKamUq5cOQC2bdtGamoqnTp1okCBAmi1WjQaDe/fv6dUqVKULl0aCwsLGUQhSd8wXUYbExMTnjx5QlRUFA8fPmTevHkIIahYsSIdO3bk6tWrREZG0qtXL0JCQihatCiQ3l5euHCBbdu24eHhQdu2bZVjG6u9zJkzJ3369CE0NJQRI0ag1WqVzBRCCGW84NSpU9y8eTNDG6hSqTAzM1P6VAkJCZ+9DtJfowumUKlU7Nixg3nz5vHu3TuqV69OSkoKv/32G3PnzgUgNjYWDw8P5s6di7u7u5FLLklfPzlxlnnlyZOHbt26ffI5Y9/7SJL0+enGrk6dOkWZMmUyZJP/VDDFmDFjcHV1pUuXLhQrVowdO3ZQokQJSpUqReXKlQHZnkjSt+pfD6TQH8CPiooiKSmJhIQEypcvr0T06tJEL1u2TAmm+OGHH3BwcKBAgQKEhoZy9OhRsmTJgqOjo5JCx1gN1507d3BwcKB69er8+uuveHh4YGlpyYEDB6hatSomJibY2dnRqVMnLl68SM6cOblz5w5Lly4lb968tG7dmpw5cwLpEW1CCNRqNdHR0Tg7OyuDIroVCBqNhtWrV8sgCkmS/mcuLi7MmDFDaVcyQ9Ss7vdv27aNJUuWUKlSJQYOHIi3tzcpKSlYWlpy584djh8/zvjx4xkzZsyfZqbIjMEGT548YcOGDTg7OzNo0CClHG5ubri5ueHi4kKfPn1Yt24d2bNnp2/fvl9EvXSioqJYu3YtefPmZciQIXh5eSnX+iFDhqBWqwkLC2PVqlUULFiQ7777LsO5y8z1+5roZxAJCwtjxYoVxMbGAlChQgV8fX1p3bo1hw8f5vz582TNmpVmzZpRpEgR5Rhnz55l6dKl2NvbK1t5ZBZNmzbl1q1bhIaGcuzYMS5evEjVqlWpVasWDg4OdOvWjRMnTrBw4UI0Gg0BAQEMHz7cYIXxmTNnSEhIoE6dOtjY2GSKNvKfJITA3NzcII27sfrPur+t/u9/9OgRZ8+epUSJEowePVrpH+sCpqOiovj999+pWbMm5cuXVwLxjh49yrNnz2jfvj0dO3Y0et0kSTIerVarZGPYvHkzbm5uBAcHY2dnR2RkJKGhoWi1WgIDA5kyZQppaWns3r2bNm3a0Lp1a9zc3EhJSWHDhg3ExMTQt29fg+Mbq01RqVT06tULCwsLZs6cyahRo4D09lGlUlGoUCEaNmzI1q1b2bNnDzly5MDBwQH4I7AE/lj8oZ+GWMr8Ps4u+ObNG7y9vcmVKxf37t3DxsYGJycnmjRpQvPmzcmVK5exiyxJkmR0uvsNIQTwx9iTvD+QpG+TbvHewIED6d69u8FCNl0wxaxZs+jWrZsSTDF27Fi8vLzw8vJSthXVkeMNkvTt+lcDKfSDKJYtW8batWt58uQJqampNGvWjKCgIOUGUDdgqh9M0bFjR+7fv8/du3eVVL46xtrH8/LlyzRr1ozAwEAmTpyobNsxe/ZsFi5cSJs2bRg5ciQAVapUoWbNmtjb27N//36io6OZMGGCsvIF0vf3trKyYtSoUWTPnl1ZRaDRaHjx4gWurq5MmzZNTixJkvSXZaYgCl05AA4cOIClpaUSRKFz5MgR7O3tyZEjB0eOHGHs2LFKZgr960pmnYx/+fIld+/epVmzZqjVaiVLkU6pUqWYOXMmnTt3Zv369dSsWdOgvJm1XjrPnj0jJiaGAQMG4OXlpaTnF0JgY2NDw4YNefPmDTNnziQsLIzAwECsrKyU92f2+n1NPs78ki9fPpo0aYJWqyV//vxAepra+fPn07FjR27cuMGMGTO4ffs2bm5uPH36lFWrVvH06VNGjRqlZAgwNl1b1rRpU1xdXenVqxdXrlyhUKFC9OvXDwcHB4QQuLm50a5dO5YuXcq7d+/w9PQ0CKI4deoUYWFhmJubU6VKFeDrW2X2qfp87v5zVFQUKpWK/PnzG0zuzZo1i+joaK5fv07nzp3x8vLKMDDx6NEjkpOTiY+P59WrVzg5OXH27FkWLVqEvb29QZ/aGHWTJMn4dN973ZZq5cqVY+nSpXTp0gWAyMhI5s+fj4mJCRUrViQkJITJkyezc+dOZdwBwNHRkdGjR9OoUSPA+P1mXZ+3a9eu2NraMmHCBMaPH49Go6FFixYAVK5cmRMnTrBo0SKsrKyoV68eOXPmVNrZ8+fPs2XLFtzd3eVE+xfo4+yCuXPnBiAoKIi+fftiYWGhZGKRJEmS/rj3+dru6SRJ+u98vJ2HjY0NZmZmLFy4EJVKRbdu3TIEUxQrVow5c+bQvn17IiMj0Wq1jBkzBgcHhwwZWWWfS5K+Xf9aIIX+Kshp06YRERGBtbU1xYoV48qVK6xbt46kpCRGjRpF1qxZcXR0zBBMceHCBe7fv8/bt29xcXExWAlprE5RcnIyuXPn5vDhw0yaNImRI0fi7OyMp6cnlpaWrFy5EkB5XKPRoNVq2bFjB1qtFjc3N+VYZ86c4ddffyV//vwUKlSIrFmzAn80+i1btqRevXp/mhZdkiTpf5FZbiZVKhVxcXEcOXKE/PnzGwRRzJkzhxMnTjBz5kzy5s3L0KFDOXr0KCNGjGDy5MnKAGJmnox/9+4dgEHwn/7fXqvVEhAQQJMmTdi4cSNPnjxRypyZ66WbULhz5w4AHz58AP643utWflhaWtKuXTt27tzJ9evXOX/+POXLl1dem1nr97Xavn07S5cuJTAw0CBDCqSvWI2KigJgyZIljB8/npMnTzJ79mzlNa6urowbN47mzZsDmSMCX/dZU6lUPHjwQJmcv3PnDgcOHCB//vxYWFhgY2NDnTp1SExMZN26daxYsYK7d+9SuHBhEhIS2LZtG2/fvmXEiBEEBgYatU5fKyEEW7du5fz584wbN04J4Hn8+DHnz5/nzJkzWFlZKW2kLtBOx9fXl4IFCyrvL1CgAJs2beLZs2eMGjUKf3//z14nSZIyB/2B0ufPn7Ns2TICAgLo3LkzAN7e3nTt2hVID6aYO3eukpkiODiYJk2a8PDhQx49eoS7uzt58uT5ZH/MGPTrdvHiRbJnz46Hhwf3799nypQpqFQqmjdvTq1atXj27BkLFixgzpw5XL16lapVq1KoUCFu377NsmXLePjwIRMnTpTbPnyhPs4uqOvz6LK7SpIkSZIkSel0/eetW7dSrlw5unfvjo2NDdOmTVO2Rfs4mEKj0VCyZEn8/f158uQJO3fu5M2bNyxcuNBgXFeSpG/bPxpIoT/goPs3PDyciIgIAgIC6NevH8WKFePcuXP07NmT7du3K1Fe+sEU5ubmLFu2jP379wPpEfeZJZ20v78/U6ZMYeLEiezatQuA4OBgateujYWFBUOHDjUIptA14D4+Phw9epTjx4+TPXt27t+/z8KFC3n27BkDBw5UgigAZc8mCwsLGUQhSdJXycHBgVy5cpGYmMj79++xsrJi/fr1hIeH06hRI0qXLk3WrFkJCAggKiqKM2fO0Lp1awYMGECdOnUwNzcHYOPGjZluMl63CvDQoUO0b9/eIIAO/giq0K0MjImJAQwzLa1du5bQ0NBMVS+dggULolKpePz4MYDBPoMqlYrU1FSsrKzw8/Pjxo0bBnty6wdRzJ49O1PW72ui0Wg4fvw4Go2Grl27Gvydd+7cyZ49ezhy5Ajv3r2jfv36dO3ale7du3PmzBmSkpIoWLAgefPmNfqWap+iUqnQaDR4eHjQqVMnHB0dmTt3LnPmzOHDhw/06dMHc3Nz3NzcaNu2LQULFmTBggUcPXqUo0ePYmVlhZubG0FBQTRu3BjIXPX7WqhUKmJjYzlz5gyTJk1ixowZrFy5ktu3bzN48GDCw8PZt28foaGhBAQEULRoUYNV4Pb29rRo0YIVK1awe/duzMzMsLW1ZcyYMbRs2RKQ502SvlW6++zQ0FDs7OxwcXGhZ8+elCxZUpls9vLyMgimmDdvHgCBgYGo1epP9j+MlflS//fr6jZz5kzWrFnDu3fvyJUrF1myZCEhIYExY8YA0Lx5czp06ICVlRWbNm1iz5497NmzRzmWjY0NI0eOVLZSNXaWDemv0c8uqJ9eWpIkSZIkSTIcE/jtt98ICgqiWrVqjBs3jrZt2yKEYPr06RmCKVJSUpRgCSEEBQoUwNPTkzJlysggCkmSDPwjd2F3797FwcEBZ2dng4br5s2brF69Gm9vb4YOHaqk7M2bNy+Ojo4kJSWxY8cOVCoVo0aNwsHBQQmm8PHx4c6dO+TNm5dKlSoBmWOg1MzMjJIlSzJ8+HCmT5/Orl270Gq1jBgxgurVqzNt2jQlmEKlUjFixAgAChcujJWVFQsXLmTlypUkJSWh1WoZPnw4DRo0AAwHNoxdT0mSpH/CpwZsNRoNqampVKhQgejoaJKSkrhz5w4RERF4eHjQqlUrJYisQIECQPqq+KdPn5KQkKAEUaxatYo5c+YoP3/Oyfj/dD0qU6YMVapU4dChQ2zfvp0WLVrg6OgIGK4wTEtLA8DLywv4I8Di1q1bLFmyhFevXvHbb7/h6en5b1cng0/VT1e+nDlzkjVrVrZt20bx4sVp1aqVEsVtYmKinJ/3799jY2NDvnz5DI5z4sQJRo4cib29PatXrzZK/b4VGo2G6OhonJ2dKVmyJJCe5jsyMpJVq1YBkC9fPqytrdm6dStarZbp06crn0l9xp5Y0pVBPxuFqakppUuXpmTJklhZWZEzZ06GDx9OWFgYKpWK3r17Y25ujqurKw0bNqRKlSpcvHiRuLg4ChQogKOjI3nz5gUyRx/za9WkSRMePXrE77//TuPGjYmJiaFRo0a4uLjQr18/tFotBw4cYOzYsUyePFnZMgjSM/s0bNiQ4sWLs2fPHjw8PMiTJ4/yeZbnTZK+bcePHyc0NBRLS0vS0tKIj48H0u/ZddeKTwVTmJmZERAQAGRsR4wRaKDfX9b9u2LFChYtWkTlypXp2LEjxYsX5+rVq0qd9YMpWrRogZ+fH1euXOH48eO8f/+e0qVL4+PjQ5kyZQDZXn4NZBCMJEmSJEmSIf1x1vj4eIoUKUKOHDk4fvw4Y8eOZezYsbRr1w5ACaYQQtC9e3clWOL48eNcuXKF6dOnU6NGDaXPLIOQJUnS+duBFJcvX6ZZs2YEBgYyadIkZTsLU1NToqOjDfZRh/QAgdmzZxMfH8+kSZOYNm0akZGRqFQqfvzxR1xcXLCzs6NMmTLKTT9kjht/Xb1MTEzw8PCgTZs2TJ48maNHjyrbfOgHU6xYsQIhBCNHjqRmzZq8e/eOHTt2cP/+fcqWLcv3339PnTp1Mk39JEmS/kn6ndmEhARev35NtmzZEEJgZ2dH7969SU1NxdHRkf379xMdHc2kSZMoUqSIcowHDx7g6OjImjVriI+PN5jgTUtL482bN/z222+fNYhCv16PHj3i5cuXpKWl4ezsrKStb9y4MVeuXGHFihVYW1vz/fff4+rqqrzv/PnzbN26lfz585MjRw6D42fLlo1WrVpRoUIFChYs+NnqpaNfv6tXr/L69WsSEhKoUqUKVlZWFChQgB9//JHg4GBCQkKwtLSkSZMmBnsHnj17VtliIWvWrAY3H+XKlaNu3bp07NhRBlF8Bjlz5uT8+fP06dMHExMTLl26xPPnz3FwcGDw4MFUq1aN2NhYunbtyqFDh3jw4AHu7u4ZbhiNffOo/7l89+4db968wcLCAiEELi4uANStWxchBCNGjGDhwoUIIejdu7dyc5w1a9ZPbuGRGYJEvmYlSpRg0qRJtGzZkhcvXuDk5EStWrXIlSsXaWlpDBgwgLS0NI4cOaIMdOiCKbRaLXZ2dhQuXJjChQsbHFeeN0mSypcvT7du3Vi6dCkajYaTJ09SpkwZsmTJYhB4px9MsXPnTmbMmEFqaiqVK1c2ajvy9u1b7O3tDa6xQgji4+PZuXMndnZ29OvXDx8fHyC9PS1RogR58uQhKCjIIJjCy8sLLy8vmjZtmmFsQY41SJIkSZIkSV8b3VbDAAsXLuS3334jMTFRyX68b98+VCpVhmCKWbNmER8fT7t27bh69SpLlizBwsICR0dHGUQhSdIn/e1AiuTkZHLnzs3hw4eZOHEiI0eOxNnZGUjPVAEYpPUODw9n48aN9OrViwYNGuDi4kLHjh3Zvn07KSkptG3bFm9v7wx7Phr7xl+/YV6wYAF79uwhOjqa5ORkAPbs2YNKpcoQTLFy5Uq0Wi2jR4+mUaNGVKlSBVNTUywtLZWBfTmwIUnS10Z/0nPNmjVERkZy9uxZnJ2d6dGjB82aNVPa+dTUVI4cOQKgrDIGOH36NJs3b8bd3R0bGxtlslSXrrl9+/bUq1fvs26BpH8tWLx4MWvWrFG25jA1NeWHH36gdu3a1KhRg4cPHxIWFsb8+fO5dOkSzZo1w9nZWcm+8eTJEyZNmkSePHkMfoezszNt27Y1CEz4XPTPW0REBMuXL+fVq1ekpaXx/fff07p1a0qUKEHjxo15/Pgx8+fPZ8SIETx9+pTatWvj7OzM6dOnCQ8P582bNwwfPtwgUER3/OnTp3/2un2LLCws6N27N2fPnmXfvn0A5MiRg06dOlGrVi1lGwUnJycKFCjAtWvXsLGxAYwfOKFP/3O5evVqdu7cydWrV7GwsMDW1pa2bdtSvXp13NzcqFevHiqViuDgYCUzxYABAzh79izPnz/H398/Q/BSZqrr1+rIkSPExsZib2/Pq1evWLFiBW5ubuTPn58CBQowZMgQhBAcPXrUIJjiP5HnTZK+bampqZibmzNo0CBMTExYuHAhq1evRq1W07RpU4AMwRTdunVDo9Gwa9cukpKSjFr+M2fOMGPGDLp06UL16tWVx1UqFcnJydy/fx9fX18liEKr1aJSqVCpVDRs2JDU1FRGjRrF+PHjEULQokULwPCaqSPHGiRJkiRJkqSvja6PO3/+fObMmYOfnx/t2rXDycmJq1evcvjwYfbu3YtKpWLMmDG0a9cOU1NTpk2bxtKlS9mwYYMybzl8+HBKly6tHFuON0iSpE8ldLlz/6K0tDTOnz/PxIkTuX37NrVq1WLEiBFkz56dw4cPM3DgQHr06EG3bt3Yv38/w4cPp2jRogQHB5M/f37evXtHixYtePDggbIv0dKlS/H39/+n6viP0jXM/v7+NGzYEGdnZ86fP8+BAweIiooyqP++ffsYOnQoycnJtG3bVtnmQ5+MbpMk6Wuj365Nnz6d8PBwsmTJQu7cubGysqJdu3bUqFFD2QIiNTWVoKAgIiMjad++PUOHDuX3339n0aJFnDt3junTp1O3bl2D36ELQDNWGzpnzhzmz59Pnjx5qFixIsnJyZw5c4aYmBhy5MjB0KFDqV27NqtXr+bXX3/l1q1bmJiYYGJiQlpaGpaWlgwePFiJiM4M1wL9oL5p06YRERGBtbU1FStW5Pz588TGxhIQEEDXrl0pU6YMHz58YNWqVUybNg0AS0tLTExMSE5OxszMjKFDh9K+fXsgc9TvW/bo0SMuXLjA27dvqVy5Ms7OzlhZWSnn/MSJE/Tq1YuyZcsyc+ZMZS/uzODP2hM/Pz80Gg3Hjx8HoHLlyrRu3ZqKFSsCsGPHDoKCgkhJSaFSpUpcunSJN2/esHr1akqUKGG0+nyroqOj2bp1K2q1mtWrV3Pq1CnKli3LmDFj8PDwQKPREBUVxbRp0zh69Ch+fn6MGzcOtVot2w9Jkv6rhQchISEsWLAAExMTJk+eTMOGDZXn9NuRa9euERcXp2wfagzJycmMHDmSyMhIunXrRv/+/Q2CHx48eECTJk1wdXVl1apV2NvbZ1gdp9VqGTp0KJGRkdja2jJ48GBatWplrCpJkiRJkiRJ0md37do1unfvjr29PbNnz1Yy32o0Gp4+fcrw4cM5c+YMNWvWZMyYMWTLlo1jx44xe/ZsNBoNrq6u1K9fn9q1awNywbMkSZ/2twMpIL2BOXXqFNOnT+fatWvUrFmTESNG4OrqyubNmylTpgy5cuViwIAB7Nu3j8WLF1OuXDnl/dWrV8fGxobKlSvj5uamrCDJbK5du0a3bt1wdHRk5syZSsOcmprKgwcPGDVqFBcuXMgQTBEUFERiYiI//PADEydONHItJEmSPo8VK1YwefJkAgMDGTBgAJ6enrx69QoHBwfMzMx4//49Go0GW1tbzp49S+/evXnz5g3Ozs7ExsYCEBQURIcOHYDPOxn/8e/SX90XFxdHgwYN8PDwYMyYMcr2G9euXWPnzp2Eh4fj7OzMxIkTqVy5MtHR0WzZsoXbt28TFxdHxYoVKV68OOXLlwcyXyd92bJlTJ06lUqVKtGnTx+KFi3KxYsXGTduHDdu3CAgIIDu3btTqlQpVCoVJ06cYPv27Tx48IAPHz5QokQJKlSooExoZ7b6SfDkyRNy584NpGd+mTVrFhcvXmTmzJl8//33Ri7dp61fv55Ro0ZRqVIlBg0apGQruHz5Mr179yY2Npbu3bvTuXNnsmTJAqSnb58wYQLv3r0jJSWFoKAgJbhH+vx02YSuXr3K1KlTOXv2LOXKlWP06NF4eHiQmprK/fv3+fnnnzl27Bi+vr6MHz9eWY0tSdK3Sb8PdvjwYe7cucONGzewsbGhevXq5M+fHzc3N+C/D6bQMWYf5fLly5w/f55GjRqRNWtWoqOjyZcvHwBJSUkMGjSIY8eOERYWRkBAgEFZdXWZMmUKy5cvV465fft2o2wLJ0mSJEmSJEnGcPjwYXr16kXv3r3p1asXQgilr6xSqUhMTKRbt26cP3+e6tWrM2rUKFxdXXn16pWywEiXMVmOX0qS9Gf+1tYeukENExMTPDw8aNOmDZMnT+bo0aNMnjyZMWPG0KhRIyB9Jdrhw4epWLGiQRDFnj17ePz4MWPGjKFly5bK45mx4YqNjVX2T/L09FQaZnNzcwoWLEhISAh9+/Zl165dyh7d1atXZ/LkyfTr108Z4JEkSfraPX36lA0bNuDq6kqfPn2USU/d1g8XLlzg4MGDWFhY0LNnT8qVK8ekSZOYNWsWaWlpeHt706hRI6NFBL9//x5ra2ul860bwN+zZw/v378nPj6etm3bUrBgQWVysHDhwuTMmRMLCwsWLFjAihUr8PX1JV++fPTv3x/ImG45s13rHj16xNq1aylYsKDBZLWbm5uSSeP3338nNTWVnj17Urp0acqVK4e/vz8WFhZKmm2dzFY/Ca5cuUKfPn1Qq9U4ODhw6NAh3r59S1BQkBJEkdkyAKSkpHDw4EGsra3p2bOnwZYPJ06c4OXLlwQEBNCkSRPS0tJ48eIFLi4ufP/997i4uJCYmIiFhYXS/5SfS+MwM0u/7fDx8WH48OFMmTKFEydOMG7cOEaPHk3+/Pnx9PRkyJAhWFlZsW/fPh48eCADKSTpG6W719b1m2bNmsXSpUtJSUnBzMyMtLQ0Nm/eTEBAAA0bNuT777+nf//+mJiYMG/ePIKDgwGUYIpPXdeMeS0oWrQovr6+mJiY8PPPP7Nv3z5Gjx5NhQoVsLGxoUyZMhw6dIhBgwaxbNkyvL290Wg0BhnZrKysKF26NDVq1CA5OVkGUUiSJEmSJEnflIcPH6LRaEhMTATSx111Yw+6IImhQ4cyYMAADh8+jFarZcyYMbi6uiqvgfR7DzlOJEnSn/nLgRT6+8QvWLCAPXv2EB0dTXJyMgC7d+9GpVIxcuRInJ2defv2LR8+fEClUilbeFy4cIHw8HCyZs1KgQIFDI6fGRsuXcP87t07IH1lnf6EkYuLCwMGDGDo0KEcPnwYExMTgoKCqFmzJnv37lUCKTLbBIUkSdI/7d27d9y/f58mTZrg6+tLamoqr169YtOmTcyfP5/U1FTltZcvX+aXX36hevXqlC5dWum86laVf+5Jz5MnTxISEqJsRaWzefNmgz3zdNsf6JfNycmJxo0bc+PGDQ4dOsTVq1epXLmy8nxm37P6yZMnREdHExQUZDBZPWfOHGJiYhgzZgz79+9n//79WFlZkZKSQsWKFbGwsECr1WJubm5wvjJb/b51QghevXpFUlISR44cAaBQoUIEBwcrE02ZMcjgzZs3XL58maJFi1K8eHHl8dDQUEJDQwkICCAoKIjU1FTat29P8+bNad26NUCGreIyY/2+NSYmJgbBFCdPnmTChAlMnTqV169fs2nTJlq2bEmLFi2oUKGCsYsrSdJndPHiReLj46lcubKyigxg4cKFhIWFUbJkSTp27EjevHm5fPkyR48eZdeuXTx//pwsWbJQoUIF+vbtixCC+fPnM2rUKIQQyuKOzEIXhGtiYsKTJ0+Iiori4cOHzJs3DyEEFStWpGPHjly9epXIyEh69epFSEiI0i9VqVRcuHCBbdu24eHhQdu2bZVjy+ucJEmSJEmS9K3w8fHB0tKSa9euAekLOHT9YV2f2N3dnSxZsvD8+XMOHDiAqakpo0ePJnv27Mpx5FydJEn/yV8OpNA1RPPnz2fOnDn4+/sTHByMs7Mz58+f58CBA+zatQuA4OBg8uTJg4uLCwcOHGDy5MmYm5uzf/9+YmJiGDlypDIxlZn5+vpibW3NlStXADJMGEH6hIS1tTWxsbHs3LmTd+/eERISQp48eQA5sCFJ0tfn4ywLkL73s0aj4cCBA1SvXp0LFy5w4sQJzp07B0D37t0pUaIEhw4dYs2aNaxbtw4vLy/s7e2B9Alf3b+fs81MTk4mIiKCCxcusH//fmWlIKSvHKxcuTKHDh0CULYf+bh8efLkoUqVKhw8eJBjx44ZBFJkdgkJCQBKJDekb/Xx66+/0r17dxo3bkzevHk5fPgwR44cIS0tjUePHtGsWTMsLCwAGTyRmalUKgIDA9m1axfPnj1DCEG2bNnImTMnkHn7KCkpKaSmpvLmzRuSk5OxtrY2CKIYPHgwhQoVYtu2bdy+fZujR48qgRQfy4z1+xapVKoMmSlatmyJEIKnT5/i5+cn9yiVpG/M6dOn6d69O8WKFcPb21tZJXbz5k1WrFhBgQIFGDVqFGq1Gki/77569SoArq6ueHh4kJSUhI2NDf369cPU1JS5c+cqgbEfL9wwFq1Wq6yS27x5M25ubgQHB2NnZ0dkZCShoaFotVoCAwOZMmUKaWlp7N69mzZt2tC6dWvc3NxISUlhw4YNxMTE0LdvX4Pjy/ZSkiRJkiRJ+lZ4eHjg7u7OqVOn+Omnnxg2bBgmJiakpaUpwRROTk64urpiYWGBEIK9e/cCKMEUcsxBkqT/y9/a2uPatWusWrWKggULMmbMGDw9PQGoUKECDRo0YNSoUezatUvZo/THH39k2LBhrF27FoAsWbIwbtw4mjdvDmT+gVJ3d3fy5cvHiRMn+Pnnn/nxxx8zNMzOzs7kzZuXQoUK8ejRI0qWLKmsWgY5sCFJ0tdHF0Rx5swZChUqhIODA0WKFKFRo0Zs2LCBHj16kJqair29PdWqVaNVq1YEBAQAUKxYMdavX8/r16+VQWX4IxL4c0cEW1hYMGDAAEqUKEGrVq0wMTFRtgnw8PBg2LBh2NraEhkZycKFC/H29jbI3KBbYahbMfj27dvPWv7/1p9lRnJxccHW1pZXr14BcOjQIRYtWkTp0qWpV68eACVLlqRkyZKcPn2a48ePc/z4cXLnzk2VKlU+ax2kvy5btmxky5bN4LHMkMbw48+l7v+zZ8+Or68vly9fJjY2lt27dxsEUei2fnB3d0elUpGUlPTJAC8pc9EFU4wcOZIpU6Zw6tQprK2tGTlypBJEAbLvLEnfgqioKAYNGgTADz/8oARRQHq2rLi4OGVbKp158+axZs0aKlSowIgRI0hLSyMsLIz27dvj5ORE7969SUpKwtHRMdMEUcAfbdr06dMJDw+nXLlyLF26lC5dugAQGRnJ/PnzMTExoWLFioSEhDB58mR27tzJ0qVLleM4OjoyevRoJduGzHopSZIkSZIkfWucnJwYNWoU3bp1Y9myZdjY2NC3b1+DMeZTp05x4cIFOnfuTGBgIMHBwezduxdTU1OGDx9ucO8hSZL0KX8rkCI2Npb4+HjatWuHp6enso+pubk5BQsWJCQkhL59+7Jjxw5UKhWTJ09m3bp1nD9/nqxZs5I3b15lsimzB1FAesM8cuRIunTpwtKlS7G2tv5kw3zmzBn69+/PvHnzlMEMObAhSdLXbM6cOcyfP5/BgwfTpEkTnJycmDBhAg4ODty7dw+NRkPHjh3x8PAw6KD+/vvvpKWlUbRoUaNeA6KiogAoUKAAnp6eeHt7Y2JiwsSJEzl27BizZs3C29sbd3d3evfurawOjIiIoFevXnh4eBjsw3fr1i0gPVAks9GfYH737h2vX7/GxsYGOzs7ihUrRkhIiDLhsG/fPl6/fk2XLl0M9t2Oi4ujQIEC9OzZk5SUFBlE8RUwdh/l48CH1NRUZfs0CwsLihQpwu+//06jRo1ITEykcuXK9OrVSwmiALh37x4AZcuWlUEUXwiVSoWXlxfh4eGcO3cOBwcHJTjtS7g3kCTpn/H48WMSEhKoWLEidevWBeDRo0e4ubnx7NkzAGXLNzDc2mnAgAHkzZuXLl26cOzYMapVq4aTkxMAQ4cOVd5j7DZF/zr3/Plzli1bRkBAAJ07dwbA29ubrl27AunBFHPnzlUyUwQHB9OkSRMePnzIo0ePcHd3J0+ePEpgibHrJkmSJEmSJEnGUqpUKUaPHs348eOZN28eDx48oE2bNjg7O3Pjxg0iIiJISUmhRIkS+Pr6Mnr0aKZMmcKuXbuwsLBgypQpcgxJkqT/6G8FUjx8+BCNRsO7d++A9JW4ukFvSF/ZOmDAAIYOHcqePXuA9G0+Pk63nBlWQf63/P39GTNmDOPGjWPevHk8fPiQtm3b4uzszPXr14mIiECr1eLp6SmDKCRJ+mZYWlpibW3N3LlzUalUNGnSBEdHR4YMGQL8MXicmpqqvOfcuXMsWbIEW1tbowYcCCHYunUr58+fZ+zYsUoQwaNHj4iOjiY6OpqJEycyatQovLy8yJcvHwMGDCAlJYVt27aRnJxMt27dlMDAs2fPsmLFCmxtbSlUqJDR6vUpWq1WuTlYtWoV27Zt4+LFi2TNmpVChQoxbdo0KlSoAKQP8h86dAh/f38qVaqkHOPw4cPcuXOH7t27G6wal4P40l+l/7lcvXo1Fy9e5N69ezRu3JhSpUpRqFAhBgwYwPnz5zl9+jT29va0bNlS+c5B+vduyZIlZMmSheLFixurKtJfoAvCLlu2rPKYbE8k6dvy/v17Pnz4wLFjx4iOjmbVqlVs2bKF9evXK9kdb9y4Qd26dZk3b94nsxLpMi29ePECyNiOGLtN0V3nQkNDsbOzw8XFhZ49e1KyZEklo5mXl5dBMMW8efMACAwMRK1WG2Tk0PmSxlIkSZIkSZIk6d9Qv3597O3tGTVqFNu3b2f79u3KcyqViqCgIMqVKweAn58fQ4YMITQ0lK5du8ogCkmS/k9/K5DC19cXa2trrly5AoC5uXmGAYtChQphbW1NbGwskZGRJCQkEBISgpWVldFSt/9d9erVUxrmbdu2sW3bNlQqlTKIERQURMWKFZXXf2n1kyRJ+m/pAsW6d++OjY0Ns2fPZvbs2QBKMAWkDx5fuHCBdevWUa1aNZ4/f87KlSu5f/8+I0aMMJhA+9xUKhWxsbGcOXOGyZMnM2PGDFauXMnt27fp06cP5ubmHDx4kLFjxzJ27FglmEK3796+ffs4cOAAVatWJSEhgevXr5OQkMCIESMoXbq00er1Mf2B9hkzZrB48WKcnZ0JDAwkJiaG2NhYg+vVu3fvePfuHW/evOH27dt4enpy7tw5wsLCsLa2zlA3OYgv/VW6z86sWbMICwvDzMyMtLQ0oqKiqFixIp07d6ZYsWLMmDGDvn37cvHiRcaMGUPnzp1xcnLixYsXrFixgpiYGEaNGqXcHEtfhk/1k2V7Iknflu+++45mzZqxbt066tatS1paGpUqVcLExISAgADc3NzYt28fz58/Z/v27QZBFLq+qKWlJZC+mAMyZzty/PhxQkNDsbS0JC0tjfj4eADMzMyUenwqmMLMzEzZFu/j8RY51iBJkiRJkiR960xNTalWrRpqtZpdu3YRFRXF69ev8fX1pUSJEpQvXx5AySZcpkwZihcvjpWVlZFLLknSl0AlhBB/9c2vXr2ic+fO3Lhxg06dOvHjjz8C6ZkpTExMlBv8zp07Y2lpyaNHj6hfv74yMPCle/z4MTt27ODOnTu8evUKX19fSpcu/aeDHJIkSV+6j9Pv67Z00rV1v/zyCyEhIXz48IEBAwbQrFkz7O3tSUxMZNy4cWzbtk15r52dHQMHDlSyFBmzzTx//jyzZ8/m9OnT5MqVi5iYGBo1akS/fv1ITEzk559/5ujRo/j5+SnBFEIIoqOjCQ0NJTIyEjMzM1xcXGjdujVubm7UrFnT6PX6lK1bt/Ljjz9SqVIlBg0ahJeXFykpKcTHxyuTDwAJCQkEBQVx8OBB/Pz8yJ07N0eOHOHNmzcEBwfTrl07I9ZC+tocPXqUfv36UaRIETp37sz9+/fZvXs3Fy5cIDAwkN69e1O0aFESExMZPnw4Bw8eJC0tTXm/i4sLvXv3pnnz5kDm+95JkiRJn6bft6xbty7R0dEIIZg2bRq1a9cmMTGRmTNnsnr1agBKlCjB8OHDKVKkiHKMM2fO0K9fP5ydnVm8eDE5cuQwSl3+GzNnzmTp0qWkpqbSpk0b+vfvr2xbop/J8ubNmyxevJidO3fi5eVFv379qFy5shFLLkmSJEmSJElfLjlOJEnSX/W3MlI4OTkxcuRIunTpwtKlS7G2tqZv377KHvEAp06d4syZM/Tv35958+Z9Vdtd5MmTh27dun3yOdkwS5L0tRFCKAPdun2rIX3Fn67N002uh4SEMHv2bFQqFU2bNlWy+BQpUoS4uDiyZcumRAWD8dvMEiVKMGnSJFq2bMmLFy9wcnKiVq1a5MyZk7S0NGWP7aNHjxpkpnB3d6d3796kpaWxe/dusmTJQqlSpZQtB3SpmjOT33//HSsrK/r06YOXlxdpaWlYWFgYBFE8fvyYDx8+0LBhQ16+fMn58+c5f/482bNnZ8KECTRt2hQw/nmTvlwf9wOfPHmChYUFQUFB+Pj4UKFCBXx8fFiwYAGHDx8GUIIp5s6dy4kTJ7h37x4vXrzA29sbDw8PuVe8JEnSF0jXt1y8eDF3794lZ86cPH36lODgYFxdXfH396dHjx7cuXOHM2fOEBcXx+vXr4mJicHFxYXDhw+zYMECXr9+zfDhwzNtEEVqairm5uYMGjQIExMTFi5cyOrVq1Gr1Uq/SpflUpeZolu3bmg0Gnbt2kVSUpKRayBJkiRJkiRJmZ+uP61bP64be5LjRJIk/VV/KyOFzm+//ca4ceNITk6mXr16tG3bFmdnZ65fv05ERARXrlxhwYIFynYXX0MQhc6fNcySJElfq7Fjx7J27VqWL19OmTJllPZPPzPFsmXLmDFjBkIIBg0aROPGjXFwcPjk8TLLpOeqVauYMGEC9vb2vH37lgoVKhAcHEz+/PnRaDRERUUxbdq0DJkptFot0dHRzJo1i7179+Ll5cXYsWPx8/MzdpUyiI+Pp1atWjg6OirZQT4O9Hj16hWhoaH8/vvvrFmzBo1Gw6VLl7CxscHJyQkvLy8g85w36cujv/o4NjYWIQQLFizg2rVr/Prrr8rzQgjOnj3LggULOH78OJUrV6Znz54UK1bsT4/9NfUxJUmSviU3b95kw4YNdOzYkbCwMNatW4elpSVLlizB39+fZ8+eMXbsWA4dOoSVlRVZs2bFzs6Oe/fuoVKpGDZsGO3btweMfy34b/pIISEhLFiwABMTEyZPnkzDhg2V5/TLf+3aNeLi4qhUqdK/WWRJkiRJkiRJkiRJkj7hHwmk0Gg0HD58mFGjRhEXF5d+4P8fXGBiYkJQUJBMAS5JkvQVSEtLY/DgwezevRtnZ2dmzpxJ6dKlgYzbfMyePZuFCxdiZmb2fwZTZAbR0dFs3boVtVrN6tWrOXXqFGXLlmXMmDF4eHj8n8EUDx8+ZObMmezZswdfX1+CgoIoWbKksatl4P379zRo0IAPHz6wefNmHB0dP/m6ESNGsHHjRqZMmUKjRo0yPG/sCQrpy6U/ubR48WJ27NjB8+fPsbGxwd7envXr12Nqaqpkc/lUMEWfPn3w9fXNcDxJkiTpy/b+/Xtln+Lhw4ezefNmLC0tiYiIoGTJkrx69Yrdu3dz5MgRrl+/jqmpKeXLl6dq1apUrVoVMP51QT9Y8PDhw9y5c4cbN25gY2ND9erVyZ8/v5LV7b8NptAxdt0kSZIkSZIkSZIk6VvzjwRS6Dx+/JgdO3Zw584dXr16ha+vL6VLlyYgIACQN/6SJElfg+TkZKZMmcK6detwcHAgJCSEMmXKAIbBFBqNhtq1a/P69Wvevn1Ljx496NWrFxYWFkauwZ/TTd5evXqVqVOncvbsWcqVK8fo0aPx8PAgNTWV+/fv8/PPP3Ps2DF8fX0ZO3asMqkbHR1NSEgIO3fupFSpUoSHh2NpaWnkWhnq0aMHhw4dom/fvnTq1Alra2tlsF5X/0OHDtGjRw8GDhxI9+7djV1k6SukC7SytrbGwsKCN2/eABAUFESHDh0APhlMcfLkScqVK0e/fv3+Y2YKSZIk6cukP2agH0yhy0yh67PEx8djZmaGnZ3dJ9/7uX0cUDxr1iyWLl1KSkoKZmZmyjUtICCAhg0b8v333wMwd+5c5s2b98lgCkmSJEmSJEmSJEmSjOsf3bg9T548dOvW7ZPPySAKSZKkL8uftdvW1tYEBQUhhGD9+vX0799fCaZQqVRotVqEEJiampKamkrZsmV58OABTk5OmTqIAv7Y5sLHx4fhw4czZcoUTpw4wbhx4xg9ejT58+fH09OTIUOGYGVlxb59+3j06JESSJEvXz769OmDpaUlnTp1MkoQxcfn7eOtpzp27MiVK1eIjIzE09OTwMBALCwslL27AR49egRA7ty5P3Pppa+V/grdGzdu8Ouvv1KpUiX69OkDpK/anTdvHlOnTsXW1pamTZsaTDyVLFmSXr16kZqayu+//06rVq2MWR1JkiTpX2JiYqL0ZaZMmQLA5s2b6dSpE8uXL8fPzw8hxCeznH3u8YaLFy8SHx9P5cqVUalUSl9r4cKFhIWFUbJkSTp27EjevHm5fPkyR48eZdeuXTx//pwsWbJQoUIF+vbtixCC+fPnM2rUKIQQn8wGJkmSJEmSJEmSJEnS5/ePZqSAP1JQfjxxI0mSJH059Cc9b9++zZMnTzA3N8fDw0OZXE9KSmLKlCmsX78eBwcHZs+eTdmyZZVjHD16lN69exMWFoZarcbJyckodfmrhBBcv36dKVOmKJkppk6dyuvXr9m0aROBgYEIIahQoUKG9+oHJXxO+udt9+7dXLx4kadPn+Lj40OrVq2ws7MjNjaWiIgIVqxYQYECBejYsSO1a9dWglzOnz/P6NGjef78OeHh4XLVv/SPunPnDm/evKFLly5ERETg7+8PpH/fIiIimD59OgDjx4+nWbNmgGFmipMnT5KYmEiNGjWMVgdJkiTp3/epzBS2traEh4dTvHhxI5cOTp8+Tffu3SlWrBg//fQTrq6uANy8eZPOnTvj4ODAzJkzUavVynvGjRvHmjVrCAwMZPTo0Tg6OmJjYwPAvHnzmDt3LgCRkZEUKFDg81dKkiRJkiRJkiRJkiQD/3gghSRJkvRl05+MDw8PZ9WqVTx9+hQzMzM8PT2ZMGEChQsXBgyDKWxsbJgxYwYlSpTg+vXrzJ07l3v37rFs2TK8vLyAT+/3nJl9HEyRK1cuhBA8ffqUmTNnUrt2bSBzZF3SL8OcOXOYP3++wfOVKlVi2rRpZM2alQcPHhAREcG2bdswMzOjRIkSVKpUibi4OH777TdiYmIYNWoUrVu3NkZVpK/Uzz//zJIlS/Dz8+PFixfs3r0bc3Nzg89ueHi4EkwxYcIEmjZtCvwRTKEvM3zvJEmSpH+Pfjs/cuRINmzYAMCxY8fIli2b0fqUUVFRtG/fnnfv3jFhwgTq1q2rPLd//3569+7NmDFjaNmypfK4LlCiQoUKjB49GiEEmzZton379kqw8bRp03B0dKRLly6fvU6SJEmSJEmSJEmSJGX0j27tIUmSJH3ZdFtyQPpgbkREBPb29lSqVIn4+HguX75M3759mTVrFsWKFcPGxobg4GBMTEz49ddf6dmzJ3ny5OHZs2ekpaUxYsQIJYgCvrwsRSqVCh8fH0aOHMmUKVM4deoU1tbWjBw5UgmigM+fSvpjH+/JHRYWhoeHB61bt0alUrFq1SqOHDlC3759mTNnDu7u7nTt2pX8+fOzceNGDh8+zOHDhwHIlSuXQTYAOVkt/VOKFi0KwKVLl3B2dubmzZsUKVLE4DW6yaPp06czatQoAGWbj48DseTnUpIk6eumv83HxIkTSUxMJH/+/Dg7Oxu1XI8fPyYhIYGKFSsqQRSPHj3Czc2NZ8+eAZAlSxbl9aGhoYSGhhIQEMCAAQPImzcvXbp04dixY1SrVk0JpBg6dKjyHtn/kiRJkiRJkiRJkiTjk4EUkiRJkkI3SfnLL78QERFBpUqVGDRoEF5eXjx79ozBgwdz7tw5BgwYQEhICEWLFsXa2ppx48bh5OTE1q1beffuHb6+vjRv3lzZ4/lLHgxWqVR4eXkRHh7OuXPncHBwUIJDMku9dOdt+/btGc4bgJmZGZMmTeL06dP079+fOXPm4ObmRosWLWjSpAkHDhwgKSkJd3d3XFxcKFiwIJB56id9+YQQ1KpVi4ULF9KjRw9evnzJ+vXrKVKkCCYmJgaZcD4Opvjw4QNt2rT54gKxJEmSpL9PP5hi9uzZyuPG7KO8f/+eDx8+cOzYMaKjo1m1ahVbtmxh/fr1WFtbA3Djxg3q1q3LvHnzlCCKwYMH4+PjA0C2bNkAePHixSfrI/tfkiRJkiRJkiRJkmR8MpBCkiRJMnDv3j3Wr1+Pu7s7AwcONAgaePv2LTY2Njx9+pT+/fsrwRQA/fv3p3Hjxpibm2Nubq4MEH8Nk/FCCMzNzSlbtqzyWGaqlxACIQQHDx7E3NycAQMGKOftw4cP7NmzhyxZsmBvb8+pU6eUc5c1a1asrKxo0KDBJ4+ZWeonfR2EEFSuXJmwsDC6d+/OunXrsLW1ZdiwYZiammYIptBqtcycOZO0tDQjl1ySJEkyJhMTE4OsRMbuo3z33Xc0a9aMdevWUbduXdLS0qhUqRImJiYEBATg5ubGvn37eP78Odu3bzcIotDVw9LSEgAXFxeljpIkSZIkSZIkSZIkZS7ybl2SJEky8Pz5c+7du0ejRo3w9vZWHl+0aBFPnjxh4cKF1K1bl6dPn9K3b18uXLigvMbNzY0cOXIoQRTGHuj+p3xqJXxmqpdKpSI+Pp7ff/+dfPnyKasdAebOncvx48eZOHEiCxcuxN3dnZMnT9KjRw8SExP/4zEl6Z+iP/kVGBjIokWLAFi6dCk///wzgBJModOtWzc2bNhAhw4dPnt5JUmSpMxFv19izD6K7jo1fvx4JYOXqakpDRs2xM3NDVtbWypWrMiDBw/Yvn07JUqUYMCAAUrfTKVScebMGfbu3UuhQoWUQApJkiRJkiRJkiRJkjKfzDMLJEmSJBmNEAKtVguk7/us0WhISkpSnl+6dClr166lVatW+Pn50bVrVzw9PXn+/Dndu3dnz549vHz5MsNx5WT8v08IAYCDgwPZsmUjKSmJV69eAbB+/XrCw8Np2LAhhQsXxt3dnTp16mBqasqFCxeoV68e69ev5/3798asgvSN0A+mqFSpEosXLwZgyZIl/PTTT0DGYApfX18ApX2SJEmSJGPSZU5avHgxd+/eJXv27Gg0GoKDgzl37hx2dnb06NGDUqVKARAXF8fr16+JiYkhLS2N/fv389NPP/H69Wu6du1Kjhw5jFkdSZIkSZIkSZIkSZL+AxlIIUmS9I3Sn5hUqVSkpKQA4O3tjY+PDwkJCQAcPnyYsLAwSpUqRcOGDbGwsMDNzU3ZA/rt27f069ePqKioz1+Jb9DHE8q6yem0tDRq1aqFWq3G1NSUa9eusWTJEvLly0fLli2VFY8uLi5oNBry5MnD06dPSUtLw8rK6rPXQ/o26QdTVKxYUQmmWLp0KdOmTQPSJ6l0AUI6mSkDjCRJkiRVrFiRNm3asHLlSpo1a8b79+/p1KkT586dw8XFhWnTplG5cmUePHhAv379aNmyJQ0aNKBPnz5cv36d4cOHU79+fYAM1zxJkiRJkiRJkiRJkjIHlZB37ZIkSd8cjUajrKjbsWMH586d48qVK3h4eFC6dGksLS3x9/cnZ86cjBgxgq1btxIWFkb58uWVY7Rr147k5GS+//57HB0dadSokbGq883QP28XL17k8ePHREVFUaJECYoWLYqFhQXv37/H0dGR9evXM2bMGCZMmECTJk2UY8ycOZP169ezYsUK0tLS8PLyMlZ1pG+YrvupUqk4evQoXbt2BaBly5aMGTPGmEWTJEmSpP/K+/fvlWDU4cOHs3nzZiwtLYmIiKBkyZK8evWK3bt3c+TIEa5fv46pqSnly5enatWqVK1aFUgPkJXBgpIkSZIkSZIkSZKUOZkZuwCSJEnS5yWEUCbjZ82aRVhYGAAWFhZcvnyZN2/eMGXKFBwdHXnx4gXbt2/Hz8+P8uXLk5aWhpmZGYcPH+b06dN0796dTp06KceWg8H/Hq1Wq5y3efPmsXz5ct6+fQukr+Dv2bMnPXr0wNraWkkdrVKpKF68uHKMM2fOsHHjRvLmzUuOHDmwtbVVji3Pm/Q5qVQqhBBKZoqwsDC6d++Ok5OTsYsmSZIkSf8VKysrpQ81ZcoUADZv3kznzp1ZsmQJ/v7+tGjRgpYtWxIfH4+ZmRl2dnbK+2X/S5IkSZIkSZIkSZIyNxlIIUmS9I3Rpdb/5ZdfCAsLo2LFivTq1YtcuXIRFRWFu7s7jo6OQPoAr4WFBW/evOHDhw9YWlpy7tw5FixYgLW1NSVLljQ4thwM/vfo/rbz589n7ty5FC5cmNatW/Pq1StevHhBnTp1MDMzU17r4OCARqNh48aNDBw4kKNHj7Jo0SLi4uIICgoyGMiX500yBv1gisDAQA4cOECuXLmMXSxJkiRJ+q+ZmJh8MpiiU6dOLF++HD8/P4QQODg4fPK9kiRJkiRJkiRJkiRlXnJrD0mSpG/Q8+fP6dmzJ7GxsSxcuBAfH59Pvu7x48d07tyZBw8eULRoUQoVKsShQ4eIi4sjODiYdu3afeaSf1uEEErgC8CFCxfo3r07+fLlY8KECajVasBwyw+dS5cu0bJlS7RaLS4uLrx48QKAoKAgOnTo8MnjS5Ix6IIpdBNKcoWuJEmS9KXRv3bptvmwtbUlPDzcIDuYJEmSJEmSJEmSJElfDjlKLUmS9A2Ki4vj9u3bVK1aFR8fH7RabYbXREVFsWLFCvLmzUv27Nm5fPkyGzduRKVSMW7cOCWI4lPvlf66+/fvc/fuXYAMQQ53797l7du3tG/fHrVajUajATAIorhy5Qrz5s3DzMyMJUuWkD9/frJkyUKlSpWYOXOmEkSh1WplEIX0t/0T33/9IIrY2FgZRCFJkiR9cXSZKQCmTJnCDz/8wLt372jZsiWxsbHI9SuSJEmSJEmSJEmS9OWRW3tIkiR9g54/f05aWhqvX78mNTU1QzYDgOzZs3Pw4EGsrKxYsmQJt27dwtHRkezZs+Pp6QnIleP/tJs3b9K4cWMqVKjA+PHjyZEjB/BHxomrV68CYGlpCXw6JfSTJ0+YO3cur1+/ZuTIkSxduhQbGxsAsmTJAsjzJv11H392Pv4c/a+fLf3Xb9y4ke3bt9O7d+8M2wZJkiRJUmanv83HxIkTSUxMJH/+/Dg7Oxu7aJIkSZIkSZIkSZIk/QUykEKSJOkbVLBgQXLkyMGDBw9ITU3F3NzcYHsIrVaLtbU1Xl5e7Nmzh7dv31K3bl2DY+ivIpf+vocPH3L27Fly5syJtbW1wV7auvPi4eEBpG+5Aunn6eMgGC8vLywtLbl69SopKSm4uroCKCsh5XmT/ir9NuLy5cs8fPiQa9euUaBAAXLlykX58uX/chDFhg0bmD59OlqtluzZs/8r5ZckSZKkf5t+MMXs2bOVx2UQqyRJkiRJkiRJkiR9eWQghSRJ0jfI0dERHx8fDhw4wNixY/n5558xNTUlLS0NExMT5T8HBwdsbGyUTAb65LYQ/5wTJ04wefJkGjVqxNSpU/H29sbKyopdu3aRL18+vLy8AChUqBDm5uYsXLiQypUrky9fPmVyWzdAny9fPrJly0ZqaqrB79CdL3nepL9CP2hn3rx5/PLLL7x588bgNW3btqVbt27/VSDEx0EUM2fORKPRsHLlStzd3f/5CkiSJEnSZ2JiYoIQQulzySBWSZIkSZIkSZIkSfoyybt5SZKkb5CdnR1Dhw7F3t6erVu3Mnr0aADMzMyUgd4zZ86wd+9ecufOja2trTGL+1W7cuUKvXv3JiYmhnz58lGqVCns7OxYvnw5AwYMYPny5dy9exeAgIAAvvvuO+Lj4+nbty+PHj0yCKIA+P3334mJiSF//vyoVCplv25J+jt0n69Zs2Yxd+5ccubMyYQJE5gwYQL9+/fH3NycFStWMGnSJJ49ewbwp/vBfyqIIiUlhZUrV6JWqz9PhSRJkiTpX6QfuCqDWCVJkiRJkiRJkiTpyyQzUkiSJH2jPDw8CA0NpXfv3qxbt44XL17QokUL8uXLx40bN1iyZAmvX79m8ODB5M6d29jF/Wrt27ePpKQk+vbtS9WqVZXH8+bNi1qtJjIyEpVKRbt27fDy8uKnn37i1atXHD9+nPbt2zN9+nTy5cuHk5MTJ06cYN68eQDUrFkTc3NzY1VL+godPHiQiIgISpUqxciRIw2CHkqVKsXkyZPZtWsXtra2TJo0CZVKZbAiF/48iGLVqlUyiEKSJEmSJEmSJEmSJEmSJEnKNGQghSRJ0jesdOnSREREMHDgQA4dOsShQ4eU56ysrBgxYgQ//PADQIYJUemfERsbC8Dt27cBmDx5MklJSUycOBGtVsuCBQvYunUrgBJMMWPGDH788UeOHj1K586dcXV1xcHBgatXr5KWlkZQUBA1atQwWp2kr9O1a9dIS0uja9euqNVqhBBKu1CyZElGjhxJv3792LhxI97e3rRp00YGUUiSJEmSJEmSJEmSJEmSJElfJJX4s7zLkiRJ0jfj2bNnHDx4kKtXrxIfH0/RokUpXrw4pUuXBgwnQKV/1s2bN+nfvz8PHjzA09OT27dvU716dcaPH4+TkxN79+4lLCyMGzduUL9+fTp06KBMOoeGhnL+/HmuXr2KnZ0dXl5e1KtXj++//x6Q5036Z+i2h2nVqhUXL15ky5YtFCpUCCDD52vv3r307duX8uXLM3fuXGxsbJQtZmQQhSRJkiRJkiRJkiRJkiRJkvSlkBkpJEmSJHLkyEHLli0/+ZycjP9n3bt3D3Nzc9zc3NBqtXh5eTFp0iR69erFnTt3yJUrFz169MDJyQlAySwRFhamZKZo3749Xl5e9OnTh6SkJGJjY7Gzs8PS0hJbW1tAnjfpn6P7HOXNm5eLFy/y+vXrDJ8tXWaK4sWL4+zszMOHD0lNTVUyUuhev3HjRhlEIUmSJEmSJEmSJEmSJEmSJGV6coZFkiRJAtInQnX/6icrkpPx/5yrV69Su3Ztxo4dS1xcnMEK/bdv32JtbU1MTAwbNmzg3bt3yvtq1KhB9+7d8fb2ZuvWrSxfvpxbt24BYGNjQ968eXFycsLGxgZIP4fyvEl/hX478PFjHh4eACxatIjHjx9/8v3Ozs5YWlpiaWmJmZlhvO5vv/3GuHHj0Gg0MohCkiRJkiRJkiTp/7V379FVlXf+x9/n5CTkJoSLXJOACHKnBC2KguCtdKBqV00R5TYIjFwUQcpQSp3Fsl6gjkQQkKBQwEFBZwYUQ0WKoMvxwm1BRIEgWBHCpQihUC5Jzjm/PzBHIu38RgSj5P36B3L23g/7WTxr5az9fPb3K0mSpO81d1kkSQCxN8cDgUDs7zq/jh49SlJSEvHx8bHKEQBFRUVkZWUxZswY0tPTWbhwIY899hjHjx+PnfP1MMXzzz9PQUFBufHP/D+UvqlwOBxbO0eOHOHQoUMcOnQo9lnv3r1p374969at48UXX2T//v1nXffWW2+xZ88errzySpKTk2NtQcLhMPHx8VSvXp25c+caopAkSZIkSZIkfa8Fome+cihJki6ogoIC6tevT2pqKq+99hrXXHMNtWrV4uDBg9SqVYtVq1bxu9/9jsLCQu644w7Gjx8fqzQBsGLFCmbPns3GjRv56U9/yoQJE0hLS6u4CemiEA6HiYuLA+Dll19m2bJl7N69m8TERB566CE6dOhAOBzm9ddfZ8qUKfzlL3/h1ltv5Z//+Z9p3LgxAGvXrmXq1Kls3LiRqVOncsMNN5T7N06cOEFJSQlVq1b9zucnSZIkSZIkSdI3YZBCkqTvQDQaLVcpYu7cuUycOJHu3bszevRoGjRoAMCpU6d47733+N3vfseePXv+bphi+fLlTJ48mT59+tC3b9/vfC66uEQikVgrmMmTJzNr1iwAatSowaFDh6hSpQqTJk3ipz/9KSdOnCAvL4958+axfft2kpOT6dixIyUlJbz//vsUFxczbtw4+vfvX5FTkiRJkiRJkiTpWzFIIUlSBXjrrbeYNGkSO3fu5Pbbb+e+++4jIyMDgOLiYt59993/NUyxa9cuMjMzgbNDGtK5mDZtGtOmTaN9+/bce++9tG7dmhkzZrBgwQISEhJ4/PHH6dGjB6dOnWLr1q0sWLCA5cuXU1xcTEpKCs2aNaNXr17ceuutQPmAhiRJkiRJkiRJPyQGKSRJ+g6d2ULhvffe49FHH+WTTz7htttu4/777/+HYYqePXsyduxYUlJSyo1niELnw3vvvceIESNo0aIF48ePp1mzZgBs3bqVQYMGcfDgQRISEmJVVMoUFBRw6tQpqlWrRkpKCjVr1gQMUUiSJEmSJEmSfth8wi1J0gUUiUT+4c8dO3Zk/PjxNGnShFdffZWnn36azz//HICEhASuvfZaHnroIRo2bMhLL73EQw89RGlpabnxDFHofNi0aRNHjx5l+PDhsRAFQG5uLpFIhOzsbIqLi/n1r3/NsmXLYuvwiiuuoE2bNmRmZlKjRg3gdLjHEIUkSZIkSZIk6YfMp9ySJF0g4XA4tqG8YsUKfv/739O3b18mT55MYWEhcHaYYtq0aWeFKcaOHcsll1xCmzZtCIVCFTYfXXyi0SiRSITNmzcDcOmll8aOTZs2jT/+8Y88+OCDPPLII9xxxx2xMMVrr73GgQMHyo1VFuox3CNJkiRJkiRJ+qEzSCFJ0gUQiURiLTyeeuopRo4cyZw5c9i4cSOvv/46Bw8eJBwOA+XDFK+88spZYYpOnTqxePFiBgwYAJze/Ja+rbK2MMFgkLp16wKwc+dOAF577TWee+45fvKTn3DVVVcB0L9/f9LS0mJhipEjR7J79+4Ku39JkiRJkiRJki4UX2uVJOkCKKtEMWPGDGbOnEmHDh0YOnQocPqN/aZNm8aCFnA6TDF27FgmTZrEK6+8QjAYZOjQoWRmZpKQkEB6ejpwOqBh2wSdi6+vnXA4HKtwcvfdd5OcnEybNm04ePAgL774IomJifTr149GjRoBEAqFKCoqonXr1iQlJXHTTTfF1qUkSZIkSZIkSRcTgxSSJF0g77//PvPmzaNNmzaMGzeOFi1axI7t3r2bHTt2sG3bNurXr8/PfvYzOnfuTCAQ4IknnmDx4sUcO3aMxx9/nNTU1Nh1hih0LsLhcCy4s3z5cjZu3MiaNWto164dV155Jd27d2f48OFUqVKFNWvW8OGHHzJgwIBYNQqADRs2ADBy5EiysrJISUkBvqpsIUmSJEmSJEnSxcIghSRJF8hnn33GkSNHGDBgAC1atKCkpIT4+HgWLVrEggUL2LlzJ6WlpQCsWbOGhx9+mE6dOhEIBPjNb35Du3btyoUopHMRjUZjIYqcnBxmz54dO/bRRx+xZcsWrr/++tha+/DDDykuLo4FJQDWr1/PvHnzqFu3LvXq1TNEIUmSJEmSJEm6qBmkkCTpPCvbXP70008B2Lt3L8eOHWP16tWsXLmSP/7xjwB07dqVWrVq8ac//YmlS5dy2223cdVVV3HdddexaNEi6tatW2486VyUrZ158+aRm5vLtddey9ChQ6lRowa7d+8mIyOjXGCnrJVHXl4e9erV469//SsLFy5k+/btPPzww1x++eVnjS1JkiRJkiRJ0sUkEI1GoxV9E5Ik/ZCVBR2+HnjIz8/nX/7lXygpKaF+/frs3LmTuLg4GjZsyH333Ue3bt0AmD59Ok8//TSTJk3i9ttv/7tjS9/Ema08APbv38+gQYM4fvw406dPp3nz5n93bf3tb38jEokwduxY3nzzzdjnoVCIcePG0bt3b8B1KUmSJEmSJEm6uFmRQpKkbyESiRAMBoHTm9fFxcUkJycD0LhxYx588EGmTp3Kn//8Z+rVq8c999xDhw4daNKkSWyMkydPEggEqF+//lnju1mt/6sNGzawZcsWevfuXS5EAaeromzfvp0BAwbQvHlzwuFwbN2W2bFjBzNnzqRVq1bk5OQwd+5ctm/fTnp6OllZWXTp0gUov+YlSZIkSZIkSboYGaSQJOkcnfnWf15eHm+99RaffPIJKSkpPProo2RmZtKzZ09uueUWDh06RJ06dcq1UABYt24dS5cuJTMzk5o1a1bENHQR2Lt3L7179yYajZKZmUnnzp2Br0IPf/3rXwE4ceIEwFlBC4CEhATy8vJ4++23yc7OZuDAgYRC5b8qGqKQJEmSJEmSJFUGBikkSToHkUgkthk9efJkZs2aBUB8fDzJycns2bOH9PR0AKpXr0716tUBKCgo4MiRI2RlZfE///M/PPPMM+zbt49HHnmExo0bV8xk9IOXlJTEgAED2L17N+3atYt9XhZ6qFOnDgBbtmzh888/JyMjo9z14XCYjIwMbr75ZlavXs2+ffvKVU35+niSJEmSJEmSJF3MDFJIknQOyjaUn332WWbNmkWnTp0YMmQIjRo1YteuXbRu3fqsTeedO3cyZMgQCgsLqV27NgcOHADg17/+NdnZ2QBEo1Hbeej/rKioiLS0NNLS0hg2bBgJCQkkJCQwdepUUlJSGDhwIAANGjSgW7duLF++nGXLljF48OByLWnOrFARFxdHQkJChcxHkiRJkiRJkqTvA4MUkiSdo/z8fBYsWEDTpk0ZPXo0LVq0AKBWrVps27aNbdu2sXr1an70ox/RqVMn6tSpQ8+ePVm5ciUnT54kKyuLW2+9lZtvvhmwbYK+mffff5/Jkydz7733ctNNN8XaxnzwwQfMmDGDpKQkkpKSuPvuu0lNTaVbt26888475OTkUKVKFbKzs0lNTY2FKNatW8d7773HZZddRmJioqEeSZIkSZIkSVKlZZBCkqRzdPDgQfbt20e/fv1o0aIFkUiE4uJiFi1axPz58zlw4AAlJSUsX76c1atXM3HiRIYMGcKgQYMoLi4mGAySmJgIGKLQN3P8+HH+67/+i/z8fJ577jni4+O5/vrrAWjatCmjRo3i6aef5sknnyQSidCnTx+6d+/OwYMHeeyxx5g4cSKffvopnTt3Jisri7Vr1zJnzhyOHj1Knz59qF27dgXPUJIkSZIkSZKkimOQQpKkc1RUVATArl272L59Oxs3bmTlypWsXr0agOzsbGrUqMHGjRtZu3Yt+fn53HLLLYRCIUKhENFoFDjdzsMQhb6J5ORkBgwYQCgUYvHixUybNo1oNEqXLl2oUaMGd955J4FAgJycHHJycohEIvTr149+/foRHx/P3Llzeemll1i0aBFVqlTh1KlTJCQkMG7cOH7xi18AtpmRJEmSJEmSJFVeBikkSfr/+PqGctnPP/7xj2nTpg0vv/wyy5cv5/DhwyQlJZGVlcXgwYO58cYbiUaj/Od//idr1qxhw4YN3HLLLbFxysZ0s1rfRNn6a9myJf3796e4uJi8vDymT59OJBLhhhtuIC0tjZ49ewKQk5PDlClTAOjXrx933XUXzZo1o6CggFWrVhGJRPjRj35EVlYW1113HWCFFEmSJEmSJElS5WaQQpKk/8WZG8rFxcWUlJQQFxdHYmIidevWZfDgwSxZsoSPPvqIVq1aMXDgQFq1akXDhg1jG94nT54kLi6Otm3bVvBsdDEIBAKUlJQQHx9P8+bNuffeewFYtmwZc+bMAfiHYYpAIEDfvn1p37497du354477iAQCBAKffWV0BCFJEmSJEmSJKmyM0ghSdI/EA6HiYuLA2Dx4sW88847FBQU0KpVK375y19y5ZVXcvPNN3PDDTdw6NAhqlatSlJSUuz6QCDAunXr+I//+A9q1qxJenp6RU1FF5FIJEJ8fDwAr7zyCuvXr2fz5s1Eo1E2bNhAaWkpwWCQLl26nBWmeOqppwgGg/Tu3RuAYDAYW+NlDFFIkiRJkiRJkiq7QLSsQbskSYo58638nJwccnNzCQaDRCIRAOrUqcPEiRPp2LEj8FW7hY0bN/LFF1/QqFEjPv30U2bMmMHHH3/MhAkT6NWrV4XNRxefyZMnM2vWLOrVq8fVV1/N8ePH2bJlC59//jmtW7fm/vvvp0uXLgAUFRXx0ksvkZOTQ9WqVbnvvvvo27dvBc9AkiRJkiRJkqTvJ4MUkiT9L6ZOncqMGTNo2bIlQ4YMITU1lcWLF7N06VJq1KjBk08+GQtTFBYWcs899/DnP/+ZUChEaWkpcXFxjB07ln79+gFfBS6kb+ONN95gxIgRXHvttYwbN46mTZsCkJ+fz9KlS3n++edp1aoVI0aMKBemePnll3nyyScBWLJkCc2bN6+wOUiSJEmSJEmS9H1law9Jkv6BlStX8oc//IGOHTsyduzY2KZzSUkJq1ev5tChQzz44IM89dRTXH311dSqVYt77rmH/Px8tm3bRps2bejSpUtsI/vMKhfSt7F582aCwSCDBw+madOmlJaWEgqFaNu2LQ0bNiQ5OZnc3FymT59ONBqla9eupKWlkZ2dzYkTJ6hWrZohCkmSJEmSJEmS/gGDFJKkSm3Xrl3ExcXRoEGDs46tWbOGkpISRo4cWW7T+eWXX6ZKlSp07tyZZcuW8cADD/Dkk09y3XXXkZ2dTc+ePSkuLiYhISF2jSEKnQ+RSIRIJMLatWuJRCJUr179rLVVrVo1evfuze7du1m2bBmzZ88GoGvXrlSvXp0hQ4bE1qbrUpIkSZIkSZKks/nkXJJUaeXn53P77bcza9YsCgsLY5+Hw2FOnjzJ22+/TUpKSrmQxZQpU/jTn/7E6NGjmThxIj/5yU8oKirigQce4J133oltSickJHBm9yw3q3U+BIPBWOUJgL179xIMBs9aX7Vr16ZTp05Eo1E2bdrE008/zcqVKwHKBXxcl5IkSZIkSZIknc2n55KkSmvHjh2kpqaSl5fHvHnz2LNnDwBxcXEkJibSvHlzIpEIBw8eBGDJkiU8++yz9OjRg2uuuYaEhARuvPFG4uPjOXbsGIMGDWLChAkUFRUBEAgEKmpqugicGcQpE4lEAGjSpAkAkydPZvv27eXOKS0tBaBDhw7UqVOHK664go8++ogTJ05c4DuWJEmSJEmSJOniYGsPSVKl1aNHDwKBADNnzmTRokUA9OvXjwYNGhCNRunatSspKSlUr16dXbt2MX/+fGrWrEmfPn2oV68eAA0aNKCkpIRmzZqxZ88eGjVqRFpaWgXOSheDcDhMXFwcAIWFhRQXFxMKhUhPTwfgl7/8JW+//TYrVqxg2rRpPPDAAzRu3JhwOEwodPrr3fr169m/fz8TJkygbt26tGjRosLmI0mSJEmSJEnSD4lBCklSpRSNRklISKB79+5Eo1FmzZoVC1P06dOHjIwMbr31Vm666SZSU1N58803+fjjjxkzZgxZWVmxcTZv3kwoFOLf//3fqV69OrVq1YqNb0UKnYtIJBILUfzhD3/gv//7vzlw4AAAw4cP54YbbiAjI4N7772Xv/zlLyxfvpyjR48ycuTIWMuPtWvX8sILL1C/fn0aNmxI48aNY2PbzkOSJEmSJEmSpP+dQQpJUqUUCARiYYp/+qd/IhAIMGPGDBYuXAh8FaZITU0lGo2yceNGgNiGNMC6detYsGABGRkZJCYmUrNmTcDNan07ZWsnJyeH3NxcQqEQDRs2ZMeOHUyaNIldu3bRv39/Wrduza9+9SumTZvGu+++y7vvvku7du2Ij49n06ZNFBcXM378+HJr1nUpSZIkSZIkSdL/n0EKSVKlUxZ0KKsYkZiYyM9//nMikQjz5s3jxRdfBL5q8xEIBLjkkksAyM3NJTMzk48//pj58+eze/duHn30UTIyMmLju1mtc3FmAOfdd9/l+eef5/rrr+e+++6jbdu2LFiwgPnz5/PCCy8QDocZOHAgV111FVOmTGH27NmsWrWKTz75hCpVqtCuXTuys7O57bbbACukSJIkSZIkSZL0TQSi0Wi0om9CkqTvSjgcjrVNWLZsGevXr2fTpk00a9aM9u3bc/LkSRYsWEBhYSF33nknffv2JT09nSNHjtC/f3+2bt0aq2YBMG7cOPr37w+4Wa1zd+baOXbsGB988AGjRo1i7ty5tG/fPnZeXl4eM2bM4NNPP6VXr17069ePRo0aAXDw4EGOHz9OYmIiVapUoVq1aoAVUiRJkiRJkiRJ+qasSCFJqjSi0WgsRFHWNgEgISGBzZs388UXX/Dwww+TkpLCzJkzWbRoEQC9e/cmMzOTZ599lscff5zDhw+Tnp5O165duemmmwA3q/XtlIUoJkyYwPLly7n88stp3bp1LERRWlpKKBSiR48eBAIBpk+fzsKFCwkEAtx9991cfvnl1KpVKzZeWdAnGo26LiVJkiRJkiRJ+oYMUkiSKo2yzer58+eTm5tL586dGTZsGPXr12fHjh1kZGRQu3ZtunfvTjQa5bnnnou1+SgLUzzxxBNEIhECgQCh0Olfo4YodL6UlpZy+PBhtmzZQnp6OgcOHKB27dqEQqHYOuvevTsA06dP58UXXyQYDHL33Xdz2WWXxcYpW+tWSJEkSZIkSZIk6ZszSCFJqlT279/PkiVLqF27NqNGjaJly5YA1KlTBzgdikhISOCaa67hb3/7G6+++mpss/quu+4iMzOTuLg4zuyMZYhC31ZZa49HHnmEqlWrMmfOHLZt28Y777zDL37xC+B0KOLrYYrc3Fyef/55jh07xrhx46hatWpFTkOSJEmSJEmSpIuCOz+SpErliy++oKCggBtvvJGWLVsSiUTKHQ8Gg+zYsYNnnnmGDz/8kNtvv53MzEzmzZvHc889x7FjxwDf9Ne3c2brDTi9nkpLSwH413/9VwYOHAjAb37zG958883YOWVhCoDu3bszePBgateuTYsWLQxRSJIkSZIkSZJ0nliRQpJUqezfvz/WPqGkpIS4uLizzrn00kv54IMPABg5ciQpKSk88cQTNGvWjNTU1O/6lnWRCYfDsXV38uRJDh06RHx8PIFAgEsvvRSAMWPGADB79myGDRvGjBkzuPHGG2MBnrLKFD/72c9o3bo1jRo1Ar6qbCFJkiRJkiRJks6dQQpJUqXSpEkT6taty2effUZJSQnx8fHlNrYjkQhJSUk0a9aMN954g71799KjRw9at25NkyZNKvju9UN35lpbuHAhr7/+OuvWrSMhIYH4+Hj69+9Px44dycrK+j+HKQxRSJIkSZIkSZJ0ftnaQ5JUqVSvXp2WLVuydetWJkyYAEBcXBylpaWxjen4+HjS0tJITk4mOTmZ+Pj4WIiirBWD9E1Fo9FYiOKJJ55gwoQJ5Ofn065dO1q1asWRI0eYOnUqv//973njjTeA05UpBg0aBMCwYcNYtWpVbLxgsPzXOEMUkiRJkiRJkiSdH1akkCRVKqmpqYwZM4Z169bx6quvkpiYyMMPP0wo9NWvxLVr17JixQoaNGjAJZdcUu56N6t1rsrWzgsvvMDs2bPp2rUrI0eOpHnz5gCsXr2apUuXkpeXRzQaJSkpic6dO/OrX/2KYDDIrFmzGDp0KFOmTKFbt24VORVJkiRJkiRJki5qBikkSZXOZZddxrRp0xg+fDgvvfQSBw4coFevXjRq1IgtW7YwZ84cDh8+zOjRo2nQoEFF364uIkePHuX111+natWqDB8+nObNm1NaWkooFKJr165kZGSQkJDAkiVLyMvLo02bNqSlpfHggw8SDoeZPXs2hYWFFT0NSZIkSZIkSZIuaoGoNcolSZVUfn4+o0aNYs+ePeU+T0xMZPTo0fTt2xc43ZLBShQ6H3bt2sXPf/5zrr76ap555pm/u7Y2btzIb3/7W3bs2MGcOXPo2LFj7NiGDRto3779d33bkiRJkiRJkiRVKlakkCRVWm3btmXBggWsWrWKzZs3U1RURNu2bcnKyqJDhw4ARCIRgsFgBd+pfsii0SjRaJRgMEhRURHHjx/n8OHDHDt2jJSUlLPOb9euHd26dWP69Om8//77dOzYMVa1oixEEQ6HiYuL+66nIkmSJEmSJElSpWCQQpJUqdWtW5e77rrr7x4zRKFzdebaCQQCnDp1isTERNLT02nVqhWFhYUcPnyY1NTUcqGIkpIS4uPjadq0KQDHjh0DIBQq/5XNEIUkSZIkSZIkSReOQQpJUqVX1l6hrNtVWasFQxQ6F2cGI5YtW8b69evZvHkzGRkZXHvttaSnp/PRRx8xfvx4nn32WapUqUJpaSlxcXHEx8cDsG/fPkKhED/+8Y8rciqSJEmSJEmSJFVKBikkSZVeWXCi7E/pXEWj0ViIIicnh9zcXAASEhLYtGkTxcXFjBo1iu3bt7NmzRpGjBhBTk4OycnJsTE2bNjAwoULqVmzJpmZmRUyD0mSJEmSJEmSKrNAtOz1W0mSJJ0X8+fP57HHHqNz584MGzaM+vXrs2PHDtLT08nMzKSgoID777+fzz77jCuuuII777yTjIwM9uzZwwsvvMD27duZMGECvXr1quipSJIkSZIkSZJU6RikkCRJOo/279/P0KFDOXjwIDNnzqRly5bljkciEYLBIPn5+YwfP569e/dy7Nix2PFq1aoxYsQIevfuDXzVekaSJEmSJEmSJH03bO0hSZJ0Hn3xxRcUFBSQnZ1Ny5YtY8GJMsFgkB07dpCXl0fTpk0ZNGgQn3/+OYcOHaJ169Y0bdqUNm3aAJx1rSRJkiRJkiRJuvAMUkiSJJ1H+/fvp7S0lMOHD1NSUkJcXNxZ51x66aWsXr2aaDTKb3/7W2rUqHHWOdFo1BCFJEmSJEmSJEkVwKfzkiRJ51GTJk2oW7cun332GSUlJQSDQcLhcOx4JBIhKSmJZs2asWvXLj7++OPY52eynYckSZIkSZIkSRXDIIUkSdJ5VL16dVq2bMnWrVuZMGECAHFxcZSWlsZadcTHx5OWlkZycjK1a9cGsPqEJEmSJEmSJEnfEz6xlyRJOo9SU1MZM2YMVatW5dVXX+Xf/u3fAAiFQrGwxNq1a1mxYgUNGjQgJSWlIm9XkiRJkiRJkiR9TSAajUYr+iYkSZIuNmvWrGH48OEcPXqUrl270qtXLxo1asSWLVuYM2cOH374IY888gjZ2dkVfauSJEmSJEmSJOkMBikkSZIukPz8fEaNGsWePXvKfZ6YmMjo0aPp27cvANFolEAgUBG3KEmSJEmSJEmSvsYghSRJ0gW0b98+Vq1axebNmykqKqJt27ZkZWXRoUMHACKRSKzlhyRJkiRJkiRJqngGKSRJkiqIIQpJkiRJkiRJkr5/fHIvSZJ0gZXlVqPRKGdmWA1RSJIkSZIkSZL0/WNFCkmSJEmSJEmSJEmSpC/5GqQkSZIkSZIkSZIkSdKXDFJIkiRJkiRJkiRJkiR9ySCFJEmSJEmSJEmSJEnSlwxSSJIkSZIkSZIkSZIkfckghSRJkiRJkiRJkiRJ0pcMUkiSJEmSJEmSJEmSJH3JIIUkSZIkSZIkSZIkSdKXDFJIkiRJkiRJkiRJkiR9ySCFJEmSJEmSJEmSJEnSlwxSSJIkSZIkSZIkSZIkfckghSRJkiRJkiRJkiRJ0pcMUkiSJEmSJEmSJEmSJH3p/wGi/COCAPmJWwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 2500x1000 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "msno.bar(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rLXwpGS73Y3n"
      },
      "outputs": [],
      "source": [
        "df=df.drop(columns=[\"latitude\",\"longitude\",\"regional\",\"delegacia\",\"uop\",\"ano\",\"municipio\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "KpFej2p74snq",
        "outputId": "8fcfdf0c-c452-42aa-ffe7-a26e59268676"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>missing</th>\n",
              "      <th>total</th>\n",
              "      <th>percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>br</th>\n",
              "      <td>894</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.045122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>km</th>\n",
              "      <td>894</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.045122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fase_dia</th>\n",
              "      <td>70</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.003533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condicao_metereologica</th>\n",
              "      <td>61</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.003079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tipo_acidente</th>\n",
              "      <td>40</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.002019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <td>26</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.001312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uf</th>\n",
              "      <td>12</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tracado_via</th>\n",
              "      <td>10</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tipo_pista</th>\n",
              "      <td>10</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uso_solo</th>\n",
              "      <td>10</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>causa_acidente</th>\n",
              "      <td>2</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quarter</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ilesos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quarter_end</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>veiculos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feridos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ignorados</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data_inversa</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feridos_graves</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feridos_leves</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mortos</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pessoas</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dia_semana</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentido_via</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>horario</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour</th>\n",
              "      <td>0</td>\n",
              "      <td>1981317</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        missing    total   percent\n",
              "br                          894  1981317  0.045122\n",
              "km                          894  1981317  0.045122\n",
              "fase_dia                     70  1981317  0.003533\n",
              "condicao_metereologica       61  1981317  0.003079\n",
              "tipo_acidente                40  1981317  0.002019\n",
              "classificacao_acidente       26  1981317  0.001312\n",
              "uf                           12  1981317  0.000606\n",
              "tracado_via                  10  1981317  0.000505\n",
              "tipo_pista                   10  1981317  0.000505\n",
              "uso_solo                     10  1981317  0.000505\n",
              "causa_acidente                2  1981317  0.000101\n",
              "quarter                       0  1981317  0.000000\n",
              "ilesos                        0  1981317  0.000000\n",
              "quarter_end                   0  1981317  0.000000\n",
              "month                         0  1981317  0.000000\n",
              "year                          0  1981317  0.000000\n",
              "veiculos                      0  1981317  0.000000\n",
              "feridos                       0  1981317  0.000000\n",
              "ignorados                     0  1981317  0.000000\n",
              "data_inversa                  0  1981317  0.000000\n",
              "feridos_graves                0  1981317  0.000000\n",
              "feridos_leves                 0  1981317  0.000000\n",
              "mortos                        0  1981317  0.000000\n",
              "pessoas                       0  1981317  0.000000\n",
              "dia_semana                    0  1981317  0.000000\n",
              "sentido_via                   0  1981317  0.000000\n",
              "horario                       0  1981317  0.000000\n",
              "hour                          0  1981317  0.000000"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.stb.missing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxs9QwrcB2B2",
        "outputId": "75170c8d-7018-4a55-b46f-ea2fe31b19c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "Index: 1981317 entries, 1256430 to 1788519\n",
            "Series name: date\n",
            "Non-Null Count    Dtype         \n",
            "--------------    -----         \n",
            "1981317 non-null  datetime64[ns]\n",
            "dtypes: datetime64[ns](1)\n",
            "memory usage: 30.2 MB\n"
          ]
        }
      ],
      "source": [
        "df[\"date\"]=pd.to_datetime(df[\"data_inversa\"])\n",
        "df[\"date\"].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GqRriZeZDclc"
      },
      "outputs": [],
      "source": [
        "df=df.set_index(\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Kc3tvPCJ5Npz"
      },
      "outputs": [],
      "source": [
        "cat_col=df.select_dtypes(exclude=\"number\").columns.to_list()\n",
        "num_col=[col for col in df if col not in cat_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x058MIZmDcCX",
        "outputId": "6212ddde-e517-40a6-cb7c-761abfe610cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['br',\n",
              " 'pessoas',\n",
              " 'mortos',\n",
              " 'feridos_leves',\n",
              " 'feridos_graves',\n",
              " 'ilesos',\n",
              " 'ignorados',\n",
              " 'feridos',\n",
              " 'veiculos',\n",
              " 'year',\n",
              " 'month',\n",
              " 'quarter']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lNzXZ1iVBfvN"
      },
      "outputs": [],
      "source": [
        "cat_col=[\n",
        "  #'data_inversa',\n",
        " 'dia_semana',\n",
        " #'horario',\n",
        " 'uf',\n",
        " #'km',\n",
        " #'municipio',\n",
        " 'causa_acidente',\n",
        " 'tipo_acidente',\n",
        " 'classificacao_acidente',\n",
        " 'fase_dia',\n",
        " 'sentido_via',\n",
        " 'condicao_metereologica',\n",
        " 'tipo_pista',\n",
        " 'tracado_via',\n",
        " 'uso_solo',\n",
        " 'hour',\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZjamqeYADbGm"
      },
      "outputs": [],
      "source": [
        "df['dia_semana']=df[\"dia_semana\"].apply(lambda x:convert_lower_case(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pYVmPPvZEnq6"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWezjnevG-Vd",
        "outputId": "1884b2af-cfa6-45fd-9b96-00290bc34f07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "causa_acidente\n",
              "Falta de atenção                      25.62\n",
              "Outras                                48.40\n",
              "Não guardar distância de segurança    56.37\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.round(df[\"causa_acidente\"].value_counts(normalize=True)*100,2).nlargest(3).cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INPKj3lnO0G9",
        "outputId": "deb302b8-47d9-4eeb-d2c9-e6fbb9a0360f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tipo_acidente\n",
              "Colisão traseira       25.86\n",
              "Colisão lateral        40.47\n",
              "Saída de Pista         52.23\n",
              "Colisão Transversal    60.13\n",
              "Tombamento             65.02\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.round(df[\"tipo_acidente\"].value_counts(normalize=True)*100,2).nlargest(5).cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hl9CCLRQPn21"
      },
      "outputs": [],
      "source": [
        "for col in cat_col:\n",
        "  df[col]=df[col].apply(lambda x: convert_lower_case(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DpQmymHJPWm",
        "outputId": "24961bf6-afdb-47d7-b962-f9f85debe802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['segunda' 'quinta' 'domingo' 'terça' 'sexta' 'quarta' 'sábado'\n",
            " 'segunda-feira' 'terça-feira' 'quarta-feira' 'quinta-feira' 'sexta-feira']\n",
            "['pa' 'mg' 'sc' 'pe' 'ro' 'go' 'se' 'rs' 'ms' 'df' 'rj' 'pr' 'ba' 'es'\n",
            " 'ce' 'pi' 'sp' 'ma' 'rn' 'pb' 'ap' 'mt' 'al' 'to' 'rr' 'am' 'ac']\n",
            "['desobediência à sinalização' 'outras' 'dormindo'\n",
            " 'não guardar distância de segurança' 'defeito mecânico em veículo'\n",
            " 'falta de atenção' 'animais na pista' 'velocidade incompatível'\n",
            " 'ingestão de álcool' 'ultrapassagem indevida' 'defeito na via'\n",
            " 'condutor dormindo' 'desobediência às normas de trânsito pelo condutor'\n",
            " 'falta de atenção à condução' 'pista escorregadia'\n",
            " 'falta de atenção do pedestre'\n",
            " 'sinalização da via insuficiente ou inadequada'\n",
            " 'avarias e/ou desgaste excessivo no pneu' 'mal súbito'\n",
            " 'defeito mecânico no veículo' 'fenômenos da natureza'\n",
            " 'restrição de visibilidade' 'objeto estático sobre o leito carroçável'\n",
            " 'carga excessiva e/ou mal acondicionada'\n",
            " 'deficiência ou não acionamento do sistema de iluminação/sinalização do veículo'\n",
            " 'ingestão de substâncias psicoativas'\n",
            " 'reação tardia ou ineficiente do condutor' 'agressão externa'\n",
            " 'desobediência às normas de trânsito pelo pedestre'\n",
            " 'acumulo de areia ou detritos sobre o pavimento'\n",
            " 'ingestão de álcool e/ou substâncias psicoativas pelo pedestre'\n",
            " 'ingestão de álcool pelo condutor' 'curva acentuada'\n",
            " 'desrespeitar a preferência no cruzamento' 'pedestre andava na pista'\n",
            " 'pista em desnível' 'chuva' 'demais falhas mecânicas ou elétricas'\n",
            " 'pista esburacada' 'ausência de reação do condutor'\n",
            " 'transitar na contramão' 'obras na pista'\n",
            " 'ingestão de álcool ou de substâncias psicoativas pelo pedestre'\n",
            " 'conversão proibida'\n",
            " 'trafegar com motocicleta (ou similar) entre as faixas'\n",
            " 'condutor deixou de manter distância do veículo da frente'\n",
            " 'iluminação deficiente'\n",
            " 'acessar a via sem observar a presença dos outros veículos'\n",
            " 'deficiência do sistema de iluminação/sinalização' 'fumaça'\n",
            " 'manobra de mudança de faixa' 'demais fenômenos da natureza'\n",
            " 'falta de acostamento' 'acesso irregular' 'transitar no acostamento'\n",
            " 'condutor usando celular'\n",
            " 'condutor desrespeitou a iluminação vermelha do semáforo'\n",
            " 'acumulo de água sobre o pavimento'\n",
            " 'afundamento ou ondulação no pavimento' 'ausência de sinalização'\n",
            " 'estacionar ou parar em local proibido' 'entrada inopinada do pedestre'\n",
            " 'mal súbito do condutor' 'demais falhas na via'\n",
            " 'acumulo de óleo sobre o pavimento' 'acostamento em desnível'\n",
            " 'pedestre cruzava a pista fora da faixa' 'sinalização mal posicionada'\n",
            " 'desvio temporário'\n",
            " 'falta de elemento de contenção que evite a saída do leito carroçável'\n",
            " 'declive acentuado' 'redutor de velocidade em desacordo'\n",
            " 'frear bruscamente' 'problema com o freio' 'retorno proibido'\n",
            " 'ingestão de substâncias psicoativas pelo condutor' 'obstrução na via'\n",
            " 'área urbana sem a presença de local apropriado para a travessia de pedestres'\n",
            " 'problema na suspensão'\n",
            " 'deixar de acionar o farol da motocicleta (ou similar)'\n",
            " 'restrição de visibilidade em curvas verticais'\n",
            " 'restrição de visibilidade em curvas horizontais' 'semáforo com defeito'\n",
            " 'faixas de trânsito com largura insuficiente' 'neblina'\n",
            " 'transitar na calçada' 'faróis desregulados' 'sinalização encoberta'\n",
            " 'participar de racha' 'modificação proibida'\n",
            " 'sistema de drenagem ineficiente']\n",
            "['colisão transversal' 'saída de pista' 'colisão lateral'\n",
            " 'colisão com bicicleta' 'colisão traseira' 'colisão com objeto fixo'\n",
            " 'capotamento' 'queda de motocicleta / bicicleta / veículo' 'incêndio'\n",
            " 'atropelamento de pessoa' 'colisão frontal' 'atropelamento de animal'\n",
            " 'colisão com objeto móvel' 'tombamento' 'derramamento de carga'\n",
            " 'danos eventuais' 'colisão com objeto estático'\n",
            " 'saída de leito carroçável' 'queda de ocupante de veículo'\n",
            " 'atropelamento de pedestre' 'engavetamento'\n",
            " 'colisão com objeto em movimento' 'colisão com objeto'\n",
            " 'colisão lateral mesmo sentido' 'eventos atípicos'\n",
            " 'colisão lateral sentido oposto']\n",
            "['com vítimas feridas' 'sem vítimas' 'com vítimas fatais' 'ignorado']\n",
            "['amanhecer' 'pleno dia' 'plena noite' 'anoitecer']\n",
            "['crescente' 'decrescente']\n",
            "['ceu claro' 'nublado' 'sol' 'chuva' 'ignorada' 'vento' 'nevoeiro/neblina'\n",
            " 'granizo' 'neve' 'céu claro' 'garoa/chuvisco' 'ignorado']\n",
            "['dupla' 'simples' 'múltipla']\n",
            "['reta' 'curva' 'cruzamento' 'desvio temporário' 'não informado'\n",
            " 'rotatória' 'viaduto' 'interseção de vias' 'ponte'\n",
            " 'retorno regulamentado' 'túnel']\n",
            "['urbano' 'rural' 'sim' 'não']\n",
            "['06' '08' '15' '14' '12' '16' '21' '18' '20' '02' '17' '22' '19' '13'\n",
            " '11' '05' '07' '04' '03' '09' '00' '01' '23' '10']\n"
          ]
        }
      ],
      "source": [
        "for col in cat_col:\n",
        "  print(df[col].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWkcp0LmKZHK",
        "outputId": "7e5e3ad1-b76b-48ed-b8bc-1a0c2b3c2d04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['segunda', 'quinta', 'domingo', 'terça', 'sexta', 'quarta',\n",
              "       'sábado'], dtype=object)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"dia_semana\"]=date_col(df[\"dia_semana\"])\n",
        "df[\"dia_semana\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U3I79aATj2y",
        "outputId": "ca440278-9cdf-46dd-f613-658cdd06ead8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tipo_acidente\n",
              "colisão traseira                              25.856050\n",
              "colisão lateral                               14.609068\n",
              "saída de pista                                11.760408\n",
              "colisão transversal                           10.493837\n",
              "tombamento                                     4.894686\n",
              "colisão com objeto fixo                        4.756975\n",
              "capotamento                                    4.655017\n",
              "colisão frontal                                4.322631\n",
              "saída de leito carroçável                      3.478436\n",
              "queda de motocicleta / bicicleta / veículo     2.648734\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"tipo_acidente\"].value_counts(normalize=True).nlargest(10)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "Qiq0GLzTU9Br",
        "outputId": "a480fa6f-10cd-46e4-91e5-58ca221594f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>br</th>\n",
              "      <th>pessoas</th>\n",
              "      <th>mortos</th>\n",
              "      <th>feridos_leves</th>\n",
              "      <th>feridos_graves</th>\n",
              "      <th>ilesos</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>quarter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>316.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>381.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>282.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>40.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>50.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>153.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>285.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>470.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>316.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>116.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1980229 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               br  pessoas  mortos  feridos_leves  feridos_graves  ilesos  \\\n",
              "date                                                                        \n",
              "2007-01-01  316.0        2       0              1               0       1   \n",
              "2007-01-01  381.0        1       0              0               0       1   \n",
              "2007-01-01  282.0        2       0              0               1       1   \n",
              "2007-01-01   40.0        2       0              0               0       2   \n",
              "2007-01-01   50.0        4       0              3               1       0   \n",
              "...           ...      ...     ...            ...             ...     ...   \n",
              "2022-12-31  153.0        1       0              1               0       0   \n",
              "2022-12-31  285.0        4       0              0               0       3   \n",
              "2022-12-31  470.0        2       0              1               1       0   \n",
              "2022-12-31  316.0        3       0              1               2       0   \n",
              "2022-12-31  116.0        2       0              1               0       1   \n",
              "\n",
              "            ignorados  feridos  veiculos  year  month  quarter  \n",
              "date                                                            \n",
              "2007-01-01          0        1         2  2007      1        1  \n",
              "2007-01-01          0        0         1  2007      1        1  \n",
              "2007-01-01          0        1         2  2007      1        1  \n",
              "2007-01-01          0        0         1  2007      1        1  \n",
              "2007-01-01          0        4         1  2007      1        1  \n",
              "...               ...      ...       ...   ...    ...      ...  \n",
              "2022-12-31          0        1         1  2022     12        4  \n",
              "2022-12-31          1        0         2  2022     12        4  \n",
              "2022-12-31          0        2         1  2022     12        4  \n",
              "2022-12-31          0        3         1  2022     12        4  \n",
              "2022-12-31          0        1         1  2022     12        4  \n",
              "\n",
              "[1980229 rows x 12 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[num_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TENjWHuCWzM6",
        "outputId": "21e02e95-85a4-421e-a48d-ff280a5ea3b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['ceu claro', 'nublado', 'sol', 'chuva', 'ignorada', 'vento',\n",
              "       'nevoeiro/neblina', 'granizo', 'neve', 'céu claro',\n",
              "       'garoa/chuvisco', 'ignorado'], dtype=object)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Diminuindo a quantidade de dados\n",
        "\n",
        "# Condições Metereologica\n",
        "df['condicao_metereologica'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KngtM1j8o3NC"
      },
      "outputs": [],
      "source": [
        "mapeamento_metereologica = {\n",
        "    'ceu claro': 'céu claro',\n",
        "    'céu claro': 'céu claro',  # Tratando as duas formas de 'céu claro' como iguais\n",
        "    'nublado': 'nublado',\n",
        "    'sol': 'sol',\n",
        "    'chuva': 'chuva',\n",
        "    'garoa/chuvisco': 'chuva',  # Agrupando garoa/chuvisco com chuva\n",
        "    'vento': 'vento',\n",
        "    'nevoeiro/neblina': 'nevoeiro',\n",
        "    'granizo': 'neve',  # Agrupando granizo com neve\n",
        "    'neve': 'neve',\n",
        "    'ignorada': 'ignorado',\n",
        "    'ignorado': 'ignorado'\n",
        "}\n",
        "\n",
        "df['condicao_metereologica'] = df['condicao_metereologica'].map(mapeamento_metereologica)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9KTFJumo5u8",
        "outputId": "9a299100-a6e6-4d61-eeb3-76c383ed9cf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['céu claro', 'nublado', 'sol', 'chuva', 'ignorado', 'vento',\n",
              "       'nevoeiro', 'neve'], dtype=object)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['condicao_metereologica'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWwVBwH0pAqq",
        "outputId": "74a66a40-8fa3-4d00-e16b-4077f7714759"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['desobediência à sinalização', 'outras', 'dormindo',\n",
              "       'não guardar distância de segurança',\n",
              "       'defeito mecânico em veículo', 'falta de atenção',\n",
              "       'animais na pista', 'velocidade incompatível',\n",
              "       'ingestão de álcool', 'ultrapassagem indevida', 'defeito na via',\n",
              "       'condutor dormindo',\n",
              "       'desobediência às normas de trânsito pelo condutor',\n",
              "       'falta de atenção à condução', 'pista escorregadia',\n",
              "       'falta de atenção do pedestre',\n",
              "       'sinalização da via insuficiente ou inadequada',\n",
              "       'avarias e/ou desgaste excessivo no pneu', 'mal súbito',\n",
              "       'defeito mecânico no veículo', 'fenômenos da natureza',\n",
              "       'restrição de visibilidade',\n",
              "       'objeto estático sobre o leito carroçável',\n",
              "       'carga excessiva e/ou mal acondicionada',\n",
              "       'deficiência ou não acionamento do sistema de iluminação/sinalização do veículo',\n",
              "       'ingestão de substâncias psicoativas',\n",
              "       'reação tardia ou ineficiente do condutor', 'agressão externa',\n",
              "       'desobediência às normas de trânsito pelo pedestre',\n",
              "       'acumulo de areia ou detritos sobre o pavimento',\n",
              "       'ingestão de álcool e/ou substâncias psicoativas pelo pedestre',\n",
              "       'ingestão de álcool pelo condutor', 'curva acentuada',\n",
              "       'desrespeitar a preferência no cruzamento',\n",
              "       'pedestre andava na pista', 'pista em desnível', 'chuva',\n",
              "       'demais falhas mecânicas ou elétricas', 'pista esburacada',\n",
              "       'ausência de reação do condutor', 'transitar na contramão',\n",
              "       'obras na pista',\n",
              "       'ingestão de álcool ou de substâncias psicoativas pelo pedestre',\n",
              "       'conversão proibida',\n",
              "       'trafegar com motocicleta (ou similar) entre as faixas',\n",
              "       'condutor deixou de manter distância do veículo da frente',\n",
              "       'iluminação deficiente',\n",
              "       'acessar a via sem observar a presença dos outros veículos',\n",
              "       'deficiência do sistema de iluminação/sinalização', 'fumaça',\n",
              "       'manobra de mudança de faixa', 'demais fenômenos da natureza',\n",
              "       'falta de acostamento', 'acesso irregular',\n",
              "       'transitar no acostamento', 'condutor usando celular',\n",
              "       'condutor desrespeitou a iluminação vermelha do semáforo',\n",
              "       'acumulo de água sobre o pavimento',\n",
              "       'afundamento ou ondulação no pavimento', 'ausência de sinalização',\n",
              "       'estacionar ou parar em local proibido',\n",
              "       'entrada inopinada do pedestre', 'mal súbito do condutor',\n",
              "       'demais falhas na via', 'acumulo de óleo sobre o pavimento',\n",
              "       'acostamento em desnível',\n",
              "       'pedestre cruzava a pista fora da faixa',\n",
              "       'sinalização mal posicionada', 'desvio temporário',\n",
              "       'falta de elemento de contenção que evite a saída do leito carroçável',\n",
              "       'declive acentuado', 'redutor de velocidade em desacordo',\n",
              "       'frear bruscamente', 'problema com o freio', 'retorno proibido',\n",
              "       'ingestão de substâncias psicoativas pelo condutor',\n",
              "       'obstrução na via',\n",
              "       'área urbana sem a presença de local apropriado para a travessia de pedestres',\n",
              "       'problema na suspensão',\n",
              "       'deixar de acionar o farol da motocicleta (ou similar)',\n",
              "       'restrição de visibilidade em curvas verticais',\n",
              "       'restrição de visibilidade em curvas horizontais',\n",
              "       'semáforo com defeito',\n",
              "       'faixas de trânsito com largura insuficiente', 'neblina',\n",
              "       'transitar na calçada', 'faróis desregulados',\n",
              "       'sinalização encoberta', 'participar de racha',\n",
              "       'modificação proibida', 'sistema de drenagem ineficiente'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Causas dos acidentes\n",
        "df['causa_acidente'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8FXBdmQs3fn",
        "outputId": "5a84962f-e451-4db4-c150-b2704d4a5202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantidade de valores únicos na coluna 'causa_acidente': 91\n",
            "Valores únicos na coluna 'causa_acidente': desobediência à sinalização, outras, dormindo, não guardar distância de segurança, defeito mecânico em veículo, falta de atenção, animais na pista, velocidade incompatível, ingestão de álcool, ultrapassagem indevida, defeito na via, condutor dormindo, desobediência às normas de trânsito pelo condutor, falta de atenção à condução, pista escorregadia, falta de atenção do pedestre, sinalização da via insuficiente ou inadequada, avarias e/ou desgaste excessivo no pneu, mal súbito, defeito mecânico no veículo, fenômenos da natureza, restrição de visibilidade, objeto estático sobre o leito carroçável, carga excessiva e/ou mal acondicionada, deficiência ou não acionamento do sistema de iluminação/sinalização do veículo, ingestão de substâncias psicoativas, reação tardia ou ineficiente do condutor, agressão externa, desobediência às normas de trânsito pelo pedestre, acumulo de areia ou detritos sobre o pavimento, ingestão de álcool e/ou substâncias psicoativas pelo pedestre, ingestão de álcool pelo condutor, curva acentuada, desrespeitar a preferência no cruzamento, pedestre andava na pista, pista em desnível, chuva, demais falhas mecânicas ou elétricas, pista esburacada, ausência de reação do condutor, transitar na contramão, obras na pista, ingestão de álcool ou de substâncias psicoativas pelo pedestre, conversão proibida, trafegar com motocicleta (ou similar) entre as faixas, condutor deixou de manter distância do veículo da frente, iluminação deficiente, acessar a via sem observar a presença dos outros veículos, deficiência do sistema de iluminação/sinalização, fumaça, manobra de mudança de faixa, demais fenômenos da natureza, falta de acostamento, acesso irregular, transitar no acostamento, condutor usando celular, condutor desrespeitou a iluminação vermelha do semáforo, acumulo de água sobre o pavimento, afundamento ou ondulação no pavimento, ausência de sinalização, estacionar ou parar em local proibido, entrada inopinada do pedestre, mal súbito do condutor, demais falhas na via, acumulo de óleo sobre o pavimento, acostamento em desnível, pedestre cruzava a pista fora da faixa, sinalização mal posicionada, desvio temporário, falta de elemento de contenção que evite a saída do leito carroçável, declive acentuado, redutor de velocidade em desacordo, frear bruscamente, problema com o freio, retorno proibido, ingestão de substâncias psicoativas pelo condutor, obstrução na via, área urbana sem a presença de local apropriado para a travessia de pedestres, problema na suspensão, deixar de acionar o farol da motocicleta (ou similar), restrição de visibilidade em curvas verticais, restrição de visibilidade em curvas horizontais, semáforo com defeito, faixas de trânsito com largura insuficiente, neblina, transitar na calçada, faróis desregulados, sinalização encoberta, participar de racha, modificação proibida, sistema de drenagem ineficiente\n",
            "Valores únicos salvos no arquivo 'valores_unicos_causa_acidente.txt'\n"
          ]
        }
      ],
      "source": [
        "quantidade_valores_unicos = df['causa_acidente'].nunique()\n",
        "print(\"Quantidade de valores únicos na coluna 'causa_acidente':\", quantidade_valores_unicos)\n",
        "\n",
        "# Exibir todos os valores únicos na coluna 'causa_acidente' em uma linha\n",
        "valores_unicos = df['causa_acidente'].unique()\n",
        "valores_unicos_str = ', '.join(valores_unicos)\n",
        "print(\"Valores únicos na coluna 'causa_acidente':\", valores_unicos_str)\n",
        "\n",
        "# Salvar todos os valores únicos em um arquivo\n",
        "with open('valores_unicos_causa_acidente.txt', 'w', encoding='utf-8') as arquivo:\n",
        "    for valor in valores_unicos:\n",
        "        arquivo.write(valor + '\\n')\n",
        "\n",
        "print(\"Valores únicos salvos no arquivo 'valores_unicos_causa_acidente.txt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ffBUwjC6pC29"
      },
      "outputs": [],
      "source": [
        "mapeamento_causas = {\n",
        "    'desobediência à sinalização': 'desobediência',\n",
        "    'outras': 'outras',\n",
        "    'dormindo': 'distração',\n",
        "    'não guardar distância de segurança': 'distância insegura',\n",
        "    'defeito mecânico em veículo': 'defeito mecânico',\n",
        "    'falta de atenção': 'distração',\n",
        "    'animais na pista': 'presença de animais',\n",
        "    'velocidade incompatível': 'velocidade inadequada',\n",
        "    'ingestão de álcool': 'álcool/drogas',\n",
        "    'ultrapassagem indevida': 'ultrapassagem',\n",
        "    'defeito na via': 'condição da via',\n",
        "    'condutor dormindo': 'distração',\n",
        "    'desobediência às normas de trânsito pelo condutor': 'desobediência',\n",
        "    'falta de atenção à condução': 'distração',\n",
        "    'pista escorregadia': 'condição da via',\n",
        "    'falta de atenção do pedestre': 'distração',\n",
        "    'sinalização da via insuficiente ou inadequada': 'sinalização inadequada',\n",
        "    'avarias e/ou desgaste excessivo no pneu': 'defeito mecânico',\n",
        "    'mal súbito': 'mal súbito',\n",
        "    'defeito mecânico no veículo': 'defeito mecânico',\n",
        "    'fenômenos da natureza': 'fenômenos naturais',\n",
        "    'restrição de visibilidade': 'visibilidade reduzida',\n",
        "    'objeto estático sobre o leito carroçável': 'obstáculo na via',\n",
        "    'carga excessiva e/ou mal acondicionada': 'carga mal acondicionada',\n",
        "    'deficiência ou não acionamento do sistema de iluminação/sinalização do veículo': 'falha nos sistemas do veículo',\n",
        "    'ingestão de substâncias psicoativas': 'álcool/drogas',\n",
        "    'reação tardia ou ineficiente do condutor': 'falha do condutor',\n",
        "    'agressão externa': 'agressão',\n",
        "    'desobediência às normas de trânsito pelo pedestre': 'desobediência',\n",
        "    'acumulo de areia ou detritos sobre o pavimento': 'condição da via',\n",
        "    'ingestão de álcool e/ou substâncias psicoativas pelo pedestre': 'álcool/drogas',\n",
        "    'ingestão de álcool pelo condutor': 'álcool/drogas',\n",
        "    'curva acentuada': 'condição da via',\n",
        "    'desrespeitar a preferência no cruzamento': 'desrespeito à preferência',\n",
        "    'pedestre andava na pista': 'pedestre na via',\n",
        "    'pista em desnível': 'condição da via',\n",
        "    'chuva': 'condição climática',\n",
        "    'demais falhas mecânicas ou elétricas': 'defeito mecânico',\n",
        "    'pista esburacada': 'condição da via',\n",
        "    'ausência de reação do condutor': 'falha do condutor',\n",
        "    'transitar na contramão': 'infração de trânsito',\n",
        "    'obras na pista': 'condição da via',\n",
        "    'ingestão de álcool ou de substâncias psicoativas pelo pedestre': 'álcool/drogas',\n",
        "    'conversão proibida': 'infração de trânsito',\n",
        "    'trafegar com motocicleta (ou similar) entre as faixas': 'infração de trânsito',\n",
        "    'condutor deixou de manter distância do veículo da frente': 'distância insegura',\n",
        "    'iluminação deficiente': 'falha nos sistemas do veículo',\n",
        "    'acessar a via sem observar a presença dos outros veículos': 'falta de atenção',\n",
        "    'deficiência do sistema de iluminação/sinalização': 'falha nos sistemas do veículo',\n",
        "    'fumaça': 'condição da via',\n",
        "    'manobra de mudança de faixa': 'manobra imprudente',\n",
        "    'demais fenômenos da natureza': 'fenômenos naturais',\n",
        "    'falta de acostamento': 'infraestrutura inadequada',\n",
        "    'acesso irregular': 'infração de trânsito',\n",
        "    'transitar no acostamento': 'infração de trânsito',\n",
        "    'condutor usando celular': 'distração',\n",
        "    'condutor desrespeitou a iluminação vermelha do semáforo': 'infração de trânsito',\n",
        "    'acumulo de água sobre o pavimento': 'condição da via',\n",
        "    'afundamento ou ondulação no pavimento': 'condição da via',\n",
        "    'ausência de sinalização': 'sinalização inadequada',\n",
        "    'estacionar ou parar em local proibido': 'infração de trânsito',\n",
        "    'entrada inopinada do pedestre': 'falta de atenção do pedestre',\n",
        "    'mal súbito do condutor': 'mal súbito',\n",
        "    'demais falhas na via': 'condição da via',\n",
        "    'acumulo de óleo sobre o pavimento': 'condição da via',\n",
        "    'acostamento em desnível': 'infraestrutura inadequada',\n",
        "    'pedestre cruzava a pista fora da faixa': 'falta de atenção do pedestre',\n",
        "    'sinalização mal posicionada': 'sinalização inadequada',\n",
        "    'desvio temporário': 'condição da via',\n",
        "    'falta de elemento de contenção que evite a saída do leito carroçável': 'infraestrutura inadequada',\n",
        "    'declive acentuado': 'condição da via',\n",
        "    'redutor de velocidade em desacordo': 'condição da via',\n",
        "    'frear bruscamente': 'manobra imprudente',\n",
        "    'problema com o freio': 'falha nos sistemas do veículo',\n",
        "    'retorno proibido': 'infração de trânsito',\n",
        "    'ingestão de substâncias psicoativas pelo condutor': 'álcool/drogas',\n",
        "    'obstrução na via': 'obstrução na via',\n",
        "    'área urbana sem a presença de local apropriado para a travessia de pedestres': 'infraestrutura inadequada',\n",
        "    'problema na suspensão': 'falha nos sistemas do veículo',\n",
        "    'deixar de acionar o farol da motocicleta (ou similar)': 'falha nos sistemas do veículo',\n",
        "    'restrição de visibilidade em curvas verticais': 'visibilidade reduzida',\n",
        "    'restrição de visibilidade em curvas horizontais': 'visibilidade reduzida',\n",
        "    'semáforo com defeito': 'sinalização inadequada',\n",
        "    'faixas de trânsito com largura insuficiente': 'infraestrutura inadequada',\n",
        "    'neblina': 'condição climática',\n",
        "    'transitar na calçada': 'infração de trânsito',\n",
        "    'faróis desregulados': 'falha nos sistemas do veículo',\n",
        "    'sinalização encoberta': 'sinalização inadequada',\n",
        "    'participar de racha': 'infração de trânsito',\n",
        "    'modificação proibida': 'infração de trânsito',\n",
        "    'sistema de drenagem ineficiente': 'infraestrutura inadequada'\n",
        "}\n",
        "\n",
        "df['causa_acidente'] = df['causa_acidente'].map(mapeamento_causas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g0roqcApC0K",
        "outputId": "89e01ed1-e9e9-435c-b527-71c6d655e4e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['desobediência', 'outras', 'distração', 'distância insegura',\n",
              "       'defeito mecânico', 'presença de animais', 'velocidade inadequada',\n",
              "       'álcool/drogas', 'ultrapassagem', 'condição da via',\n",
              "       'sinalização inadequada', 'mal súbito', 'fenômenos naturais',\n",
              "       'visibilidade reduzida', 'obstáculo na via',\n",
              "       'carga mal acondicionada', 'falha nos sistemas do veículo',\n",
              "       'falha do condutor', 'agressão', 'desrespeito à preferência',\n",
              "       'pedestre na via', 'condição climática', 'infração de trânsito',\n",
              "       'falta de atenção', 'manobra imprudente',\n",
              "       'infraestrutura inadequada', 'falta de atenção do pedestre',\n",
              "       'obstrução na via'], dtype=object)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['causa_acidente'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhomC-qOpVzX",
        "outputId": "e668ec57-27c0-4fc6-a329-d446e70766d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['colisão transversal', 'saída de pista', 'colisão lateral',\n",
              "       'colisão com bicicleta', 'colisão traseira',\n",
              "       'colisão com objeto fixo', 'capotamento',\n",
              "       'queda de motocicleta / bicicleta / veículo', 'incêndio',\n",
              "       'atropelamento de pessoa', 'colisão frontal',\n",
              "       'atropelamento de animal', 'colisão com objeto móvel',\n",
              "       'tombamento', 'derramamento de carga', 'danos eventuais',\n",
              "       'colisão com objeto estático', 'saída de leito carroçável',\n",
              "       'queda de ocupante de veículo', 'atropelamento de pedestre',\n",
              "       'engavetamento', 'colisão com objeto em movimento',\n",
              "       'colisão com objeto', 'colisão lateral mesmo sentido',\n",
              "       'eventos atípicos', 'colisão lateral sentido oposto'], dtype=object)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tipos de acidentes\n",
        "df['tipo_acidente'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6iNnmTBnpXlf"
      },
      "outputs": [],
      "source": [
        "mapeamento_acidente = {\n",
        "    'colisão transversal': 'colisão',\n",
        "    'saída de pista': 'saída de pista',\n",
        "    'colisão lateral': 'colisão',\n",
        "    'colisão com bicicleta': 'colisão',\n",
        "    'colisão traseira': 'colisão',\n",
        "    'colisão com objeto fixo': 'colisão',\n",
        "    'capotamento': 'capotamento',\n",
        "    'queda de motocicleta / bicicleta / veículo': 'queda',\n",
        "    'incêndio': 'incêndio',\n",
        "    'atropelamento de pessoa': 'atropelamento de pessoa',\n",
        "    'colisão frontal': 'colisão',\n",
        "    'atropelamento de animal': 'atropelamento de animal',\n",
        "    'colisão com objeto móvel': 'colisão',\n",
        "    'tombamento': 'tombamento',\n",
        "    'derramamento de carga': 'derramamento de carga',\n",
        "    'danos eventuais': 'outros',\n",
        "    'colisão com objeto estático': 'colisão',\n",
        "    'saída de leito carroçável': 'saída de pista',\n",
        "    'queda de ocupante de veículo': 'queda',\n",
        "    'atropelamento de pedestre': 'atropelamento de pessoa',\n",
        "    'engavetamento': 'colisão',\n",
        "    'colisão com objeto em movimento': 'colisão',\n",
        "    'colisão com objeto': 'colisão',\n",
        "    'colisão lateral mesmo sentido': 'colisão',\n",
        "    'eventos atípicos': 'outros',\n",
        "    'colisão lateral sentido oposto': 'colisão'\n",
        "}\n",
        "\n",
        "df['tipo_acidente'] = df['tipo_acidente'].map(mapeamento_acidente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x7NbZ74pX-3",
        "outputId": "86087b6c-5db1-4ea8-ebf7-4fc817bf23e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['colisão', 'saída de pista', 'capotamento', 'queda', 'incêndio',\n",
              "       'atropelamento de pessoa', 'atropelamento de animal', 'tombamento',\n",
              "       'derramamento de carga', 'outros'], dtype=object)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['tipo_acidente'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48agJzGWAubP"
      },
      "source": [
        "#####Questão 1  - Classificando as colunas em Quantitativa Discreta,Quantitiva Contínua, Qualitativa Ordinal ou Qualitativa Nominal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAenciAsA9f1"
      },
      "source": [
        "Colunas Quantitativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "Oh1-OPdbA78t",
        "outputId": "91eb0af3-7973-4eee-e243-f693b5083707"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>br</th>\n",
              "      <th>pessoas</th>\n",
              "      <th>mortos</th>\n",
              "      <th>feridos_leves</th>\n",
              "      <th>feridos_graves</th>\n",
              "      <th>ilesos</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>quarter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>316.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>381.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>282.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>40.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>50.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>153.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>285.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>470.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>316.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>116.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1980229 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               br  pessoas  mortos  feridos_leves  feridos_graves  ilesos  \\\n",
              "date                                                                        \n",
              "2007-01-01  316.0        2       0              1               0       1   \n",
              "2007-01-01  381.0        1       0              0               0       1   \n",
              "2007-01-01  282.0        2       0              0               1       1   \n",
              "2007-01-01   40.0        2       0              0               0       2   \n",
              "2007-01-01   50.0        4       0              3               1       0   \n",
              "...           ...      ...     ...            ...             ...     ...   \n",
              "2022-12-31  153.0        1       0              1               0       0   \n",
              "2022-12-31  285.0        4       0              0               0       3   \n",
              "2022-12-31  470.0        2       0              1               1       0   \n",
              "2022-12-31  316.0        3       0              1               2       0   \n",
              "2022-12-31  116.0        2       0              1               0       1   \n",
              "\n",
              "            ignorados  feridos  veiculos  year  month  quarter  \n",
              "date                                                            \n",
              "2007-01-01          0        1         2  2007      1        1  \n",
              "2007-01-01          0        0         1  2007      1        1  \n",
              "2007-01-01          0        1         2  2007      1        1  \n",
              "2007-01-01          0        0         1  2007      1        1  \n",
              "2007-01-01          0        4         1  2007      1        1  \n",
              "...               ...      ...       ...   ...    ...      ...  \n",
              "2022-12-31          0        1         1  2022     12        4  \n",
              "2022-12-31          1        0         2  2022     12        4  \n",
              "2022-12-31          0        2         1  2022     12        4  \n",
              "2022-12-31          0        3         1  2022     12        4  \n",
              "2022-12-31          0        1         1  2022     12        4  \n",
              "\n",
              "[1980229 rows x 12 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[num_col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whcMDQqkO09h"
      },
      "source": [
        "*  Coluna date é classificada como Qualitativa ordinal, pode ser subdividida em dia, mês e ano, e segue uma ordem cronológico.\n",
        "* Coluna br é classificada como Quantitativa Discreta, trata-se de valores contáveis e não podem ser divididos em partes menores significativas, mesmo sendo decimal. E nome de rodovias só pode assumir um valor.Exemplos: BR 153, BR 277.\n",
        "*   Colunas pessoas,mortos,feridos_leves,feridos_graves,ilesos,ignorados,feridos,veiculos,quarter(trimestre) são Quantitativas Discretas. Ano e mês nesse caso se enquadram em Discretos também, já que são nesse caso representados como númericos, e representam uma quantidade específica, é unidade individual e não pode ser dividida em partes menores significativas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb59EBSFO4vI"
      },
      "source": [
        "Qualitativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "1hqM7YWCO6mQ",
        "outputId": "1f7bf947-902a-45a5-b682-0bb637fc19ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>uf</th>\n",
              "      <th>causa_acidente</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <th>fase_dia</th>\n",
              "      <th>sentido_via</th>\n",
              "      <th>condicao_metereologica</th>\n",
              "      <th>tipo_pista</th>\n",
              "      <th>tracado_via</th>\n",
              "      <th>uso_solo</th>\n",
              "      <th>hour</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>segunda</td>\n",
              "      <td>pa</td>\n",
              "      <td>desobediência</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>amanhecer</td>\n",
              "      <td>crescente</td>\n",
              "      <td>céu claro</td>\n",
              "      <td>dupla</td>\n",
              "      <td>reta</td>\n",
              "      <td>urbano</td>\n",
              "      <td>06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>segunda</td>\n",
              "      <td>mg</td>\n",
              "      <td>outras</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>decrescente</td>\n",
              "      <td>nublado</td>\n",
              "      <td>simples</td>\n",
              "      <td>curva</td>\n",
              "      <td>rural</td>\n",
              "      <td>08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>segunda</td>\n",
              "      <td>sc</td>\n",
              "      <td>desobediência</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>decrescente</td>\n",
              "      <td>nublado</td>\n",
              "      <td>simples</td>\n",
              "      <td>cruzamento</td>\n",
              "      <td>rural</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>segunda</td>\n",
              "      <td>mg</td>\n",
              "      <td>distração</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>crescente</td>\n",
              "      <td>sol</td>\n",
              "      <td>dupla</td>\n",
              "      <td>reta</td>\n",
              "      <td>rural</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>segunda</td>\n",
              "      <td>mg</td>\n",
              "      <td>outras</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>decrescente</td>\n",
              "      <td>nublado</td>\n",
              "      <td>simples</td>\n",
              "      <td>curva</td>\n",
              "      <td>rural</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>sábado</td>\n",
              "      <td>pr</td>\n",
              "      <td>álcool/drogas</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>crescente</td>\n",
              "      <td>céu claro</td>\n",
              "      <td>simples</td>\n",
              "      <td>não informado</td>\n",
              "      <td>não</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>sábado</td>\n",
              "      <td>rs</td>\n",
              "      <td>falta de atenção</td>\n",
              "      <td>colisão</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>decrescente</td>\n",
              "      <td>céu claro</td>\n",
              "      <td>simples</td>\n",
              "      <td>não informado</td>\n",
              "      <td>não</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>sábado</td>\n",
              "      <td>sc</td>\n",
              "      <td>defeito mecânico</td>\n",
              "      <td>tombamento</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>decrescente</td>\n",
              "      <td>céu claro</td>\n",
              "      <td>dupla</td>\n",
              "      <td>reta</td>\n",
              "      <td>não</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>sábado</td>\n",
              "      <td>al</td>\n",
              "      <td>presença de animais</td>\n",
              "      <td>atropelamento de animal</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>crescente</td>\n",
              "      <td>céu claro</td>\n",
              "      <td>simples</td>\n",
              "      <td>reta</td>\n",
              "      <td>não</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-31</th>\n",
              "      <td>sábado</td>\n",
              "      <td>sc</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>decrescente</td>\n",
              "      <td>céu claro</td>\n",
              "      <td>simples</td>\n",
              "      <td>reta</td>\n",
              "      <td>não</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1980229 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           dia_semana  uf         causa_acidente            tipo_acidente  \\\n",
              "date                                                                        \n",
              "2007-01-01    segunda  pa          desobediência                  colisão   \n",
              "2007-01-01    segunda  mg                 outras           saída de pista   \n",
              "2007-01-01    segunda  sc          desobediência                  colisão   \n",
              "2007-01-01    segunda  mg              distração           saída de pista   \n",
              "2007-01-01    segunda  mg                 outras           saída de pista   \n",
              "...               ...  ..                    ...                      ...   \n",
              "2022-12-31     sábado  pr          álcool/drogas           saída de pista   \n",
              "2022-12-31     sábado  rs       falta de atenção                  colisão   \n",
              "2022-12-31     sábado  sc       defeito mecânico               tombamento   \n",
              "2022-12-31     sábado  al    presença de animais  atropelamento de animal   \n",
              "2022-12-31     sábado  sc  velocidade inadequada           saída de pista   \n",
              "\n",
              "           classificacao_acidente     fase_dia  sentido_via  \\\n",
              "date                                                          \n",
              "2007-01-01    com vítimas feridas    amanhecer    crescente   \n",
              "2007-01-01            sem vítimas    pleno dia  decrescente   \n",
              "2007-01-01    com vítimas feridas    pleno dia  decrescente   \n",
              "2007-01-01            sem vítimas    pleno dia    crescente   \n",
              "2007-01-01    com vítimas feridas    pleno dia  decrescente   \n",
              "...                           ...          ...          ...   \n",
              "2022-12-31    com vítimas feridas    pleno dia    crescente   \n",
              "2022-12-31            sem vítimas    pleno dia  decrescente   \n",
              "2022-12-31    com vítimas feridas    pleno dia  decrescente   \n",
              "2022-12-31    com vítimas feridas  plena noite    crescente   \n",
              "2022-12-31    com vítimas feridas  plena noite  decrescente   \n",
              "\n",
              "           condicao_metereologica tipo_pista    tracado_via uso_solo hour  \n",
              "date                                                                       \n",
              "2007-01-01              céu claro      dupla           reta   urbano   06  \n",
              "2007-01-01                nublado    simples          curva    rural   08  \n",
              "2007-01-01                nublado    simples     cruzamento    rural   15  \n",
              "2007-01-01                    sol      dupla           reta    rural   14  \n",
              "2007-01-01                nublado    simples          curva    rural   12  \n",
              "...                           ...        ...            ...      ...  ...  \n",
              "2022-12-31              céu claro    simples  não informado      não   16  \n",
              "2022-12-31              céu claro    simples  não informado      não   17  \n",
              "2022-12-31              céu claro      dupla           reta      não   16  \n",
              "2022-12-31              céu claro    simples           reta      não   18  \n",
              "2022-12-31              céu claro    simples           reta      não   20  \n",
              "\n",
              "[1980229 rows x 12 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[cat_col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDNcJqI6qGsr"
      },
      "source": [
        "# Machine Learning - Classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-WNuLn_v8n_"
      },
      "source": [
        "Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WOi6i09_vtF8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler      # padrozinação\n",
        "from sklearn.preprocessing import LabelEncoder        # tratamento dados categóricos\n",
        "from sklearn.preprocessing import OneHotEncoder       # tratamento dados categóricos\n",
        "from sklearn.compose import ColumnTransformer         # tratamento dados categóricos\n",
        "from sklearn.model_selection import train_test_split  # divisão treino e teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCgEABtJwAXV"
      },
      "source": [
        "Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lm7jOHmIvwt5"
      },
      "outputs": [],
      "source": [
        "# metricas e matrix de confusão\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "\n",
        "# modelos\n",
        "from sklearn.naive_bayes import GaussianNB            # Naive Bayes\n",
        "from sklearn.tree import DecisionTreeClassifier       # Arvore de decisão\n",
        "from sklearn import tree                              # Arvore de decisão\n",
        "from sklearn.ensemble import RandomForestClassifier   # Random Forest\n",
        "from sklearn.neighbors import KNeighborsClassifier    # KNN - baseada em instâncias\n",
        "from sklearn.linear_model import LogisticRegression   # Regressão logística\n",
        "from sklearn.svm import SVC                           # SVM\n",
        "from sklearn.neural_network import MLPClassifier      # Redes neurais artificiais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ09LRCOwBd9"
      },
      "source": [
        "Avaliação de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "0uNr1d5gv4Yd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV            # Melhores parametros\n",
        "from sklearn.model_selection import cross_val_score, KFold  # Validação cruzada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skRDjLf3qXZV"
      },
      "source": [
        "## Tratamento de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "VIA1xls8qgVD",
        "outputId": "6adced4e-2757-4bd3-fc3a-6a2913b8fe31"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_inversa</th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>horario</th>\n",
              "      <th>uf</th>\n",
              "      <th>br</th>\n",
              "      <th>km</th>\n",
              "      <th>causa_acidente</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <th>fase_dia</th>\n",
              "      <th>...</th>\n",
              "      <th>feridos_graves</th>\n",
              "      <th>ilesos</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>quarter</th>\n",
              "      <th>quarter_end</th>\n",
              "      <th>hour</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>2007-01-01</td>\n",
              "      <td>segunda</td>\n",
              "      <td>06:30:00</td>\n",
              "      <td>pa</td>\n",
              "      <td>316.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>desobediência</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>amanhecer</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>2007-01-01</td>\n",
              "      <td>segunda</td>\n",
              "      <td>08:40:00</td>\n",
              "      <td>mg</td>\n",
              "      <td>381.0</td>\n",
              "      <td>397.4</td>\n",
              "      <td>outras</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>2007-01-01</td>\n",
              "      <td>segunda</td>\n",
              "      <td>15:20:00</td>\n",
              "      <td>sc</td>\n",
              "      <td>282.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>desobediência</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>2007-01-01</td>\n",
              "      <td>segunda</td>\n",
              "      <td>14:00:00</td>\n",
              "      <td>mg</td>\n",
              "      <td>40.0</td>\n",
              "      <td>120.8</td>\n",
              "      <td>distração</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-01</th>\n",
              "      <td>2007-01-01</td>\n",
              "      <td>segunda</td>\n",
              "      <td>12:00:00</td>\n",
              "      <td>mg</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>outras</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           data_inversa dia_semana   horario  uf     br     km causa_acidente  \\\n",
              "date                                                                            \n",
              "2007-01-01   2007-01-01    segunda  06:30:00  pa  316.0    9.0  desobediência   \n",
              "2007-01-01   2007-01-01    segunda  08:40:00  mg  381.0  397.4         outras   \n",
              "2007-01-01   2007-01-01    segunda  15:20:00  sc  282.0   23.0  desobediência   \n",
              "2007-01-01   2007-01-01    segunda  14:00:00  mg   40.0  120.8      distração   \n",
              "2007-01-01   2007-01-01    segunda  12:00:00  mg   50.0    0.2         outras   \n",
              "\n",
              "             tipo_acidente classificacao_acidente   fase_dia  ...  \\\n",
              "date                                                          ...   \n",
              "2007-01-01         colisão    com vítimas feridas  amanhecer  ...   \n",
              "2007-01-01  saída de pista            sem vítimas  pleno dia  ...   \n",
              "2007-01-01         colisão    com vítimas feridas  pleno dia  ...   \n",
              "2007-01-01  saída de pista            sem vítimas  pleno dia  ...   \n",
              "2007-01-01  saída de pista    com vítimas feridas  pleno dia  ...   \n",
              "\n",
              "           feridos_graves ilesos ignorados feridos veiculos  year  month  \\\n",
              "date                                                                       \n",
              "2007-01-01              0      1         0       1        2  2007      1   \n",
              "2007-01-01              0      1         0       0        1  2007      1   \n",
              "2007-01-01              1      1         0       1        2  2007      1   \n",
              "2007-01-01              0      2         0       0        1  2007      1   \n",
              "2007-01-01              1      0         0       4        1  2007      1   \n",
              "\n",
              "            quarter  quarter_end  hour  \n",
              "date                                    \n",
              "2007-01-01        1        False    06  \n",
              "2007-01-01        1        False    08  \n",
              "2007-01-01        1        False    15  \n",
              "2007-01-01        1        False    14  \n",
              "2007-01-01        1        False    12  \n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ0vquLDq-gd"
      },
      "source": [
        "Excluir dos dados 'outros' na coluna 'tipo_acidente'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKcWa0c-qvf8",
        "outputId": "02d2c433-c0fc-453c-e97e-384ca3705b5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tipo_acidente\n",
              "colisão                    64.460929\n",
              "saída de pista             15.238844\n",
              "tombamento                  4.894686\n",
              "capotamento                 4.655017\n",
              "queda                       3.562164\n",
              "atropelamento de pessoa     3.186248\n",
              "atropelamento de animal     2.216360\n",
              "incêndio                    0.716786\n",
              "outros                      0.568924\n",
              "derramamento de carga       0.500043\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['tipo_acidente'].value_counts(normalize = True) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Hmtvo0MWmERE"
      },
      "outputs": [],
      "source": [
        "df = df.drop(df[df['tipo_acidente'] == 'outros'].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0XibUOqrZt2"
      },
      "source": [
        "Exclusão dos dados 'ignorado' na coluna de condições meteorológica e mapeamento para classificar em condições favoráveis (céu claro e sol) e condições adversas (chuva, nublado, nevoeiro, vento, neve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5oU-FAMrwB7",
        "outputId": "d36ad828-be75-49e7-9b50-65ba5d88296f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "condicao_metereologica\n",
              "céu claro    53.837433\n",
              "nublado      17.876704\n",
              "chuva        15.992799\n",
              "sol           9.454970\n",
              "ignorado      1.664487\n",
              "nevoeiro      0.913400\n",
              "vento         0.248112\n",
              "neve          0.012096\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['condicao_metereologica'].value_counts(normalize = True) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ieoT-EjspaZ7"
      },
      "outputs": [],
      "source": [
        "df = df.drop(df[df['condicao_metereologica'] == 'ignorado'].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "zVzO4WVGpaXP",
        "outputId": "7b15aa9d-dd1c-4f83-c6d5-8afc7d1e070b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_inversa</th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>horario</th>\n",
              "      <th>uf</th>\n",
              "      <th>br</th>\n",
              "      <th>km</th>\n",
              "      <th>causa_acidente</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <th>fase_dia</th>\n",
              "      <th>...</th>\n",
              "      <th>feridos_graves</th>\n",
              "      <th>ilesos</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>quarter</th>\n",
              "      <th>quarter_end</th>\n",
              "      <th>hour</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>18:30:00</td>\n",
              "      <td>ma</td>\n",
              "      <td>135.0</td>\n",
              "      <td>12</td>\n",
              "      <td>álcool/drogas</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>13:00:00</td>\n",
              "      <td>al</td>\n",
              "      <td>101.0</td>\n",
              "      <td>180</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas fatais</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>06:00:00</td>\n",
              "      <td>pe</td>\n",
              "      <td>424.0</td>\n",
              "      <td>0</td>\n",
              "      <td>distância insegura</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>amanhecer</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>16:40:00</td>\n",
              "      <td>sp</td>\n",
              "      <td>116.0</td>\n",
              "      <td>125,6</td>\n",
              "      <td>outras</td>\n",
              "      <td>colisão</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>04:00:00</td>\n",
              "      <td>ba</td>\n",
              "      <td>101.0</td>\n",
              "      <td>724,8</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>ignorado</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           data_inversa dia_semana   horario  uf     br     km  \\\n",
              "date                                                             \n",
              "2016-03-16   2016-03-16     quarta  18:30:00  ma  135.0     12   \n",
              "2016-03-16   2016-03-16     quarta  13:00:00  al  101.0    180   \n",
              "2016-03-16   2016-03-16     quarta  06:00:00  pe  424.0      0   \n",
              "2016-03-16   2016-03-16     quarta  16:40:00  sp  116.0  125,6   \n",
              "2016-03-16   2016-03-16     quarta  04:00:00  ba  101.0  724,8   \n",
              "\n",
              "                   causa_acidente   tipo_acidente classificacao_acidente  \\\n",
              "date                                                                       \n",
              "2016-03-16          álcool/drogas  saída de pista    com vítimas feridas   \n",
              "2016-03-16  velocidade inadequada  saída de pista     com vítimas fatais   \n",
              "2016-03-16     distância insegura         colisão    com vítimas feridas   \n",
              "2016-03-16                 outras         colisão            sem vítimas   \n",
              "2016-03-16  velocidade inadequada  saída de pista               ignorado   \n",
              "\n",
              "               fase_dia  ... feridos_graves ilesos ignorados feridos veiculos  \\\n",
              "date                     ...                                                    \n",
              "2016-03-16  plena noite  ...              0      0         0       1        1   \n",
              "2016-03-16    pleno dia  ...              0      0         0       0        1   \n",
              "2016-03-16    amanhecer  ...              1      0         0       3        2   \n",
              "2016-03-16    pleno dia  ...              0      2         1       0        3   \n",
              "2016-03-16  plena noite  ...              0      0         1       0        1   \n",
              "\n",
              "            year  month  quarter  quarter_end  hour  \n",
              "date                                                 \n",
              "2016-03-16  2016      3        1        False    18  \n",
              "2016-03-16  2016      3        1        False    13  \n",
              "2016-03-16  2016      3        1        False    06  \n",
              "2016-03-16  2016      3        1        False    16  \n",
              "2016-03-16  2016      3        1        False    04  \n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mapeamento = {\n",
        "    'céu claro': 'Condições Favoráveis',\n",
        "    'chuva': 'Condições Adversas',\n",
        "    'nublado': 'Condições Adversas',\n",
        "    'sol': 'Condições Favoráveis',\n",
        "    'nevoeiro': 'Condições Adversas',\n",
        "    'vento': 'Condições Adversas',\n",
        "    'neve': 'Condições Adversas'\n",
        "}\n",
        "\n",
        "df['condicao_metereologica'] = df['condicao_metereologica'].map(mapeamento)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wjpgd8Rr_xz"
      },
      "source": [
        "Criação da coluna de acidente fatal com 0 para acidentes que não tiveram mortes e 1 para acidentes com mortes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_inversa</th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>horario</th>\n",
              "      <th>uf</th>\n",
              "      <th>br</th>\n",
              "      <th>km</th>\n",
              "      <th>causa_acidente</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <th>fase_dia</th>\n",
              "      <th>...</th>\n",
              "      <th>ilesos</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>quarter</th>\n",
              "      <th>quarter_end</th>\n",
              "      <th>hour</th>\n",
              "      <th>acidente_grave</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>18:30:00</td>\n",
              "      <td>ma</td>\n",
              "      <td>135.0</td>\n",
              "      <td>12</td>\n",
              "      <td>álcool/drogas</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>13:00:00</td>\n",
              "      <td>al</td>\n",
              "      <td>101.0</td>\n",
              "      <td>180</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas fatais</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>06:00:00</td>\n",
              "      <td>pe</td>\n",
              "      <td>424.0</td>\n",
              "      <td>0</td>\n",
              "      <td>distância insegura</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>amanhecer</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>06</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>16:40:00</td>\n",
              "      <td>sp</td>\n",
              "      <td>116.0</td>\n",
              "      <td>125,6</td>\n",
              "      <td>outras</td>\n",
              "      <td>colisão</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>04:00:00</td>\n",
              "      <td>ba</td>\n",
              "      <td>101.0</td>\n",
              "      <td>724,8</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>ignorado</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>04</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           data_inversa dia_semana   horario  uf     br     km  \\\n",
              "date                                                             \n",
              "2016-03-16   2016-03-16     quarta  18:30:00  ma  135.0     12   \n",
              "2016-03-16   2016-03-16     quarta  13:00:00  al  101.0    180   \n",
              "2016-03-16   2016-03-16     quarta  06:00:00  pe  424.0      0   \n",
              "2016-03-16   2016-03-16     quarta  16:40:00  sp  116.0  125,6   \n",
              "2016-03-16   2016-03-16     quarta  04:00:00  ba  101.0  724,8   \n",
              "\n",
              "                   causa_acidente   tipo_acidente classificacao_acidente  \\\n",
              "date                                                                       \n",
              "2016-03-16          álcool/drogas  saída de pista    com vítimas feridas   \n",
              "2016-03-16  velocidade inadequada  saída de pista     com vítimas fatais   \n",
              "2016-03-16     distância insegura         colisão    com vítimas feridas   \n",
              "2016-03-16                 outras         colisão            sem vítimas   \n",
              "2016-03-16  velocidade inadequada  saída de pista               ignorado   \n",
              "\n",
              "               fase_dia  ... ilesos ignorados feridos veiculos  year  month  \\\n",
              "date                     ...                                                  \n",
              "2016-03-16  plena noite  ...      0         0       1        1  2016      3   \n",
              "2016-03-16    pleno dia  ...      0         0       0        1  2016      3   \n",
              "2016-03-16    amanhecer  ...      0         0       3        2  2016      3   \n",
              "2016-03-16    pleno dia  ...      2         1       0        3  2016      3   \n",
              "2016-03-16  plena noite  ...      0         1       0        1  2016      3   \n",
              "\n",
              "            quarter  quarter_end  hour  acidente_grave  \n",
              "date                                                    \n",
              "2016-03-16        1        False    18               0  \n",
              "2016-03-16        1        False    13               0  \n",
              "2016-03-16        1        False    06               1  \n",
              "2016-03-16        1        False    16               0  \n",
              "2016-03-16        1        False    04               0  \n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mapeamento = lambda x: 1 if x > 0 else 0\n",
        "df['acidente_grave'] = df['feridos_graves'].map(mapeamento)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "kQZmmkV7kDMa",
        "outputId": "79fbf6eb-49cc-47ea-8ff0-b8177ed58634"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_inversa</th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>horario</th>\n",
              "      <th>uf</th>\n",
              "      <th>br</th>\n",
              "      <th>km</th>\n",
              "      <th>causa_acidente</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <th>fase_dia</th>\n",
              "      <th>...</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>quarter</th>\n",
              "      <th>quarter_end</th>\n",
              "      <th>hour</th>\n",
              "      <th>acidente_grave</th>\n",
              "      <th>acidente_fatal</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>18:30:00</td>\n",
              "      <td>ma</td>\n",
              "      <td>135.0</td>\n",
              "      <td>12</td>\n",
              "      <td>álcool/drogas</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>13:00:00</td>\n",
              "      <td>al</td>\n",
              "      <td>101.0</td>\n",
              "      <td>180</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas fatais</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>06:00:00</td>\n",
              "      <td>pe</td>\n",
              "      <td>424.0</td>\n",
              "      <td>0</td>\n",
              "      <td>distância insegura</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>amanhecer</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>06</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>16:40:00</td>\n",
              "      <td>sp</td>\n",
              "      <td>116.0</td>\n",
              "      <td>125,6</td>\n",
              "      <td>outras</td>\n",
              "      <td>colisão</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>pleno dia</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>04:00:00</td>\n",
              "      <td>ba</td>\n",
              "      <td>101.0</td>\n",
              "      <td>724,8</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>ignorado</td>\n",
              "      <td>plena noite</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>04</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           data_inversa dia_semana   horario  uf     br     km  \\\n",
              "date                                                             \n",
              "2016-03-16   2016-03-16     quarta  18:30:00  ma  135.0     12   \n",
              "2016-03-16   2016-03-16     quarta  13:00:00  al  101.0    180   \n",
              "2016-03-16   2016-03-16     quarta  06:00:00  pe  424.0      0   \n",
              "2016-03-16   2016-03-16     quarta  16:40:00  sp  116.0  125,6   \n",
              "2016-03-16   2016-03-16     quarta  04:00:00  ba  101.0  724,8   \n",
              "\n",
              "                   causa_acidente   tipo_acidente classificacao_acidente  \\\n",
              "date                                                                       \n",
              "2016-03-16          álcool/drogas  saída de pista    com vítimas feridas   \n",
              "2016-03-16  velocidade inadequada  saída de pista     com vítimas fatais   \n",
              "2016-03-16     distância insegura         colisão    com vítimas feridas   \n",
              "2016-03-16                 outras         colisão            sem vítimas   \n",
              "2016-03-16  velocidade inadequada  saída de pista               ignorado   \n",
              "\n",
              "               fase_dia  ... ignorados feridos veiculos  year month  quarter  \\\n",
              "date                     ...                                                   \n",
              "2016-03-16  plena noite  ...         0       1        1  2016     3        1   \n",
              "2016-03-16    pleno dia  ...         0       0        1  2016     3        1   \n",
              "2016-03-16    amanhecer  ...         0       3        2  2016     3        1   \n",
              "2016-03-16    pleno dia  ...         1       0        3  2016     3        1   \n",
              "2016-03-16  plena noite  ...         1       0        1  2016     3        1   \n",
              "\n",
              "            quarter_end  hour  acidente_grave  acidente_fatal  \n",
              "date                                                           \n",
              "2016-03-16        False    18               0               0  \n",
              "2016-03-16        False    13               0               1  \n",
              "2016-03-16        False    06               1               0  \n",
              "2016-03-16        False    16               0               0  \n",
              "2016-03-16        False    04               0               0  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mapeamento = lambda x: 1 if x > 0 else 0\n",
        "df['acidente_fatal'] = df['mortos'].map(mapeamento)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XMhcCdJsY-P"
      },
      "source": [
        "Correção da coluna da fase do dia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "sZlc9nUXmx0A",
        "outputId": "3bed4320-e5dd-4b58-a6a3-b6911abc5a5e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_inversa</th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>horario</th>\n",
              "      <th>uf</th>\n",
              "      <th>br</th>\n",
              "      <th>km</th>\n",
              "      <th>causa_acidente</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>classificacao_acidente</th>\n",
              "      <th>fase_dia</th>\n",
              "      <th>...</th>\n",
              "      <th>ignorados</th>\n",
              "      <th>feridos</th>\n",
              "      <th>veiculos</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>quarter</th>\n",
              "      <th>quarter_end</th>\n",
              "      <th>hour</th>\n",
              "      <th>acidente_grave</th>\n",
              "      <th>acidente_fatal</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>18:30:00</td>\n",
              "      <td>ma</td>\n",
              "      <td>135.0</td>\n",
              "      <td>12</td>\n",
              "      <td>álcool/drogas</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>Noite</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>13:00:00</td>\n",
              "      <td>al</td>\n",
              "      <td>101.0</td>\n",
              "      <td>180</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>com vítimas fatais</td>\n",
              "      <td>Tarde</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>06:00:00</td>\n",
              "      <td>pe</td>\n",
              "      <td>424.0</td>\n",
              "      <td>0</td>\n",
              "      <td>distância insegura</td>\n",
              "      <td>colisão</td>\n",
              "      <td>com vítimas feridas</td>\n",
              "      <td>Manhã</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>16:40:00</td>\n",
              "      <td>sp</td>\n",
              "      <td>116.0</td>\n",
              "      <td>125,6</td>\n",
              "      <td>outras</td>\n",
              "      <td>colisão</td>\n",
              "      <td>sem vítimas</td>\n",
              "      <td>Tarde</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-03-16</th>\n",
              "      <td>2016-03-16</td>\n",
              "      <td>quarta</td>\n",
              "      <td>04:00:00</td>\n",
              "      <td>ba</td>\n",
              "      <td>101.0</td>\n",
              "      <td>724,8</td>\n",
              "      <td>velocidade inadequada</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>ignorado</td>\n",
              "      <td>Madrugada</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           data_inversa dia_semana   horario  uf     br     km  \\\n",
              "date                                                             \n",
              "2016-03-16   2016-03-16     quarta  18:30:00  ma  135.0     12   \n",
              "2016-03-16   2016-03-16     quarta  13:00:00  al  101.0    180   \n",
              "2016-03-16   2016-03-16     quarta  06:00:00  pe  424.0      0   \n",
              "2016-03-16   2016-03-16     quarta  16:40:00  sp  116.0  125,6   \n",
              "2016-03-16   2016-03-16     quarta  04:00:00  ba  101.0  724,8   \n",
              "\n",
              "                   causa_acidente   tipo_acidente classificacao_acidente  \\\n",
              "date                                                                       \n",
              "2016-03-16          álcool/drogas  saída de pista    com vítimas feridas   \n",
              "2016-03-16  velocidade inadequada  saída de pista     com vítimas fatais   \n",
              "2016-03-16     distância insegura         colisão    com vítimas feridas   \n",
              "2016-03-16                 outras         colisão            sem vítimas   \n",
              "2016-03-16  velocidade inadequada  saída de pista               ignorado   \n",
              "\n",
              "             fase_dia  ... ignorados feridos veiculos  year month  quarter  \\\n",
              "date                   ...                                                   \n",
              "2016-03-16      Noite  ...         0       1        1  2016     3        1   \n",
              "2016-03-16      Tarde  ...         0       0        1  2016     3        1   \n",
              "2016-03-16      Manhã  ...         0       3        2  2016     3        1   \n",
              "2016-03-16      Tarde  ...         1       0        3  2016     3        1   \n",
              "2016-03-16  Madrugada  ...         1       0        1  2016     3        1   \n",
              "\n",
              "            quarter_end  hour  acidente_grave  acidente_fatal  \n",
              "date                                                           \n",
              "2016-03-16        False    18               0               0  \n",
              "2016-03-16        False    13               0               1  \n",
              "2016-03-16        False     6               1               0  \n",
              "2016-03-16        False    16               0               0  \n",
              "2016-03-16        False     4               0               0  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mapeamento_horario = lambda x: (\n",
        "    'Madrugada' if 0 <= x < 6\n",
        "    else 'Manhã' if 6 <= x < 12\n",
        "    else 'Tarde' if 12 <= x < 18\n",
        "    else 'Noite'\n",
        ")\n",
        "\n",
        "df['hour'] = df['hour'].astype(int)\n",
        "df['fase_dia'] = df['hour'].apply(mapeamento_horario)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INICIO TESTE - TESTANDO ACIDENTES GRAVES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "update_acidente_grave = lambda x: 1 if x['acidente_fatal'] == 1 else x['acidente_grave']\n",
        "df['acidente_grave'] = df.apply(update_acidente_grave, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "acidente_grave\n",
              "0    11167\n",
              "1     4061\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['acidente_grave'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "acidente_grave\n",
              "0    0.73332\n",
              "1    0.26668\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['acidente_grave'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[df['acidente_grave'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4061, 8)"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FIM TESTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFnT0cBdsjJb"
      },
      "source": [
        "Selecionar as colunas qualitativas.\n",
        "\n",
        "Algumas das colunas não foram selecionada por opção de acordo com a importância dos dados para o algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "7eUqMzydg-VR",
        "outputId": "1c7bc919-9789-4933-9476-19eee9a386ad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dia_semana</th>\n",
              "      <th>tipo_acidente</th>\n",
              "      <th>fase_dia</th>\n",
              "      <th>condicao_metereologica</th>\n",
              "      <th>tipo_pista</th>\n",
              "      <th>tracado_via</th>\n",
              "      <th>uso_solo</th>\n",
              "      <th>acidente_fatal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>quarta</td>\n",
              "      <td>saída de pista</td>\n",
              "      <td>Tarde</td>\n",
              "      <td>Condições Favoráveis</td>\n",
              "      <td>dupla</td>\n",
              "      <td>curva</td>\n",
              "      <td>urbano</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quarta</td>\n",
              "      <td>colisão</td>\n",
              "      <td>Manhã</td>\n",
              "      <td>Condições Favoráveis</td>\n",
              "      <td>simples</td>\n",
              "      <td>curva</td>\n",
              "      <td>urbano</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>quarta</td>\n",
              "      <td>colisão</td>\n",
              "      <td>Noite</td>\n",
              "      <td>Condições Favoráveis</td>\n",
              "      <td>simples</td>\n",
              "      <td>curva</td>\n",
              "      <td>rural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>quarta</td>\n",
              "      <td>colisão</td>\n",
              "      <td>Tarde</td>\n",
              "      <td>Condições Favoráveis</td>\n",
              "      <td>simples</td>\n",
              "      <td>reta</td>\n",
              "      <td>rural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>quarta</td>\n",
              "      <td>atropelamento de animal</td>\n",
              "      <td>Manhã</td>\n",
              "      <td>Condições Favoráveis</td>\n",
              "      <td>simples</td>\n",
              "      <td>reta</td>\n",
              "      <td>rural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  dia_semana            tipo_acidente fase_dia condicao_metereologica  \\\n",
              "0     quarta           saída de pista    Tarde   Condições Favoráveis   \n",
              "1     quarta                  colisão    Manhã   Condições Favoráveis   \n",
              "2     quarta                  colisão    Noite   Condições Favoráveis   \n",
              "3     quarta                  colisão    Tarde   Condições Favoráveis   \n",
              "4     quarta  atropelamento de animal    Manhã   Condições Favoráveis   \n",
              "\n",
              "  tipo_pista tracado_via uso_solo  acidente_fatal  \n",
              "0      dupla       curva   urbano               1  \n",
              "1    simples       curva   urbano               0  \n",
              "2    simples       curva    rural               0  \n",
              "3    simples        reta    rural               0  \n",
              "4    simples        reta    rural               0  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[['dia_semana', 'tipo_acidente', 'fase_dia', 'condicao_metereologica', 'tipo_pista', 'tracado_via', 'uso_solo', 'acidente_fatal']]\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDkZ9XkjuTHb"
      },
      "source": [
        "## Visualização dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv7_ASvsub_N",
        "outputId": "d077c3e8-2c99-4831-cf88-799087389306"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4061, 8)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "acidente_fatal\n",
              "0    3049\n",
              "1    1012\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['acidente_fatal'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "acidente_fatal\n",
              "0    0.7508\n",
              "1    0.2492\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['acidente_fatal'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "f9TKZ4vpuhsD",
        "outputId": "5c0567ac-46cf-4afb-929a-5234e9a85552"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.countplot(x = df['acidente_fatal']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17R8qTZyusDd"
      },
      "source": [
        "## Divisão features e target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0ShAkccUgxsW"
      },
      "outputs": [],
      "source": [
        "x = df.iloc[:, :7].values\n",
        "y = df.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PyNiElqu5eI",
        "outputId": "e1be14d2-26ff-4f11-800f-923a65134f89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4061, 7), (4061,))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR9MlvwtvdBp"
      },
      "source": [
        "## Tratamento de atributos categóricos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8guiXFpzviVq"
      },
      "source": [
        "### LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3uGG_UIvZWF",
        "outputId": "abc51471-557f-442b-9b8a-0f671bfa313d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['quarta', 'saída de pista', 'Tarde', 'Condições Favoráveis',\n",
              "       'dupla', 'curva', 'urbano'], dtype=object)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcmoIuTMvZON",
        "outputId": "4da26252-d640-42d1-9242-fb44606c3241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['dia_semana', 'tipo_acidente', 'fase_dia', 'condicao_metereologica',\n",
              "       'tipo_pista', 'tracado_via', 'uso_solo', 'acidente_fatal'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "r-DN2RUKvZTd"
      },
      "outputs": [],
      "source": [
        "label_encoder_dia_semana = LabelEncoder()\n",
        "label_encoder_tipo_acidente = LabelEncoder()\n",
        "label_encoder_fase_dia = LabelEncoder()\n",
        "label_encoder_condicao_metereologica = LabelEncoder()\n",
        "label_encoder_tipo_pista = LabelEncoder()\n",
        "label_encoder_tracado_via = LabelEncoder()\n",
        "label_encoder_uso_solo = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "TgX4rMVxy0eg"
      },
      "outputs": [],
      "source": [
        "x[:, 0] = label_encoder_dia_semana.fit_transform(x[:, 0])\n",
        "x[:, 1] = label_encoder_tipo_acidente.fit_transform(x[:, 1])\n",
        "x[:, 2] = label_encoder_fase_dia.fit_transform(x[:, 2])\n",
        "x[:, 3] = label_encoder_condicao_metereologica.fit_transform(x[:, 3])\n",
        "x[:, 4] = label_encoder_tipo_pista.fit_transform(x[:, 4])\n",
        "x[:, 5] = label_encoder_tracado_via.fit_transform(x[:, 5])\n",
        "x[:, 6] = label_encoder_uso_solo.fit_transform(x[:, 6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBVwrUfj1c13",
        "outputId": "b1afe584-e580-4867-a6f6-fba2d54dbbf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 7, 3, 1, 0, 1, 3], dtype=object)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0c_bjSz1fWo",
        "outputId": "83cd43a3-a725-400e-d2aa-8e6b64a9b006"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 7, 3, ..., 0, 1, 3],\n",
              "       [1, 3, 1, ..., 2, 1, 3],\n",
              "       [1, 3, 2, ..., 2, 1, 1],\n",
              "       ...,\n",
              "       [5, 3, 3, ..., 0, 4, 0],\n",
              "       [5, 7, 1, ..., 2, 4, 0],\n",
              "       [5, 1, 3, ..., 1, 6, 2]], dtype=object)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxVndRlV1u11"
      },
      "source": [
        "### OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "0keKIpeT1oPj"
      },
      "outputs": [],
      "source": [
        "onehotencoder_x = ColumnTransformer(transformers = [('OneHot', OneHotEncoder(), [0, 1, 2, 3, 4, 5, 6])], remainder = 'passthrough')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UjtQnHV5UOd",
        "outputId": "925be395-2fd0-474a-d5fa-747748d87f02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = onehotencoder_x.fit_transform(x).toarray()\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Su2Q45i5UMA",
        "outputId": "631d6cf9-0139-4f82-df20-c526a3ce0ae6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4061, 40)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfxWFX_H6xev"
      },
      "source": [
        "## Escalonamento dos valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "UJzFbNU35UJY"
      },
      "outputs": [],
      "source": [
        "# Não foi necessário mas caso adicione a coluna de feridos será necessário\n",
        "# scaler_x = StandardScaler()\n",
        "# x = scaler_x.fit_transform(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsLkgQ8J6-8M"
      },
      "source": [
        "## Divisão das bases em treinamento e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "tF5S7xBO5UG_"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0, stratify = y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSzC6ICm7RqD",
        "outputId": "53788360-c34b-42a7-f0cc-3f36ef57de2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3248, 40), (3248,))"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okb5IuPn7US-",
        "outputId": "d4031f90-94f9-448b-df17-0e52d4f3ef2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((813, 40), (813,))"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctuy3OiI7bJg"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXbGBEtT8Kpp"
      },
      "source": [
        "### Árvores de decisão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "3mps6Nmx8BWc"
      },
      "outputs": [],
      "source": [
        "arvore_modelo = DecisionTreeClassifier(criterion = 'log_loss', min_samples_leaf = 1, min_samples_split = 2 , splitter = 'best')\n",
        "arvore_modelo.fit(x_train, y_train)\n",
        "previsoes = arvore_modelo.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNu9RqSY8Wu4",
        "outputId": "553fbbbf-512e-42bc-c06a-505a450ba716"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7023370233702337"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "zoNv3unR8Z5K",
        "outputId": "e6252909-ea29-4d30-e17c-df683510b45e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7023370233702337"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(arvore_modelo)\n",
        "cm.fit(x_train, y_train)\n",
        "cm.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5FzppuU8g1W",
        "outputId": "a14e54fa-5e3e-4aa4-de90-0ade8bd98f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.86      0.81       610\n",
            "           1       0.36      0.24      0.29       203\n",
            "\n",
            "    accuracy                           0.70       813\n",
            "   macro avg       0.56      0.55      0.55       813\n",
            "weighted avg       0.67      0.70      0.68       813\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81LACgMZ9AvC"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "rvAFDR3c8yqv"
      },
      "outputs": [],
      "source": [
        "random_modelo = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', class_weight = \"balanced\",\n",
        "                                       min_samples_leaf = 5, min_samples_split = 5)\n",
        "random_modelo.fit(x_train, y_train)\n",
        "previsoes = random_modelo.predict(x_test)\n",
        "\n",
        "# n_estimators = 100, criterion = 'entropy', class_weight = \"balanced\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnNDHC3F9Msm",
        "outputId": "c5f2b683-8ceb-4a5d-90c2-7046e155aa8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6568265682656826"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "xck5Fb959O4I",
        "outputId": "eb32b2f8-ba12-463a-b156-04624c024996"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6568265682656826"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(random_modelo)\n",
        "cm.fit(x_train, y_train)\n",
        "cm.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz3Ssa8D9WA0",
        "outputId": "6ca6ac24-b4ce-48a7-ac45-e2451c51c1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.68      0.75       610\n",
            "           1       0.38      0.58      0.46       203\n",
            "\n",
            "    accuracy                           0.66       813\n",
            "   macro avg       0.60      0.63      0.60       813\n",
            "weighted avg       0.72      0.66      0.68       813\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9FjBQbW9t6K"
      },
      "source": [
        "### K-Nearest Neighbour (KNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "mmvNeXD49aHV"
      },
      "outputs": [],
      "source": [
        "knn_modelo = KNeighborsClassifier(n_neighbors = 20, p = 1)\n",
        "knn_modelo.fit(x_train, y_train)\n",
        "previsoes = knn_modelo.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVgjH6za_f3w",
        "outputId": "f6935282-fab6-4696-f284-378411e7def6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7539975399753998"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "ZFS6EzHH_huY",
        "outputId": "cea28760-8759-4f4a-b5a5-d15f5686ddb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7539975399753998"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(knn_modelo)\n",
        "cm.fit(x_train, y_train)\n",
        "cm.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSI8H3pr_nx0",
        "outputId": "5bbb2fad-4620-41cb-aed9-70ccbe9f2de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.98      0.86       610\n",
            "           1       0.55      0.08      0.14       203\n",
            "\n",
            "    accuracy                           0.75       813\n",
            "   macro avg       0.66      0.53      0.50       813\n",
            "weighted avg       0.71      0.75      0.68       813\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIenLVOAA_J"
      },
      "source": [
        "### Regressão logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "MK-uoigY_-4x"
      },
      "outputs": [],
      "source": [
        "logistic_modelo = LogisticRegression(C = 1.0, solver = 'lbfgs', tol = 0.00001, class_weight = 'balanced')\n",
        "logistic_modelo.fit(x_train, y_train)\n",
        "previsoes = logistic_modelo.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8WXN6m-ASoz",
        "outputId": "3f5f18cc-a3ab-46cb-f00c-78f87194e94a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6039360393603936"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "kJwy-vZgAUmM",
        "outputId": "40544250-7f21-4ba0-f71a-a199892d2089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6039360393603936"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(logistic_modelo)\n",
        "cm.fit(x_train, y_train)\n",
        "cm.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABoH_mnTAaWA",
        "outputId": "b2475d89-6064-49c3-939d-a483123828dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.58      0.69       610\n",
            "           1       0.35      0.67      0.46       203\n",
            "\n",
            "    accuracy                           0.60       813\n",
            "   macro avg       0.59      0.63      0.57       813\n",
            "weighted avg       0.72      0.60      0.63       813\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCKXRp6CAkUz"
      },
      "source": [
        "### SVM (support vector machines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "odTVSQvCAeBJ"
      },
      "outputs": [],
      "source": [
        "svm_modelo = SVC(kernel = 'rbf', C = 1.0, tol = 0.001)\n",
        "svm_modelo.fit(x_train, y_train)\n",
        "previsoes = svm_modelo.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS5GuNkqBgFm",
        "outputId": "1f23e673-5892-4618-d81d-55af4bd1af09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7699876998769988"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "hucz4TpZBip3",
        "outputId": "165800ef-8c44-47c8-a44b-c0a73c4f1ef6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7699876998769988"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(svm_modelo)\n",
        "cm.fit(x_train, y_train)\n",
        "cm.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ccWtUOMBoZj",
        "outputId": "962fde2a-6d32-41cb-dd9e-dd7076041ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.99      0.87       610\n",
            "           1       0.81      0.10      0.18       203\n",
            "\n",
            "    accuracy                           0.77       813\n",
            "   macro avg       0.79      0.55      0.52       813\n",
            "weighted avg       0.78      0.77      0.70       813\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_zYqQg9CP0f"
      },
      "source": [
        "### Redes neurais artificiais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP0FV-cJBsGF",
        "outputId": "7dcf7b08-f99f-44c0-8ac6-b50ba66ec69f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# neuronios\n",
        "(7 + 1) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-sJtkqjjCa14",
        "outputId": "83228d24-443a-46b4-874c-0d97b545cca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.56928595\n",
            "Iteration 2, loss = 0.56306767\n",
            "Iteration 3, loss = 0.56326418\n",
            "Iteration 4, loss = 0.56257496\n",
            "Iteration 5, loss = 0.56276612\n",
            "Iteration 6, loss = 0.56296583\n",
            "Iteration 7, loss = 0.56294383\n",
            "Iteration 8, loss = 0.56256325\n",
            "Iteration 9, loss = 0.56289621\n",
            "Iteration 10, loss = 0.56203731\n",
            "Iteration 11, loss = 0.56307729\n",
            "Iteration 12, loss = 0.56277337\n",
            "Iteration 13, loss = 0.56337856\n",
            "Iteration 14, loss = 0.56250510\n",
            "Iteration 15, loss = 0.56189751\n",
            "Iteration 16, loss = 0.56205625\n",
            "Iteration 17, loss = 0.56253971\n",
            "Iteration 18, loss = 0.56228249\n",
            "Iteration 19, loss = 0.56193103\n",
            "Iteration 20, loss = 0.56242269\n",
            "Iteration 21, loss = 0.56193794\n",
            "Iteration 22, loss = 0.56147466\n",
            "Iteration 23, loss = 0.56188305\n",
            "Iteration 24, loss = 0.56163158\n",
            "Iteration 25, loss = 0.56239972\n",
            "Iteration 26, loss = 0.56165083\n",
            "Iteration 27, loss = 0.56168769\n",
            "Iteration 28, loss = 0.56118725\n",
            "Iteration 29, loss = 0.56266354\n",
            "Iteration 30, loss = 0.56149206\n",
            "Iteration 31, loss = 0.56075944\n",
            "Iteration 32, loss = 0.56139143\n",
            "Iteration 33, loss = 0.56067563\n",
            "Iteration 34, loss = 0.56074699\n",
            "Iteration 35, loss = 0.56257698\n",
            "Iteration 36, loss = 0.56079678\n",
            "Iteration 37, loss = 0.56120667\n",
            "Iteration 38, loss = 0.56112580\n",
            "Iteration 39, loss = 0.56099293\n",
            "Iteration 40, loss = 0.56085976\n",
            "Iteration 41, loss = 0.56056937\n",
            "Iteration 42, loss = 0.56070593\n",
            "Iteration 43, loss = 0.56001924\n",
            "Iteration 44, loss = 0.56041753\n",
            "Iteration 45, loss = 0.55984254\n",
            "Iteration 46, loss = 0.56016364\n",
            "Iteration 47, loss = 0.55923170\n",
            "Iteration 48, loss = 0.55940916\n",
            "Iteration 49, loss = 0.55899404\n",
            "Iteration 50, loss = 0.55963257\n",
            "Iteration 51, loss = 0.55879866\n",
            "Iteration 52, loss = 0.55891753\n",
            "Iteration 53, loss = 0.55902294\n",
            "Iteration 54, loss = 0.55875489\n",
            "Iteration 55, loss = 0.55715652\n",
            "Iteration 56, loss = 0.55900243\n",
            "Iteration 57, loss = 0.55865913\n",
            "Iteration 58, loss = 0.55799718\n",
            "Iteration 59, loss = 0.55843183\n",
            "Iteration 60, loss = 0.55775070\n",
            "Iteration 61, loss = 0.55661551\n",
            "Iteration 62, loss = 0.55639585\n",
            "Iteration 63, loss = 0.55747422\n",
            "Iteration 64, loss = 0.55695067\n",
            "Iteration 65, loss = 0.55537151\n",
            "Iteration 66, loss = 0.55627301\n",
            "Iteration 67, loss = 0.55570375\n",
            "Iteration 68, loss = 0.55583959\n",
            "Iteration 69, loss = 0.55479030\n",
            "Iteration 70, loss = 0.55431059\n",
            "Iteration 71, loss = 0.55451520\n",
            "Iteration 72, loss = 0.55431528\n",
            "Iteration 73, loss = 0.55337529\n",
            "Iteration 74, loss = 0.55389866\n",
            "Iteration 75, loss = 0.55328154\n",
            "Iteration 76, loss = 0.55240491\n",
            "Iteration 77, loss = 0.55241053\n",
            "Iteration 78, loss = 0.55219199\n",
            "Iteration 79, loss = 0.55125543\n",
            "Iteration 80, loss = 0.55106828\n",
            "Iteration 81, loss = 0.55011051\n",
            "Iteration 82, loss = 0.54977580\n",
            "Iteration 83, loss = 0.54892573\n",
            "Iteration 84, loss = 0.54819489\n",
            "Iteration 85, loss = 0.54819725\n",
            "Iteration 86, loss = 0.54721178\n",
            "Iteration 87, loss = 0.54633072\n",
            "Iteration 88, loss = 0.54632101\n",
            "Iteration 89, loss = 0.54585996\n",
            "Iteration 90, loss = 0.54531236\n",
            "Iteration 91, loss = 0.54417450\n",
            "Iteration 92, loss = 0.54291868\n",
            "Iteration 93, loss = 0.54245021\n",
            "Iteration 94, loss = 0.54147988\n",
            "Iteration 95, loss = 0.54155678\n",
            "Iteration 96, loss = 0.54114866\n",
            "Iteration 97, loss = 0.54013675\n",
            "Iteration 98, loss = 0.53878836\n",
            "Iteration 99, loss = 0.53788268\n",
            "Iteration 100, loss = 0.53865032\n",
            "Iteration 101, loss = 0.53712137\n",
            "Iteration 102, loss = 0.53626302\n",
            "Iteration 103, loss = 0.53704659\n",
            "Iteration 104, loss = 0.53600321\n",
            "Iteration 105, loss = 0.53538008\n",
            "Iteration 106, loss = 0.53480991\n",
            "Iteration 107, loss = 0.53374796\n",
            "Iteration 108, loss = 0.53377202\n",
            "Iteration 109, loss = 0.53241554\n",
            "Iteration 110, loss = 0.53277115\n",
            "Iteration 111, loss = 0.53248738\n",
            "Iteration 112, loss = 0.53138885\n",
            "Iteration 113, loss = 0.53106586\n",
            "Iteration 114, loss = 0.53058965\n",
            "Iteration 115, loss = 0.53024514\n",
            "Iteration 116, loss = 0.52908674\n",
            "Iteration 117, loss = 0.52970111\n",
            "Iteration 118, loss = 0.52838325\n",
            "Iteration 119, loss = 0.52882586\n",
            "Iteration 120, loss = 0.52858649\n",
            "Iteration 121, loss = 0.52919437\n",
            "Iteration 122, loss = 0.52794756\n",
            "Iteration 123, loss = 0.52885112\n",
            "Iteration 124, loss = 0.52728045\n",
            "Iteration 125, loss = 0.52806203\n",
            "Iteration 126, loss = 0.52666088\n",
            "Iteration 127, loss = 0.52605920\n",
            "Iteration 128, loss = 0.52622091\n",
            "Iteration 129, loss = 0.52695346\n",
            "Iteration 130, loss = 0.52531889\n",
            "Iteration 131, loss = 0.52576293\n",
            "Iteration 132, loss = 0.52477744\n",
            "Iteration 133, loss = 0.52550423\n",
            "Iteration 134, loss = 0.52555943\n",
            "Iteration 135, loss = 0.52544362\n",
            "Iteration 136, loss = 0.52483836\n",
            "Iteration 137, loss = 0.52437516\n",
            "Iteration 138, loss = 0.52476122\n",
            "Iteration 139, loss = 0.52521109\n",
            "Iteration 140, loss = 0.52387561\n",
            "Iteration 141, loss = 0.52424730\n",
            "Iteration 142, loss = 0.52460578\n",
            "Iteration 143, loss = 0.52358872\n",
            "Iteration 144, loss = 0.52388711\n",
            "Iteration 145, loss = 0.52439327\n",
            "Iteration 146, loss = 0.52337412\n",
            "Iteration 147, loss = 0.52290278\n",
            "Iteration 148, loss = 0.52311217\n",
            "Iteration 149, loss = 0.52249293\n",
            "Iteration 150, loss = 0.52321270\n",
            "Iteration 151, loss = 0.52235455\n",
            "Iteration 152, loss = 0.52260853\n",
            "Iteration 153, loss = 0.52223979\n",
            "Iteration 154, loss = 0.52274786\n",
            "Iteration 155, loss = 0.52224617\n",
            "Iteration 156, loss = 0.52241023\n",
            "Iteration 157, loss = 0.52128780\n",
            "Iteration 158, loss = 0.52235540\n",
            "Iteration 159, loss = 0.52155629\n",
            "Iteration 160, loss = 0.52096784\n",
            "Iteration 161, loss = 0.52164695\n",
            "Iteration 162, loss = 0.52106774\n",
            "Iteration 163, loss = 0.52088303\n",
            "Iteration 164, loss = 0.52120729\n",
            "Iteration 165, loss = 0.52177829\n",
            "Iteration 166, loss = 0.52132088\n",
            "Iteration 167, loss = 0.52122876\n",
            "Iteration 168, loss = 0.52064483\n",
            "Iteration 169, loss = 0.52107812\n",
            "Iteration 170, loss = 0.52047505\n",
            "Iteration 171, loss = 0.52081373\n",
            "Iteration 172, loss = 0.52063940\n",
            "Iteration 173, loss = 0.52061720\n",
            "Iteration 174, loss = 0.52074645\n",
            "Iteration 175, loss = 0.52026626\n",
            "Iteration 176, loss = 0.51982608\n",
            "Iteration 177, loss = 0.52078893\n",
            "Iteration 178, loss = 0.52042523\n",
            "Iteration 179, loss = 0.52064998\n",
            "Iteration 180, loss = 0.51926481\n",
            "Iteration 181, loss = 0.51971926\n",
            "Iteration 182, loss = 0.51998780\n",
            "Iteration 183, loss = 0.52042114\n",
            "Iteration 184, loss = 0.51955528\n",
            "Iteration 185, loss = 0.51888809\n",
            "Iteration 186, loss = 0.51919876\n",
            "Iteration 187, loss = 0.51971754\n",
            "Iteration 188, loss = 0.51811398\n",
            "Iteration 189, loss = 0.51984315\n",
            "Iteration 190, loss = 0.51903343\n",
            "Iteration 191, loss = 0.51937854\n",
            "Iteration 192, loss = 0.51915487\n",
            "Iteration 193, loss = 0.51909991\n",
            "Iteration 194, loss = 0.51943808\n",
            "Iteration 195, loss = 0.51948878\n",
            "Iteration 196, loss = 0.51939749\n",
            "Iteration 197, loss = 0.51896154\n",
            "Iteration 198, loss = 0.51930772\n",
            "Iteration 199, loss = 0.51841742\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-4 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-4 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-4 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-4 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-4 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=10,\n",
              "              hidden_layer_sizes=(100, 100), max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=10,\n",
              "              hidden_layer_sizes=(100, 100), max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              tol=1e-05, verbose=True)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(activation='logistic', batch_size=10,\n",
              "              hidden_layer_sizes=(100, 100), max_iter=1000, solver='sgd',\n",
              "              tol=1e-05, verbose=True)"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rede_neural_modelo = MLPClassifier(max_iter = 1000, verbose = True, tol = 0.0000100,\n",
        "                                   solver = 'sgd', activation = 'logistic',\n",
        "                                   hidden_layer_sizes = (100, 100),\n",
        "                                   batch_size = 10)\n",
        "rede_neural_modelo.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "-DByZmEhCi8z"
      },
      "outputs": [],
      "source": [
        "previsoes = rede_neural_modelo.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-d9AZTtCvGy",
        "outputId": "3fedf553-9780-4dc8-b929-9d0c749377e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7662976629766297"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "yfp0wUndCw_y",
        "outputId": "68e550f0-5a7d-422f-a662-f59c97b283a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7662976629766297"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(rede_neural_modelo)\n",
        "cm.fit(x_train, y_train)\n",
        "cm.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY3jlyZZDFLW",
        "outputId": "2d2ac40d-00eb-4211-b634-a4055ee4319f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.99      0.86       610\n",
            "           1       0.74      0.10      0.17       203\n",
            "\n",
            "    accuracy                           0.77       813\n",
            "   macro avg       0.75      0.54      0.52       813\n",
            "weighted avg       0.76      0.77      0.69       813\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W42yknTuECH3"
      },
      "source": [
        "## Validação cruzada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8j8SaQpEF8b"
      },
      "source": [
        "### Árvore de decisão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "-SA4lqRcDGr2"
      },
      "outputs": [],
      "source": [
        "parametros = {'criterion': ['gini', 'entropy'],\n",
        "              'splitter': ['best', 'random'],\n",
        "              'min_samples_split': [2, 5, 10],\n",
        "              'min_samples_leaf': [1, 5, 10]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAq0L6CuEKHU",
        "outputId": "af8c7a43-7973-435c-dacc-a0f196211dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'criterion': 'gini', 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
            "0.1976637565234356\n"
          ]
        }
      ],
      "source": [
        "grid_search = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = parametros, scoring = 'recall')\n",
        "grid_search.fit(x, y)\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_resultado = grid_search.best_score_\n",
        "print(melhores_parametros)\n",
        "print(melhor_resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kGseJlnEVQs"
      },
      "source": [
        "### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "W3EWJrioESHA"
      },
      "outputs": [],
      "source": [
        "parametros = {'criterion': ['gini', 'entropy'],\n",
        "              'n_estimators': [10, 40, 100, 150],\n",
        "              'min_samples_split': [2, 5, 10],\n",
        "              'min_samples_leaf': [1, 5, 10]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgJmsqVYEazu",
        "outputId": "2416674f-4edd-4b3a-9573-7743d4cc4c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'criterion': 'entropy', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 40}\n",
            "0.759171771520671\n"
          ]
        }
      ],
      "source": [
        "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid = parametros)\n",
        "grid_search.fit(x, y)\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_resultado = grid_search.best_score_\n",
        "print(melhores_parametros)\n",
        "print(melhor_resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRYDHXKnEf3-"
      },
      "source": [
        "### Knn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "GtmbeNTdEebH"
      },
      "outputs": [],
      "source": [
        "parametros = {'n_neighbors': [3, 5, 10, 20],\n",
        "              'p': [1, 2]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PgB1_FlEjEr",
        "outputId": "9ed6d1ca-f1d8-44f3-d700-ffb693b60228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_neighbors': 20, 'p': 1}\n",
            "0.7480913602239471\n"
          ]
        }
      ],
      "source": [
        "grid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = parametros)\n",
        "grid_search.fit(x, y)\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_resultado = grid_search.best_score_\n",
        "print(melhores_parametros)\n",
        "print(melhor_resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pYHKVUnEoUz"
      },
      "source": [
        "### Regressão logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "n-StliumEm4J"
      },
      "outputs": [],
      "source": [
        "parametros = {'tol': [0.0001, 0.00001, 0.000001],\n",
        "              'C': [1.0, 1.5, 2.0],\n",
        "              'solver': ['lbfgs', 'sag', 'saga', 'adam']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXVztCYhEqdE",
        "outputId": "500f8ed3-726f-4db1-e900-66806dcbd727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1.0, 'solver': 'lbfgs', 'tol': 1e-05}\n",
            "0.756465138543011\n"
          ]
        }
      ],
      "source": [
        "grid_search = GridSearchCV(estimator = LogisticRegression(), param_grid = parametros)\n",
        "grid_search.fit(x, y)\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_resultado = grid_search.best_score_\n",
        "print(melhores_parametros)\n",
        "print(melhor_resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFqOZVY1ExDR"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "iUHBY67hEvxv"
      },
      "outputs": [],
      "source": [
        "parametros = {'tol': [0.001, 0.0001, 0.00001],\n",
        "              'C': [1.0, 1.5, 2.0],\n",
        "              'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "jv9k-XMCE0zr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1.0, 'kernel': 'rbf', 'tol': 0.001}\n",
            "0.7586815843527892\n"
          ]
        }
      ],
      "source": [
        "grid_search = GridSearchCV(estimator=SVC(), param_grid=parametros)\n",
        "grid_search.fit(x,y)\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_resultado = grid_search.best_score_\n",
        "print(melhores_parametros)\n",
        "print(melhor_resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiCqbStKE5Qv"
      },
      "source": [
        "### Redes neurais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "RvobBrJGE3gN"
      },
      "outputs": [],
      "source": [
        "parametros = {'activation': ['relu', 'logistic', 'tahn'],\n",
        "              'solver': ['adam', 'sgd'],\n",
        "              'batch_size': [10, 56]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "HBSrhVIpE7cH"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(estimator = MLPClassifier(), param_grid = parametros)\n",
        "grid_search.fit(x, y)\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_resultado = grid_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "_fvrmLU_E-XM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'activation': 'logistic', 'batch_size': 10, 'solver': 'sgd'}\n",
            "0.7589266779367302\n"
          ]
        }
      ],
      "source": [
        "print(melhores_parametros)\n",
        "print(melhor_resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "YhIbhXJTFDxM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Iteration 1, loss = 0.57058772\n",
            "Iteration 2, loss = 0.56185621\n",
            "Iteration 3, loss = 0.56384521\n",
            "Iteration 4, loss = 0.56220127\n",
            "Iteration 5, loss = 0.56295987\n",
            "Iteration 6, loss = 0.56217820\n",
            "Iteration 7, loss = 0.56249356\n",
            "Iteration 8, loss = 0.56279695\n",
            "Iteration 9, loss = 0.56255788\n",
            "Iteration 10, loss = 0.56228693\n",
            "Iteration 11, loss = 0.56326878\n",
            "Iteration 12, loss = 0.56320085\n",
            "Iteration 13, loss = 0.56220427\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57494276\n",
            "Iteration 2, loss = 0.56329096\n",
            "Iteration 3, loss = 0.56356483\n",
            "Iteration 4, loss = 0.56315196\n",
            "Iteration 5, loss = 0.56356285\n",
            "Iteration 6, loss = 0.56344983\n",
            "Iteration 7, loss = 0.56319616\n",
            "Iteration 8, loss = 0.56283185\n",
            "Iteration 9, loss = 0.56362553\n",
            "Iteration 10, loss = 0.56349686\n",
            "Iteration 11, loss = 0.56257526\n",
            "Iteration 12, loss = 0.56263051\n",
            "Iteration 13, loss = 0.56405682\n",
            "Iteration 14, loss = 0.56284186\n",
            "Iteration 15, loss = 0.56270300\n",
            "Iteration 16, loss = 0.56337702\n",
            "Iteration 17, loss = 0.56294546\n",
            "Iteration 18, loss = 0.56277421\n",
            "Iteration 19, loss = 0.56294360\n",
            "Iteration 20, loss = 0.56266273\n",
            "Iteration 21, loss = 0.56265517\n",
            "Iteration 22, loss = 0.56293808\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56411182\n",
            "Iteration 2, loss = 0.56161283\n",
            "Iteration 3, loss = 0.56190952\n",
            "Iteration 4, loss = 0.56214868\n",
            "Iteration 5, loss = 0.56274946\n",
            "Iteration 6, loss = 0.56143897\n",
            "Iteration 7, loss = 0.56101334\n",
            "Iteration 8, loss = 0.56235192\n",
            "Iteration 9, loss = 0.56073274\n",
            "Iteration 10, loss = 0.56145663\n",
            "Iteration 11, loss = 0.56151122\n",
            "Iteration 12, loss = 0.56161633\n",
            "Iteration 13, loss = 0.56104591\n",
            "Iteration 14, loss = 0.56081522\n",
            "Iteration 15, loss = 0.56146342\n",
            "Iteration 16, loss = 0.56102998\n",
            "Iteration 17, loss = 0.56215660\n",
            "Iteration 18, loss = 0.56171362\n",
            "Iteration 19, loss = 0.56099599\n",
            "Iteration 20, loss = 0.56030796\n",
            "Iteration 21, loss = 0.56136284\n",
            "Iteration 22, loss = 0.56122987\n",
            "Iteration 23, loss = 0.56043031\n",
            "Iteration 24, loss = 0.56103478\n",
            "Iteration 25, loss = 0.56039822\n",
            "Iteration 26, loss = 0.56079087\n",
            "Iteration 27, loss = 0.56047622\n",
            "Iteration 28, loss = 0.56059027\n",
            "Iteration 29, loss = 0.56147013\n",
            "Iteration 30, loss = 0.56066930\n",
            "Iteration 31, loss = 0.56083360\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56653646\n",
            "Iteration 2, loss = 0.56430912\n",
            "Iteration 3, loss = 0.56512189\n",
            "Iteration 4, loss = 0.56515161\n",
            "Iteration 5, loss = 0.56565542\n",
            "Iteration 6, loss = 0.56469836\n",
            "Iteration 7, loss = 0.56469513\n",
            "Iteration 8, loss = 0.56517205\n",
            "Iteration 9, loss = 0.56524977\n",
            "Iteration 10, loss = 0.56382790\n",
            "Iteration 11, loss = 0.56508034\n",
            "Iteration 12, loss = 0.56445055\n",
            "Iteration 13, loss = 0.56442123\n",
            "Iteration 14, loss = 0.56508643\n",
            "Iteration 15, loss = 0.56419309\n",
            "Iteration 16, loss = 0.56405925\n",
            "Iteration 17, loss = 0.56425287\n",
            "Iteration 18, loss = 0.56426803\n",
            "Iteration 19, loss = 0.56502248\n",
            "Iteration 20, loss = 0.56419690\n",
            "Iteration 21, loss = 0.56490664\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56906446\n",
            "Iteration 2, loss = 0.55834690\n",
            "Iteration 3, loss = 0.55803437\n",
            "Iteration 4, loss = 0.55815317\n",
            "Iteration 5, loss = 0.55746662\n",
            "Iteration 6, loss = 0.55778165\n",
            "Iteration 7, loss = 0.55749430\n",
            "Iteration 8, loss = 0.55758609\n",
            "Iteration 9, loss = 0.55793495\n",
            "Iteration 10, loss = 0.55782908\n",
            "Iteration 11, loss = 0.55831690\n",
            "Iteration 12, loss = 0.55722146\n",
            "Iteration 13, loss = 0.55773770\n",
            "Iteration 14, loss = 0.55682122\n",
            "Iteration 15, loss = 0.55703170\n",
            "Iteration 16, loss = 0.55704822\n",
            "Iteration 17, loss = 0.55676455\n",
            "Iteration 18, loss = 0.55774971\n",
            "Iteration 19, loss = 0.55801266\n",
            "Iteration 20, loss = 0.55690978\n",
            "Iteration 21, loss = 0.55672814\n",
            "Iteration 22, loss = 0.55629500\n",
            "Iteration 23, loss = 0.55652660\n",
            "Iteration 24, loss = 0.55712850\n",
            "Iteration 25, loss = 0.55602032\n",
            "Iteration 26, loss = 0.55664560\n",
            "Iteration 27, loss = 0.55643802\n",
            "Iteration 28, loss = 0.55628522\n",
            "Iteration 29, loss = 0.55628856\n",
            "Iteration 30, loss = 0.55623853\n",
            "Iteration 31, loss = 0.55587401\n",
            "Iteration 32, loss = 0.55621022\n",
            "Iteration 33, loss = 0.55585901\n",
            "Iteration 34, loss = 0.55557257\n",
            "Iteration 35, loss = 0.55528086\n",
            "Iteration 36, loss = 0.55537835\n",
            "Iteration 37, loss = 0.55637562\n",
            "Iteration 38, loss = 0.55535880\n",
            "Iteration 39, loss = 0.55505598\n",
            "Iteration 40, loss = 0.55516582\n",
            "Iteration 41, loss = 0.55526524\n",
            "Iteration 42, loss = 0.55501742\n",
            "Iteration 43, loss = 0.55522767\n",
            "Iteration 44, loss = 0.55514558\n",
            "Iteration 45, loss = 0.55483104\n",
            "Iteration 46, loss = 0.55448698\n",
            "Iteration 47, loss = 0.55455184\n",
            "Iteration 48, loss = 0.55454935\n",
            "Iteration 49, loss = 0.55378779\n",
            "Iteration 50, loss = 0.55381432\n",
            "Iteration 51, loss = 0.55243071\n",
            "Iteration 52, loss = 0.55375931\n",
            "Iteration 53, loss = 0.55331332\n",
            "Iteration 54, loss = 0.55262370\n",
            "Iteration 55, loss = 0.55173107\n",
            "Iteration 56, loss = 0.55237873\n",
            "Iteration 57, loss = 0.55269993\n",
            "Iteration 58, loss = 0.55202075\n",
            "Iteration 59, loss = 0.55119103\n",
            "Iteration 60, loss = 0.55116035\n",
            "Iteration 61, loss = 0.55152558\n",
            "Iteration 62, loss = 0.55032866\n",
            "Iteration 63, loss = 0.54905654\n",
            "Iteration 64, loss = 0.55011435\n",
            "Iteration 65, loss = 0.54939954\n",
            "Iteration 66, loss = 0.54941609\n",
            "Iteration 67, loss = 0.54835453\n",
            "Iteration 68, loss = 0.54839646\n",
            "Iteration 69, loss = 0.54745206\n",
            "Iteration 70, loss = 0.54758194\n",
            "Iteration 71, loss = 0.54579399\n",
            "Iteration 72, loss = 0.54567283\n",
            "Iteration 73, loss = 0.54561460\n",
            "Iteration 74, loss = 0.54452518\n",
            "Iteration 75, loss = 0.54376703\n",
            "Iteration 76, loss = 0.54328459\n",
            "Iteration 77, loss = 0.54319009\n",
            "Iteration 78, loss = 0.54166966\n",
            "Iteration 79, loss = 0.54137367\n",
            "Iteration 80, loss = 0.54136699\n",
            "Iteration 81, loss = 0.53941260\n",
            "Iteration 82, loss = 0.54000461\n",
            "Iteration 83, loss = 0.53876284\n",
            "Iteration 84, loss = 0.53810766\n",
            "Iteration 85, loss = 0.53675561\n",
            "Iteration 86, loss = 0.53597485\n",
            "Iteration 87, loss = 0.53546224\n",
            "Iteration 88, loss = 0.53492765\n",
            "Iteration 89, loss = 0.53383387\n",
            "Iteration 90, loss = 0.53264651\n",
            "Iteration 91, loss = 0.53185619\n",
            "Iteration 92, loss = 0.53167598\n",
            "Iteration 93, loss = 0.53068412\n",
            "Iteration 94, loss = 0.52997617\n",
            "Iteration 95, loss = 0.52868224\n",
            "Iteration 96, loss = 0.52885857\n",
            "Iteration 97, loss = 0.52777412\n",
            "Iteration 98, loss = 0.52699150\n",
            "Iteration 99, loss = 0.52707205\n",
            "Iteration 100, loss = 0.52607329\n",
            "Iteration 101, loss = 0.52616712\n",
            "Iteration 102, loss = 0.52572194\n",
            "Iteration 103, loss = 0.52398016\n",
            "Iteration 104, loss = 0.52433102\n",
            "Iteration 105, loss = 0.52388215\n",
            "Iteration 106, loss = 0.52427317\n",
            "Iteration 107, loss = 0.52376483\n",
            "Iteration 108, loss = 0.52265921\n",
            "Iteration 109, loss = 0.52238198\n",
            "Iteration 110, loss = 0.52150705\n",
            "Iteration 111, loss = 0.52174370\n",
            "Iteration 112, loss = 0.52089761\n",
            "Iteration 113, loss = 0.51966658\n",
            "Iteration 114, loss = 0.52114789\n",
            "Iteration 115, loss = 0.51931582\n",
            "Iteration 116, loss = 0.52050159\n",
            "Iteration 117, loss = 0.51897573\n",
            "Iteration 118, loss = 0.51897866\n",
            "Iteration 119, loss = 0.51874651\n",
            "Iteration 120, loss = 0.51863779\n",
            "Iteration 121, loss = 0.51945824\n",
            "Iteration 122, loss = 0.51833035\n",
            "Iteration 123, loss = 0.51787118\n",
            "Iteration 124, loss = 0.51826109\n",
            "Iteration 125, loss = 0.51776763\n",
            "Iteration 126, loss = 0.51742167\n",
            "Iteration 127, loss = 0.51742215\n",
            "Iteration 128, loss = 0.51737943\n",
            "Iteration 129, loss = 0.51797707\n",
            "Iteration 130, loss = 0.51681633\n",
            "Iteration 131, loss = 0.51682990\n",
            "Iteration 132, loss = 0.51641687\n",
            "Iteration 133, loss = 0.51661490\n",
            "Iteration 134, loss = 0.51532310\n",
            "Iteration 135, loss = 0.51567615\n",
            "Iteration 136, loss = 0.51538974\n",
            "Iteration 137, loss = 0.51583876\n",
            "Iteration 138, loss = 0.51609330\n",
            "Iteration 139, loss = 0.51502500\n",
            "Iteration 140, loss = 0.51548903\n",
            "Iteration 141, loss = 0.51450520\n",
            "Iteration 142, loss = 0.51479083\n",
            "Iteration 143, loss = 0.51424269\n",
            "Iteration 144, loss = 0.51447587\n",
            "Iteration 145, loss = 0.51498163\n",
            "Iteration 146, loss = 0.51419990\n",
            "Iteration 147, loss = 0.51458629\n",
            "Iteration 148, loss = 0.51458459\n",
            "Iteration 149, loss = 0.51429356\n",
            "Iteration 150, loss = 0.51357295\n",
            "Iteration 151, loss = 0.51453419\n",
            "Iteration 152, loss = 0.51316867\n",
            "Iteration 153, loss = 0.51362237\n",
            "Iteration 154, loss = 0.51393360\n",
            "Iteration 155, loss = 0.51369134\n",
            "Iteration 156, loss = 0.51344611\n",
            "Iteration 157, loss = 0.51408129\n",
            "Iteration 158, loss = 0.51333720\n",
            "Iteration 159, loss = 0.51364453\n",
            "Iteration 160, loss = 0.51280739\n",
            "Iteration 161, loss = 0.51333666\n",
            "Iteration 162, loss = 0.51326564\n",
            "Iteration 163, loss = 0.51221746\n",
            "Iteration 164, loss = 0.51277322\n",
            "Iteration 165, loss = 0.51231030\n",
            "Iteration 166, loss = 0.51307143\n",
            "Iteration 167, loss = 0.51325188\n",
            "Iteration 168, loss = 0.51289890\n",
            "Iteration 169, loss = 0.51261560\n",
            "Iteration 170, loss = 0.51250765\n",
            "Iteration 171, loss = 0.51219464\n",
            "Iteration 172, loss = 0.51310000\n",
            "Iteration 173, loss = 0.51296621\n",
            "Iteration 174, loss = 0.51290936\n",
            "Iteration 175, loss = 0.51181618\n",
            "Iteration 176, loss = 0.51185172\n",
            "Iteration 177, loss = 0.51276549\n",
            "Iteration 178, loss = 0.51199633\n",
            "Iteration 179, loss = 0.51245752\n",
            "Iteration 180, loss = 0.51154992\n",
            "Iteration 181, loss = 0.51247278\n",
            "Iteration 182, loss = 0.51254909\n",
            "Iteration 183, loss = 0.51159132\n",
            "Iteration 184, loss = 0.51298265\n",
            "Iteration 185, loss = 0.51175251\n",
            "Iteration 186, loss = 0.51229247\n",
            "Iteration 187, loss = 0.51208764\n",
            "Iteration 188, loss = 0.51233820\n",
            "Iteration 189, loss = 0.51134644\n",
            "Iteration 190, loss = 0.51337703\n",
            "Iteration 191, loss = 0.51181294\n",
            "Iteration 192, loss = 0.51139137\n",
            "Iteration 193, loss = 0.51143956\n",
            "Iteration 194, loss = 0.51124532\n",
            "Iteration 195, loss = 0.51126384\n",
            "Iteration 196, loss = 0.51271922\n",
            "Iteration 197, loss = 0.51025368\n",
            "Iteration 198, loss = 0.51232678\n",
            "Iteration 199, loss = 0.51088959\n",
            "Iteration 200, loss = 0.51155160\n",
            "Iteration 201, loss = 0.51044912\n",
            "Iteration 202, loss = 0.51131939\n",
            "Iteration 203, loss = 0.51160438\n",
            "Iteration 204, loss = 0.51162274\n",
            "Iteration 205, loss = 0.51134889\n",
            "Iteration 206, loss = 0.51087939\n",
            "Iteration 207, loss = 0.51190735\n",
            "Iteration 208, loss = 0.51102750\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56773328\n",
            "Iteration 2, loss = 0.56525570\n",
            "Iteration 3, loss = 0.56555055\n",
            "Iteration 4, loss = 0.56543511\n",
            "Iteration 5, loss = 0.56663714\n",
            "Iteration 6, loss = 0.56511322\n",
            "Iteration 7, loss = 0.56578887\n",
            "Iteration 8, loss = 0.56497135\n",
            "Iteration 9, loss = 0.56545665\n",
            "Iteration 10, loss = 0.56554541\n",
            "Iteration 11, loss = 0.56559453\n",
            "Iteration 12, loss = 0.56530802\n",
            "Iteration 13, loss = 0.56480070\n",
            "Iteration 14, loss = 0.56480191\n",
            "Iteration 15, loss = 0.56507235\n",
            "Iteration 16, loss = 0.56540935\n",
            "Iteration 17, loss = 0.56444167\n",
            "Iteration 18, loss = 0.56493297\n",
            "Iteration 19, loss = 0.56462095\n",
            "Iteration 20, loss = 0.56430410\n",
            "Iteration 21, loss = 0.56510935\n",
            "Iteration 22, loss = 0.56507104\n",
            "Iteration 23, loss = 0.56521306\n",
            "Iteration 24, loss = 0.56426164\n",
            "Iteration 25, loss = 0.56432070\n",
            "Iteration 26, loss = 0.56381649\n",
            "Iteration 27, loss = 0.56397640\n",
            "Iteration 28, loss = 0.56452502\n",
            "Iteration 29, loss = 0.56373329\n",
            "Iteration 30, loss = 0.56369212\n",
            "Iteration 31, loss = 0.56373443\n",
            "Iteration 32, loss = 0.56338056\n",
            "Iteration 33, loss = 0.56363712\n",
            "Iteration 34, loss = 0.56379019\n",
            "Iteration 35, loss = 0.56425647\n",
            "Iteration 36, loss = 0.56377389\n",
            "Iteration 37, loss = 0.56340362\n",
            "Iteration 38, loss = 0.56383225\n",
            "Iteration 39, loss = 0.56349792\n",
            "Iteration 40, loss = 0.56347698\n",
            "Iteration 41, loss = 0.56310914\n",
            "Iteration 42, loss = 0.56332750\n",
            "Iteration 43, loss = 0.56279911\n",
            "Iteration 44, loss = 0.56292925\n",
            "Iteration 45, loss = 0.56244381\n",
            "Iteration 46, loss = 0.56318891\n",
            "Iteration 47, loss = 0.56241740\n",
            "Iteration 48, loss = 0.56194909\n",
            "Iteration 49, loss = 0.56248292\n",
            "Iteration 50, loss = 0.56166580\n",
            "Iteration 51, loss = 0.56105343\n",
            "Iteration 52, loss = 0.56138828\n",
            "Iteration 53, loss = 0.56121718\n",
            "Iteration 54, loss = 0.56179934\n",
            "Iteration 55, loss = 0.56108378\n",
            "Iteration 56, loss = 0.56098526\n",
            "Iteration 57, loss = 0.56034111\n",
            "Iteration 58, loss = 0.56027034\n",
            "Iteration 59, loss = 0.55982379\n",
            "Iteration 60, loss = 0.55994755\n",
            "Iteration 61, loss = 0.56016240\n",
            "Iteration 62, loss = 0.55878575\n",
            "Iteration 63, loss = 0.55898428\n",
            "Iteration 64, loss = 0.55848521\n",
            "Iteration 65, loss = 0.55878000\n",
            "Iteration 66, loss = 0.55763634\n",
            "Iteration 67, loss = 0.55750827\n",
            "Iteration 68, loss = 0.55680783\n",
            "Iteration 69, loss = 0.55709022\n",
            "Iteration 70, loss = 0.55579041\n",
            "Iteration 71, loss = 0.55555874\n",
            "Iteration 72, loss = 0.55534858\n",
            "Iteration 73, loss = 0.55494964\n",
            "Iteration 74, loss = 0.55454726\n",
            "Iteration 75, loss = 0.55316379\n",
            "Iteration 76, loss = 0.55306514\n",
            "Iteration 77, loss = 0.55262037\n",
            "Iteration 78, loss = 0.55171210\n",
            "Iteration 79, loss = 0.55185011\n",
            "Iteration 80, loss = 0.55120981\n",
            "Iteration 81, loss = 0.55087477\n",
            "Iteration 82, loss = 0.54948618\n",
            "Iteration 83, loss = 0.54908770\n",
            "Iteration 84, loss = 0.54926641\n",
            "Iteration 85, loss = 0.54814349\n",
            "Iteration 86, loss = 0.54734733\n",
            "Iteration 87, loss = 0.54739016\n",
            "Iteration 88, loss = 0.54647620\n",
            "Iteration 89, loss = 0.54561330\n",
            "Iteration 90, loss = 0.54519141\n",
            "Iteration 91, loss = 0.54431195\n",
            "Iteration 92, loss = 0.54321045\n",
            "Iteration 93, loss = 0.54341441\n",
            "Iteration 94, loss = 0.54207845\n",
            "Iteration 95, loss = 0.54134307\n",
            "Iteration 96, loss = 0.54153190\n",
            "Iteration 97, loss = 0.54129981\n",
            "Iteration 98, loss = 0.53885919\n",
            "Iteration 99, loss = 0.53965832\n",
            "Iteration 100, loss = 0.53917034\n",
            "Iteration 101, loss = 0.53899906\n",
            "Iteration 102, loss = 0.53838101\n",
            "Iteration 103, loss = 0.53870741\n",
            "Iteration 104, loss = 0.53770403\n",
            "Iteration 105, loss = 0.53711707\n",
            "Iteration 106, loss = 0.53638143\n",
            "Iteration 107, loss = 0.53622146\n",
            "Iteration 108, loss = 0.53634002\n",
            "Iteration 109, loss = 0.53645450\n",
            "Iteration 110, loss = 0.53512943\n",
            "Iteration 111, loss = 0.53476159\n",
            "Iteration 112, loss = 0.53506432\n",
            "Iteration 113, loss = 0.53537433\n",
            "Iteration 114, loss = 0.53469343\n",
            "Iteration 115, loss = 0.53428469\n",
            "Iteration 116, loss = 0.53317313\n",
            "Iteration 117, loss = 0.53286766\n",
            "Iteration 118, loss = 0.53318878\n",
            "Iteration 119, loss = 0.53353079\n",
            "Iteration 120, loss = 0.53279323\n",
            "Iteration 121, loss = 0.53255080\n",
            "Iteration 122, loss = 0.53229222\n",
            "Iteration 123, loss = 0.53143347\n",
            "Iteration 124, loss = 0.53131427\n",
            "Iteration 125, loss = 0.53276696\n",
            "Iteration 126, loss = 0.53185359\n",
            "Iteration 127, loss = 0.53118898\n",
            "Iteration 128, loss = 0.53080799\n",
            "Iteration 129, loss = 0.53095062\n",
            "Iteration 130, loss = 0.53034098\n",
            "Iteration 131, loss = 0.53003833\n",
            "Iteration 132, loss = 0.52975399\n",
            "Iteration 133, loss = 0.53025446\n",
            "Iteration 134, loss = 0.52900693\n",
            "Iteration 135, loss = 0.52936214\n",
            "Iteration 136, loss = 0.52888934\n",
            "Iteration 137, loss = 0.52921231\n",
            "Iteration 138, loss = 0.52951609\n",
            "Iteration 139, loss = 0.52847386\n",
            "Iteration 140, loss = 0.52925708\n",
            "Iteration 141, loss = 0.52887643\n",
            "Iteration 142, loss = 0.52835121\n",
            "Iteration 143, loss = 0.52780472\n",
            "Iteration 144, loss = 0.52801945\n",
            "Iteration 145, loss = 0.52708795\n",
            "Iteration 146, loss = 0.52748551\n",
            "Iteration 147, loss = 0.52702829\n",
            "Iteration 148, loss = 0.52667543\n",
            "Iteration 149, loss = 0.52728997\n",
            "Iteration 150, loss = 0.52651640\n",
            "Iteration 151, loss = 0.52683536\n",
            "Iteration 152, loss = 0.52730471\n",
            "Iteration 153, loss = 0.52733764\n",
            "Iteration 154, loss = 0.52632426\n",
            "Iteration 155, loss = 0.52637428\n",
            "Iteration 156, loss = 0.52664114\n",
            "Iteration 157, loss = 0.52679812\n",
            "Iteration 158, loss = 0.52601049\n",
            "Iteration 159, loss = 0.52703788\n",
            "Iteration 160, loss = 0.52624488\n",
            "Iteration 161, loss = 0.52613445\n",
            "Iteration 162, loss = 0.52622241\n",
            "Iteration 163, loss = 0.52575022\n",
            "Iteration 164, loss = 0.52538777\n",
            "Iteration 165, loss = 0.52593621\n",
            "Iteration 166, loss = 0.52577904\n",
            "Iteration 167, loss = 0.52565652\n",
            "Iteration 168, loss = 0.52576851\n",
            "Iteration 169, loss = 0.52587654\n",
            "Iteration 170, loss = 0.52553263\n",
            "Iteration 171, loss = 0.52518801\n",
            "Iteration 172, loss = 0.52513765\n",
            "Iteration 173, loss = 0.52531949\n",
            "Iteration 174, loss = 0.52537436\n",
            "Iteration 175, loss = 0.52479109\n",
            "Iteration 176, loss = 0.52504787\n",
            "Iteration 177, loss = 0.52474833\n",
            "Iteration 178, loss = 0.52497846\n",
            "Iteration 179, loss = 0.52419512\n",
            "Iteration 180, loss = 0.52510145\n",
            "Iteration 181, loss = 0.52459691\n",
            "Iteration 182, loss = 0.52439302\n",
            "Iteration 183, loss = 0.52421520\n",
            "Iteration 184, loss = 0.52497467\n",
            "Iteration 185, loss = 0.52478434\n",
            "Iteration 186, loss = 0.52501421\n",
            "Iteration 187, loss = 0.52433709\n",
            "Iteration 188, loss = 0.52473974\n",
            "Iteration 189, loss = 0.52474788\n",
            "Iteration 190, loss = 0.52461354\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56480351\n",
            "Iteration 2, loss = 0.56077563\n",
            "Iteration 3, loss = 0.56085806\n",
            "Iteration 4, loss = 0.56122611\n",
            "Iteration 5, loss = 0.55973281\n",
            "Iteration 6, loss = 0.56130213\n",
            "Iteration 7, loss = 0.56006955\n",
            "Iteration 8, loss = 0.56016835\n",
            "Iteration 9, loss = 0.55997767\n",
            "Iteration 10, loss = 0.55989895\n",
            "Iteration 11, loss = 0.55956493\n",
            "Iteration 12, loss = 0.56018204\n",
            "Iteration 13, loss = 0.56084459\n",
            "Iteration 14, loss = 0.55997146\n",
            "Iteration 15, loss = 0.56044056\n",
            "Iteration 16, loss = 0.55941356\n",
            "Iteration 17, loss = 0.56069635\n",
            "Iteration 18, loss = 0.55975481\n",
            "Iteration 19, loss = 0.55949734\n",
            "Iteration 20, loss = 0.55877925\n",
            "Iteration 21, loss = 0.55980576\n",
            "Iteration 22, loss = 0.55964293\n",
            "Iteration 23, loss = 0.55900583\n",
            "Iteration 24, loss = 0.55836479\n",
            "Iteration 25, loss = 0.55934536\n",
            "Iteration 26, loss = 0.55907309\n",
            "Iteration 27, loss = 0.55853015\n",
            "Iteration 28, loss = 0.55885945\n",
            "Iteration 29, loss = 0.55783985\n",
            "Iteration 30, loss = 0.55928440\n",
            "Iteration 31, loss = 0.55868525\n",
            "Iteration 32, loss = 0.55820576\n",
            "Iteration 33, loss = 0.55863867\n",
            "Iteration 34, loss = 0.55846905\n",
            "Iteration 35, loss = 0.55757434\n",
            "Iteration 36, loss = 0.55814306\n",
            "Iteration 37, loss = 0.55729713\n",
            "Iteration 38, loss = 0.55772837\n",
            "Iteration 39, loss = 0.55743034\n",
            "Iteration 40, loss = 0.55790667\n",
            "Iteration 41, loss = 0.55712231\n",
            "Iteration 42, loss = 0.55809946\n",
            "Iteration 43, loss = 0.55677405\n",
            "Iteration 44, loss = 0.55639056\n",
            "Iteration 45, loss = 0.55713402\n",
            "Iteration 46, loss = 0.55693559\n",
            "Iteration 47, loss = 0.55647444\n",
            "Iteration 48, loss = 0.55660390\n",
            "Iteration 49, loss = 0.55575723\n",
            "Iteration 50, loss = 0.55530026\n",
            "Iteration 51, loss = 0.55645332\n",
            "Iteration 52, loss = 0.55533729\n",
            "Iteration 53, loss = 0.55473840\n",
            "Iteration 54, loss = 0.55455977\n",
            "Iteration 55, loss = 0.55406569\n",
            "Iteration 56, loss = 0.55421490\n",
            "Iteration 57, loss = 0.55259469\n",
            "Iteration 58, loss = 0.55379555\n",
            "Iteration 59, loss = 0.55284440\n",
            "Iteration 60, loss = 0.55218539\n",
            "Iteration 61, loss = 0.55222138\n",
            "Iteration 62, loss = 0.55097621\n",
            "Iteration 63, loss = 0.54981334\n",
            "Iteration 64, loss = 0.54980831\n",
            "Iteration 65, loss = 0.55036040\n",
            "Iteration 66, loss = 0.54986809\n",
            "Iteration 67, loss = 0.54879056\n",
            "Iteration 68, loss = 0.54836605\n",
            "Iteration 69, loss = 0.54725554\n",
            "Iteration 70, loss = 0.54736473\n",
            "Iteration 71, loss = 0.54658813\n",
            "Iteration 72, loss = 0.54651901\n",
            "Iteration 73, loss = 0.54489500\n",
            "Iteration 74, loss = 0.54427491\n",
            "Iteration 75, loss = 0.54399606\n",
            "Iteration 76, loss = 0.54299980\n",
            "Iteration 77, loss = 0.54202769\n",
            "Iteration 78, loss = 0.54087985\n",
            "Iteration 79, loss = 0.54060995\n",
            "Iteration 80, loss = 0.53941117\n",
            "Iteration 81, loss = 0.53863983\n",
            "Iteration 82, loss = 0.53710620\n",
            "Iteration 83, loss = 0.53681990\n",
            "Iteration 84, loss = 0.53676394\n",
            "Iteration 85, loss = 0.53466613\n",
            "Iteration 86, loss = 0.53486384\n",
            "Iteration 87, loss = 0.53363570\n",
            "Iteration 88, loss = 0.53254114\n",
            "Iteration 89, loss = 0.53155052\n",
            "Iteration 90, loss = 0.53217166\n",
            "Iteration 91, loss = 0.53039055\n",
            "Iteration 92, loss = 0.53024343\n",
            "Iteration 93, loss = 0.52987777\n",
            "Iteration 94, loss = 0.52946383\n",
            "Iteration 95, loss = 0.52872158\n",
            "Iteration 96, loss = 0.52812508\n",
            "Iteration 97, loss = 0.52758298\n",
            "Iteration 98, loss = 0.52710175\n",
            "Iteration 99, loss = 0.52678250\n",
            "Iteration 100, loss = 0.52599220\n",
            "Iteration 101, loss = 0.52716971\n",
            "Iteration 102, loss = 0.52528055\n",
            "Iteration 103, loss = 0.52549976\n",
            "Iteration 104, loss = 0.52500254\n",
            "Iteration 105, loss = 0.52508486\n",
            "Iteration 106, loss = 0.52383594\n",
            "Iteration 107, loss = 0.52368823\n",
            "Iteration 108, loss = 0.52349885\n",
            "Iteration 109, loss = 0.52337404\n",
            "Iteration 110, loss = 0.52313118\n",
            "Iteration 111, loss = 0.52180539\n",
            "Iteration 112, loss = 0.52339351\n",
            "Iteration 113, loss = 0.52308231\n",
            "Iteration 114, loss = 0.52267610\n",
            "Iteration 115, loss = 0.52129749\n",
            "Iteration 116, loss = 0.52177482\n",
            "Iteration 117, loss = 0.52138839\n",
            "Iteration 118, loss = 0.52102414\n",
            "Iteration 119, loss = 0.52153053\n",
            "Iteration 120, loss = 0.52116751\n",
            "Iteration 121, loss = 0.52133721\n",
            "Iteration 122, loss = 0.51963541\n",
            "Iteration 123, loss = 0.51987474\n",
            "Iteration 124, loss = 0.52068084\n",
            "Iteration 125, loss = 0.52002346\n",
            "Iteration 126, loss = 0.51972943\n",
            "Iteration 127, loss = 0.52035628\n",
            "Iteration 128, loss = 0.52042716\n",
            "Iteration 129, loss = 0.51878983\n",
            "Iteration 130, loss = 0.51925629\n",
            "Iteration 131, loss = 0.51894284\n",
            "Iteration 132, loss = 0.51959768\n",
            "Iteration 133, loss = 0.51852297\n",
            "Iteration 134, loss = 0.51907545\n",
            "Iteration 135, loss = 0.51864267\n",
            "Iteration 136, loss = 0.51893876\n",
            "Iteration 137, loss = 0.51867276\n",
            "Iteration 138, loss = 0.51823092\n",
            "Iteration 139, loss = 0.51858001\n",
            "Iteration 140, loss = 0.51777243\n",
            "Iteration 141, loss = 0.51792606\n",
            "Iteration 142, loss = 0.51820956\n",
            "Iteration 143, loss = 0.51690394\n",
            "Iteration 144, loss = 0.51762904\n",
            "Iteration 145, loss = 0.51760082\n",
            "Iteration 146, loss = 0.51792655\n",
            "Iteration 147, loss = 0.51766516\n",
            "Iteration 148, loss = 0.51737943\n",
            "Iteration 149, loss = 0.51632124\n",
            "Iteration 150, loss = 0.51693121\n",
            "Iteration 151, loss = 0.51733552\n",
            "Iteration 152, loss = 0.51810939\n",
            "Iteration 153, loss = 0.51675037\n",
            "Iteration 154, loss = 0.51657994\n",
            "Iteration 155, loss = 0.51709812\n",
            "Iteration 156, loss = 0.51629483\n",
            "Iteration 157, loss = 0.51622572\n",
            "Iteration 158, loss = 0.51722044\n",
            "Iteration 159, loss = 0.51559534\n",
            "Iteration 160, loss = 0.51724931\n",
            "Iteration 161, loss = 0.51688215\n",
            "Iteration 162, loss = 0.51580487\n",
            "Iteration 163, loss = 0.51646253\n",
            "Iteration 164, loss = 0.51484235\n",
            "Iteration 165, loss = 0.51717946\n",
            "Iteration 166, loss = 0.51706702\n",
            "Iteration 167, loss = 0.51625624\n",
            "Iteration 168, loss = 0.51621504\n",
            "Iteration 169, loss = 0.51757805\n",
            "Iteration 170, loss = 0.51588766\n",
            "Iteration 171, loss = 0.51638328\n",
            "Iteration 172, loss = 0.51558783\n",
            "Iteration 173, loss = 0.51510158\n",
            "Iteration 174, loss = 0.51675212\n",
            "Iteration 175, loss = 0.51639865\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56463353\n",
            "Iteration 2, loss = 0.56336080\n",
            "Iteration 3, loss = 0.56356586\n",
            "Iteration 4, loss = 0.56383629\n",
            "Iteration 5, loss = 0.56411865\n",
            "Iteration 6, loss = 0.56357562\n",
            "Iteration 7, loss = 0.56325526\n",
            "Iteration 8, loss = 0.56313422\n",
            "Iteration 9, loss = 0.56329761\n",
            "Iteration 10, loss = 0.56321104\n",
            "Iteration 11, loss = 0.56388791\n",
            "Iteration 12, loss = 0.56373910\n",
            "Iteration 13, loss = 0.56313487\n",
            "Iteration 14, loss = 0.56323697\n",
            "Iteration 15, loss = 0.56290929\n",
            "Iteration 16, loss = 0.56324483\n",
            "Iteration 17, loss = 0.56317561\n",
            "Iteration 18, loss = 0.56340914\n",
            "Iteration 19, loss = 0.56294338\n",
            "Iteration 20, loss = 0.56302609\n",
            "Iteration 21, loss = 0.56309750\n",
            "Iteration 22, loss = 0.56230293\n",
            "Iteration 23, loss = 0.56329951\n",
            "Iteration 24, loss = 0.56197317\n",
            "Iteration 25, loss = 0.56250323\n",
            "Iteration 26, loss = 0.56238503\n",
            "Iteration 27, loss = 0.56207991\n",
            "Iteration 28, loss = 0.56182150\n",
            "Iteration 29, loss = 0.56124768\n",
            "Iteration 30, loss = 0.56225655\n",
            "Iteration 31, loss = 0.56204510\n",
            "Iteration 32, loss = 0.56195497\n",
            "Iteration 33, loss = 0.56200484\n",
            "Iteration 34, loss = 0.56136054\n",
            "Iteration 35, loss = 0.56225199\n",
            "Iteration 36, loss = 0.56199362\n",
            "Iteration 37, loss = 0.56195743\n",
            "Iteration 38, loss = 0.56161919\n",
            "Iteration 39, loss = 0.56162330\n",
            "Iteration 40, loss = 0.56061704\n",
            "Iteration 41, loss = 0.56073622\n",
            "Iteration 42, loss = 0.56167805\n",
            "Iteration 43, loss = 0.56040736\n",
            "Iteration 44, loss = 0.56095529\n",
            "Iteration 45, loss = 0.56038598\n",
            "Iteration 46, loss = 0.56069815\n",
            "Iteration 47, loss = 0.56004320\n",
            "Iteration 48, loss = 0.56007091\n",
            "Iteration 49, loss = 0.56073175\n",
            "Iteration 50, loss = 0.55870788\n",
            "Iteration 51, loss = 0.55948129\n",
            "Iteration 52, loss = 0.56007324\n",
            "Iteration 53, loss = 0.55846986\n",
            "Iteration 54, loss = 0.55988117\n",
            "Iteration 55, loss = 0.55880697\n",
            "Iteration 56, loss = 0.55834159\n",
            "Iteration 57, loss = 0.55895493\n",
            "Iteration 58, loss = 0.55842907\n",
            "Iteration 59, loss = 0.55700700\n",
            "Iteration 60, loss = 0.55770295\n",
            "Iteration 61, loss = 0.55809602\n",
            "Iteration 62, loss = 0.55746059\n",
            "Iteration 63, loss = 0.55672645\n",
            "Iteration 64, loss = 0.55574284\n",
            "Iteration 65, loss = 0.55524614\n",
            "Iteration 66, loss = 0.55457678\n",
            "Iteration 67, loss = 0.55563029\n",
            "Iteration 68, loss = 0.55523244\n",
            "Iteration 69, loss = 0.55395664\n",
            "Iteration 70, loss = 0.55479989\n",
            "Iteration 71, loss = 0.55305207\n",
            "Iteration 72, loss = 0.55273541\n",
            "Iteration 73, loss = 0.55360804\n",
            "Iteration 74, loss = 0.55238337\n",
            "Iteration 75, loss = 0.55227534\n",
            "Iteration 76, loss = 0.55081583\n",
            "Iteration 77, loss = 0.55069870\n",
            "Iteration 78, loss = 0.54892411\n",
            "Iteration 79, loss = 0.54847006\n",
            "Iteration 80, loss = 0.54892682\n",
            "Iteration 81, loss = 0.54804971\n",
            "Iteration 82, loss = 0.54686221\n",
            "Iteration 83, loss = 0.54520845\n",
            "Iteration 84, loss = 0.54601276\n",
            "Iteration 85, loss = 0.54452504\n",
            "Iteration 86, loss = 0.54393735\n",
            "Iteration 87, loss = 0.54367866\n",
            "Iteration 88, loss = 0.54297366\n",
            "Iteration 89, loss = 0.54217829\n",
            "Iteration 90, loss = 0.54142465\n",
            "Iteration 91, loss = 0.54026896\n",
            "Iteration 92, loss = 0.53957635\n",
            "Iteration 93, loss = 0.53940599\n",
            "Iteration 94, loss = 0.53816474\n",
            "Iteration 95, loss = 0.53762502\n",
            "Iteration 96, loss = 0.53699979\n",
            "Iteration 97, loss = 0.53582709\n",
            "Iteration 98, loss = 0.53521115\n",
            "Iteration 99, loss = 0.53535837\n",
            "Iteration 100, loss = 0.53461054\n",
            "Iteration 101, loss = 0.53469901\n",
            "Iteration 102, loss = 0.53366238\n",
            "Iteration 103, loss = 0.53300087\n",
            "Iteration 104, loss = 0.53278070\n",
            "Iteration 105, loss = 0.53213673\n",
            "Iteration 106, loss = 0.53112208\n",
            "Iteration 107, loss = 0.53109818\n",
            "Iteration 108, loss = 0.53159487\n",
            "Iteration 109, loss = 0.53073863\n",
            "Iteration 110, loss = 0.52928557\n",
            "Iteration 111, loss = 0.53005634\n",
            "Iteration 112, loss = 0.52976111\n",
            "Iteration 113, loss = 0.52896871\n",
            "Iteration 114, loss = 0.52893602\n",
            "Iteration 115, loss = 0.52857869\n",
            "Iteration 116, loss = 0.52724879\n",
            "Iteration 117, loss = 0.52775805\n",
            "Iteration 118, loss = 0.52778056\n",
            "Iteration 119, loss = 0.52713894\n",
            "Iteration 120, loss = 0.52672637\n",
            "Iteration 121, loss = 0.52745727\n",
            "Iteration 122, loss = 0.52675583\n",
            "Iteration 123, loss = 0.52640996\n",
            "Iteration 124, loss = 0.52627769\n",
            "Iteration 125, loss = 0.52590953\n",
            "Iteration 126, loss = 0.52534814\n",
            "Iteration 127, loss = 0.52558780\n",
            "Iteration 128, loss = 0.52417514\n",
            "Iteration 129, loss = 0.52477479\n",
            "Iteration 130, loss = 0.52459521\n",
            "Iteration 131, loss = 0.52383287\n",
            "Iteration 132, loss = 0.52502836\n",
            "Iteration 133, loss = 0.52386998\n",
            "Iteration 134, loss = 0.52400480\n",
            "Iteration 135, loss = 0.52400422\n",
            "Iteration 136, loss = 0.52439575\n",
            "Iteration 137, loss = 0.52307297\n",
            "Iteration 138, loss = 0.52397034\n",
            "Iteration 139, loss = 0.52343787\n",
            "Iteration 140, loss = 0.52294677\n",
            "Iteration 141, loss = 0.52263978\n",
            "Iteration 142, loss = 0.52278790\n",
            "Iteration 143, loss = 0.52322757\n",
            "Iteration 144, loss = 0.52272009\n",
            "Iteration 145, loss = 0.52208549\n",
            "Iteration 146, loss = 0.52185455\n",
            "Iteration 147, loss = 0.52274929\n",
            "Iteration 148, loss = 0.52233745\n",
            "Iteration 149, loss = 0.52246614\n",
            "Iteration 150, loss = 0.52173736\n",
            "Iteration 151, loss = 0.52204250\n",
            "Iteration 152, loss = 0.52104780\n",
            "Iteration 153, loss = 0.52185220\n",
            "Iteration 154, loss = 0.52199357\n",
            "Iteration 155, loss = 0.52127898\n",
            "Iteration 156, loss = 0.52091656\n",
            "Iteration 157, loss = 0.52111652\n",
            "Iteration 158, loss = 0.52098710\n",
            "Iteration 159, loss = 0.52105695\n",
            "Iteration 160, loss = 0.52089821\n",
            "Iteration 161, loss = 0.52094597\n",
            "Iteration 162, loss = 0.52073316\n",
            "Iteration 163, loss = 0.52056787\n",
            "Iteration 164, loss = 0.52033704\n",
            "Iteration 165, loss = 0.52132793\n",
            "Iteration 166, loss = 0.52015593\n",
            "Iteration 167, loss = 0.52024936\n",
            "Iteration 168, loss = 0.52084185\n",
            "Iteration 169, loss = 0.52022690\n",
            "Iteration 170, loss = 0.52014481\n",
            "Iteration 171, loss = 0.52033464\n",
            "Iteration 172, loss = 0.52039003\n",
            "Iteration 173, loss = 0.51998390\n",
            "Iteration 174, loss = 0.52104254\n",
            "Iteration 175, loss = 0.52033754\n",
            "Iteration 176, loss = 0.52039704\n",
            "Iteration 177, loss = 0.51956158\n",
            "Iteration 178, loss = 0.52005795\n",
            "Iteration 179, loss = 0.51924787\n",
            "Iteration 180, loss = 0.52066907\n",
            "Iteration 181, loss = 0.51925851\n",
            "Iteration 182, loss = 0.52034935\n",
            "Iteration 183, loss = 0.51959432\n",
            "Iteration 184, loss = 0.51988076\n",
            "Iteration 185, loss = 0.51989459\n",
            "Iteration 186, loss = 0.52055103\n",
            "Iteration 187, loss = 0.51941939\n",
            "Iteration 188, loss = 0.51948610\n",
            "Iteration 189, loss = 0.51886469\n",
            "Iteration 190, loss = 0.51954914\n",
            "Iteration 191, loss = 0.52008290\n",
            "Iteration 192, loss = 0.51975555\n",
            "Iteration 193, loss = 0.51992503\n",
            "Iteration 194, loss = 0.51907422\n",
            "Iteration 195, loss = 0.52021920\n",
            "Iteration 196, loss = 0.51936531\n",
            "Iteration 197, loss = 0.51964989\n",
            "Iteration 198, loss = 0.51948730\n",
            "Iteration 199, loss = 0.52006230\n",
            "Iteration 200, loss = 0.51984763\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57412401\n",
            "Iteration 2, loss = 0.56397966\n",
            "Iteration 3, loss = 0.56323483\n",
            "Iteration 4, loss = 0.56375480\n",
            "Iteration 5, loss = 0.56365290\n",
            "Iteration 6, loss = 0.56316491\n",
            "Iteration 7, loss = 0.56261734\n",
            "Iteration 8, loss = 0.56318370\n",
            "Iteration 9, loss = 0.56398143\n",
            "Iteration 10, loss = 0.56305755\n",
            "Iteration 11, loss = 0.56402177\n",
            "Iteration 12, loss = 0.56226210\n",
            "Iteration 13, loss = 0.56361025\n",
            "Iteration 14, loss = 0.56294798\n",
            "Iteration 15, loss = 0.56325179\n",
            "Iteration 16, loss = 0.56359146\n",
            "Iteration 17, loss = 0.56289675\n",
            "Iteration 18, loss = 0.56346937\n",
            "Iteration 19, loss = 0.56266801\n",
            "Iteration 20, loss = 0.56272513\n",
            "Iteration 21, loss = 0.56352531\n",
            "Iteration 22, loss = 0.56374008\n",
            "Iteration 23, loss = 0.56261673\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57151353\n",
            "Iteration 2, loss = 0.56398792\n",
            "Iteration 3, loss = 0.56463707\n",
            "Iteration 4, loss = 0.56374409\n",
            "Iteration 5, loss = 0.56368151\n",
            "Iteration 6, loss = 0.56391017\n",
            "Iteration 7, loss = 0.56415790\n",
            "Iteration 8, loss = 0.56406013\n",
            "Iteration 9, loss = 0.56458925\n",
            "Iteration 10, loss = 0.56410015\n",
            "Iteration 11, loss = 0.56355658\n",
            "Iteration 12, loss = 0.56305578\n",
            "Iteration 13, loss = 0.56385953\n",
            "Iteration 14, loss = 0.56449812\n",
            "Iteration 15, loss = 0.56374259\n",
            "Iteration 16, loss = 0.56368613\n",
            "Iteration 17, loss = 0.56394101\n",
            "Iteration 18, loss = 0.56368173\n",
            "Iteration 19, loss = 0.56350198\n",
            "Iteration 20, loss = 0.56366993\n",
            "Iteration 21, loss = 0.56271908\n",
            "Iteration 22, loss = 0.56319577\n",
            "Iteration 23, loss = 0.56306000\n",
            "Iteration 24, loss = 0.56363096\n",
            "Iteration 25, loss = 0.56353798\n",
            "Iteration 26, loss = 0.56305064\n",
            "Iteration 27, loss = 0.56309259\n",
            "Iteration 28, loss = 0.56382597\n",
            "Iteration 29, loss = 0.56315178\n",
            "Iteration 30, loss = 0.56336482\n",
            "Iteration 31, loss = 0.56308383\n",
            "Iteration 32, loss = 0.56269102\n",
            "Iteration 33, loss = 0.56321152\n",
            "Iteration 34, loss = 0.56222235\n",
            "Iteration 35, loss = 0.56329816\n",
            "Iteration 36, loss = 0.56285701\n",
            "Iteration 37, loss = 0.56267977\n",
            "Iteration 38, loss = 0.56219541\n",
            "Iteration 39, loss = 0.56241167\n",
            "Iteration 40, loss = 0.56215265\n",
            "Iteration 41, loss = 0.56233794\n",
            "Iteration 42, loss = 0.56227804\n",
            "Iteration 43, loss = 0.56218892\n",
            "Iteration 44, loss = 0.56150865\n",
            "Iteration 45, loss = 0.56147118\n",
            "Iteration 46, loss = 0.56208310\n",
            "Iteration 47, loss = 0.56110158\n",
            "Iteration 48, loss = 0.56147348\n",
            "Iteration 49, loss = 0.56185369\n",
            "Iteration 50, loss = 0.56094900\n",
            "Iteration 51, loss = 0.56154551\n",
            "Iteration 52, loss = 0.56146697\n",
            "Iteration 53, loss = 0.56061792\n",
            "Iteration 54, loss = 0.56049600\n",
            "Iteration 55, loss = 0.56042140\n",
            "Iteration 56, loss = 0.56049702\n",
            "Iteration 57, loss = 0.56054835\n",
            "Iteration 58, loss = 0.56059660\n",
            "Iteration 59, loss = 0.55910492\n",
            "Iteration 60, loss = 0.55880548\n",
            "Iteration 61, loss = 0.55951215\n",
            "Iteration 62, loss = 0.55844513\n",
            "Iteration 63, loss = 0.55840958\n",
            "Iteration 64, loss = 0.55777171\n",
            "Iteration 65, loss = 0.55885206\n",
            "Iteration 66, loss = 0.55806705\n",
            "Iteration 67, loss = 0.55757613\n",
            "Iteration 68, loss = 0.55742561\n",
            "Iteration 69, loss = 0.55742527\n",
            "Iteration 70, loss = 0.55787819\n",
            "Iteration 71, loss = 0.55683970\n",
            "Iteration 72, loss = 0.55627079\n",
            "Iteration 73, loss = 0.55671390\n",
            "Iteration 74, loss = 0.55554652\n",
            "Iteration 75, loss = 0.55502112\n",
            "Iteration 76, loss = 0.55460476\n",
            "Iteration 77, loss = 0.55418517\n",
            "Iteration 78, loss = 0.55365793\n",
            "Iteration 79, loss = 0.55265690\n",
            "Iteration 80, loss = 0.55271739\n",
            "Iteration 81, loss = 0.55196408\n",
            "Iteration 82, loss = 0.55090627\n",
            "Iteration 83, loss = 0.55109700\n",
            "Iteration 84, loss = 0.55043435\n",
            "Iteration 85, loss = 0.54994070\n",
            "Iteration 86, loss = 0.54953304\n",
            "Iteration 87, loss = 0.54892242\n",
            "Iteration 88, loss = 0.54800039\n",
            "Iteration 89, loss = 0.54718956\n",
            "Iteration 90, loss = 0.54775933\n",
            "Iteration 91, loss = 0.54599410\n",
            "Iteration 92, loss = 0.54494632\n",
            "Iteration 93, loss = 0.54534259\n",
            "Iteration 94, loss = 0.54429869\n",
            "Iteration 95, loss = 0.54279557\n",
            "Iteration 96, loss = 0.54267808\n",
            "Iteration 97, loss = 0.54121922\n",
            "Iteration 98, loss = 0.54089590\n",
            "Iteration 99, loss = 0.54006976\n",
            "Iteration 100, loss = 0.53963312\n",
            "Iteration 101, loss = 0.53878596\n",
            "Iteration 102, loss = 0.53896477\n",
            "Iteration 103, loss = 0.53712918\n",
            "Iteration 104, loss = 0.53750686\n",
            "Iteration 105, loss = 0.53622818\n",
            "Iteration 106, loss = 0.53606606\n",
            "Iteration 107, loss = 0.53479218\n",
            "Iteration 108, loss = 0.53486594\n",
            "Iteration 109, loss = 0.53420537\n",
            "Iteration 110, loss = 0.53485193\n",
            "Iteration 111, loss = 0.53416818\n",
            "Iteration 112, loss = 0.53365738\n",
            "Iteration 113, loss = 0.53296296\n",
            "Iteration 114, loss = 0.53240217\n",
            "Iteration 115, loss = 0.53219694\n",
            "Iteration 116, loss = 0.53078371\n",
            "Iteration 117, loss = 0.53166088\n",
            "Iteration 118, loss = 0.53138472\n",
            "Iteration 119, loss = 0.53129862\n",
            "Iteration 120, loss = 0.53010771\n",
            "Iteration 121, loss = 0.52956080\n",
            "Iteration 122, loss = 0.52984893\n",
            "Iteration 123, loss = 0.52972697\n",
            "Iteration 124, loss = 0.52977061\n",
            "Iteration 125, loss = 0.52811287\n",
            "Iteration 126, loss = 0.52962140\n",
            "Iteration 127, loss = 0.52913186\n",
            "Iteration 128, loss = 0.52844917\n",
            "Iteration 129, loss = 0.52853571\n",
            "Iteration 130, loss = 0.52764231\n",
            "Iteration 131, loss = 0.52745143\n",
            "Iteration 132, loss = 0.52787196\n",
            "Iteration 133, loss = 0.52729904\n",
            "Iteration 134, loss = 0.52671072\n",
            "Iteration 135, loss = 0.52641729\n",
            "Iteration 136, loss = 0.52588559\n",
            "Iteration 137, loss = 0.52725019\n",
            "Iteration 138, loss = 0.52585689\n",
            "Iteration 139, loss = 0.52511442\n",
            "Iteration 140, loss = 0.52644848\n",
            "Iteration 141, loss = 0.52432795\n",
            "Iteration 142, loss = 0.52615902\n",
            "Iteration 143, loss = 0.52603247\n",
            "Iteration 144, loss = 0.52546869\n",
            "Iteration 145, loss = 0.52541304\n",
            "Iteration 146, loss = 0.52428961\n",
            "Iteration 147, loss = 0.52535886\n",
            "Iteration 148, loss = 0.52444925\n",
            "Iteration 149, loss = 0.52488230\n",
            "Iteration 150, loss = 0.52446664\n",
            "Iteration 151, loss = 0.52495412\n",
            "Iteration 152, loss = 0.52372671\n",
            "Iteration 153, loss = 0.52377468\n",
            "Iteration 154, loss = 0.52402127\n",
            "Iteration 155, loss = 0.52474143\n",
            "Iteration 156, loss = 0.52381910\n",
            "Iteration 157, loss = 0.52295237\n",
            "Iteration 158, loss = 0.52429663\n",
            "Iteration 159, loss = 0.52387345\n",
            "Iteration 160, loss = 0.52373811\n",
            "Iteration 161, loss = 0.52352331\n",
            "Iteration 162, loss = 0.52287454\n",
            "Iteration 163, loss = 0.52341344\n",
            "Iteration 164, loss = 0.52226184\n",
            "Iteration 165, loss = 0.52350332\n",
            "Iteration 166, loss = 0.52271769\n",
            "Iteration 167, loss = 0.52255035\n",
            "Iteration 168, loss = 0.52163351\n",
            "Iteration 169, loss = 0.52307312\n",
            "Iteration 170, loss = 0.52252600\n",
            "Iteration 171, loss = 0.52239486\n",
            "Iteration 172, loss = 0.52234716\n",
            "Iteration 173, loss = 0.52175521\n",
            "Iteration 174, loss = 0.52232617\n",
            "Iteration 175, loss = 0.52251195\n",
            "Iteration 176, loss = 0.52163736\n",
            "Iteration 177, loss = 0.52207344\n",
            "Iteration 178, loss = 0.52250357\n",
            "Iteration 179, loss = 0.52166155\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "1\n",
            "Iteration 1, loss = 0.56750780\n",
            "Iteration 2, loss = 0.56389079\n",
            "Iteration 3, loss = 0.56364647\n",
            "Iteration 4, loss = 0.56378698\n",
            "Iteration 5, loss = 0.56371062\n",
            "Iteration 6, loss = 0.56439447\n",
            "Iteration 7, loss = 0.56394141\n",
            "Iteration 8, loss = 0.56309365\n",
            "Iteration 9, loss = 0.56399478\n",
            "Iteration 10, loss = 0.56363467\n",
            "Iteration 11, loss = 0.56323613\n",
            "Iteration 12, loss = 0.56301131\n",
            "Iteration 13, loss = 0.56292781\n",
            "Iteration 14, loss = 0.56365654\n",
            "Iteration 15, loss = 0.56347699\n",
            "Iteration 16, loss = 0.56393173\n",
            "Iteration 17, loss = 0.56289897\n",
            "Iteration 18, loss = 0.56297559\n",
            "Iteration 19, loss = 0.56292535\n",
            "Iteration 20, loss = 0.56214023\n",
            "Iteration 21, loss = 0.56214391\n",
            "Iteration 22, loss = 0.56285169\n",
            "Iteration 23, loss = 0.56213903\n",
            "Iteration 24, loss = 0.56250719\n",
            "Iteration 25, loss = 0.56203991\n",
            "Iteration 26, loss = 0.56239350\n",
            "Iteration 27, loss = 0.56263942\n",
            "Iteration 28, loss = 0.56225164\n",
            "Iteration 29, loss = 0.56222416\n",
            "Iteration 30, loss = 0.56190039\n",
            "Iteration 31, loss = 0.56241319\n",
            "Iteration 32, loss = 0.56190536\n",
            "Iteration 33, loss = 0.56189190\n",
            "Iteration 34, loss = 0.56134215\n",
            "Iteration 35, loss = 0.56273933\n",
            "Iteration 36, loss = 0.56141208\n",
            "Iteration 37, loss = 0.56158542\n",
            "Iteration 38, loss = 0.56196257\n",
            "Iteration 39, loss = 0.56116910\n",
            "Iteration 40, loss = 0.56120693\n",
            "Iteration 41, loss = 0.56049029\n",
            "Iteration 42, loss = 0.56121962\n",
            "Iteration 43, loss = 0.56034611\n",
            "Iteration 44, loss = 0.56031971\n",
            "Iteration 45, loss = 0.56047054\n",
            "Iteration 46, loss = 0.56097686\n",
            "Iteration 47, loss = 0.55972061\n",
            "Iteration 48, loss = 0.55943287\n",
            "Iteration 49, loss = 0.55938350\n",
            "Iteration 50, loss = 0.55890033\n",
            "Iteration 51, loss = 0.55935770\n",
            "Iteration 52, loss = 0.55919545\n",
            "Iteration 53, loss = 0.55818084\n",
            "Iteration 54, loss = 0.55898351\n",
            "Iteration 55, loss = 0.55850175\n",
            "Iteration 56, loss = 0.55769771\n",
            "Iteration 57, loss = 0.55787865\n",
            "Iteration 58, loss = 0.55802682\n",
            "Iteration 59, loss = 0.55754438\n",
            "Iteration 60, loss = 0.55772215\n",
            "Iteration 61, loss = 0.55772210\n",
            "Iteration 62, loss = 0.55620789\n",
            "Iteration 63, loss = 0.55594195\n",
            "Iteration 64, loss = 0.55468864\n",
            "Iteration 65, loss = 0.55475564\n",
            "Iteration 66, loss = 0.55446068\n",
            "Iteration 67, loss = 0.55441194\n",
            "Iteration 68, loss = 0.55399553\n",
            "Iteration 69, loss = 0.55236225\n",
            "Iteration 70, loss = 0.55212614\n",
            "Iteration 71, loss = 0.55159829\n",
            "Iteration 72, loss = 0.55179246\n",
            "Iteration 73, loss = 0.55045178\n",
            "Iteration 74, loss = 0.55000693\n",
            "Iteration 75, loss = 0.54968254\n",
            "Iteration 76, loss = 0.54896450\n",
            "Iteration 77, loss = 0.54945224\n",
            "Iteration 78, loss = 0.54798338\n",
            "Iteration 79, loss = 0.54715506\n",
            "Iteration 80, loss = 0.54655776\n",
            "Iteration 81, loss = 0.54575879\n",
            "Iteration 82, loss = 0.54471946\n",
            "Iteration 83, loss = 0.54394655\n",
            "Iteration 84, loss = 0.54308250\n",
            "Iteration 85, loss = 0.54253204\n",
            "Iteration 86, loss = 0.54230703\n",
            "Iteration 87, loss = 0.54197058\n",
            "Iteration 88, loss = 0.53955542\n",
            "Iteration 89, loss = 0.54028372\n",
            "Iteration 90, loss = 0.53963795\n",
            "Iteration 91, loss = 0.53861889\n",
            "Iteration 92, loss = 0.53833795\n",
            "Iteration 93, loss = 0.53697871\n",
            "Iteration 94, loss = 0.53648389\n",
            "Iteration 95, loss = 0.53620428\n",
            "Iteration 96, loss = 0.53537359\n",
            "Iteration 97, loss = 0.53530380\n",
            "Iteration 98, loss = 0.53300433\n",
            "Iteration 99, loss = 0.53410460\n",
            "Iteration 100, loss = 0.53397553\n",
            "Iteration 101, loss = 0.53309889\n",
            "Iteration 102, loss = 0.53160932\n",
            "Iteration 103, loss = 0.53147754\n",
            "Iteration 104, loss = 0.53132711\n",
            "Iteration 105, loss = 0.53097155\n",
            "Iteration 106, loss = 0.52974713\n",
            "Iteration 107, loss = 0.53032017\n",
            "Iteration 108, loss = 0.53034455\n",
            "Iteration 109, loss = 0.52866754\n",
            "Iteration 110, loss = 0.52842414\n",
            "Iteration 111, loss = 0.52849104\n",
            "Iteration 112, loss = 0.52791360\n",
            "Iteration 113, loss = 0.52814847\n",
            "Iteration 114, loss = 0.52755465\n",
            "Iteration 115, loss = 0.52687682\n",
            "Iteration 116, loss = 0.52609726\n",
            "Iteration 117, loss = 0.52587922\n",
            "Iteration 118, loss = 0.52611149\n",
            "Iteration 119, loss = 0.52572146\n",
            "Iteration 120, loss = 0.52537235\n",
            "Iteration 121, loss = 0.52605453\n",
            "Iteration 122, loss = 0.52539198\n",
            "Iteration 123, loss = 0.52568407\n",
            "Iteration 124, loss = 0.52341701\n",
            "Iteration 125, loss = 0.52477055\n",
            "Iteration 126, loss = 0.52410700\n",
            "Iteration 127, loss = 0.52464861\n",
            "Iteration 128, loss = 0.52388122\n",
            "Iteration 129, loss = 0.52382462\n",
            "Iteration 130, loss = 0.52387788\n",
            "Iteration 131, loss = 0.52341466\n",
            "Iteration 132, loss = 0.52334367\n",
            "Iteration 133, loss = 0.52324673\n",
            "Iteration 134, loss = 0.52338455\n",
            "Iteration 135, loss = 0.52227964\n",
            "Iteration 136, loss = 0.52269011\n",
            "Iteration 137, loss = 0.52279278\n",
            "Iteration 138, loss = 0.52245181\n",
            "Iteration 139, loss = 0.52196821\n",
            "Iteration 140, loss = 0.52159072\n",
            "Iteration 141, loss = 0.52247760\n",
            "Iteration 142, loss = 0.52167199\n",
            "Iteration 143, loss = 0.52146432\n",
            "Iteration 144, loss = 0.52188752\n",
            "Iteration 145, loss = 0.52078423\n",
            "Iteration 146, loss = 0.52114912\n",
            "Iteration 147, loss = 0.52118359\n",
            "Iteration 148, loss = 0.52023967\n",
            "Iteration 149, loss = 0.52099632\n",
            "Iteration 150, loss = 0.52088114\n",
            "Iteration 151, loss = 0.52022049\n",
            "Iteration 152, loss = 0.52052444\n",
            "Iteration 153, loss = 0.52036946\n",
            "Iteration 154, loss = 0.51989942\n",
            "Iteration 155, loss = 0.51995349\n",
            "Iteration 156, loss = 0.51974580\n",
            "Iteration 157, loss = 0.52002695\n",
            "Iteration 158, loss = 0.52039657\n",
            "Iteration 159, loss = 0.51951016\n",
            "Iteration 160, loss = 0.51952192\n",
            "Iteration 161, loss = 0.51904792\n",
            "Iteration 162, loss = 0.51964077\n",
            "Iteration 163, loss = 0.51884731\n",
            "Iteration 164, loss = 0.52061971\n",
            "Iteration 165, loss = 0.51875951\n",
            "Iteration 166, loss = 0.51947078\n",
            "Iteration 167, loss = 0.51885098\n",
            "Iteration 168, loss = 0.52060978\n",
            "Iteration 169, loss = 0.51956517\n",
            "Iteration 170, loss = 0.51817985\n",
            "Iteration 171, loss = 0.51885641\n",
            "Iteration 172, loss = 0.51930222\n",
            "Iteration 173, loss = 0.51895310\n",
            "Iteration 174, loss = 0.51931901\n",
            "Iteration 175, loss = 0.51911117\n",
            "Iteration 176, loss = 0.51862751\n",
            "Iteration 177, loss = 0.51867580\n",
            "Iteration 178, loss = 0.51804664\n",
            "Iteration 179, loss = 0.51977238\n",
            "Iteration 180, loss = 0.51871593\n",
            "Iteration 181, loss = 0.51865876\n",
            "Iteration 182, loss = 0.51814276\n",
            "Iteration 183, loss = 0.51796621\n",
            "Iteration 184, loss = 0.51825240\n",
            "Iteration 185, loss = 0.51859584\n",
            "Iteration 186, loss = 0.51788073\n",
            "Iteration 187, loss = 0.51858724\n",
            "Iteration 188, loss = 0.51747065\n",
            "Iteration 189, loss = 0.51784603\n",
            "Iteration 190, loss = 0.51800139\n",
            "Iteration 191, loss = 0.51793835\n",
            "Iteration 192, loss = 0.51857416\n",
            "Iteration 193, loss = 0.51882487\n",
            "Iteration 194, loss = 0.51845528\n",
            "Iteration 195, loss = 0.51842090\n",
            "Iteration 196, loss = 0.51839747\n",
            "Iteration 197, loss = 0.51779352\n",
            "Iteration 198, loss = 0.51798984\n",
            "Iteration 199, loss = 0.51776768\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56565864\n",
            "Iteration 2, loss = 0.56356547\n",
            "Iteration 3, loss = 0.56364398\n",
            "Iteration 4, loss = 0.56290033\n",
            "Iteration 5, loss = 0.56330074\n",
            "Iteration 6, loss = 0.56349731\n",
            "Iteration 7, loss = 0.56262549\n",
            "Iteration 8, loss = 0.56308732\n",
            "Iteration 9, loss = 0.56284327\n",
            "Iteration 10, loss = 0.56345080\n",
            "Iteration 11, loss = 0.56301922\n",
            "Iteration 12, loss = 0.56284281\n",
            "Iteration 13, loss = 0.56232543\n",
            "Iteration 14, loss = 0.56278752\n",
            "Iteration 15, loss = 0.56249071\n",
            "Iteration 16, loss = 0.56275421\n",
            "Iteration 17, loss = 0.56265805\n",
            "Iteration 18, loss = 0.56222287\n",
            "Iteration 19, loss = 0.56187827\n",
            "Iteration 20, loss = 0.56299105\n",
            "Iteration 21, loss = 0.56117634\n",
            "Iteration 22, loss = 0.56254402\n",
            "Iteration 23, loss = 0.56209829\n",
            "Iteration 24, loss = 0.56220333\n",
            "Iteration 25, loss = 0.56202776\n",
            "Iteration 26, loss = 0.56185889\n",
            "Iteration 27, loss = 0.56194956\n",
            "Iteration 28, loss = 0.56153660\n",
            "Iteration 29, loss = 0.56176101\n",
            "Iteration 30, loss = 0.56152000\n",
            "Iteration 31, loss = 0.56203367\n",
            "Iteration 32, loss = 0.56158878\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57281533\n",
            "Iteration 2, loss = 0.56543387\n",
            "Iteration 3, loss = 0.56530577\n",
            "Iteration 4, loss = 0.56567744\n",
            "Iteration 5, loss = 0.56503263\n",
            "Iteration 6, loss = 0.56490186\n",
            "Iteration 7, loss = 0.56487687\n",
            "Iteration 8, loss = 0.56533662\n",
            "Iteration 9, loss = 0.56517266\n",
            "Iteration 10, loss = 0.56482525\n",
            "Iteration 11, loss = 0.56552756\n",
            "Iteration 12, loss = 0.56525413\n",
            "Iteration 13, loss = 0.56455858\n",
            "Iteration 14, loss = 0.56351428\n",
            "Iteration 15, loss = 0.56497270\n",
            "Iteration 16, loss = 0.56466723\n",
            "Iteration 17, loss = 0.56440196\n",
            "Iteration 18, loss = 0.56412046\n",
            "Iteration 19, loss = 0.56488593\n",
            "Iteration 20, loss = 0.56477210\n",
            "Iteration 21, loss = 0.56516276\n",
            "Iteration 22, loss = 0.56491296\n",
            "Iteration 23, loss = 0.56463718\n",
            "Iteration 24, loss = 0.56474374\n",
            "Iteration 25, loss = 0.56494437\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56438151\n",
            "Iteration 2, loss = 0.56353987\n",
            "Iteration 3, loss = 0.56379939\n",
            "Iteration 4, loss = 0.56335840\n",
            "Iteration 5, loss = 0.56298808\n",
            "Iteration 6, loss = 0.56425518\n",
            "Iteration 7, loss = 0.56385024\n",
            "Iteration 8, loss = 0.56337653\n",
            "Iteration 9, loss = 0.56328980\n",
            "Iteration 10, loss = 0.56338943\n",
            "Iteration 11, loss = 0.56333201\n",
            "Iteration 12, loss = 0.56354127\n",
            "Iteration 13, loss = 0.56301548\n",
            "Iteration 14, loss = 0.56309267\n",
            "Iteration 15, loss = 0.56397248\n",
            "Iteration 16, loss = 0.56302169\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57239984\n",
            "Iteration 2, loss = 0.56553210\n",
            "Iteration 3, loss = 0.56587601\n",
            "Iteration 4, loss = 0.56560493\n",
            "Iteration 5, loss = 0.56598146\n",
            "Iteration 6, loss = 0.56495968\n",
            "Iteration 7, loss = 0.56548989\n",
            "Iteration 8, loss = 0.56548795\n",
            "Iteration 9, loss = 0.56621004\n",
            "Iteration 10, loss = 0.56564787\n",
            "Iteration 11, loss = 0.56571755\n",
            "Iteration 12, loss = 0.56519984\n",
            "Iteration 13, loss = 0.56476010\n",
            "Iteration 14, loss = 0.56518510\n",
            "Iteration 15, loss = 0.56531701\n",
            "Iteration 16, loss = 0.56523767\n",
            "Iteration 17, loss = 0.56442372\n",
            "Iteration 18, loss = 0.56418808\n",
            "Iteration 19, loss = 0.56494327\n",
            "Iteration 20, loss = 0.56461376\n",
            "Iteration 21, loss = 0.56370043\n",
            "Iteration 22, loss = 0.56465513\n",
            "Iteration 23, loss = 0.56418701\n",
            "Iteration 24, loss = 0.56427278\n",
            "Iteration 25, loss = 0.56446899\n",
            "Iteration 26, loss = 0.56402293\n",
            "Iteration 27, loss = 0.56454096\n",
            "Iteration 28, loss = 0.56408407\n",
            "Iteration 29, loss = 0.56457321\n",
            "Iteration 30, loss = 0.56344383\n",
            "Iteration 31, loss = 0.56353421\n",
            "Iteration 32, loss = 0.56381684\n",
            "Iteration 33, loss = 0.56394213\n",
            "Iteration 34, loss = 0.56321093\n",
            "Iteration 35, loss = 0.56327669\n",
            "Iteration 36, loss = 0.56375076\n",
            "Iteration 37, loss = 0.56248137\n",
            "Iteration 38, loss = 0.56279078\n",
            "Iteration 39, loss = 0.56213656\n",
            "Iteration 40, loss = 0.56341459\n",
            "Iteration 41, loss = 0.56236998\n",
            "Iteration 42, loss = 0.56327025\n",
            "Iteration 43, loss = 0.56261703\n",
            "Iteration 44, loss = 0.56253538\n",
            "Iteration 45, loss = 0.56097092\n",
            "Iteration 46, loss = 0.56175179\n",
            "Iteration 47, loss = 0.56156963\n",
            "Iteration 48, loss = 0.56183269\n",
            "Iteration 49, loss = 0.56098426\n",
            "Iteration 50, loss = 0.56047531\n",
            "Iteration 51, loss = 0.56061506\n",
            "Iteration 52, loss = 0.56033761\n",
            "Iteration 53, loss = 0.55960683\n",
            "Iteration 54, loss = 0.55987038\n",
            "Iteration 55, loss = 0.56039970\n",
            "Iteration 56, loss = 0.55879976\n",
            "Iteration 57, loss = 0.55950152\n",
            "Iteration 58, loss = 0.55833142\n",
            "Iteration 59, loss = 0.55854792\n",
            "Iteration 60, loss = 0.55729622\n",
            "Iteration 61, loss = 0.55722138\n",
            "Iteration 62, loss = 0.55763685\n",
            "Iteration 63, loss = 0.55645549\n",
            "Iteration 64, loss = 0.55624222\n",
            "Iteration 65, loss = 0.55511437\n",
            "Iteration 66, loss = 0.55504883\n",
            "Iteration 67, loss = 0.55384587\n",
            "Iteration 68, loss = 0.55472309\n",
            "Iteration 69, loss = 0.55352427\n",
            "Iteration 70, loss = 0.55314445\n",
            "Iteration 71, loss = 0.55215112\n",
            "Iteration 72, loss = 0.55256401\n",
            "Iteration 73, loss = 0.54988708\n",
            "Iteration 74, loss = 0.55004084\n",
            "Iteration 75, loss = 0.54962385\n",
            "Iteration 76, loss = 0.54952709\n",
            "Iteration 77, loss = 0.54823112\n",
            "Iteration 78, loss = 0.54815265\n",
            "Iteration 79, loss = 0.54777021\n",
            "Iteration 80, loss = 0.54603636\n",
            "Iteration 81, loss = 0.54588524\n",
            "Iteration 82, loss = 0.54457215\n",
            "Iteration 83, loss = 0.54412559\n",
            "Iteration 84, loss = 0.54353207\n",
            "Iteration 85, loss = 0.54188655\n",
            "Iteration 86, loss = 0.54213060\n",
            "Iteration 87, loss = 0.54103807\n",
            "Iteration 88, loss = 0.54035008\n",
            "Iteration 89, loss = 0.53947705\n",
            "Iteration 90, loss = 0.53886170\n",
            "Iteration 91, loss = 0.53810044\n",
            "Iteration 92, loss = 0.53785653\n",
            "Iteration 93, loss = 0.53684338\n",
            "Iteration 94, loss = 0.53629935\n",
            "Iteration 95, loss = 0.53585210\n",
            "Iteration 96, loss = 0.53524641\n",
            "Iteration 97, loss = 0.53591425\n",
            "Iteration 98, loss = 0.53453212\n",
            "Iteration 99, loss = 0.53359413\n",
            "Iteration 100, loss = 0.53327291\n",
            "Iteration 101, loss = 0.53368956\n",
            "Iteration 102, loss = 0.53291248\n",
            "Iteration 103, loss = 0.53300551\n",
            "Iteration 104, loss = 0.53288971\n",
            "Iteration 105, loss = 0.53273256\n",
            "Iteration 106, loss = 0.53197303\n",
            "Iteration 107, loss = 0.53137679\n",
            "Iteration 108, loss = 0.53088490\n",
            "Iteration 109, loss = 0.53124259\n",
            "Iteration 110, loss = 0.53011781\n",
            "Iteration 111, loss = 0.53026447\n",
            "Iteration 112, loss = 0.53021483\n",
            "Iteration 113, loss = 0.52923981\n",
            "Iteration 114, loss = 0.52972316\n",
            "Iteration 115, loss = 0.52929599\n",
            "Iteration 116, loss = 0.52857767\n",
            "Iteration 117, loss = 0.52874432\n",
            "Iteration 118, loss = 0.52904003\n",
            "Iteration 119, loss = 0.52841775\n",
            "Iteration 120, loss = 0.52690176\n",
            "Iteration 121, loss = 0.52794390\n",
            "Iteration 122, loss = 0.52762147\n",
            "Iteration 123, loss = 0.52587790\n",
            "Iteration 124, loss = 0.52632635\n",
            "Iteration 125, loss = 0.52720968\n",
            "Iteration 126, loss = 0.52684810\n",
            "Iteration 127, loss = 0.52656689\n",
            "Iteration 128, loss = 0.52727044\n",
            "Iteration 129, loss = 0.52659635\n",
            "Iteration 130, loss = 0.52624888\n",
            "Iteration 131, loss = 0.52670232\n",
            "Iteration 132, loss = 0.52678606\n",
            "Iteration 133, loss = 0.52639539\n",
            "Iteration 134, loss = 0.52594369\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56716907\n",
            "Iteration 2, loss = 0.56281843\n",
            "Iteration 3, loss = 0.56209301\n",
            "Iteration 4, loss = 0.56192782\n",
            "Iteration 5, loss = 0.56144002\n",
            "Iteration 6, loss = 0.56250045\n",
            "Iteration 7, loss = 0.56190534\n",
            "Iteration 8, loss = 0.56149781\n",
            "Iteration 9, loss = 0.56221560\n",
            "Iteration 10, loss = 0.56167167\n",
            "Iteration 11, loss = 0.56230568\n",
            "Iteration 12, loss = 0.56150684\n",
            "Iteration 13, loss = 0.56132847\n",
            "Iteration 14, loss = 0.56165783\n",
            "Iteration 15, loss = 0.56217441\n",
            "Iteration 16, loss = 0.56135127\n",
            "Iteration 17, loss = 0.56152619\n",
            "Iteration 18, loss = 0.56126121\n",
            "Iteration 19, loss = 0.56107015\n",
            "Iteration 20, loss = 0.56133470\n",
            "Iteration 21, loss = 0.56156352\n",
            "Iteration 22, loss = 0.56096892\n",
            "Iteration 23, loss = 0.56124575\n",
            "Iteration 24, loss = 0.56026831\n",
            "Iteration 25, loss = 0.56068721\n",
            "Iteration 26, loss = 0.56085594\n",
            "Iteration 27, loss = 0.56030849\n",
            "Iteration 28, loss = 0.56048002\n",
            "Iteration 29, loss = 0.56018563\n",
            "Iteration 30, loss = 0.56038562\n",
            "Iteration 31, loss = 0.56026043\n",
            "Iteration 32, loss = 0.55958180\n",
            "Iteration 33, loss = 0.55970949\n",
            "Iteration 34, loss = 0.56037243\n",
            "Iteration 35, loss = 0.55934917\n",
            "Iteration 36, loss = 0.55884885\n",
            "Iteration 37, loss = 0.55857113\n",
            "Iteration 38, loss = 0.55957647\n",
            "Iteration 39, loss = 0.55868042\n",
            "Iteration 40, loss = 0.55807776\n",
            "Iteration 41, loss = 0.55846330\n",
            "Iteration 42, loss = 0.55839913\n",
            "Iteration 43, loss = 0.55846185\n",
            "Iteration 44, loss = 0.55718624\n",
            "Iteration 45, loss = 0.55846222\n",
            "Iteration 46, loss = 0.55802627\n",
            "Iteration 47, loss = 0.55714245\n",
            "Iteration 48, loss = 0.55747149\n",
            "Iteration 49, loss = 0.55716570\n",
            "Iteration 50, loss = 0.55649563\n",
            "Iteration 51, loss = 0.55652199\n",
            "Iteration 52, loss = 0.55590481\n",
            "Iteration 53, loss = 0.55608952\n",
            "Iteration 54, loss = 0.55573435\n",
            "Iteration 55, loss = 0.55569610\n",
            "Iteration 56, loss = 0.55539647\n",
            "Iteration 57, loss = 0.55409181\n",
            "Iteration 58, loss = 0.55409104\n",
            "Iteration 59, loss = 0.55352880\n",
            "Iteration 60, loss = 0.55301693\n",
            "Iteration 61, loss = 0.55249714\n",
            "Iteration 62, loss = 0.55307136\n",
            "Iteration 63, loss = 0.55185594\n",
            "Iteration 64, loss = 0.55120112\n",
            "Iteration 65, loss = 0.55030164\n",
            "Iteration 66, loss = 0.55022585\n",
            "Iteration 67, loss = 0.54916665\n",
            "Iteration 68, loss = 0.54917262\n",
            "Iteration 69, loss = 0.54846802\n",
            "Iteration 70, loss = 0.54702504\n",
            "Iteration 71, loss = 0.54761329\n",
            "Iteration 72, loss = 0.54681249\n",
            "Iteration 73, loss = 0.54598783\n",
            "Iteration 74, loss = 0.54546885\n",
            "Iteration 75, loss = 0.54431521\n",
            "Iteration 76, loss = 0.54410059\n",
            "Iteration 77, loss = 0.54336739\n",
            "Iteration 78, loss = 0.54247094\n",
            "Iteration 79, loss = 0.54184060\n",
            "Iteration 80, loss = 0.54082038\n",
            "Iteration 81, loss = 0.53966641\n",
            "Iteration 82, loss = 0.53976001\n",
            "Iteration 83, loss = 0.53930814\n",
            "Iteration 84, loss = 0.53804506\n",
            "Iteration 85, loss = 0.53750257\n",
            "Iteration 86, loss = 0.53724668\n",
            "Iteration 87, loss = 0.53674876\n",
            "Iteration 88, loss = 0.53545630\n",
            "Iteration 89, loss = 0.53467301\n",
            "Iteration 90, loss = 0.53408581\n",
            "Iteration 91, loss = 0.53243656\n",
            "Iteration 92, loss = 0.53397879\n",
            "Iteration 93, loss = 0.53248114\n",
            "Iteration 94, loss = 0.53183914\n",
            "Iteration 95, loss = 0.53145741\n",
            "Iteration 96, loss = 0.53027788\n",
            "Iteration 97, loss = 0.53009686\n",
            "Iteration 98, loss = 0.53108435\n",
            "Iteration 99, loss = 0.52922987\n",
            "Iteration 100, loss = 0.53051034\n",
            "Iteration 101, loss = 0.53039293\n",
            "Iteration 102, loss = 0.52823926\n",
            "Iteration 103, loss = 0.52793014\n",
            "Iteration 104, loss = 0.52796838\n",
            "Iteration 105, loss = 0.52794404\n",
            "Iteration 106, loss = 0.52770168\n",
            "Iteration 107, loss = 0.52732259\n",
            "Iteration 108, loss = 0.52671417\n",
            "Iteration 109, loss = 0.52676007\n",
            "Iteration 110, loss = 0.52730570\n",
            "Iteration 111, loss = 0.52737670\n",
            "Iteration 112, loss = 0.52673144\n",
            "Iteration 113, loss = 0.52620094\n",
            "Iteration 114, loss = 0.52529414\n",
            "Iteration 115, loss = 0.52570054\n",
            "Iteration 116, loss = 0.52553529\n",
            "Iteration 117, loss = 0.52513484\n",
            "Iteration 118, loss = 0.52481201\n",
            "Iteration 119, loss = 0.52464713\n",
            "Iteration 120, loss = 0.52379808\n",
            "Iteration 121, loss = 0.52437053\n",
            "Iteration 122, loss = 0.52421171\n",
            "Iteration 123, loss = 0.52307962\n",
            "Iteration 124, loss = 0.52418711\n",
            "Iteration 125, loss = 0.52381032\n",
            "Iteration 126, loss = 0.52350874\n",
            "Iteration 127, loss = 0.52386844\n",
            "Iteration 128, loss = 0.52282133\n",
            "Iteration 129, loss = 0.52294862\n",
            "Iteration 130, loss = 0.52280559\n",
            "Iteration 131, loss = 0.52234916\n",
            "Iteration 132, loss = 0.52218945\n",
            "Iteration 133, loss = 0.52262835\n",
            "Iteration 134, loss = 0.52240788\n",
            "Iteration 135, loss = 0.52152648\n",
            "Iteration 136, loss = 0.52164963\n",
            "Iteration 137, loss = 0.52167887\n",
            "Iteration 138, loss = 0.52207499\n",
            "Iteration 139, loss = 0.52161408\n",
            "Iteration 140, loss = 0.52245758\n",
            "Iteration 141, loss = 0.52141079\n",
            "Iteration 142, loss = 0.52223852\n",
            "Iteration 143, loss = 0.52177269\n",
            "Iteration 144, loss = 0.52160974\n",
            "Iteration 145, loss = 0.52053484\n",
            "Iteration 146, loss = 0.52084768\n",
            "Iteration 147, loss = 0.52058755\n",
            "Iteration 148, loss = 0.52135158\n",
            "Iteration 149, loss = 0.52075146\n",
            "Iteration 150, loss = 0.52108108\n",
            "Iteration 151, loss = 0.52002707\n",
            "Iteration 152, loss = 0.52078096\n",
            "Iteration 153, loss = 0.52033782\n",
            "Iteration 154, loss = 0.51974764\n",
            "Iteration 155, loss = 0.52111619\n",
            "Iteration 156, loss = 0.52034021\n",
            "Iteration 157, loss = 0.52060579\n",
            "Iteration 158, loss = 0.51973662\n",
            "Iteration 159, loss = 0.51986430\n",
            "Iteration 160, loss = 0.51899520\n",
            "Iteration 161, loss = 0.52005786\n",
            "Iteration 162, loss = 0.52032593\n",
            "Iteration 163, loss = 0.51948462\n",
            "Iteration 164, loss = 0.52033117\n",
            "Iteration 165, loss = 0.52016468\n",
            "Iteration 166, loss = 0.51941867\n",
            "Iteration 167, loss = 0.51871007\n",
            "Iteration 168, loss = 0.52005079\n",
            "Iteration 169, loss = 0.51955788\n",
            "Iteration 170, loss = 0.51980038\n",
            "Iteration 171, loss = 0.52003131\n",
            "Iteration 172, loss = 0.51898765\n",
            "Iteration 173, loss = 0.51890035\n",
            "Iteration 174, loss = 0.51962694\n",
            "Iteration 175, loss = 0.51947546\n",
            "Iteration 176, loss = 0.51940408\n",
            "Iteration 177, loss = 0.51854938\n",
            "Iteration 178, loss = 0.51915871\n",
            "Iteration 179, loss = 0.51899216\n",
            "Iteration 180, loss = 0.51903700\n",
            "Iteration 181, loss = 0.51888075\n",
            "Iteration 182, loss = 0.51895988\n",
            "Iteration 183, loss = 0.51977685\n",
            "Iteration 184, loss = 0.51904651\n",
            "Iteration 185, loss = 0.51916579\n",
            "Iteration 186, loss = 0.51791657\n",
            "Iteration 187, loss = 0.51932847\n",
            "Iteration 188, loss = 0.51909966\n",
            "Iteration 189, loss = 0.51787918\n",
            "Iteration 190, loss = 0.51907562\n",
            "Iteration 191, loss = 0.51810064\n",
            "Iteration 192, loss = 0.51861993\n",
            "Iteration 193, loss = 0.51926658\n",
            "Iteration 194, loss = 0.51870363\n",
            "Iteration 195, loss = 0.51901917\n",
            "Iteration 196, loss = 0.51814712\n",
            "Iteration 197, loss = 0.51850780\n",
            "Iteration 198, loss = 0.51782721\n",
            "Iteration 199, loss = 0.51799439\n",
            "Iteration 200, loss = 0.51869853\n",
            "Iteration 201, loss = 0.51915152\n",
            "Iteration 202, loss = 0.51686631\n",
            "Iteration 203, loss = 0.51885687\n",
            "Iteration 204, loss = 0.51793891\n",
            "Iteration 205, loss = 0.51889541\n",
            "Iteration 206, loss = 0.51849183\n",
            "Iteration 207, loss = 0.51969421\n",
            "Iteration 208, loss = 0.51782130\n",
            "Iteration 209, loss = 0.51864674\n",
            "Iteration 210, loss = 0.51804075\n",
            "Iteration 211, loss = 0.51834861\n",
            "Iteration 212, loss = 0.51914629\n",
            "Iteration 213, loss = 0.51741395\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56861630\n",
            "Iteration 2, loss = 0.56149426\n",
            "Iteration 3, loss = 0.56227058\n",
            "Iteration 4, loss = 0.56238099\n",
            "Iteration 5, loss = 0.56275024\n",
            "Iteration 6, loss = 0.56230688\n",
            "Iteration 7, loss = 0.56283653\n",
            "Iteration 8, loss = 0.56169212\n",
            "Iteration 9, loss = 0.56263768\n",
            "Iteration 10, loss = 0.56221751\n",
            "Iteration 11, loss = 0.56184327\n",
            "Iteration 12, loss = 0.56156851\n",
            "Iteration 13, loss = 0.56180693\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56122164\n",
            "Iteration 2, loss = 0.56057502\n",
            "Iteration 3, loss = 0.55997191\n",
            "Iteration 4, loss = 0.56058247\n",
            "Iteration 5, loss = 0.56005173\n",
            "Iteration 6, loss = 0.56024789\n",
            "Iteration 7, loss = 0.56045905\n",
            "Iteration 8, loss = 0.56046541\n",
            "Iteration 9, loss = 0.56044347\n",
            "Iteration 10, loss = 0.56006696\n",
            "Iteration 11, loss = 0.56011452\n",
            "Iteration 12, loss = 0.55969301\n",
            "Iteration 13, loss = 0.56007716\n",
            "Iteration 14, loss = 0.55944790\n",
            "Iteration 15, loss = 0.56160670\n",
            "Iteration 16, loss = 0.55940136\n",
            "Iteration 17, loss = 0.55951065\n",
            "Iteration 18, loss = 0.55943568\n",
            "Iteration 19, loss = 0.55915062\n",
            "Iteration 20, loss = 0.55944620\n",
            "Iteration 21, loss = 0.55929069\n",
            "Iteration 22, loss = 0.55923768\n",
            "Iteration 23, loss = 0.55930479\n",
            "Iteration 24, loss = 0.55877722\n",
            "Iteration 25, loss = 0.55844434\n",
            "Iteration 26, loss = 0.55924407\n",
            "Iteration 27, loss = 0.55899027\n",
            "Iteration 28, loss = 0.55898308\n",
            "Iteration 29, loss = 0.55921127\n",
            "Iteration 30, loss = 0.55970177\n",
            "Iteration 31, loss = 0.55861030\n",
            "Iteration 32, loss = 0.55839183\n",
            "Iteration 33, loss = 0.55825552\n",
            "Iteration 34, loss = 0.55811682\n",
            "Iteration 35, loss = 0.55833626\n",
            "Iteration 36, loss = 0.55833975\n",
            "Iteration 37, loss = 0.55798843\n",
            "Iteration 38, loss = 0.55779274\n",
            "Iteration 39, loss = 0.55835089\n",
            "Iteration 40, loss = 0.55802240\n",
            "Iteration 41, loss = 0.55777983\n",
            "Iteration 42, loss = 0.55680775\n",
            "Iteration 43, loss = 0.55776190\n",
            "Iteration 44, loss = 0.55706164\n",
            "Iteration 45, loss = 0.55769808\n",
            "Iteration 46, loss = 0.55723122\n",
            "Iteration 47, loss = 0.55683085\n",
            "Iteration 48, loss = 0.55705060\n",
            "Iteration 49, loss = 0.55624777\n",
            "Iteration 50, loss = 0.55607491\n",
            "Iteration 51, loss = 0.55574091\n",
            "Iteration 52, loss = 0.55506288\n",
            "Iteration 53, loss = 0.55514371\n",
            "Iteration 54, loss = 0.55664609\n",
            "Iteration 55, loss = 0.55563378\n",
            "Iteration 56, loss = 0.55468841\n",
            "Iteration 57, loss = 0.55500175\n",
            "Iteration 58, loss = 0.55402219\n",
            "Iteration 59, loss = 0.55452123\n",
            "Iteration 60, loss = 0.55383899\n",
            "Iteration 61, loss = 0.55420169\n",
            "Iteration 62, loss = 0.55213445\n",
            "Iteration 63, loss = 0.55355733\n",
            "Iteration 64, loss = 0.55226406\n",
            "Iteration 65, loss = 0.55236365\n",
            "Iteration 66, loss = 0.55236095\n",
            "Iteration 67, loss = 0.55210312\n",
            "Iteration 68, loss = 0.55105788\n",
            "Iteration 69, loss = 0.55107475\n",
            "Iteration 70, loss = 0.55030203\n",
            "Iteration 71, loss = 0.55022399\n",
            "Iteration 72, loss = 0.54946945\n",
            "Iteration 73, loss = 0.54963442\n",
            "Iteration 74, loss = 0.54914469\n",
            "Iteration 75, loss = 0.54770472\n",
            "Iteration 76, loss = 0.54798122\n",
            "Iteration 77, loss = 0.54660703\n",
            "Iteration 78, loss = 0.54722248\n",
            "Iteration 79, loss = 0.54517962\n",
            "Iteration 80, loss = 0.54497835\n",
            "Iteration 81, loss = 0.54490730\n",
            "Iteration 82, loss = 0.54329539\n",
            "Iteration 83, loss = 0.54245749\n",
            "Iteration 84, loss = 0.54145095\n",
            "Iteration 85, loss = 0.54179841\n",
            "Iteration 86, loss = 0.54089198\n",
            "Iteration 87, loss = 0.54021617\n",
            "Iteration 88, loss = 0.53959570\n",
            "Iteration 89, loss = 0.53948550\n",
            "Iteration 90, loss = 0.53795761\n",
            "Iteration 91, loss = 0.53696197\n",
            "Iteration 92, loss = 0.53697216\n",
            "Iteration 93, loss = 0.53547478\n",
            "Iteration 94, loss = 0.53523961\n",
            "Iteration 95, loss = 0.53470363\n",
            "Iteration 96, loss = 0.53405991\n",
            "Iteration 97, loss = 0.53299319\n",
            "Iteration 98, loss = 0.53295835\n",
            "Iteration 99, loss = 0.53213158\n",
            "Iteration 100, loss = 0.53215636\n",
            "Iteration 101, loss = 0.52975479\n",
            "Iteration 102, loss = 0.52979145\n",
            "Iteration 103, loss = 0.52977357\n",
            "Iteration 104, loss = 0.52940763\n",
            "Iteration 105, loss = 0.52885890\n",
            "Iteration 106, loss = 0.52857115\n",
            "Iteration 107, loss = 0.52800636\n",
            "Iteration 108, loss = 0.52712805\n",
            "Iteration 109, loss = 0.52690766\n",
            "Iteration 110, loss = 0.52741898\n",
            "Iteration 111, loss = 0.52688053\n",
            "Iteration 112, loss = 0.52600983\n",
            "Iteration 113, loss = 0.52543080\n",
            "Iteration 114, loss = 0.52622432\n",
            "Iteration 115, loss = 0.52532343\n",
            "Iteration 116, loss = 0.52473693\n",
            "Iteration 117, loss = 0.52369205\n",
            "Iteration 118, loss = 0.52437049\n",
            "Iteration 119, loss = 0.52345296\n",
            "Iteration 120, loss = 0.52410938\n",
            "Iteration 121, loss = 0.52386889\n",
            "Iteration 122, loss = 0.52358382\n",
            "Iteration 123, loss = 0.52300925\n",
            "Iteration 124, loss = 0.52247070\n",
            "Iteration 125, loss = 0.52247620\n",
            "Iteration 126, loss = 0.52217112\n",
            "Iteration 127, loss = 0.52239460\n",
            "Iteration 128, loss = 0.52236132\n",
            "Iteration 129, loss = 0.52211489\n",
            "Iteration 130, loss = 0.52181855\n",
            "Iteration 131, loss = 0.52103928\n",
            "Iteration 132, loss = 0.52083198\n",
            "Iteration 133, loss = 0.52062054\n",
            "Iteration 134, loss = 0.52047064\n",
            "Iteration 135, loss = 0.51993873\n",
            "Iteration 136, loss = 0.52006894\n",
            "Iteration 137, loss = 0.51979870\n",
            "Iteration 138, loss = 0.52090608\n",
            "Iteration 139, loss = 0.51985432\n",
            "Iteration 140, loss = 0.52011675\n",
            "Iteration 141, loss = 0.51979027\n",
            "Iteration 142, loss = 0.52035572\n",
            "Iteration 143, loss = 0.51887557\n",
            "Iteration 144, loss = 0.51946169\n",
            "Iteration 145, loss = 0.51950465\n",
            "Iteration 146, loss = 0.51926791\n",
            "Iteration 147, loss = 0.52021391\n",
            "Iteration 148, loss = 0.51902649\n",
            "Iteration 149, loss = 0.51857086\n",
            "Iteration 150, loss = 0.51867130\n",
            "Iteration 151, loss = 0.51856552\n",
            "Iteration 152, loss = 0.51907516\n",
            "Iteration 153, loss = 0.51803628\n",
            "Iteration 154, loss = 0.51729227\n",
            "Iteration 155, loss = 0.51850940\n",
            "Iteration 156, loss = 0.51812514\n",
            "Iteration 157, loss = 0.51705449\n",
            "Iteration 158, loss = 0.51729683\n",
            "Iteration 159, loss = 0.51768251\n",
            "Iteration 160, loss = 0.51831811\n",
            "Iteration 161, loss = 0.51748368\n",
            "Iteration 162, loss = 0.51752937\n",
            "Iteration 163, loss = 0.51691686\n",
            "Iteration 164, loss = 0.51719669\n",
            "Iteration 165, loss = 0.51794387\n",
            "Iteration 166, loss = 0.51719580\n",
            "Iteration 167, loss = 0.51695254\n",
            "Iteration 168, loss = 0.51757706\n",
            "Iteration 169, loss = 0.51768393\n",
            "Iteration 170, loss = 0.51684016\n",
            "Iteration 171, loss = 0.51778373\n",
            "Iteration 172, loss = 0.51689732\n",
            "Iteration 173, loss = 0.51733224\n",
            "Iteration 174, loss = 0.51672551\n",
            "Iteration 175, loss = 0.51764891\n",
            "Iteration 176, loss = 0.51686529\n",
            "Iteration 177, loss = 0.51686830\n",
            "Iteration 178, loss = 0.51715335\n",
            "Iteration 179, loss = 0.51679255\n",
            "Iteration 180, loss = 0.51660019\n",
            "Iteration 181, loss = 0.51650724\n",
            "Iteration 182, loss = 0.51699778\n",
            "Iteration 183, loss = 0.51533220\n",
            "Iteration 184, loss = 0.51640300\n",
            "Iteration 185, loss = 0.51607372\n",
            "Iteration 186, loss = 0.51669879\n",
            "Iteration 187, loss = 0.51683692\n",
            "Iteration 188, loss = 0.51608704\n",
            "Iteration 189, loss = 0.51590952\n",
            "Iteration 190, loss = 0.51633734\n",
            "Iteration 191, loss = 0.51681505\n",
            "Iteration 192, loss = 0.51620286\n",
            "Iteration 193, loss = 0.51582599\n",
            "Iteration 194, loss = 0.51685747\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57048850\n",
            "Iteration 2, loss = 0.56298706\n",
            "Iteration 3, loss = 0.56260112\n",
            "Iteration 4, loss = 0.56135225\n",
            "Iteration 5, loss = 0.56197486\n",
            "Iteration 6, loss = 0.56273129\n",
            "Iteration 7, loss = 0.56232038\n",
            "Iteration 8, loss = 0.56284520\n",
            "Iteration 9, loss = 0.56195017\n",
            "Iteration 10, loss = 0.56273015\n",
            "Iteration 11, loss = 0.56203147\n",
            "Iteration 12, loss = 0.56184037\n",
            "Iteration 13, loss = 0.56177357\n",
            "Iteration 14, loss = 0.56186110\n",
            "Iteration 15, loss = 0.56406990\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56356744\n",
            "Iteration 2, loss = 0.56120767\n",
            "Iteration 3, loss = 0.56083221\n",
            "Iteration 4, loss = 0.56017142\n",
            "Iteration 5, loss = 0.56024701\n",
            "Iteration 6, loss = 0.56002023\n",
            "Iteration 7, loss = 0.56094219\n",
            "Iteration 8, loss = 0.56015287\n",
            "Iteration 9, loss = 0.56013738\n",
            "Iteration 10, loss = 0.55979595\n",
            "Iteration 11, loss = 0.55977278\n",
            "Iteration 12, loss = 0.55954538\n",
            "Iteration 13, loss = 0.55983512\n",
            "Iteration 14, loss = 0.56106515\n",
            "Iteration 15, loss = 0.55922669\n",
            "Iteration 16, loss = 0.55951202\n",
            "Iteration 17, loss = 0.56032776\n",
            "Iteration 18, loss = 0.55928145\n",
            "Iteration 19, loss = 0.55948903\n",
            "Iteration 20, loss = 0.55967134\n",
            "Iteration 21, loss = 0.55924887\n",
            "Iteration 22, loss = 0.55936145\n",
            "Iteration 23, loss = 0.55981821\n",
            "Iteration 24, loss = 0.56015818\n",
            "Iteration 25, loss = 0.55996818\n",
            "Iteration 26, loss = 0.55930262\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "2\n",
            "Iteration 1, loss = 0.57748949\n",
            "Iteration 2, loss = 0.56741234\n",
            "Iteration 3, loss = 0.56679141\n",
            "Iteration 4, loss = 0.56776980\n",
            "Iteration 5, loss = 0.56587109\n",
            "Iteration 6, loss = 0.56669029\n",
            "Iteration 7, loss = 0.56705862\n",
            "Iteration 8, loss = 0.56701711\n",
            "Iteration 9, loss = 0.56629675\n",
            "Iteration 10, loss = 0.56655924\n",
            "Iteration 11, loss = 0.56662310\n",
            "Iteration 12, loss = 0.56577523\n",
            "Iteration 13, loss = 0.56573172\n",
            "Iteration 14, loss = 0.56676547\n",
            "Iteration 15, loss = 0.56629017\n",
            "Iteration 16, loss = 0.56658812\n",
            "Iteration 17, loss = 0.56581997\n",
            "Iteration 18, loss = 0.56581755\n",
            "Iteration 19, loss = 0.56528827\n",
            "Iteration 20, loss = 0.56574780\n",
            "Iteration 21, loss = 0.56571112\n",
            "Iteration 22, loss = 0.56638076\n",
            "Iteration 23, loss = 0.56585190\n",
            "Iteration 24, loss = 0.56573350\n",
            "Iteration 25, loss = 0.56626034\n",
            "Iteration 26, loss = 0.56504049\n",
            "Iteration 27, loss = 0.56571979\n",
            "Iteration 28, loss = 0.56544917\n",
            "Iteration 29, loss = 0.56491863\n",
            "Iteration 30, loss = 0.56482361\n",
            "Iteration 31, loss = 0.56521906\n",
            "Iteration 32, loss = 0.56482550\n",
            "Iteration 33, loss = 0.56475709\n",
            "Iteration 34, loss = 0.56447531\n",
            "Iteration 35, loss = 0.56424989\n",
            "Iteration 36, loss = 0.56490734\n",
            "Iteration 37, loss = 0.56443983\n",
            "Iteration 38, loss = 0.56480721\n",
            "Iteration 39, loss = 0.56477176\n",
            "Iteration 40, loss = 0.56480407\n",
            "Iteration 41, loss = 0.56431664\n",
            "Iteration 42, loss = 0.56378502\n",
            "Iteration 43, loss = 0.56350774\n",
            "Iteration 44, loss = 0.56379231\n",
            "Iteration 45, loss = 0.56337818\n",
            "Iteration 46, loss = 0.56320401\n",
            "Iteration 47, loss = 0.56354828\n",
            "Iteration 48, loss = 0.56335079\n",
            "Iteration 49, loss = 0.56260480\n",
            "Iteration 50, loss = 0.56274871\n",
            "Iteration 51, loss = 0.56236685\n",
            "Iteration 52, loss = 0.56169217\n",
            "Iteration 53, loss = 0.56201092\n",
            "Iteration 54, loss = 0.56168774\n",
            "Iteration 55, loss = 0.56108855\n",
            "Iteration 56, loss = 0.56120667\n",
            "Iteration 57, loss = 0.56096838\n",
            "Iteration 58, loss = 0.56012709\n",
            "Iteration 59, loss = 0.56055169\n",
            "Iteration 60, loss = 0.56037965\n",
            "Iteration 61, loss = 0.55999970\n",
            "Iteration 62, loss = 0.55959202\n",
            "Iteration 63, loss = 0.55952240\n",
            "Iteration 64, loss = 0.55876708\n",
            "Iteration 65, loss = 0.55815855\n",
            "Iteration 66, loss = 0.55771077\n",
            "Iteration 67, loss = 0.55661470\n",
            "Iteration 68, loss = 0.55698430\n",
            "Iteration 69, loss = 0.55648216\n",
            "Iteration 70, loss = 0.55548003\n",
            "Iteration 71, loss = 0.55551403\n",
            "Iteration 72, loss = 0.55552965\n",
            "Iteration 73, loss = 0.55482811\n",
            "Iteration 74, loss = 0.55357006\n",
            "Iteration 75, loss = 0.55332786\n",
            "Iteration 76, loss = 0.55273199\n",
            "Iteration 77, loss = 0.55137806\n",
            "Iteration 78, loss = 0.55151293\n",
            "Iteration 79, loss = 0.55042061\n",
            "Iteration 80, loss = 0.54989114\n",
            "Iteration 81, loss = 0.54971417\n",
            "Iteration 82, loss = 0.54822249\n",
            "Iteration 83, loss = 0.54863927\n",
            "Iteration 84, loss = 0.54772754\n",
            "Iteration 85, loss = 0.54647072\n",
            "Iteration 86, loss = 0.54585753\n",
            "Iteration 87, loss = 0.54498957\n",
            "Iteration 88, loss = 0.54452430\n",
            "Iteration 89, loss = 0.54342258\n",
            "Iteration 90, loss = 0.54314680\n",
            "Iteration 91, loss = 0.54263746\n",
            "Iteration 92, loss = 0.54163664\n",
            "Iteration 93, loss = 0.54117885\n",
            "Iteration 94, loss = 0.53997887\n",
            "Iteration 95, loss = 0.53952913\n",
            "Iteration 96, loss = 0.53913760\n",
            "Iteration 97, loss = 0.53882143\n",
            "Iteration 98, loss = 0.53820345\n",
            "Iteration 99, loss = 0.53664316\n",
            "Iteration 100, loss = 0.53674615\n",
            "Iteration 101, loss = 0.53739827\n",
            "Iteration 102, loss = 0.53625347\n",
            "Iteration 103, loss = 0.53548825\n",
            "Iteration 104, loss = 0.53572298\n",
            "Iteration 105, loss = 0.53482712\n",
            "Iteration 106, loss = 0.53567977\n",
            "Iteration 107, loss = 0.53520679\n",
            "Iteration 108, loss = 0.53449995\n",
            "Iteration 109, loss = 0.53367971\n",
            "Iteration 110, loss = 0.53346308\n",
            "Iteration 111, loss = 0.53320456\n",
            "Iteration 112, loss = 0.53319373\n",
            "Iteration 113, loss = 0.53264705\n",
            "Iteration 114, loss = 0.53163353\n",
            "Iteration 115, loss = 0.53197656\n",
            "Iteration 116, loss = 0.53202012\n",
            "Iteration 117, loss = 0.53141528\n",
            "Iteration 118, loss = 0.53068578\n",
            "Iteration 119, loss = 0.53084543\n",
            "Iteration 120, loss = 0.53070213\n",
            "Iteration 121, loss = 0.53046144\n",
            "Iteration 122, loss = 0.52958076\n",
            "Iteration 123, loss = 0.53008690\n",
            "Iteration 124, loss = 0.53099498\n",
            "Iteration 125, loss = 0.53014204\n",
            "Iteration 126, loss = 0.53002569\n",
            "Iteration 127, loss = 0.52869455\n",
            "Iteration 128, loss = 0.52836082\n",
            "Iteration 129, loss = 0.52902893\n",
            "Iteration 130, loss = 0.52946258\n",
            "Iteration 131, loss = 0.52877584\n",
            "Iteration 132, loss = 0.52923961\n",
            "Iteration 133, loss = 0.52811577\n",
            "Iteration 134, loss = 0.52867865\n",
            "Iteration 135, loss = 0.52791746\n",
            "Iteration 136, loss = 0.52962765\n",
            "Iteration 137, loss = 0.52848535\n",
            "Iteration 138, loss = 0.52746121\n",
            "Iteration 139, loss = 0.52809348\n",
            "Iteration 140, loss = 0.52737983\n",
            "Iteration 141, loss = 0.52755893\n",
            "Iteration 142, loss = 0.52867697\n",
            "Iteration 143, loss = 0.52755492\n",
            "Iteration 144, loss = 0.52731978\n",
            "Iteration 145, loss = 0.52713043\n",
            "Iteration 146, loss = 0.52765052\n",
            "Iteration 147, loss = 0.52731357\n",
            "Iteration 148, loss = 0.52733674\n",
            "Iteration 149, loss = 0.52682438\n",
            "Iteration 150, loss = 0.52721622\n",
            "Iteration 151, loss = 0.52684261\n",
            "Iteration 152, loss = 0.52739121\n",
            "Iteration 153, loss = 0.52614097\n",
            "Iteration 154, loss = 0.52662317\n",
            "Iteration 155, loss = 0.52605054\n",
            "Iteration 156, loss = 0.52644415\n",
            "Iteration 157, loss = 0.52622370\n",
            "Iteration 158, loss = 0.52567971\n",
            "Iteration 159, loss = 0.52557392\n",
            "Iteration 160, loss = 0.52556994\n",
            "Iteration 161, loss = 0.52628981\n",
            "Iteration 162, loss = 0.52594464\n",
            "Iteration 163, loss = 0.52561529\n",
            "Iteration 164, loss = 0.52592813\n",
            "Iteration 165, loss = 0.52541093\n",
            "Iteration 166, loss = 0.52512518\n",
            "Iteration 167, loss = 0.52613701\n",
            "Iteration 168, loss = 0.52545684\n",
            "Iteration 169, loss = 0.52532546\n",
            "Iteration 170, loss = 0.52604375\n",
            "Iteration 171, loss = 0.52600150\n",
            "Iteration 172, loss = 0.52542413\n",
            "Iteration 173, loss = 0.52536492\n",
            "Iteration 174, loss = 0.52551029\n",
            "Iteration 175, loss = 0.52567802\n",
            "Iteration 176, loss = 0.52548181\n",
            "Iteration 177, loss = 0.52513073\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56362815\n",
            "Iteration 2, loss = 0.56306447\n",
            "Iteration 3, loss = 0.56316593\n",
            "Iteration 4, loss = 0.56282605\n",
            "Iteration 5, loss = 0.56268349\n",
            "Iteration 6, loss = 0.56218479\n",
            "Iteration 7, loss = 0.56187662\n",
            "Iteration 8, loss = 0.56331724\n",
            "Iteration 9, loss = 0.56264224\n",
            "Iteration 10, loss = 0.56246696\n",
            "Iteration 11, loss = 0.56265172\n",
            "Iteration 12, loss = 0.56300318\n",
            "Iteration 13, loss = 0.56242841\n",
            "Iteration 14, loss = 0.56239389\n",
            "Iteration 15, loss = 0.56232716\n",
            "Iteration 16, loss = 0.56242369\n",
            "Iteration 17, loss = 0.56267405\n",
            "Iteration 18, loss = 0.56245241\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57484876\n",
            "Iteration 2, loss = 0.56471212\n",
            "Iteration 3, loss = 0.56446207\n",
            "Iteration 4, loss = 0.56425220\n",
            "Iteration 5, loss = 0.56490202\n",
            "Iteration 6, loss = 0.56498223\n",
            "Iteration 7, loss = 0.56360001\n",
            "Iteration 8, loss = 0.56404330\n",
            "Iteration 9, loss = 0.56392085\n",
            "Iteration 10, loss = 0.56410033\n",
            "Iteration 11, loss = 0.56459285\n",
            "Iteration 12, loss = 0.56450172\n",
            "Iteration 13, loss = 0.56436076\n",
            "Iteration 14, loss = 0.56417144\n",
            "Iteration 15, loss = 0.56388511\n",
            "Iteration 16, loss = 0.56460349\n",
            "Iteration 17, loss = 0.56410624\n",
            "Iteration 18, loss = 0.56424181\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57377823\n",
            "Iteration 2, loss = 0.56061574\n",
            "Iteration 3, loss = 0.55957949\n",
            "Iteration 4, loss = 0.56038327\n",
            "Iteration 5, loss = 0.56032590\n",
            "Iteration 6, loss = 0.55897469\n",
            "Iteration 7, loss = 0.56047311\n",
            "Iteration 8, loss = 0.55873391\n",
            "Iteration 9, loss = 0.56029311\n",
            "Iteration 10, loss = 0.56016114\n",
            "Iteration 11, loss = 0.56006536\n",
            "Iteration 12, loss = 0.56044392\n",
            "Iteration 13, loss = 0.56002481\n",
            "Iteration 14, loss = 0.55959376\n",
            "Iteration 15, loss = 0.56057203\n",
            "Iteration 16, loss = 0.55972741\n",
            "Iteration 17, loss = 0.56064779\n",
            "Iteration 18, loss = 0.55946122\n",
            "Iteration 19, loss = 0.55949856\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57219274\n",
            "Iteration 2, loss = 0.56055247\n",
            "Iteration 3, loss = 0.56156812\n",
            "Iteration 4, loss = 0.56142466\n",
            "Iteration 5, loss = 0.56228795\n",
            "Iteration 6, loss = 0.56112625\n",
            "Iteration 7, loss = 0.56104785\n",
            "Iteration 8, loss = 0.56095923\n",
            "Iteration 9, loss = 0.56139899\n",
            "Iteration 10, loss = 0.56020693\n",
            "Iteration 11, loss = 0.56104231\n",
            "Iteration 12, loss = 0.56086651\n",
            "Iteration 13, loss = 0.56122643\n",
            "Iteration 14, loss = 0.56062180\n",
            "Iteration 15, loss = 0.56101980\n",
            "Iteration 16, loss = 0.56148478\n",
            "Iteration 17, loss = 0.55981487\n",
            "Iteration 18, loss = 0.56111486\n",
            "Iteration 19, loss = 0.56022859\n",
            "Iteration 20, loss = 0.56048410\n",
            "Iteration 21, loss = 0.55958877\n",
            "Iteration 22, loss = 0.55999819\n",
            "Iteration 23, loss = 0.56029934\n",
            "Iteration 24, loss = 0.55943135\n",
            "Iteration 25, loss = 0.56011348\n",
            "Iteration 26, loss = 0.56046999\n",
            "Iteration 27, loss = 0.55974027\n",
            "Iteration 28, loss = 0.56017399\n",
            "Iteration 29, loss = 0.55975531\n",
            "Iteration 30, loss = 0.55873767\n",
            "Iteration 31, loss = 0.55964548\n",
            "Iteration 32, loss = 0.55945995\n",
            "Iteration 33, loss = 0.55927825\n",
            "Iteration 34, loss = 0.55911783\n",
            "Iteration 35, loss = 0.55919047\n",
            "Iteration 36, loss = 0.55886578\n",
            "Iteration 37, loss = 0.55933119\n",
            "Iteration 38, loss = 0.55840946\n",
            "Iteration 39, loss = 0.55842713\n",
            "Iteration 40, loss = 0.55799254\n",
            "Iteration 41, loss = 0.55798032\n",
            "Iteration 42, loss = 0.55760115\n",
            "Iteration 43, loss = 0.55742077\n",
            "Iteration 44, loss = 0.55756044\n",
            "Iteration 45, loss = 0.55780415\n",
            "Iteration 46, loss = 0.55662558\n",
            "Iteration 47, loss = 0.55623409\n",
            "Iteration 48, loss = 0.55565870\n",
            "Iteration 49, loss = 0.55613643\n",
            "Iteration 50, loss = 0.55600596\n",
            "Iteration 51, loss = 0.55521546\n",
            "Iteration 52, loss = 0.55575730\n",
            "Iteration 53, loss = 0.55493322\n",
            "Iteration 54, loss = 0.55472624\n",
            "Iteration 55, loss = 0.55435700\n",
            "Iteration 56, loss = 0.55443961\n",
            "Iteration 57, loss = 0.55356073\n",
            "Iteration 58, loss = 0.55333845\n",
            "Iteration 59, loss = 0.55291120\n",
            "Iteration 60, loss = 0.55031281\n",
            "Iteration 61, loss = 0.55220174\n",
            "Iteration 62, loss = 0.55142897\n",
            "Iteration 63, loss = 0.55097946\n",
            "Iteration 64, loss = 0.55038204\n",
            "Iteration 65, loss = 0.54926435\n",
            "Iteration 66, loss = 0.54872742\n",
            "Iteration 67, loss = 0.54838286\n",
            "Iteration 68, loss = 0.54735657\n",
            "Iteration 69, loss = 0.54688149\n",
            "Iteration 70, loss = 0.54582250\n",
            "Iteration 71, loss = 0.54595779\n",
            "Iteration 72, loss = 0.54465435\n",
            "Iteration 73, loss = 0.54367837\n",
            "Iteration 74, loss = 0.54230799\n",
            "Iteration 75, loss = 0.54248899\n",
            "Iteration 76, loss = 0.54202248\n",
            "Iteration 77, loss = 0.54114885\n",
            "Iteration 78, loss = 0.54005817\n",
            "Iteration 79, loss = 0.53877027\n",
            "Iteration 80, loss = 0.53899585\n",
            "Iteration 81, loss = 0.53756156\n",
            "Iteration 82, loss = 0.53742733\n",
            "Iteration 83, loss = 0.53609854\n",
            "Iteration 84, loss = 0.53425053\n",
            "Iteration 85, loss = 0.53403650\n",
            "Iteration 86, loss = 0.53469780\n",
            "Iteration 87, loss = 0.53242876\n",
            "Iteration 88, loss = 0.53137811\n",
            "Iteration 89, loss = 0.53252844\n",
            "Iteration 90, loss = 0.53042096\n",
            "Iteration 91, loss = 0.52949864\n",
            "Iteration 92, loss = 0.52908662\n",
            "Iteration 93, loss = 0.52915181\n",
            "Iteration 94, loss = 0.52834695\n",
            "Iteration 95, loss = 0.52768967\n",
            "Iteration 96, loss = 0.52731298\n",
            "Iteration 97, loss = 0.52700102\n",
            "Iteration 98, loss = 0.52587404\n",
            "Iteration 99, loss = 0.52536052\n",
            "Iteration 100, loss = 0.52566697\n",
            "Iteration 101, loss = 0.52514595\n",
            "Iteration 102, loss = 0.52482533\n",
            "Iteration 103, loss = 0.52441097\n",
            "Iteration 104, loss = 0.52469670\n",
            "Iteration 105, loss = 0.52359785\n",
            "Iteration 106, loss = 0.52188649\n",
            "Iteration 107, loss = 0.52371261\n",
            "Iteration 108, loss = 0.52289029\n",
            "Iteration 109, loss = 0.52266531\n",
            "Iteration 110, loss = 0.52248398\n",
            "Iteration 111, loss = 0.52196997\n",
            "Iteration 112, loss = 0.52176556\n",
            "Iteration 113, loss = 0.52175018\n",
            "Iteration 114, loss = 0.52178926\n",
            "Iteration 115, loss = 0.52198755\n",
            "Iteration 116, loss = 0.52102580\n",
            "Iteration 117, loss = 0.52008972\n",
            "Iteration 118, loss = 0.52028154\n",
            "Iteration 119, loss = 0.52053395\n",
            "Iteration 120, loss = 0.52014252\n",
            "Iteration 121, loss = 0.51980346\n",
            "Iteration 122, loss = 0.52005520\n",
            "Iteration 123, loss = 0.51948586\n",
            "Iteration 124, loss = 0.51924898\n",
            "Iteration 125, loss = 0.51942808\n",
            "Iteration 126, loss = 0.51982671\n",
            "Iteration 127, loss = 0.51910779\n",
            "Iteration 128, loss = 0.51800781\n",
            "Iteration 129, loss = 0.51897974\n",
            "Iteration 130, loss = 0.51828889\n",
            "Iteration 131, loss = 0.51835830\n",
            "Iteration 132, loss = 0.51873324\n",
            "Iteration 133, loss = 0.51774873\n",
            "Iteration 134, loss = 0.51799393\n",
            "Iteration 135, loss = 0.51834906\n",
            "Iteration 136, loss = 0.51795447\n",
            "Iteration 137, loss = 0.51749404\n",
            "Iteration 138, loss = 0.51716333\n",
            "Iteration 139, loss = 0.51745554\n",
            "Iteration 140, loss = 0.51744930\n",
            "Iteration 141, loss = 0.51763005\n",
            "Iteration 142, loss = 0.51702184\n",
            "Iteration 143, loss = 0.51693125\n",
            "Iteration 144, loss = 0.51660419\n",
            "Iteration 145, loss = 0.51626820\n",
            "Iteration 146, loss = 0.51600832\n",
            "Iteration 147, loss = 0.51690912\n",
            "Iteration 148, loss = 0.51662512\n",
            "Iteration 149, loss = 0.51678546\n",
            "Iteration 150, loss = 0.51648880\n",
            "Iteration 151, loss = 0.51587044\n",
            "Iteration 152, loss = 0.51664416\n",
            "Iteration 153, loss = 0.51570694\n",
            "Iteration 154, loss = 0.51586303\n",
            "Iteration 155, loss = 0.51555317\n",
            "Iteration 156, loss = 0.51529198\n",
            "Iteration 157, loss = 0.51630857\n",
            "Iteration 158, loss = 0.51534983\n",
            "Iteration 159, loss = 0.51655375\n",
            "Iteration 160, loss = 0.51583921\n",
            "Iteration 161, loss = 0.51612088\n",
            "Iteration 162, loss = 0.51517232\n",
            "Iteration 163, loss = 0.51528461\n",
            "Iteration 164, loss = 0.51583795\n",
            "Iteration 165, loss = 0.51521421\n",
            "Iteration 166, loss = 0.51616499\n",
            "Iteration 167, loss = 0.51507790\n",
            "Iteration 168, loss = 0.51509078\n",
            "Iteration 169, loss = 0.51500799\n",
            "Iteration 170, loss = 0.51473294\n",
            "Iteration 171, loss = 0.51520947\n",
            "Iteration 172, loss = 0.51480280\n",
            "Iteration 173, loss = 0.51594558\n",
            "Iteration 174, loss = 0.51513907\n",
            "Iteration 175, loss = 0.51588917\n",
            "Iteration 176, loss = 0.51488326\n",
            "Iteration 177, loss = 0.51564673\n",
            "Iteration 178, loss = 0.51518537\n",
            "Iteration 179, loss = 0.51478810\n",
            "Iteration 180, loss = 0.51459273\n",
            "Iteration 181, loss = 0.51507272\n",
            "Iteration 182, loss = 0.51516790\n",
            "Iteration 183, loss = 0.51451764\n",
            "Iteration 184, loss = 0.51469919\n",
            "Iteration 185, loss = 0.51420499\n",
            "Iteration 186, loss = 0.51475017\n",
            "Iteration 187, loss = 0.51451923\n",
            "Iteration 188, loss = 0.51401258\n",
            "Iteration 189, loss = 0.51509280\n",
            "Iteration 190, loss = 0.51483226\n",
            "Iteration 191, loss = 0.51462161\n",
            "Iteration 192, loss = 0.51447738\n",
            "Iteration 193, loss = 0.51397925\n",
            "Iteration 194, loss = 0.51459020\n",
            "Iteration 195, loss = 0.51479890\n",
            "Iteration 196, loss = 0.51436868\n",
            "Iteration 197, loss = 0.51546580\n",
            "Iteration 198, loss = 0.51451611\n",
            "Iteration 199, loss = 0.51425019\n",
            "Iteration 200, loss = 0.51411559\n",
            "Iteration 201, loss = 0.51466428\n",
            "Iteration 202, loss = 0.51580466\n",
            "Iteration 203, loss = 0.51484066\n",
            "Iteration 204, loss = 0.51356390\n",
            "Iteration 205, loss = 0.51509709\n",
            "Iteration 206, loss = 0.51394432\n",
            "Iteration 207, loss = 0.51489099\n",
            "Iteration 208, loss = 0.51419808\n",
            "Iteration 209, loss = 0.51446315\n",
            "Iteration 210, loss = 0.51462702\n",
            "Iteration 211, loss = 0.51489895\n",
            "Iteration 212, loss = 0.51423109\n",
            "Iteration 213, loss = 0.51380702\n",
            "Iteration 214, loss = 0.51461315\n",
            "Iteration 215, loss = 0.51433122\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56566248\n",
            "Iteration 2, loss = 0.56237953\n",
            "Iteration 3, loss = 0.56203573\n",
            "Iteration 4, loss = 0.56223536\n",
            "Iteration 5, loss = 0.56205553\n",
            "Iteration 6, loss = 0.56238016\n",
            "Iteration 7, loss = 0.56185283\n",
            "Iteration 8, loss = 0.56164995\n",
            "Iteration 9, loss = 0.56153548\n",
            "Iteration 10, loss = 0.56170602\n",
            "Iteration 11, loss = 0.56129284\n",
            "Iteration 12, loss = 0.56155409\n",
            "Iteration 13, loss = 0.56198188\n",
            "Iteration 14, loss = 0.56086853\n",
            "Iteration 15, loss = 0.56127047\n",
            "Iteration 16, loss = 0.56191442\n",
            "Iteration 17, loss = 0.56154970\n",
            "Iteration 18, loss = 0.56093676\n",
            "Iteration 19, loss = 0.55920337\n",
            "Iteration 20, loss = 0.56098290\n",
            "Iteration 21, loss = 0.56157427\n",
            "Iteration 22, loss = 0.56090420\n",
            "Iteration 23, loss = 0.56090956\n",
            "Iteration 24, loss = 0.56155197\n",
            "Iteration 25, loss = 0.56111426\n",
            "Iteration 26, loss = 0.56098474\n",
            "Iteration 27, loss = 0.56100485\n",
            "Iteration 28, loss = 0.56109878\n",
            "Iteration 29, loss = 0.56095614\n",
            "Iteration 30, loss = 0.56099035\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56650488\n",
            "Iteration 2, loss = 0.56516583\n",
            "Iteration 3, loss = 0.56339071\n",
            "Iteration 4, loss = 0.56441933\n",
            "Iteration 5, loss = 0.56428550\n",
            "Iteration 6, loss = 0.56421775\n",
            "Iteration 7, loss = 0.56445792\n",
            "Iteration 8, loss = 0.56421253\n",
            "Iteration 9, loss = 0.56409434\n",
            "Iteration 10, loss = 0.56305033\n",
            "Iteration 11, loss = 0.56427425\n",
            "Iteration 12, loss = 0.56414267\n",
            "Iteration 13, loss = 0.56386141\n",
            "Iteration 14, loss = 0.56408792\n",
            "Iteration 15, loss = 0.56395703\n",
            "Iteration 16, loss = 0.56314877\n",
            "Iteration 17, loss = 0.56317245\n",
            "Iteration 18, loss = 0.56333530\n",
            "Iteration 19, loss = 0.56353672\n",
            "Iteration 20, loss = 0.56466408\n",
            "Iteration 21, loss = 0.56348221\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56407806\n",
            "Iteration 2, loss = 0.56002777\n",
            "Iteration 3, loss = 0.56077971\n",
            "Iteration 4, loss = 0.56003720\n",
            "Iteration 5, loss = 0.55998560\n",
            "Iteration 6, loss = 0.56050358\n",
            "Iteration 7, loss = 0.56101486\n",
            "Iteration 8, loss = 0.55990061\n",
            "Iteration 9, loss = 0.56064330\n",
            "Iteration 10, loss = 0.56087060\n",
            "Iteration 11, loss = 0.56025530\n",
            "Iteration 12, loss = 0.56002687\n",
            "Iteration 13, loss = 0.55993888\n",
            "Iteration 14, loss = 0.55967484\n",
            "Iteration 15, loss = 0.55969217\n",
            "Iteration 16, loss = 0.56054247\n",
            "Iteration 17, loss = 0.56005459\n",
            "Iteration 18, loss = 0.55953041\n",
            "Iteration 19, loss = 0.56014157\n",
            "Iteration 20, loss = 0.55941049\n",
            "Iteration 21, loss = 0.56019771\n",
            "Iteration 22, loss = 0.55864489\n",
            "Iteration 23, loss = 0.55923184\n",
            "Iteration 24, loss = 0.55902259\n",
            "Iteration 25, loss = 0.55970952\n",
            "Iteration 26, loss = 0.55872450\n",
            "Iteration 27, loss = 0.55922364\n",
            "Iteration 28, loss = 0.55964358\n",
            "Iteration 29, loss = 0.55999055\n",
            "Iteration 30, loss = 0.55903529\n",
            "Iteration 31, loss = 0.55921432\n",
            "Iteration 32, loss = 0.55891493\n",
            "Iteration 33, loss = 0.55931553\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56631666\n",
            "Iteration 2, loss = 0.56212700\n",
            "Iteration 3, loss = 0.56151755\n",
            "Iteration 4, loss = 0.56196643\n",
            "Iteration 5, loss = 0.56226086\n",
            "Iteration 6, loss = 0.56193218\n",
            "Iteration 7, loss = 0.56148026\n",
            "Iteration 8, loss = 0.56110199\n",
            "Iteration 9, loss = 0.56281366\n",
            "Iteration 10, loss = 0.56136372\n",
            "Iteration 11, loss = 0.56208369\n",
            "Iteration 12, loss = 0.56177318\n",
            "Iteration 13, loss = 0.56133666\n",
            "Iteration 14, loss = 0.56187708\n",
            "Iteration 15, loss = 0.56155092\n",
            "Iteration 16, loss = 0.56123393\n",
            "Iteration 17, loss = 0.56119769\n",
            "Iteration 18, loss = 0.56168769\n",
            "Iteration 19, loss = 0.56101473\n",
            "Iteration 20, loss = 0.56040657\n",
            "Iteration 21, loss = 0.56198000\n",
            "Iteration 22, loss = 0.56065246\n",
            "Iteration 23, loss = 0.56117884\n",
            "Iteration 24, loss = 0.55987856\n",
            "Iteration 25, loss = 0.56066407\n",
            "Iteration 26, loss = 0.56079065\n",
            "Iteration 27, loss = 0.56086922\n",
            "Iteration 28, loss = 0.56070870\n",
            "Iteration 29, loss = 0.56064811\n",
            "Iteration 30, loss = 0.56079405\n",
            "Iteration 31, loss = 0.56054126\n",
            "Iteration 32, loss = 0.56039408\n",
            "Iteration 33, loss = 0.56031167\n",
            "Iteration 34, loss = 0.56027624\n",
            "Iteration 35, loss = 0.55991687\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56593558\n",
            "Iteration 2, loss = 0.56486859\n",
            "Iteration 3, loss = 0.56403999\n",
            "Iteration 4, loss = 0.56405664\n",
            "Iteration 5, loss = 0.56454756\n",
            "Iteration 6, loss = 0.56376268\n",
            "Iteration 7, loss = 0.56327099\n",
            "Iteration 8, loss = 0.56386736\n",
            "Iteration 9, loss = 0.56309749\n",
            "Iteration 10, loss = 0.56298977\n",
            "Iteration 11, loss = 0.56301657\n",
            "Iteration 12, loss = 0.56305669\n",
            "Iteration 13, loss = 0.56314553\n",
            "Iteration 14, loss = 0.56263729\n",
            "Iteration 15, loss = 0.56286197\n",
            "Iteration 16, loss = 0.56322370\n",
            "Iteration 17, loss = 0.56298209\n",
            "Iteration 18, loss = 0.56315968\n",
            "Iteration 19, loss = 0.56271300\n",
            "Iteration 20, loss = 0.56372967\n",
            "Iteration 21, loss = 0.56284087\n",
            "Iteration 22, loss = 0.56304624\n",
            "Iteration 23, loss = 0.56203319\n",
            "Iteration 24, loss = 0.56245213\n",
            "Iteration 25, loss = 0.56185340\n",
            "Iteration 26, loss = 0.56263317\n",
            "Iteration 27, loss = 0.56266864\n",
            "Iteration 28, loss = 0.56227064\n",
            "Iteration 29, loss = 0.56208399\n",
            "Iteration 30, loss = 0.56205300\n",
            "Iteration 31, loss = 0.56229382\n",
            "Iteration 32, loss = 0.56167928\n",
            "Iteration 33, loss = 0.56220215\n",
            "Iteration 34, loss = 0.56204565\n",
            "Iteration 35, loss = 0.56206681\n",
            "Iteration 36, loss = 0.56135514\n",
            "Iteration 37, loss = 0.56099458\n",
            "Iteration 38, loss = 0.56042145\n",
            "Iteration 39, loss = 0.56228892\n",
            "Iteration 40, loss = 0.56188955\n",
            "Iteration 41, loss = 0.56112400\n",
            "Iteration 42, loss = 0.56107533\n",
            "Iteration 43, loss = 0.56047487\n",
            "Iteration 44, loss = 0.56095210\n",
            "Iteration 45, loss = 0.56065358\n",
            "Iteration 46, loss = 0.55983691\n",
            "Iteration 47, loss = 0.56018619\n",
            "Iteration 48, loss = 0.56004090\n",
            "Iteration 49, loss = 0.55941039\n",
            "Iteration 50, loss = 0.55939152\n",
            "Iteration 51, loss = 0.55975967\n",
            "Iteration 52, loss = 0.55942103\n",
            "Iteration 53, loss = 0.55901393\n",
            "Iteration 54, loss = 0.55781836\n",
            "Iteration 55, loss = 0.55881125\n",
            "Iteration 56, loss = 0.55766821\n",
            "Iteration 57, loss = 0.55778827\n",
            "Iteration 58, loss = 0.55718318\n",
            "Iteration 59, loss = 0.55808214\n",
            "Iteration 60, loss = 0.55647493\n",
            "Iteration 61, loss = 0.55674137\n",
            "Iteration 62, loss = 0.55635151\n",
            "Iteration 63, loss = 0.55594708\n",
            "Iteration 64, loss = 0.55464864\n",
            "Iteration 65, loss = 0.55527750\n",
            "Iteration 66, loss = 0.55527811\n",
            "Iteration 67, loss = 0.55547513\n",
            "Iteration 68, loss = 0.55387176\n",
            "Iteration 69, loss = 0.55405703\n",
            "Iteration 70, loss = 0.55180927\n",
            "Iteration 71, loss = 0.55278123\n",
            "Iteration 72, loss = 0.55175529\n",
            "Iteration 73, loss = 0.55160446\n",
            "Iteration 74, loss = 0.55164027\n",
            "Iteration 75, loss = 0.55035308\n",
            "Iteration 76, loss = 0.54918486\n",
            "Iteration 77, loss = 0.54969447\n",
            "Iteration 78, loss = 0.54947856\n",
            "Iteration 79, loss = 0.54790897\n",
            "Iteration 80, loss = 0.54683302\n",
            "Iteration 81, loss = 0.54667208\n",
            "Iteration 82, loss = 0.54592699\n",
            "Iteration 83, loss = 0.54552741\n",
            "Iteration 84, loss = 0.54474964\n",
            "Iteration 85, loss = 0.54378894\n",
            "Iteration 86, loss = 0.54352201\n",
            "Iteration 87, loss = 0.54245616\n",
            "Iteration 88, loss = 0.54178387\n",
            "Iteration 89, loss = 0.54133312\n",
            "Iteration 90, loss = 0.54205853\n",
            "Iteration 91, loss = 0.53942431\n",
            "Iteration 92, loss = 0.54016321\n",
            "Iteration 93, loss = 0.53805506\n",
            "Iteration 94, loss = 0.53773708\n",
            "Iteration 95, loss = 0.53773347\n",
            "Iteration 96, loss = 0.53698284\n",
            "Iteration 97, loss = 0.53642592\n",
            "Iteration 98, loss = 0.53529574\n",
            "Iteration 99, loss = 0.53509488\n",
            "Iteration 100, loss = 0.53429807\n",
            "Iteration 101, loss = 0.53439233\n",
            "Iteration 102, loss = 0.53357573\n",
            "Iteration 103, loss = 0.53335727\n",
            "Iteration 104, loss = 0.53292743\n",
            "Iteration 105, loss = 0.53129908\n",
            "Iteration 106, loss = 0.53269733\n",
            "Iteration 107, loss = 0.53238005\n",
            "Iteration 108, loss = 0.53113985\n",
            "Iteration 109, loss = 0.53112419\n",
            "Iteration 110, loss = 0.53081251\n",
            "Iteration 111, loss = 0.52913279\n",
            "Iteration 112, loss = 0.53003488\n",
            "Iteration 113, loss = 0.52974239\n",
            "Iteration 114, loss = 0.52973264\n",
            "Iteration 115, loss = 0.52919081\n",
            "Iteration 116, loss = 0.52950188\n",
            "Iteration 117, loss = 0.52914569\n",
            "Iteration 118, loss = 0.52881210\n",
            "Iteration 119, loss = 0.52933087\n",
            "Iteration 120, loss = 0.52883387\n",
            "Iteration 121, loss = 0.52946028\n",
            "Iteration 122, loss = 0.52745084\n",
            "Iteration 123, loss = 0.52749851\n",
            "Iteration 124, loss = 0.52800698\n",
            "Iteration 125, loss = 0.52767732\n",
            "Iteration 126, loss = 0.52686003\n",
            "Iteration 127, loss = 0.52706356\n",
            "Iteration 128, loss = 0.52669619\n",
            "Iteration 129, loss = 0.52680965\n",
            "Iteration 130, loss = 0.52717649\n",
            "Iteration 131, loss = 0.52627001\n",
            "Iteration 132, loss = 0.52488259\n",
            "Iteration 133, loss = 0.52651214\n",
            "Iteration 134, loss = 0.52643506\n",
            "Iteration 135, loss = 0.52609918\n",
            "Iteration 136, loss = 0.52589831\n",
            "Iteration 137, loss = 0.52586035\n",
            "Iteration 138, loss = 0.52583638\n",
            "Iteration 139, loss = 0.52630256\n",
            "Iteration 140, loss = 0.52545624\n",
            "Iteration 141, loss = 0.52535909\n",
            "Iteration 142, loss = 0.52448510\n",
            "Iteration 143, loss = 0.52536839\n",
            "Iteration 144, loss = 0.52424154\n",
            "Iteration 145, loss = 0.52475714\n",
            "Iteration 146, loss = 0.52501009\n",
            "Iteration 147, loss = 0.52449888\n",
            "Iteration 148, loss = 0.52427616\n",
            "Iteration 149, loss = 0.52444602\n",
            "Iteration 150, loss = 0.52464587\n",
            "Iteration 151, loss = 0.52480598\n",
            "Iteration 152, loss = 0.52384652\n",
            "Iteration 153, loss = 0.52382541\n",
            "Iteration 154, loss = 0.52369156\n",
            "Iteration 155, loss = 0.52389010\n",
            "Iteration 156, loss = 0.52386077\n",
            "Iteration 157, loss = 0.52293223\n",
            "Iteration 158, loss = 0.52365921\n",
            "Iteration 159, loss = 0.52335452\n",
            "Iteration 160, loss = 0.52408726\n",
            "Iteration 161, loss = 0.52438206\n",
            "Iteration 162, loss = 0.52316763\n",
            "Iteration 163, loss = 0.52390108\n",
            "Iteration 164, loss = 0.52325578\n",
            "Iteration 165, loss = 0.52263953\n",
            "Iteration 166, loss = 0.52270545\n",
            "Iteration 167, loss = 0.52307579\n",
            "Iteration 168, loss = 0.52292623\n",
            "Iteration 169, loss = 0.52371420\n",
            "Iteration 170, loss = 0.52343963\n",
            "Iteration 171, loss = 0.52300183\n",
            "Iteration 172, loss = 0.52254840\n",
            "Iteration 173, loss = 0.52194052\n",
            "Iteration 174, loss = 0.52313205\n",
            "Iteration 175, loss = 0.52359691\n",
            "Iteration 176, loss = 0.52442018\n",
            "Iteration 177, loss = 0.52267473\n",
            "Iteration 178, loss = 0.52285098\n",
            "Iteration 179, loss = 0.52277298\n",
            "Iteration 180, loss = 0.52302706\n",
            "Iteration 181, loss = 0.52297696\n",
            "Iteration 182, loss = 0.52292047\n",
            "Iteration 183, loss = 0.52202844\n",
            "Iteration 184, loss = 0.52374856\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "3\n",
            "Iteration 1, loss = 0.56848816\n",
            "Iteration 2, loss = 0.56085980\n",
            "Iteration 3, loss = 0.56144256\n",
            "Iteration 4, loss = 0.56141781\n",
            "Iteration 5, loss = 0.56207822\n",
            "Iteration 6, loss = 0.56069200\n",
            "Iteration 7, loss = 0.56108515\n",
            "Iteration 8, loss = 0.56146427\n",
            "Iteration 9, loss = 0.56171386\n",
            "Iteration 10, loss = 0.56124016\n",
            "Iteration 11, loss = 0.56103689\n",
            "Iteration 12, loss = 0.56096320\n",
            "Iteration 13, loss = 0.56122875\n",
            "Iteration 14, loss = 0.56125915\n",
            "Iteration 15, loss = 0.56053603\n",
            "Iteration 16, loss = 0.56006028\n",
            "Iteration 17, loss = 0.56003052\n",
            "Iteration 18, loss = 0.56031885\n",
            "Iteration 19, loss = 0.56130810\n",
            "Iteration 20, loss = 0.55974893\n",
            "Iteration 21, loss = 0.56125198\n",
            "Iteration 22, loss = 0.56077374\n",
            "Iteration 23, loss = 0.56070340\n",
            "Iteration 24, loss = 0.56050996\n",
            "Iteration 25, loss = 0.56035400\n",
            "Iteration 26, loss = 0.56033779\n",
            "Iteration 27, loss = 0.56130600\n",
            "Iteration 28, loss = 0.56059470\n",
            "Iteration 29, loss = 0.55969075\n",
            "Iteration 30, loss = 0.55959828\n",
            "Iteration 31, loss = 0.56030585\n",
            "Iteration 32, loss = 0.55996924\n",
            "Iteration 33, loss = 0.56012179\n",
            "Iteration 34, loss = 0.56117291\n",
            "Iteration 35, loss = 0.55978174\n",
            "Iteration 36, loss = 0.55936208\n",
            "Iteration 37, loss = 0.55917630\n",
            "Iteration 38, loss = 0.56015143\n",
            "Iteration 39, loss = 0.55873827\n",
            "Iteration 40, loss = 0.56045697\n",
            "Iteration 41, loss = 0.55950740\n",
            "Iteration 42, loss = 0.55939600\n",
            "Iteration 43, loss = 0.55894917\n",
            "Iteration 44, loss = 0.55919488\n",
            "Iteration 45, loss = 0.55958700\n",
            "Iteration 46, loss = 0.55853681\n",
            "Iteration 47, loss = 0.55906892\n",
            "Iteration 48, loss = 0.55879418\n",
            "Iteration 49, loss = 0.55924779\n",
            "Iteration 50, loss = 0.55869997\n",
            "Iteration 51, loss = 0.55839360\n",
            "Iteration 52, loss = 0.55862924\n",
            "Iteration 53, loss = 0.55801828\n",
            "Iteration 54, loss = 0.55831996\n",
            "Iteration 55, loss = 0.55806800\n",
            "Iteration 56, loss = 0.55787229\n",
            "Iteration 57, loss = 0.55780785\n",
            "Iteration 58, loss = 0.55747818\n",
            "Iteration 59, loss = 0.55778081\n",
            "Iteration 60, loss = 0.55693804\n",
            "Iteration 61, loss = 0.55720592\n",
            "Iteration 62, loss = 0.55705702\n",
            "Iteration 63, loss = 0.55635489\n",
            "Iteration 64, loss = 0.55746938\n",
            "Iteration 65, loss = 0.55661304\n",
            "Iteration 66, loss = 0.55621217\n",
            "Iteration 67, loss = 0.55584565\n",
            "Iteration 68, loss = 0.55575768\n",
            "Iteration 69, loss = 0.55561957\n",
            "Iteration 70, loss = 0.55464093\n",
            "Iteration 71, loss = 0.55444543\n",
            "Iteration 72, loss = 0.55436273\n",
            "Iteration 73, loss = 0.55478170\n",
            "Iteration 74, loss = 0.55342902\n",
            "Iteration 75, loss = 0.55359789\n",
            "Iteration 76, loss = 0.55310038\n",
            "Iteration 77, loss = 0.55250568\n",
            "Iteration 78, loss = 0.55202217\n",
            "Iteration 79, loss = 0.55107020\n",
            "Iteration 80, loss = 0.55138713\n",
            "Iteration 81, loss = 0.55049064\n",
            "Iteration 82, loss = 0.55028754\n",
            "Iteration 83, loss = 0.54983221\n",
            "Iteration 84, loss = 0.54939421\n",
            "Iteration 85, loss = 0.54918371\n",
            "Iteration 86, loss = 0.54793518\n",
            "Iteration 87, loss = 0.54811524\n",
            "Iteration 88, loss = 0.54642074\n",
            "Iteration 89, loss = 0.54677626\n",
            "Iteration 90, loss = 0.54652384\n",
            "Iteration 91, loss = 0.54517232\n",
            "Iteration 92, loss = 0.54416496\n",
            "Iteration 93, loss = 0.54432403\n",
            "Iteration 94, loss = 0.54271324\n",
            "Iteration 95, loss = 0.54192679\n",
            "Iteration 96, loss = 0.54174543\n",
            "Iteration 97, loss = 0.54114005\n",
            "Iteration 98, loss = 0.54169900\n",
            "Iteration 99, loss = 0.53948722\n",
            "Iteration 100, loss = 0.53849611\n",
            "Iteration 101, loss = 0.53893652\n",
            "Iteration 102, loss = 0.53767160\n",
            "Iteration 103, loss = 0.53739178\n",
            "Iteration 104, loss = 0.53594323\n",
            "Iteration 105, loss = 0.53564775\n",
            "Iteration 106, loss = 0.53481565\n",
            "Iteration 107, loss = 0.53424466\n",
            "Iteration 108, loss = 0.53404303\n",
            "Iteration 109, loss = 0.53354581\n",
            "Iteration 110, loss = 0.53321137\n",
            "Iteration 111, loss = 0.53211190\n",
            "Iteration 112, loss = 0.53251910\n",
            "Iteration 113, loss = 0.53145766\n",
            "Iteration 114, loss = 0.53099136\n",
            "Iteration 115, loss = 0.53035928\n",
            "Iteration 116, loss = 0.53035818\n",
            "Iteration 117, loss = 0.52924259\n",
            "Iteration 118, loss = 0.52957591\n",
            "Iteration 119, loss = 0.52933451\n",
            "Iteration 120, loss = 0.52821854\n",
            "Iteration 121, loss = 0.52906534\n",
            "Iteration 122, loss = 0.52743123\n",
            "Iteration 123, loss = 0.52697205\n",
            "Iteration 124, loss = 0.52710478\n",
            "Iteration 125, loss = 0.52691457\n",
            "Iteration 126, loss = 0.52604275\n",
            "Iteration 127, loss = 0.52567261\n",
            "Iteration 128, loss = 0.52568621\n",
            "Iteration 129, loss = 0.52520652\n",
            "Iteration 130, loss = 0.52500507\n",
            "Iteration 131, loss = 0.52571750\n",
            "Iteration 132, loss = 0.52456552\n",
            "Iteration 133, loss = 0.52359070\n",
            "Iteration 134, loss = 0.52488393\n",
            "Iteration 135, loss = 0.52409319\n",
            "Iteration 136, loss = 0.52318570\n",
            "Iteration 137, loss = 0.52440597\n",
            "Iteration 138, loss = 0.52447358\n",
            "Iteration 139, loss = 0.52347145\n",
            "Iteration 140, loss = 0.52335376\n",
            "Iteration 141, loss = 0.52341829\n",
            "Iteration 142, loss = 0.52424944\n",
            "Iteration 143, loss = 0.52242307\n",
            "Iteration 144, loss = 0.52282718\n",
            "Iteration 145, loss = 0.52306028\n",
            "Iteration 146, loss = 0.52242400\n",
            "Iteration 147, loss = 0.52275018\n",
            "Iteration 148, loss = 0.52228945\n",
            "Iteration 149, loss = 0.52178062\n",
            "Iteration 150, loss = 0.52234233\n",
            "Iteration 151, loss = 0.52172029\n",
            "Iteration 152, loss = 0.52068232\n",
            "Iteration 153, loss = 0.52108602\n",
            "Iteration 154, loss = 0.52174726\n",
            "Iteration 155, loss = 0.52097806\n",
            "Iteration 156, loss = 0.52198964\n",
            "Iteration 157, loss = 0.52067695\n",
            "Iteration 158, loss = 0.52095894\n",
            "Iteration 159, loss = 0.52136438\n",
            "Iteration 160, loss = 0.52037927\n",
            "Iteration 161, loss = 0.52100501\n",
            "Iteration 162, loss = 0.52046453\n",
            "Iteration 163, loss = 0.51986388\n",
            "Iteration 164, loss = 0.52099319\n",
            "Iteration 165, loss = 0.52052995\n",
            "Iteration 166, loss = 0.52031902\n",
            "Iteration 167, loss = 0.51997655\n",
            "Iteration 168, loss = 0.52025743\n",
            "Iteration 169, loss = 0.51973167\n",
            "Iteration 170, loss = 0.52019550\n",
            "Iteration 171, loss = 0.52014483\n",
            "Iteration 172, loss = 0.51952741\n",
            "Iteration 173, loss = 0.51928999\n",
            "Iteration 174, loss = 0.51933947\n",
            "Iteration 175, loss = 0.51913476\n",
            "Iteration 176, loss = 0.51978532\n",
            "Iteration 177, loss = 0.51997672\n",
            "Iteration 178, loss = 0.51935087\n",
            "Iteration 179, loss = 0.51878038\n",
            "Iteration 180, loss = 0.51846780\n",
            "Iteration 181, loss = 0.51896463\n",
            "Iteration 182, loss = 0.51891822\n",
            "Iteration 183, loss = 0.51977764\n",
            "Iteration 184, loss = 0.51926036\n",
            "Iteration 185, loss = 0.51882843\n",
            "Iteration 186, loss = 0.51938020\n",
            "Iteration 187, loss = 0.51819034\n",
            "Iteration 188, loss = 0.51908034\n",
            "Iteration 189, loss = 0.51933959\n",
            "Iteration 190, loss = 0.51966974\n",
            "Iteration 191, loss = 0.51936141\n",
            "Iteration 192, loss = 0.51874330\n",
            "Iteration 193, loss = 0.51880251\n",
            "Iteration 194, loss = 0.51971750\n",
            "Iteration 195, loss = 0.51812560\n",
            "Iteration 196, loss = 0.51894414\n",
            "Iteration 197, loss = 0.51920007\n",
            "Iteration 198, loss = 0.51835904\n",
            "Iteration 199, loss = 0.51863940\n",
            "Iteration 200, loss = 0.51770934\n",
            "Iteration 201, loss = 0.51823114\n",
            "Iteration 202, loss = 0.51903407\n",
            "Iteration 203, loss = 0.51812262\n",
            "Iteration 204, loss = 0.51853982\n",
            "Iteration 205, loss = 0.51933205\n",
            "Iteration 206, loss = 0.51879025\n",
            "Iteration 207, loss = 0.51857655\n",
            "Iteration 208, loss = 0.51822759\n",
            "Iteration 209, loss = 0.51775677\n",
            "Iteration 210, loss = 0.51771458\n",
            "Iteration 211, loss = 0.51667148\n",
            "Iteration 212, loss = 0.51911091\n",
            "Iteration 213, loss = 0.51859565\n",
            "Iteration 214, loss = 0.51801002\n",
            "Iteration 215, loss = 0.51840445\n",
            "Iteration 216, loss = 0.51754331\n",
            "Iteration 217, loss = 0.51768789\n",
            "Iteration 218, loss = 0.51855954\n",
            "Iteration 219, loss = 0.51785746\n",
            "Iteration 220, loss = 0.51859067\n",
            "Iteration 221, loss = 0.51958298\n",
            "Iteration 222, loss = 0.51786253\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57187508\n",
            "Iteration 2, loss = 0.56446638\n",
            "Iteration 3, loss = 0.56438950\n",
            "Iteration 4, loss = 0.56508191\n",
            "Iteration 5, loss = 0.56556530\n",
            "Iteration 6, loss = 0.56257975\n",
            "Iteration 7, loss = 0.56582542\n",
            "Iteration 8, loss = 0.56428098\n",
            "Iteration 9, loss = 0.56407451\n",
            "Iteration 10, loss = 0.56414905\n",
            "Iteration 11, loss = 0.56329259\n",
            "Iteration 12, loss = 0.56388488\n",
            "Iteration 13, loss = 0.56457742\n",
            "Iteration 14, loss = 0.56444221\n",
            "Iteration 15, loss = 0.56397370\n",
            "Iteration 16, loss = 0.56383687\n",
            "Iteration 17, loss = 0.56559185\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56548545\n",
            "Iteration 2, loss = 0.56492151\n",
            "Iteration 3, loss = 0.56441887\n",
            "Iteration 4, loss = 0.56525957\n",
            "Iteration 5, loss = 0.56444257\n",
            "Iteration 6, loss = 0.56451735\n",
            "Iteration 7, loss = 0.56474366\n",
            "Iteration 8, loss = 0.56416587\n",
            "Iteration 9, loss = 0.56423664\n",
            "Iteration 10, loss = 0.56404023\n",
            "Iteration 11, loss = 0.56425376\n",
            "Iteration 12, loss = 0.56436481\n",
            "Iteration 13, loss = 0.56367274\n",
            "Iteration 14, loss = 0.56417890\n",
            "Iteration 15, loss = 0.56381474\n",
            "Iteration 16, loss = 0.56383707\n",
            "Iteration 17, loss = 0.56325784\n",
            "Iteration 18, loss = 0.56281355\n",
            "Iteration 19, loss = 0.56379443\n",
            "Iteration 20, loss = 0.56392545\n",
            "Iteration 21, loss = 0.56312740\n",
            "Iteration 22, loss = 0.56392994\n",
            "Iteration 23, loss = 0.56353873\n",
            "Iteration 24, loss = 0.56305023\n",
            "Iteration 25, loss = 0.56323031\n",
            "Iteration 26, loss = 0.56366568\n",
            "Iteration 27, loss = 0.56340341\n",
            "Iteration 28, loss = 0.56260587\n",
            "Iteration 29, loss = 0.56339421\n",
            "Iteration 30, loss = 0.56355891\n",
            "Iteration 31, loss = 0.56324064\n",
            "Iteration 32, loss = 0.56342705\n",
            "Iteration 33, loss = 0.56221496\n",
            "Iteration 34, loss = 0.56220917\n",
            "Iteration 35, loss = 0.56292666\n",
            "Iteration 36, loss = 0.56367694\n",
            "Iteration 37, loss = 0.56241806\n",
            "Iteration 38, loss = 0.56279540\n",
            "Iteration 39, loss = 0.56201706\n",
            "Iteration 40, loss = 0.56098652\n",
            "Iteration 41, loss = 0.56146893\n",
            "Iteration 42, loss = 0.56143886\n",
            "Iteration 43, loss = 0.56165284\n",
            "Iteration 44, loss = 0.56139859\n",
            "Iteration 45, loss = 0.56259334\n",
            "Iteration 46, loss = 0.56094341\n",
            "Iteration 47, loss = 0.56194123\n",
            "Iteration 48, loss = 0.56031567\n",
            "Iteration 49, loss = 0.56154284\n",
            "Iteration 50, loss = 0.56076084\n",
            "Iteration 51, loss = 0.56021252\n",
            "Iteration 52, loss = 0.56066486\n",
            "Iteration 53, loss = 0.56021901\n",
            "Iteration 54, loss = 0.56022296\n",
            "Iteration 55, loss = 0.55992601\n",
            "Iteration 56, loss = 0.55940756\n",
            "Iteration 57, loss = 0.55915336\n",
            "Iteration 58, loss = 0.55889329\n",
            "Iteration 59, loss = 0.55840480\n",
            "Iteration 60, loss = 0.55809245\n",
            "Iteration 61, loss = 0.55838559\n",
            "Iteration 62, loss = 0.55877139\n",
            "Iteration 63, loss = 0.55650144\n",
            "Iteration 64, loss = 0.55647714\n",
            "Iteration 65, loss = 0.55632670\n",
            "Iteration 66, loss = 0.55638332\n",
            "Iteration 67, loss = 0.55496436\n",
            "Iteration 68, loss = 0.55549171\n",
            "Iteration 69, loss = 0.55469980\n",
            "Iteration 70, loss = 0.55429264\n",
            "Iteration 71, loss = 0.55341634\n",
            "Iteration 72, loss = 0.55379619\n",
            "Iteration 73, loss = 0.55382303\n",
            "Iteration 74, loss = 0.55226260\n",
            "Iteration 75, loss = 0.55186634\n",
            "Iteration 76, loss = 0.55107440\n",
            "Iteration 77, loss = 0.54990712\n",
            "Iteration 78, loss = 0.55001434\n",
            "Iteration 79, loss = 0.54911682\n",
            "Iteration 80, loss = 0.54823204\n",
            "Iteration 81, loss = 0.54741462\n",
            "Iteration 82, loss = 0.54699015\n",
            "Iteration 83, loss = 0.54722749\n",
            "Iteration 84, loss = 0.54553166\n",
            "Iteration 85, loss = 0.54525330\n",
            "Iteration 86, loss = 0.54463130\n",
            "Iteration 87, loss = 0.54407614\n",
            "Iteration 88, loss = 0.54279105\n",
            "Iteration 89, loss = 0.54265917\n",
            "Iteration 90, loss = 0.54191490\n",
            "Iteration 91, loss = 0.54099546\n",
            "Iteration 92, loss = 0.54051049\n",
            "Iteration 93, loss = 0.53999811\n",
            "Iteration 94, loss = 0.53989554\n",
            "Iteration 95, loss = 0.53864374\n",
            "Iteration 96, loss = 0.53782632\n",
            "Iteration 97, loss = 0.53701340\n",
            "Iteration 98, loss = 0.53726925\n",
            "Iteration 99, loss = 0.53648409\n",
            "Iteration 100, loss = 0.53563020\n",
            "Iteration 101, loss = 0.53517387\n",
            "Iteration 102, loss = 0.53453435\n",
            "Iteration 103, loss = 0.53442594\n",
            "Iteration 104, loss = 0.53477718\n",
            "Iteration 105, loss = 0.53315488\n",
            "Iteration 106, loss = 0.53321799\n",
            "Iteration 107, loss = 0.53175191\n",
            "Iteration 108, loss = 0.53190668\n",
            "Iteration 109, loss = 0.53195578\n",
            "Iteration 110, loss = 0.53099496\n",
            "Iteration 111, loss = 0.53146237\n",
            "Iteration 112, loss = 0.53101452\n",
            "Iteration 113, loss = 0.53051751\n",
            "Iteration 114, loss = 0.53031009\n",
            "Iteration 115, loss = 0.52964742\n",
            "Iteration 116, loss = 0.52934186\n",
            "Iteration 117, loss = 0.52938577\n",
            "Iteration 118, loss = 0.52927643\n",
            "Iteration 119, loss = 0.52842868\n",
            "Iteration 120, loss = 0.52826743\n",
            "Iteration 121, loss = 0.52850037\n",
            "Iteration 122, loss = 0.52859591\n",
            "Iteration 123, loss = 0.52793156\n",
            "Iteration 124, loss = 0.52707751\n",
            "Iteration 125, loss = 0.52763732\n",
            "Iteration 126, loss = 0.52763192\n",
            "Iteration 127, loss = 0.52566352\n",
            "Iteration 128, loss = 0.52777286\n",
            "Iteration 129, loss = 0.52686539\n",
            "Iteration 130, loss = 0.52663468\n",
            "Iteration 131, loss = 0.52603333\n",
            "Iteration 132, loss = 0.52607144\n",
            "Iteration 133, loss = 0.52645575\n",
            "Iteration 134, loss = 0.52610816\n",
            "Iteration 135, loss = 0.52551807\n",
            "Iteration 136, loss = 0.52659981\n",
            "Iteration 137, loss = 0.52572111\n",
            "Iteration 138, loss = 0.52492256\n",
            "Iteration 139, loss = 0.52541370\n",
            "Iteration 140, loss = 0.52569265\n",
            "Iteration 141, loss = 0.52463002\n",
            "Iteration 142, loss = 0.52571937\n",
            "Iteration 143, loss = 0.52504923\n",
            "Iteration 144, loss = 0.52449863\n",
            "Iteration 145, loss = 0.52440962\n",
            "Iteration 146, loss = 0.52464135\n",
            "Iteration 147, loss = 0.52338685\n",
            "Iteration 148, loss = 0.52486118\n",
            "Iteration 149, loss = 0.52399919\n",
            "Iteration 150, loss = 0.52410718\n",
            "Iteration 151, loss = 0.52401304\n",
            "Iteration 152, loss = 0.52308356\n",
            "Iteration 153, loss = 0.52354518\n",
            "Iteration 154, loss = 0.52430237\n",
            "Iteration 155, loss = 0.52385455\n",
            "Iteration 156, loss = 0.52331987\n",
            "Iteration 157, loss = 0.52359979\n",
            "Iteration 158, loss = 0.52328201\n",
            "Iteration 159, loss = 0.52331513\n",
            "Iteration 160, loss = 0.52349717\n",
            "Iteration 161, loss = 0.52292092\n",
            "Iteration 162, loss = 0.52285406\n",
            "Iteration 163, loss = 0.52264305\n",
            "Iteration 164, loss = 0.52288703\n",
            "Iteration 165, loss = 0.52275820\n",
            "Iteration 166, loss = 0.52297851\n",
            "Iteration 167, loss = 0.52159633\n",
            "Iteration 168, loss = 0.52201615\n",
            "Iteration 169, loss = 0.52312003\n",
            "Iteration 170, loss = 0.52226295\n",
            "Iteration 171, loss = 0.52229297\n",
            "Iteration 172, loss = 0.52192530\n",
            "Iteration 173, loss = 0.52178594\n",
            "Iteration 174, loss = 0.52170356\n",
            "Iteration 175, loss = 0.52266326\n",
            "Iteration 176, loss = 0.52275234\n",
            "Iteration 177, loss = 0.52147050\n",
            "Iteration 178, loss = 0.52247564\n",
            "Iteration 179, loss = 0.52238998\n",
            "Iteration 180, loss = 0.52224473\n",
            "Iteration 181, loss = 0.52149882\n",
            "Iteration 182, loss = 0.52148809\n",
            "Iteration 183, loss = 0.52159167\n",
            "Iteration 184, loss = 0.52143973\n",
            "Iteration 185, loss = 0.52132886\n",
            "Iteration 186, loss = 0.52170116\n",
            "Iteration 187, loss = 0.52134690\n",
            "Iteration 188, loss = 0.52179389\n",
            "Iteration 189, loss = 0.52140137\n",
            "Iteration 190, loss = 0.52115655\n",
            "Iteration 191, loss = 0.52140486\n",
            "Iteration 192, loss = 0.52162880\n",
            "Iteration 193, loss = 0.52048219\n",
            "Iteration 194, loss = 0.52014053\n",
            "Iteration 195, loss = 0.52142821\n",
            "Iteration 196, loss = 0.52104214\n",
            "Iteration 197, loss = 0.52114921\n",
            "Iteration 198, loss = 0.52141371\n",
            "Iteration 199, loss = 0.52002484\n",
            "Iteration 200, loss = 0.52177341\n",
            "Iteration 201, loss = 0.52090152\n",
            "Iteration 202, loss = 0.52140373\n",
            "Iteration 203, loss = 0.52127399\n",
            "Iteration 204, loss = 0.52103561\n",
            "Iteration 205, loss = 0.52130541\n",
            "Iteration 206, loss = 0.52160377\n",
            "Iteration 207, loss = 0.52134386\n",
            "Iteration 208, loss = 0.52077630\n",
            "Iteration 209, loss = 0.52070483\n",
            "Iteration 210, loss = 0.52088472\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57130940\n",
            "Iteration 2, loss = 0.56223028\n",
            "Iteration 3, loss = 0.56234646\n",
            "Iteration 4, loss = 0.56173040\n",
            "Iteration 5, loss = 0.56187347\n",
            "Iteration 6, loss = 0.56251643\n",
            "Iteration 7, loss = 0.56157941\n",
            "Iteration 8, loss = 0.56264627\n",
            "Iteration 9, loss = 0.56330247\n",
            "Iteration 10, loss = 0.56080230\n",
            "Iteration 11, loss = 0.56160468\n",
            "Iteration 12, loss = 0.56157227\n",
            "Iteration 13, loss = 0.56225980\n",
            "Iteration 14, loss = 0.56126398\n",
            "Iteration 15, loss = 0.56158885\n",
            "Iteration 16, loss = 0.56183667\n",
            "Iteration 17, loss = 0.56142170\n",
            "Iteration 18, loss = 0.56178564\n",
            "Iteration 19, loss = 0.56159257\n",
            "Iteration 20, loss = 0.56111908\n",
            "Iteration 21, loss = 0.56081263\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56669548\n",
            "Iteration 2, loss = 0.56180686\n",
            "Iteration 3, loss = 0.56086006\n",
            "Iteration 4, loss = 0.56125972\n",
            "Iteration 5, loss = 0.56073222\n",
            "Iteration 6, loss = 0.56069182\n",
            "Iteration 7, loss = 0.56085297\n",
            "Iteration 8, loss = 0.56138500\n",
            "Iteration 9, loss = 0.56071107\n",
            "Iteration 10, loss = 0.56137634\n",
            "Iteration 11, loss = 0.56083481\n",
            "Iteration 12, loss = 0.56049932\n",
            "Iteration 13, loss = 0.56006017\n",
            "Iteration 14, loss = 0.56118757\n",
            "Iteration 15, loss = 0.56009497\n",
            "Iteration 16, loss = 0.56053506\n",
            "Iteration 17, loss = 0.55996060\n",
            "Iteration 18, loss = 0.56019950\n",
            "Iteration 19, loss = 0.56042815\n",
            "Iteration 20, loss = 0.56101042\n",
            "Iteration 21, loss = 0.56032999\n",
            "Iteration 22, loss = 0.56011537\n",
            "Iteration 23, loss = 0.56022734\n",
            "Iteration 24, loss = 0.56118380\n",
            "Iteration 25, loss = 0.55918691\n",
            "Iteration 26, loss = 0.55946261\n",
            "Iteration 27, loss = 0.55950476\n",
            "Iteration 28, loss = 0.55945153\n",
            "Iteration 29, loss = 0.55950797\n",
            "Iteration 30, loss = 0.55908969\n",
            "Iteration 31, loss = 0.55980641\n",
            "Iteration 32, loss = 0.55909371\n",
            "Iteration 33, loss = 0.55966695\n",
            "Iteration 34, loss = 0.55887531\n",
            "Iteration 35, loss = 0.55947960\n",
            "Iteration 36, loss = 0.55860914\n",
            "Iteration 37, loss = 0.55901205\n",
            "Iteration 38, loss = 0.55886156\n",
            "Iteration 39, loss = 0.55809291\n",
            "Iteration 40, loss = 0.55859809\n",
            "Iteration 41, loss = 0.55827905\n",
            "Iteration 42, loss = 0.55795104\n",
            "Iteration 43, loss = 0.55809221\n",
            "Iteration 44, loss = 0.55733390\n",
            "Iteration 45, loss = 0.55777590\n",
            "Iteration 46, loss = 0.55847872\n",
            "Iteration 47, loss = 0.55717047\n",
            "Iteration 48, loss = 0.55661843\n",
            "Iteration 49, loss = 0.55673143\n",
            "Iteration 50, loss = 0.55648947\n",
            "Iteration 51, loss = 0.55630661\n",
            "Iteration 52, loss = 0.55577935\n",
            "Iteration 53, loss = 0.55630209\n",
            "Iteration 54, loss = 0.55600603\n",
            "Iteration 55, loss = 0.55568104\n",
            "Iteration 56, loss = 0.55525062\n",
            "Iteration 57, loss = 0.55507098\n",
            "Iteration 58, loss = 0.55535113\n",
            "Iteration 59, loss = 0.55438706\n",
            "Iteration 60, loss = 0.55414748\n",
            "Iteration 61, loss = 0.55337219\n",
            "Iteration 62, loss = 0.55349593\n",
            "Iteration 63, loss = 0.55293867\n",
            "Iteration 64, loss = 0.55280333\n",
            "Iteration 65, loss = 0.55307080\n",
            "Iteration 66, loss = 0.55204303\n",
            "Iteration 67, loss = 0.55098077\n",
            "Iteration 68, loss = 0.55121111\n",
            "Iteration 69, loss = 0.55077816\n",
            "Iteration 70, loss = 0.54972440\n",
            "Iteration 71, loss = 0.54917274\n",
            "Iteration 72, loss = 0.54757603\n",
            "Iteration 73, loss = 0.54784330\n",
            "Iteration 74, loss = 0.54863233\n",
            "Iteration 75, loss = 0.54688622\n",
            "Iteration 76, loss = 0.54647162\n",
            "Iteration 77, loss = 0.54578386\n",
            "Iteration 78, loss = 0.54427442\n",
            "Iteration 79, loss = 0.54450517\n",
            "Iteration 80, loss = 0.54336532\n",
            "Iteration 81, loss = 0.54243516\n",
            "Iteration 82, loss = 0.54215229\n",
            "Iteration 83, loss = 0.54211155\n",
            "Iteration 84, loss = 0.54107817\n",
            "Iteration 85, loss = 0.53963959\n",
            "Iteration 86, loss = 0.53889839\n",
            "Iteration 87, loss = 0.53824208\n",
            "Iteration 88, loss = 0.53721495\n",
            "Iteration 89, loss = 0.53668656\n",
            "Iteration 90, loss = 0.53510434\n",
            "Iteration 91, loss = 0.53577982\n",
            "Iteration 92, loss = 0.53461029\n",
            "Iteration 93, loss = 0.53372828\n",
            "Iteration 94, loss = 0.53322953\n",
            "Iteration 95, loss = 0.53234179\n",
            "Iteration 96, loss = 0.53215153\n",
            "Iteration 97, loss = 0.53219294\n",
            "Iteration 98, loss = 0.53061489\n",
            "Iteration 99, loss = 0.52987747\n",
            "Iteration 100, loss = 0.52977529\n",
            "Iteration 101, loss = 0.52912925\n",
            "Iteration 102, loss = 0.52861717\n",
            "Iteration 103, loss = 0.52856014\n",
            "Iteration 104, loss = 0.52715341\n",
            "Iteration 105, loss = 0.52647020\n",
            "Iteration 106, loss = 0.52780395\n",
            "Iteration 107, loss = 0.52691003\n",
            "Iteration 108, loss = 0.52617659\n",
            "Iteration 109, loss = 0.52513333\n",
            "Iteration 110, loss = 0.52515724\n",
            "Iteration 111, loss = 0.52427150\n",
            "Iteration 112, loss = 0.52429005\n",
            "Iteration 113, loss = 0.52423727\n",
            "Iteration 114, loss = 0.52487161\n",
            "Iteration 115, loss = 0.52399968\n",
            "Iteration 116, loss = 0.52409590\n",
            "Iteration 117, loss = 0.52318445\n",
            "Iteration 118, loss = 0.52279600\n",
            "Iteration 119, loss = 0.52267942\n",
            "Iteration 120, loss = 0.52228998\n",
            "Iteration 121, loss = 0.52345725\n",
            "Iteration 122, loss = 0.52346488\n",
            "Iteration 123, loss = 0.52115740\n",
            "Iteration 124, loss = 0.52216659\n",
            "Iteration 125, loss = 0.52093979\n",
            "Iteration 126, loss = 0.52046676\n",
            "Iteration 127, loss = 0.52208291\n",
            "Iteration 128, loss = 0.52072091\n",
            "Iteration 129, loss = 0.52066150\n",
            "Iteration 130, loss = 0.52047753\n",
            "Iteration 131, loss = 0.51976278\n",
            "Iteration 132, loss = 0.52011888\n",
            "Iteration 133, loss = 0.51947620\n",
            "Iteration 134, loss = 0.51979140\n",
            "Iteration 135, loss = 0.51957545\n",
            "Iteration 136, loss = 0.51921110\n",
            "Iteration 137, loss = 0.51922709\n",
            "Iteration 138, loss = 0.51870495\n",
            "Iteration 139, loss = 0.51885022\n",
            "Iteration 140, loss = 0.51823641\n",
            "Iteration 141, loss = 0.51919337\n",
            "Iteration 142, loss = 0.51825015\n",
            "Iteration 143, loss = 0.51850832\n",
            "Iteration 144, loss = 0.51854968\n",
            "Iteration 145, loss = 0.51886897\n",
            "Iteration 146, loss = 0.51738302\n",
            "Iteration 147, loss = 0.51808131\n",
            "Iteration 148, loss = 0.51834586\n",
            "Iteration 149, loss = 0.51781718\n",
            "Iteration 150, loss = 0.51921537\n",
            "Iteration 151, loss = 0.51779048\n",
            "Iteration 152, loss = 0.51763595\n",
            "Iteration 153, loss = 0.51676918\n",
            "Iteration 154, loss = 0.51707836\n",
            "Iteration 155, loss = 0.51696183\n",
            "Iteration 156, loss = 0.51677486\n",
            "Iteration 157, loss = 0.51647681\n",
            "Iteration 158, loss = 0.51728977\n",
            "Iteration 159, loss = 0.51685000\n",
            "Iteration 160, loss = 0.51614006\n",
            "Iteration 161, loss = 0.51621380\n",
            "Iteration 162, loss = 0.51716385\n",
            "Iteration 163, loss = 0.51682745\n",
            "Iteration 164, loss = 0.51624241\n",
            "Iteration 165, loss = 0.51668513\n",
            "Iteration 166, loss = 0.51638323\n",
            "Iteration 167, loss = 0.51672300\n",
            "Iteration 168, loss = 0.51606718\n",
            "Iteration 169, loss = 0.51659607\n",
            "Iteration 170, loss = 0.51553158\n",
            "Iteration 171, loss = 0.51597924\n",
            "Iteration 172, loss = 0.51612982\n",
            "Iteration 173, loss = 0.51629177\n",
            "Iteration 174, loss = 0.51508503\n",
            "Iteration 175, loss = 0.51606430\n",
            "Iteration 176, loss = 0.51591498\n",
            "Iteration 177, loss = 0.51533637\n",
            "Iteration 178, loss = 0.51543398\n",
            "Iteration 179, loss = 0.51572367\n",
            "Iteration 180, loss = 0.51656810\n",
            "Iteration 181, loss = 0.51544270\n",
            "Iteration 182, loss = 0.51533731\n",
            "Iteration 183, loss = 0.51521414\n",
            "Iteration 184, loss = 0.51564386\n",
            "Iteration 185, loss = 0.51592571\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56236458\n",
            "Iteration 2, loss = 0.55957690\n",
            "Iteration 3, loss = 0.55937185\n",
            "Iteration 4, loss = 0.55979306\n",
            "Iteration 5, loss = 0.55834854\n",
            "Iteration 6, loss = 0.55930690\n",
            "Iteration 7, loss = 0.55981608\n",
            "Iteration 8, loss = 0.55890586\n",
            "Iteration 9, loss = 0.55863377\n",
            "Iteration 10, loss = 0.55991138\n",
            "Iteration 11, loss = 0.55882607\n",
            "Iteration 12, loss = 0.55945661\n",
            "Iteration 13, loss = 0.55914972\n",
            "Iteration 14, loss = 0.55909428\n",
            "Iteration 15, loss = 0.55829403\n",
            "Iteration 16, loss = 0.55835569\n",
            "Iteration 17, loss = 0.55843044\n",
            "Iteration 18, loss = 0.55842166\n",
            "Iteration 19, loss = 0.55849609\n",
            "Iteration 20, loss = 0.55895483\n",
            "Iteration 21, loss = 0.55868799\n",
            "Iteration 22, loss = 0.55814060\n",
            "Iteration 23, loss = 0.55814378\n",
            "Iteration 24, loss = 0.55816552\n",
            "Iteration 25, loss = 0.55839386\n",
            "Iteration 26, loss = 0.55813816\n",
            "Iteration 27, loss = 0.55788379\n",
            "Iteration 28, loss = 0.55774372\n",
            "Iteration 29, loss = 0.55722636\n",
            "Iteration 30, loss = 0.55722087\n",
            "Iteration 31, loss = 0.55785694\n",
            "Iteration 32, loss = 0.55768154\n",
            "Iteration 33, loss = 0.55696649\n",
            "Iteration 34, loss = 0.55734898\n",
            "Iteration 35, loss = 0.55744190\n",
            "Iteration 36, loss = 0.55696429\n",
            "Iteration 37, loss = 0.55720681\n",
            "Iteration 38, loss = 0.55720236\n",
            "Iteration 39, loss = 0.55709057\n",
            "Iteration 40, loss = 0.55652332\n",
            "Iteration 41, loss = 0.55612518\n",
            "Iteration 42, loss = 0.55584632\n",
            "Iteration 43, loss = 0.55622836\n",
            "Iteration 44, loss = 0.55611087\n",
            "Iteration 45, loss = 0.55586268\n",
            "Iteration 46, loss = 0.55561727\n",
            "Iteration 47, loss = 0.55510660\n",
            "Iteration 48, loss = 0.55555533\n",
            "Iteration 49, loss = 0.55472339\n",
            "Iteration 50, loss = 0.55520218\n",
            "Iteration 51, loss = 0.55454184\n",
            "Iteration 52, loss = 0.55384641\n",
            "Iteration 53, loss = 0.55431403\n",
            "Iteration 54, loss = 0.55349229\n",
            "Iteration 55, loss = 0.55370350\n",
            "Iteration 56, loss = 0.55372290\n",
            "Iteration 57, loss = 0.55315540\n",
            "Iteration 58, loss = 0.55309807\n",
            "Iteration 59, loss = 0.55208443\n",
            "Iteration 60, loss = 0.55173080\n",
            "Iteration 61, loss = 0.55147638\n",
            "Iteration 62, loss = 0.55160207\n",
            "Iteration 63, loss = 0.55056927\n",
            "Iteration 64, loss = 0.54966631\n",
            "Iteration 65, loss = 0.55017846\n",
            "Iteration 66, loss = 0.54906214\n",
            "Iteration 67, loss = 0.54897218\n",
            "Iteration 68, loss = 0.54825909\n",
            "Iteration 69, loss = 0.54774466\n",
            "Iteration 70, loss = 0.54749294\n",
            "Iteration 71, loss = 0.54707304\n",
            "Iteration 72, loss = 0.54602870\n",
            "Iteration 73, loss = 0.54462186\n",
            "Iteration 74, loss = 0.54591786\n",
            "Iteration 75, loss = 0.54510536\n",
            "Iteration 76, loss = 0.54424343\n",
            "Iteration 77, loss = 0.54286553\n",
            "Iteration 78, loss = 0.54238274\n",
            "Iteration 79, loss = 0.54129596\n",
            "Iteration 80, loss = 0.54047784\n",
            "Iteration 81, loss = 0.54012240\n",
            "Iteration 82, loss = 0.53920126\n",
            "Iteration 83, loss = 0.53803792\n",
            "Iteration 84, loss = 0.53762773\n",
            "Iteration 85, loss = 0.53851603\n",
            "Iteration 86, loss = 0.53642983\n",
            "Iteration 87, loss = 0.53562423\n",
            "Iteration 88, loss = 0.53502941\n",
            "Iteration 89, loss = 0.53407663\n",
            "Iteration 90, loss = 0.53411636\n",
            "Iteration 91, loss = 0.53244857\n",
            "Iteration 92, loss = 0.53307871\n",
            "Iteration 93, loss = 0.53165415\n",
            "Iteration 94, loss = 0.53216362\n",
            "Iteration 95, loss = 0.53060409\n",
            "Iteration 96, loss = 0.52975160\n",
            "Iteration 97, loss = 0.53047225\n",
            "Iteration 98, loss = 0.52984992\n",
            "Iteration 99, loss = 0.52829868\n",
            "Iteration 100, loss = 0.52881243\n",
            "Iteration 101, loss = 0.52758576\n",
            "Iteration 102, loss = 0.52752187\n",
            "Iteration 103, loss = 0.52731307\n",
            "Iteration 104, loss = 0.52678734\n",
            "Iteration 105, loss = 0.52698250\n",
            "Iteration 106, loss = 0.52632643\n",
            "Iteration 107, loss = 0.52511716\n",
            "Iteration 108, loss = 0.52551041\n",
            "Iteration 109, loss = 0.52414078\n",
            "Iteration 110, loss = 0.52479694\n",
            "Iteration 111, loss = 0.52531478\n",
            "Iteration 112, loss = 0.52476657\n",
            "Iteration 113, loss = 0.52392888\n",
            "Iteration 114, loss = 0.52477536\n",
            "Iteration 115, loss = 0.52402170\n",
            "Iteration 116, loss = 0.52241496\n",
            "Iteration 117, loss = 0.52322596\n",
            "Iteration 118, loss = 0.52290488\n",
            "Iteration 119, loss = 0.52265722\n",
            "Iteration 120, loss = 0.52297653\n",
            "Iteration 121, loss = 0.52243518\n",
            "Iteration 122, loss = 0.52332894\n",
            "Iteration 123, loss = 0.52227601\n",
            "Iteration 124, loss = 0.52180754\n",
            "Iteration 125, loss = 0.52175721\n",
            "Iteration 126, loss = 0.52072962\n",
            "Iteration 127, loss = 0.52140921\n",
            "Iteration 128, loss = 0.52098059\n",
            "Iteration 129, loss = 0.52145293\n",
            "Iteration 130, loss = 0.52008535\n",
            "Iteration 131, loss = 0.52021508\n",
            "Iteration 132, loss = 0.51984750\n",
            "Iteration 133, loss = 0.52112584\n",
            "Iteration 134, loss = 0.51956903\n",
            "Iteration 135, loss = 0.52021667\n",
            "Iteration 136, loss = 0.52013829\n",
            "Iteration 137, loss = 0.51956422\n",
            "Iteration 138, loss = 0.52046566\n",
            "Iteration 139, loss = 0.51981416\n",
            "Iteration 140, loss = 0.51889495\n",
            "Iteration 141, loss = 0.51930763\n",
            "Iteration 142, loss = 0.51799870\n",
            "Iteration 143, loss = 0.51945092\n",
            "Iteration 144, loss = 0.51956848\n",
            "Iteration 145, loss = 0.51898835\n",
            "Iteration 146, loss = 0.51853208\n",
            "Iteration 147, loss = 0.51889270\n",
            "Iteration 148, loss = 0.51839340\n",
            "Iteration 149, loss = 0.51824882\n",
            "Iteration 150, loss = 0.51776164\n",
            "Iteration 151, loss = 0.51813537\n",
            "Iteration 152, loss = 0.51851235\n",
            "Iteration 153, loss = 0.51895916\n",
            "Iteration 154, loss = 0.51813031\n",
            "Iteration 155, loss = 0.51812467\n",
            "Iteration 156, loss = 0.51754693\n",
            "Iteration 157, loss = 0.51703935\n",
            "Iteration 158, loss = 0.51711267\n",
            "Iteration 159, loss = 0.51733579\n",
            "Iteration 160, loss = 0.51774222\n",
            "Iteration 161, loss = 0.51732346\n",
            "Iteration 162, loss = 0.51670381\n",
            "Iteration 163, loss = 0.51840105\n",
            "Iteration 164, loss = 0.51774035\n",
            "Iteration 165, loss = 0.51651225\n",
            "Iteration 166, loss = 0.51764715\n",
            "Iteration 167, loss = 0.51693792\n",
            "Iteration 168, loss = 0.51771133\n",
            "Iteration 169, loss = 0.51769950\n",
            "Iteration 170, loss = 0.51702588\n",
            "Iteration 171, loss = 0.51733526\n",
            "Iteration 172, loss = 0.51691254\n",
            "Iteration 173, loss = 0.51721336\n",
            "Iteration 174, loss = 0.51717104\n",
            "Iteration 175, loss = 0.51694074\n",
            "Iteration 176, loss = 0.51734764\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57389438\n",
            "Iteration 2, loss = 0.56300507\n",
            "Iteration 3, loss = 0.56266274\n",
            "Iteration 4, loss = 0.56225070\n",
            "Iteration 5, loss = 0.56265603\n",
            "Iteration 6, loss = 0.56233587\n",
            "Iteration 7, loss = 0.56226199\n",
            "Iteration 8, loss = 0.56191066\n",
            "Iteration 9, loss = 0.56253539\n",
            "Iteration 10, loss = 0.56209991\n",
            "Iteration 11, loss = 0.56171380\n",
            "Iteration 12, loss = 0.56199588\n",
            "Iteration 13, loss = 0.56196599\n",
            "Iteration 14, loss = 0.56225999\n",
            "Iteration 15, loss = 0.56204251\n",
            "Iteration 16, loss = 0.56135540\n",
            "Iteration 17, loss = 0.56183460\n",
            "Iteration 18, loss = 0.56141109\n",
            "Iteration 19, loss = 0.56137830\n",
            "Iteration 20, loss = 0.56203013\n",
            "Iteration 21, loss = 0.56109897\n",
            "Iteration 22, loss = 0.56123952\n",
            "Iteration 23, loss = 0.56130549\n",
            "Iteration 24, loss = 0.56142616\n",
            "Iteration 25, loss = 0.56134683\n",
            "Iteration 26, loss = 0.56096989\n",
            "Iteration 27, loss = 0.56110750\n",
            "Iteration 28, loss = 0.56057340\n",
            "Iteration 29, loss = 0.56159582\n",
            "Iteration 30, loss = 0.56024623\n",
            "Iteration 31, loss = 0.56083929\n",
            "Iteration 32, loss = 0.56040769\n",
            "Iteration 33, loss = 0.55997237\n",
            "Iteration 34, loss = 0.55786753\n",
            "Iteration 35, loss = 0.55977503\n",
            "Iteration 36, loss = 0.55983336\n",
            "Iteration 37, loss = 0.55992010\n",
            "Iteration 38, loss = 0.55932065\n",
            "Iteration 39, loss = 0.55801002\n",
            "Iteration 40, loss = 0.55830148\n",
            "Iteration 41, loss = 0.55895306\n",
            "Iteration 42, loss = 0.55896597\n",
            "Iteration 43, loss = 0.55879589\n",
            "Iteration 44, loss = 0.55830137\n",
            "Iteration 45, loss = 0.55806205\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56794883\n",
            "Iteration 2, loss = 0.56658773\n",
            "Iteration 3, loss = 0.56600034\n",
            "Iteration 4, loss = 0.56544660\n",
            "Iteration 5, loss = 0.56628776\n",
            "Iteration 6, loss = 0.56549379\n",
            "Iteration 7, loss = 0.56580742\n",
            "Iteration 8, loss = 0.56582008\n",
            "Iteration 9, loss = 0.56607934\n",
            "Iteration 10, loss = 0.56566057\n",
            "Iteration 11, loss = 0.56592640\n",
            "Iteration 12, loss = 0.56557471\n",
            "Iteration 13, loss = 0.56555888\n",
            "Iteration 14, loss = 0.56560980\n",
            "Iteration 15, loss = 0.56597045\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56923033\n",
            "Iteration 2, loss = 0.56337641\n",
            "Iteration 3, loss = 0.56273875\n",
            "Iteration 4, loss = 0.56213408\n",
            "Iteration 5, loss = 0.56286066\n",
            "Iteration 6, loss = 0.56278917\n",
            "Iteration 7, loss = 0.56326265\n",
            "Iteration 8, loss = 0.56327484\n",
            "Iteration 9, loss = 0.56241572\n",
            "Iteration 10, loss = 0.56284194\n",
            "Iteration 11, loss = 0.56235705\n",
            "Iteration 12, loss = 0.56324821\n",
            "Iteration 13, loss = 0.56175152\n",
            "Iteration 14, loss = 0.56266654\n",
            "Iteration 15, loss = 0.56231483\n",
            "Iteration 16, loss = 0.56229935\n",
            "Iteration 17, loss = 0.56216445\n",
            "Iteration 18, loss = 0.56189774\n",
            "Iteration 19, loss = 0.56043859\n",
            "Iteration 20, loss = 0.56179783\n",
            "Iteration 21, loss = 0.56187652\n",
            "Iteration 22, loss = 0.56140217\n",
            "Iteration 23, loss = 0.56062869\n",
            "Iteration 24, loss = 0.56155773\n",
            "Iteration 25, loss = 0.56078540\n",
            "Iteration 26, loss = 0.56214893\n",
            "Iteration 27, loss = 0.56061951\n",
            "Iteration 28, loss = 0.56134704\n",
            "Iteration 29, loss = 0.56127305\n",
            "Iteration 30, loss = 0.56166169\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56847769\n",
            "Iteration 2, loss = 0.56514693\n",
            "Iteration 3, loss = 0.56455547\n",
            "Iteration 4, loss = 0.56441584\n",
            "Iteration 5, loss = 0.56436027\n",
            "Iteration 6, loss = 0.56459716\n",
            "Iteration 7, loss = 0.56394430\n",
            "Iteration 8, loss = 0.56442468\n",
            "Iteration 9, loss = 0.56494398\n",
            "Iteration 10, loss = 0.56431573\n",
            "Iteration 11, loss = 0.56450011\n",
            "Iteration 12, loss = 0.56350982\n",
            "Iteration 13, loss = 0.56335866\n",
            "Iteration 14, loss = 0.56434533\n",
            "Iteration 15, loss = 0.56334602\n",
            "Iteration 16, loss = 0.56332279\n",
            "Iteration 17, loss = 0.56357483\n",
            "Iteration 18, loss = 0.56380635\n",
            "Iteration 19, loss = 0.56330935\n",
            "Iteration 20, loss = 0.56376099\n",
            "Iteration 21, loss = 0.56367247\n",
            "Iteration 22, loss = 0.56334858\n",
            "Iteration 23, loss = 0.56398219\n",
            "Iteration 24, loss = 0.56328976\n",
            "Iteration 25, loss = 0.56307573\n",
            "Iteration 26, loss = 0.56342667\n",
            "Iteration 27, loss = 0.56330055\n",
            "Iteration 28, loss = 0.56311251\n",
            "Iteration 29, loss = 0.56256825\n",
            "Iteration 30, loss = 0.56329811\n",
            "Iteration 31, loss = 0.56263322\n",
            "Iteration 32, loss = 0.56312514\n",
            "Iteration 33, loss = 0.56255361\n",
            "Iteration 34, loss = 0.56279105\n",
            "Iteration 35, loss = 0.56184245\n",
            "Iteration 36, loss = 0.56225314\n",
            "Iteration 37, loss = 0.56212475\n",
            "Iteration 38, loss = 0.56125489\n",
            "Iteration 39, loss = 0.56252757\n",
            "Iteration 40, loss = 0.56224281\n",
            "Iteration 41, loss = 0.56075716\n",
            "Iteration 42, loss = 0.56189824\n",
            "Iteration 43, loss = 0.56101997\n",
            "Iteration 44, loss = 0.56037326\n",
            "Iteration 45, loss = 0.56061699\n",
            "Iteration 46, loss = 0.56095895\n",
            "Iteration 47, loss = 0.56157223\n",
            "Iteration 48, loss = 0.56022538\n",
            "Iteration 49, loss = 0.56067335\n",
            "Iteration 50, loss = 0.56086647\n",
            "Iteration 51, loss = 0.55944534\n",
            "Iteration 52, loss = 0.56035533\n",
            "Iteration 53, loss = 0.55943237\n",
            "Iteration 54, loss = 0.55994154\n",
            "Iteration 55, loss = 0.55928583\n",
            "Iteration 56, loss = 0.55897591\n",
            "Iteration 57, loss = 0.55890641\n",
            "Iteration 58, loss = 0.55866257\n",
            "Iteration 59, loss = 0.55885528\n",
            "Iteration 60, loss = 0.55822430\n",
            "Iteration 61, loss = 0.55752824\n",
            "Iteration 62, loss = 0.55680418\n",
            "Iteration 63, loss = 0.55715473\n",
            "Iteration 64, loss = 0.55665143\n",
            "Iteration 65, loss = 0.55725818\n",
            "Iteration 66, loss = 0.55621324\n",
            "Iteration 67, loss = 0.55602119\n",
            "Iteration 68, loss = 0.55454791\n",
            "Iteration 69, loss = 0.55428542\n",
            "Iteration 70, loss = 0.55424074\n",
            "Iteration 71, loss = 0.55393984\n",
            "Iteration 72, loss = 0.55247351\n",
            "Iteration 73, loss = 0.55298408\n",
            "Iteration 74, loss = 0.55198052\n",
            "Iteration 75, loss = 0.55145125\n",
            "Iteration 76, loss = 0.55052514\n",
            "Iteration 77, loss = 0.55091267\n",
            "Iteration 78, loss = 0.54837184\n",
            "Iteration 79, loss = 0.54923647\n",
            "Iteration 80, loss = 0.54861309\n",
            "Iteration 81, loss = 0.54733188\n",
            "Iteration 82, loss = 0.54688310\n",
            "Iteration 83, loss = 0.54594171\n",
            "Iteration 84, loss = 0.54598886\n",
            "Iteration 85, loss = 0.54477186\n",
            "Iteration 86, loss = 0.54336790\n",
            "Iteration 87, loss = 0.54336688\n",
            "Iteration 88, loss = 0.54263768\n",
            "Iteration 89, loss = 0.54147617\n",
            "Iteration 90, loss = 0.54158973\n",
            "Iteration 91, loss = 0.54028130\n",
            "Iteration 92, loss = 0.53971162\n",
            "Iteration 93, loss = 0.53903246\n",
            "Iteration 94, loss = 0.53768748\n",
            "Iteration 95, loss = 0.53733318\n",
            "Iteration 96, loss = 0.53658304\n",
            "Iteration 97, loss = 0.53654995\n",
            "Iteration 98, loss = 0.53545011\n",
            "Iteration 99, loss = 0.53486729\n",
            "Iteration 100, loss = 0.53489036\n",
            "Iteration 101, loss = 0.53398065\n",
            "Iteration 102, loss = 0.53391046\n",
            "Iteration 103, loss = 0.53328821\n",
            "Iteration 104, loss = 0.53258394\n",
            "Iteration 105, loss = 0.53230271\n",
            "Iteration 106, loss = 0.53255810\n",
            "Iteration 107, loss = 0.53161117\n",
            "Iteration 108, loss = 0.53121856\n",
            "Iteration 109, loss = 0.53130313\n",
            "Iteration 110, loss = 0.52968267\n",
            "Iteration 111, loss = 0.52940533\n",
            "Iteration 112, loss = 0.53079014\n",
            "Iteration 113, loss = 0.52978534\n",
            "Iteration 114, loss = 0.52949612\n",
            "Iteration 115, loss = 0.52820398\n",
            "Iteration 116, loss = 0.52854730\n",
            "Iteration 117, loss = 0.52896360\n",
            "Iteration 118, loss = 0.52889433\n",
            "Iteration 119, loss = 0.52747485\n",
            "Iteration 120, loss = 0.52798121\n",
            "Iteration 121, loss = 0.52769086\n",
            "Iteration 122, loss = 0.52700866\n",
            "Iteration 123, loss = 0.52736864\n",
            "Iteration 124, loss = 0.52699460\n",
            "Iteration 125, loss = 0.52601507\n",
            "Iteration 126, loss = 0.52735481\n",
            "Iteration 127, loss = 0.52670769\n",
            "Iteration 128, loss = 0.52632716\n",
            "Iteration 129, loss = 0.52706350\n",
            "Iteration 130, loss = 0.52704936\n",
            "Iteration 131, loss = 0.52545977\n",
            "Iteration 132, loss = 0.52497492\n",
            "Iteration 133, loss = 0.52538886\n",
            "Iteration 134, loss = 0.52558119\n",
            "Iteration 135, loss = 0.52497880\n",
            "Iteration 136, loss = 0.52430376\n",
            "Iteration 137, loss = 0.52452008\n",
            "Iteration 138, loss = 0.52480371\n",
            "Iteration 139, loss = 0.52420359\n",
            "Iteration 140, loss = 0.52461561\n",
            "Iteration 141, loss = 0.52438744\n",
            "Iteration 142, loss = 0.52432302\n",
            "Iteration 143, loss = 0.52278130\n",
            "Iteration 144, loss = 0.52290151\n",
            "Iteration 145, loss = 0.52468068\n",
            "Iteration 146, loss = 0.52320971\n",
            "Iteration 147, loss = 0.52449879\n",
            "Iteration 148, loss = 0.52355407\n",
            "Iteration 149, loss = 0.52402115\n",
            "Iteration 150, loss = 0.52384672\n",
            "Iteration 151, loss = 0.52310435\n",
            "Iteration 152, loss = 0.52348869\n",
            "Iteration 153, loss = 0.52290208\n",
            "Iteration 154, loss = 0.52283164\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "4\n",
            "Iteration 1, loss = 0.56594897\n",
            "Iteration 2, loss = 0.56470847\n",
            "Iteration 3, loss = 0.56466675\n",
            "Iteration 4, loss = 0.56537849\n",
            "Iteration 5, loss = 0.56512702\n",
            "Iteration 6, loss = 0.56495253\n",
            "Iteration 7, loss = 0.56483090\n",
            "Iteration 8, loss = 0.56498807\n",
            "Iteration 9, loss = 0.56483445\n",
            "Iteration 10, loss = 0.56406480\n",
            "Iteration 11, loss = 0.56496697\n",
            "Iteration 12, loss = 0.56516741\n",
            "Iteration 13, loss = 0.56449977\n",
            "Iteration 14, loss = 0.56446243\n",
            "Iteration 15, loss = 0.56344288\n",
            "Iteration 16, loss = 0.56478785\n",
            "Iteration 17, loss = 0.56412421\n",
            "Iteration 18, loss = 0.56378571\n",
            "Iteration 19, loss = 0.56427843\n",
            "Iteration 20, loss = 0.56444649\n",
            "Iteration 21, loss = 0.56351583\n",
            "Iteration 22, loss = 0.56455779\n",
            "Iteration 23, loss = 0.56390834\n",
            "Iteration 24, loss = 0.56404874\n",
            "Iteration 25, loss = 0.56404476\n",
            "Iteration 26, loss = 0.56353820\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57235629\n",
            "Iteration 2, loss = 0.56605386\n",
            "Iteration 3, loss = 0.56530864\n",
            "Iteration 4, loss = 0.56580537\n",
            "Iteration 5, loss = 0.56548238\n",
            "Iteration 6, loss = 0.56550089\n",
            "Iteration 7, loss = 0.56509125\n",
            "Iteration 8, loss = 0.56582490\n",
            "Iteration 9, loss = 0.56516190\n",
            "Iteration 10, loss = 0.56519262\n",
            "Iteration 11, loss = 0.56560586\n",
            "Iteration 12, loss = 0.56510433\n",
            "Iteration 13, loss = 0.56516527\n",
            "Iteration 14, loss = 0.56581124\n",
            "Iteration 15, loss = 0.56528009\n",
            "Iteration 16, loss = 0.56540699\n",
            "Iteration 17, loss = 0.56469519\n",
            "Iteration 18, loss = 0.56470270\n",
            "Iteration 19, loss = 0.56405724\n",
            "Iteration 20, loss = 0.56471876\n",
            "Iteration 21, loss = 0.56465723\n",
            "Iteration 22, loss = 0.56475759\n",
            "Iteration 23, loss = 0.56493169\n",
            "Iteration 24, loss = 0.56439368\n",
            "Iteration 25, loss = 0.56528219\n",
            "Iteration 26, loss = 0.56439722\n",
            "Iteration 27, loss = 0.56428845\n",
            "Iteration 28, loss = 0.56516592\n",
            "Iteration 29, loss = 0.56400973\n",
            "Iteration 30, loss = 0.56466150\n",
            "Iteration 31, loss = 0.56437135\n",
            "Iteration 32, loss = 0.56416317\n",
            "Iteration 33, loss = 0.56485392\n",
            "Iteration 34, loss = 0.56423584\n",
            "Iteration 35, loss = 0.56451966\n",
            "Iteration 36, loss = 0.56445736\n",
            "Iteration 37, loss = 0.56389654\n",
            "Iteration 38, loss = 0.56373762\n",
            "Iteration 39, loss = 0.56379300\n",
            "Iteration 40, loss = 0.56381743\n",
            "Iteration 41, loss = 0.56317365\n",
            "Iteration 42, loss = 0.56339303\n",
            "Iteration 43, loss = 0.56340008\n",
            "Iteration 44, loss = 0.56307863\n",
            "Iteration 45, loss = 0.56230500\n",
            "Iteration 46, loss = 0.56271602\n",
            "Iteration 47, loss = 0.56148094\n",
            "Iteration 48, loss = 0.56281952\n",
            "Iteration 49, loss = 0.56204815\n",
            "Iteration 50, loss = 0.56233681\n",
            "Iteration 51, loss = 0.56200998\n",
            "Iteration 52, loss = 0.56248489\n",
            "Iteration 53, loss = 0.56191231\n",
            "Iteration 54, loss = 0.56195267\n",
            "Iteration 55, loss = 0.56148974\n",
            "Iteration 56, loss = 0.56060327\n",
            "Iteration 57, loss = 0.56049854\n",
            "Iteration 58, loss = 0.56071245\n",
            "Iteration 59, loss = 0.56035891\n",
            "Iteration 60, loss = 0.56001392\n",
            "Iteration 61, loss = 0.55958393\n",
            "Iteration 62, loss = 0.56069280\n",
            "Iteration 63, loss = 0.55910163\n",
            "Iteration 64, loss = 0.55881699\n",
            "Iteration 65, loss = 0.55940758\n",
            "Iteration 66, loss = 0.55858748\n",
            "Iteration 67, loss = 0.55719372\n",
            "Iteration 68, loss = 0.55857535\n",
            "Iteration 69, loss = 0.55820006\n",
            "Iteration 70, loss = 0.55683686\n",
            "Iteration 71, loss = 0.55752233\n",
            "Iteration 72, loss = 0.55663536\n",
            "Iteration 73, loss = 0.55550197\n",
            "Iteration 74, loss = 0.55512984\n",
            "Iteration 75, loss = 0.55481520\n",
            "Iteration 76, loss = 0.55399722\n",
            "Iteration 77, loss = 0.55360296\n",
            "Iteration 78, loss = 0.55307797\n",
            "Iteration 79, loss = 0.55288566\n",
            "Iteration 80, loss = 0.55237087\n",
            "Iteration 81, loss = 0.55130515\n",
            "Iteration 82, loss = 0.55081096\n",
            "Iteration 83, loss = 0.55001458\n",
            "Iteration 84, loss = 0.54961529\n",
            "Iteration 85, loss = 0.54873951\n",
            "Iteration 86, loss = 0.54871089\n",
            "Iteration 87, loss = 0.54733409\n",
            "Iteration 88, loss = 0.54665779\n",
            "Iteration 89, loss = 0.54586178\n",
            "Iteration 90, loss = 0.54551095\n",
            "Iteration 91, loss = 0.54452280\n",
            "Iteration 92, loss = 0.54427680\n",
            "Iteration 93, loss = 0.54350611\n",
            "Iteration 94, loss = 0.54279146\n",
            "Iteration 95, loss = 0.54208088\n",
            "Iteration 96, loss = 0.54184548\n",
            "Iteration 97, loss = 0.54110570\n",
            "Iteration 98, loss = 0.54008775\n",
            "Iteration 99, loss = 0.53841339\n",
            "Iteration 100, loss = 0.53963356\n",
            "Iteration 101, loss = 0.53869125\n",
            "Iteration 102, loss = 0.53802721\n",
            "Iteration 103, loss = 0.53702380\n",
            "Iteration 104, loss = 0.53611901\n",
            "Iteration 105, loss = 0.53585005\n",
            "Iteration 106, loss = 0.53538277\n",
            "Iteration 107, loss = 0.53496619\n",
            "Iteration 108, loss = 0.53510628\n",
            "Iteration 109, loss = 0.53377174\n",
            "Iteration 110, loss = 0.53344932\n",
            "Iteration 111, loss = 0.53358303\n",
            "Iteration 112, loss = 0.53292571\n",
            "Iteration 113, loss = 0.53279433\n",
            "Iteration 114, loss = 0.53333671\n",
            "Iteration 115, loss = 0.53189145\n",
            "Iteration 116, loss = 0.53214366\n",
            "Iteration 117, loss = 0.53142898\n",
            "Iteration 118, loss = 0.53185386\n",
            "Iteration 119, loss = 0.53028811\n",
            "Iteration 120, loss = 0.53026371\n",
            "Iteration 121, loss = 0.53015510\n",
            "Iteration 122, loss = 0.52958721\n",
            "Iteration 123, loss = 0.53015761\n",
            "Iteration 124, loss = 0.52914609\n",
            "Iteration 125, loss = 0.52967265\n",
            "Iteration 126, loss = 0.52840158\n",
            "Iteration 127, loss = 0.52916427\n",
            "Iteration 128, loss = 0.52815598\n",
            "Iteration 129, loss = 0.52888343\n",
            "Iteration 130, loss = 0.52885526\n",
            "Iteration 131, loss = 0.52850294\n",
            "Iteration 132, loss = 0.52835477\n",
            "Iteration 133, loss = 0.52763380\n",
            "Iteration 134, loss = 0.52761683\n",
            "Iteration 135, loss = 0.52743064\n",
            "Iteration 136, loss = 0.52775660\n",
            "Iteration 137, loss = 0.52756055\n",
            "Iteration 138, loss = 0.52658838\n",
            "Iteration 139, loss = 0.52710823\n",
            "Iteration 140, loss = 0.52512568\n",
            "Iteration 141, loss = 0.52678173\n",
            "Iteration 142, loss = 0.52581793\n",
            "Iteration 143, loss = 0.52611891\n",
            "Iteration 144, loss = 0.52631790\n",
            "Iteration 145, loss = 0.52618906\n",
            "Iteration 146, loss = 0.52597229\n",
            "Iteration 147, loss = 0.52684436\n",
            "Iteration 148, loss = 0.52645278\n",
            "Iteration 149, loss = 0.52545129\n",
            "Iteration 150, loss = 0.52493706\n",
            "Iteration 151, loss = 0.52555830\n",
            "Iteration 152, loss = 0.52522223\n",
            "Iteration 153, loss = 0.52577826\n",
            "Iteration 154, loss = 0.52529412\n",
            "Iteration 155, loss = 0.52549190\n",
            "Iteration 156, loss = 0.52445720\n",
            "Iteration 157, loss = 0.52479081\n",
            "Iteration 158, loss = 0.52538799\n",
            "Iteration 159, loss = 0.52469005\n",
            "Iteration 160, loss = 0.52473154\n",
            "Iteration 161, loss = 0.52442717\n",
            "Iteration 162, loss = 0.52438302\n",
            "Iteration 163, loss = 0.52447750\n",
            "Iteration 164, loss = 0.52338412\n",
            "Iteration 165, loss = 0.52472663\n",
            "Iteration 166, loss = 0.52375738\n",
            "Iteration 167, loss = 0.52336566\n",
            "Iteration 168, loss = 0.52388933\n",
            "Iteration 169, loss = 0.52444205\n",
            "Iteration 170, loss = 0.52337241\n",
            "Iteration 171, loss = 0.52411324\n",
            "Iteration 172, loss = 0.52411944\n",
            "Iteration 173, loss = 0.52361087\n",
            "Iteration 174, loss = 0.52358331\n",
            "Iteration 175, loss = 0.52430652\n",
            "Iteration 176, loss = 0.52382559\n",
            "Iteration 177, loss = 0.52278314\n",
            "Iteration 178, loss = 0.52348245\n",
            "Iteration 179, loss = 0.52350547\n",
            "Iteration 180, loss = 0.52342183\n",
            "Iteration 181, loss = 0.52313648\n",
            "Iteration 182, loss = 0.52351431\n",
            "Iteration 183, loss = 0.52338787\n",
            "Iteration 184, loss = 0.52444488\n",
            "Iteration 185, loss = 0.52339249\n",
            "Iteration 186, loss = 0.52331942\n",
            "Iteration 187, loss = 0.52271922\n",
            "Iteration 188, loss = 0.52323360\n",
            "Iteration 189, loss = 0.52311506\n",
            "Iteration 190, loss = 0.52353641\n",
            "Iteration 191, loss = 0.52399785\n",
            "Iteration 192, loss = 0.52293829\n",
            "Iteration 193, loss = 0.52419581\n",
            "Iteration 194, loss = 0.52255520\n",
            "Iteration 195, loss = 0.52228002\n",
            "Iteration 196, loss = 0.52314622\n",
            "Iteration 197, loss = 0.52279479\n",
            "Iteration 198, loss = 0.52305349\n",
            "Iteration 199, loss = 0.52346269\n",
            "Iteration 200, loss = 0.52214274\n",
            "Iteration 201, loss = 0.52323195\n",
            "Iteration 202, loss = 0.52381043\n",
            "Iteration 203, loss = 0.52339785\n",
            "Iteration 204, loss = 0.52293929\n",
            "Iteration 205, loss = 0.52255089\n",
            "Iteration 206, loss = 0.52274061\n",
            "Iteration 207, loss = 0.52287719\n",
            "Iteration 208, loss = 0.52225161\n",
            "Iteration 209, loss = 0.52433233\n",
            "Iteration 210, loss = 0.52278558\n",
            "Iteration 211, loss = 0.52207087\n",
            "Iteration 212, loss = 0.52247400\n",
            "Iteration 213, loss = 0.52248520\n",
            "Iteration 214, loss = 0.52235918\n",
            "Iteration 215, loss = 0.52234160\n",
            "Iteration 216, loss = 0.52198478\n",
            "Iteration 217, loss = 0.52221630\n",
            "Iteration 218, loss = 0.52229557\n",
            "Iteration 219, loss = 0.52263971\n",
            "Iteration 220, loss = 0.52236961\n",
            "Iteration 221, loss = 0.52248614\n",
            "Iteration 222, loss = 0.52237248\n",
            "Iteration 223, loss = 0.52286716\n",
            "Iteration 224, loss = 0.52336483\n",
            "Iteration 225, loss = 0.52197865\n",
            "Iteration 226, loss = 0.52210474\n",
            "Iteration 227, loss = 0.52262063\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56811177\n",
            "Iteration 2, loss = 0.56197023\n",
            "Iteration 3, loss = 0.56333759\n",
            "Iteration 4, loss = 0.56187342\n",
            "Iteration 5, loss = 0.56190243\n",
            "Iteration 6, loss = 0.56157204\n",
            "Iteration 7, loss = 0.56254288\n",
            "Iteration 8, loss = 0.56207171\n",
            "Iteration 9, loss = 0.56155444\n",
            "Iteration 10, loss = 0.56145449\n",
            "Iteration 11, loss = 0.56230849\n",
            "Iteration 12, loss = 0.56134756\n",
            "Iteration 13, loss = 0.56142805\n",
            "Iteration 14, loss = 0.56152716\n",
            "Iteration 15, loss = 0.56147442\n",
            "Iteration 16, loss = 0.56187750\n",
            "Iteration 17, loss = 0.56126205\n",
            "Iteration 18, loss = 0.56131167\n",
            "Iteration 19, loss = 0.56170834\n",
            "Iteration 20, loss = 0.56138672\n",
            "Iteration 21, loss = 0.56090009\n",
            "Iteration 22, loss = 0.56156269\n",
            "Iteration 23, loss = 0.56139236\n",
            "Iteration 24, loss = 0.55986325\n",
            "Iteration 25, loss = 0.56107133\n",
            "Iteration 26, loss = 0.56040598\n",
            "Iteration 27, loss = 0.56063488\n",
            "Iteration 28, loss = 0.56012339\n",
            "Iteration 29, loss = 0.56119729\n",
            "Iteration 30, loss = 0.56075190\n",
            "Iteration 31, loss = 0.56079811\n",
            "Iteration 32, loss = 0.56121363\n",
            "Iteration 33, loss = 0.56126761\n",
            "Iteration 34, loss = 0.56014389\n",
            "Iteration 35, loss = 0.56033874\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56359427\n",
            "Iteration 2, loss = 0.56281561\n",
            "Iteration 3, loss = 0.56091720\n",
            "Iteration 4, loss = 0.56126011\n",
            "Iteration 5, loss = 0.56083120\n",
            "Iteration 6, loss = 0.56055290\n",
            "Iteration 7, loss = 0.56116523\n",
            "Iteration 8, loss = 0.56181711\n",
            "Iteration 9, loss = 0.56129180\n",
            "Iteration 10, loss = 0.56092593\n",
            "Iteration 11, loss = 0.56111888\n",
            "Iteration 12, loss = 0.56111048\n",
            "Iteration 13, loss = 0.56169166\n",
            "Iteration 14, loss = 0.56144697\n",
            "Iteration 15, loss = 0.56056405\n",
            "Iteration 16, loss = 0.56100376\n",
            "Iteration 17, loss = 0.56112856\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56763670\n",
            "Iteration 2, loss = 0.56785510\n",
            "Iteration 3, loss = 0.56836759\n",
            "Iteration 4, loss = 0.56779440\n",
            "Iteration 5, loss = 0.56839475\n",
            "Iteration 6, loss = 0.56793117\n",
            "Iteration 7, loss = 0.56732431\n",
            "Iteration 8, loss = 0.56770529\n",
            "Iteration 9, loss = 0.56786652\n",
            "Iteration 10, loss = 0.56771273\n",
            "Iteration 11, loss = 0.56868927\n",
            "Iteration 12, loss = 0.56848328\n",
            "Iteration 13, loss = 0.56683918\n",
            "Iteration 14, loss = 0.56783405\n",
            "Iteration 15, loss = 0.56714671\n",
            "Iteration 16, loss = 0.56721546\n",
            "Iteration 17, loss = 0.56712014\n",
            "Iteration 18, loss = 0.56674412\n",
            "Iteration 19, loss = 0.56703220\n",
            "Iteration 20, loss = 0.56725632\n",
            "Iteration 21, loss = 0.56655298\n",
            "Iteration 22, loss = 0.56634905\n",
            "Iteration 23, loss = 0.56646535\n",
            "Iteration 24, loss = 0.56651343\n",
            "Iteration 25, loss = 0.56681676\n",
            "Iteration 26, loss = 0.56642052\n",
            "Iteration 27, loss = 0.56619285\n",
            "Iteration 28, loss = 0.56723874\n",
            "Iteration 29, loss = 0.56634476\n",
            "Iteration 30, loss = 0.56640862\n",
            "Iteration 31, loss = 0.56629839\n",
            "Iteration 32, loss = 0.56636070\n",
            "Iteration 33, loss = 0.56608707\n",
            "Iteration 34, loss = 0.56582620\n",
            "Iteration 35, loss = 0.56610465\n",
            "Iteration 36, loss = 0.56588374\n",
            "Iteration 37, loss = 0.56477078\n",
            "Iteration 38, loss = 0.56557723\n",
            "Iteration 39, loss = 0.56496603\n",
            "Iteration 40, loss = 0.56599133\n",
            "Iteration 41, loss = 0.56532740\n",
            "Iteration 42, loss = 0.56496413\n",
            "Iteration 43, loss = 0.56544036\n",
            "Iteration 44, loss = 0.56481370\n",
            "Iteration 45, loss = 0.56514038\n",
            "Iteration 46, loss = 0.56504747\n",
            "Iteration 47, loss = 0.56481369\n",
            "Iteration 48, loss = 0.56381373\n",
            "Iteration 49, loss = 0.56430213\n",
            "Iteration 50, loss = 0.56400072\n",
            "Iteration 51, loss = 0.56287542\n",
            "Iteration 52, loss = 0.56287359\n",
            "Iteration 53, loss = 0.56300292\n",
            "Iteration 54, loss = 0.56295069\n",
            "Iteration 55, loss = 0.56221751\n",
            "Iteration 56, loss = 0.56242574\n",
            "Iteration 57, loss = 0.56124432\n",
            "Iteration 58, loss = 0.56125482\n",
            "Iteration 59, loss = 0.56122393\n",
            "Iteration 60, loss = 0.56013932\n",
            "Iteration 61, loss = 0.56048359\n",
            "Iteration 62, loss = 0.56063657\n",
            "Iteration 63, loss = 0.55996509\n",
            "Iteration 64, loss = 0.55957939\n",
            "Iteration 65, loss = 0.55904224\n",
            "Iteration 66, loss = 0.55947918\n",
            "Iteration 67, loss = 0.55818824\n",
            "Iteration 68, loss = 0.55663902\n",
            "Iteration 69, loss = 0.55702652\n",
            "Iteration 70, loss = 0.55678985\n",
            "Iteration 71, loss = 0.55538324\n",
            "Iteration 72, loss = 0.55568062\n",
            "Iteration 73, loss = 0.55487857\n",
            "Iteration 74, loss = 0.55369321\n",
            "Iteration 75, loss = 0.55328759\n",
            "Iteration 76, loss = 0.55291826\n",
            "Iteration 77, loss = 0.55211258\n",
            "Iteration 78, loss = 0.55110251\n",
            "Iteration 79, loss = 0.55080803\n",
            "Iteration 80, loss = 0.54990792\n",
            "Iteration 81, loss = 0.54847391\n",
            "Iteration 82, loss = 0.54845684\n",
            "Iteration 83, loss = 0.54707537\n",
            "Iteration 84, loss = 0.54643206\n",
            "Iteration 85, loss = 0.54554520\n",
            "Iteration 86, loss = 0.54506627\n",
            "Iteration 87, loss = 0.54319439\n",
            "Iteration 88, loss = 0.54296456\n",
            "Iteration 89, loss = 0.54232548\n",
            "Iteration 90, loss = 0.54185590\n",
            "Iteration 91, loss = 0.54158661\n",
            "Iteration 92, loss = 0.53973059\n",
            "Iteration 93, loss = 0.53915415\n",
            "Iteration 94, loss = 0.53832022\n",
            "Iteration 95, loss = 0.53824938\n",
            "Iteration 96, loss = 0.53717647\n",
            "Iteration 97, loss = 0.53656226\n",
            "Iteration 98, loss = 0.53592110\n",
            "Iteration 99, loss = 0.53546331\n",
            "Iteration 100, loss = 0.53597409\n",
            "Iteration 101, loss = 0.53492814\n",
            "Iteration 102, loss = 0.53451302\n",
            "Iteration 103, loss = 0.53340655\n",
            "Iteration 104, loss = 0.53342162\n",
            "Iteration 105, loss = 0.53354940\n",
            "Iteration 106, loss = 0.53270364\n",
            "Iteration 107, loss = 0.53283709\n",
            "Iteration 108, loss = 0.53182071\n",
            "Iteration 109, loss = 0.53139662\n",
            "Iteration 110, loss = 0.53142103\n",
            "Iteration 111, loss = 0.53153178\n",
            "Iteration 112, loss = 0.53047344\n",
            "Iteration 113, loss = 0.53110636\n",
            "Iteration 114, loss = 0.53044274\n",
            "Iteration 115, loss = 0.53013224\n",
            "Iteration 116, loss = 0.52983815\n",
            "Iteration 117, loss = 0.52921508\n",
            "Iteration 118, loss = 0.52823132\n",
            "Iteration 119, loss = 0.52938562\n",
            "Iteration 120, loss = 0.52813202\n",
            "Iteration 121, loss = 0.52822614\n",
            "Iteration 122, loss = 0.52867150\n",
            "Iteration 123, loss = 0.52841123\n",
            "Iteration 124, loss = 0.52862323\n",
            "Iteration 125, loss = 0.52807522\n",
            "Iteration 126, loss = 0.52717234\n",
            "Iteration 127, loss = 0.52700648\n",
            "Iteration 128, loss = 0.52801705\n",
            "Iteration 129, loss = 0.52723581\n",
            "Iteration 130, loss = 0.52735016\n",
            "Iteration 131, loss = 0.52661126\n",
            "Iteration 132, loss = 0.52805118\n",
            "Iteration 133, loss = 0.52628378\n",
            "Iteration 134, loss = 0.52613614\n",
            "Iteration 135, loss = 0.52506081\n",
            "Iteration 136, loss = 0.52650044\n",
            "Iteration 137, loss = 0.52681347\n",
            "Iteration 138, loss = 0.52631701\n",
            "Iteration 139, loss = 0.52554703\n",
            "Iteration 140, loss = 0.52641712\n",
            "Iteration 141, loss = 0.52584657\n",
            "Iteration 142, loss = 0.52551026\n",
            "Iteration 143, loss = 0.52539697\n",
            "Iteration 144, loss = 0.52545906\n",
            "Iteration 145, loss = 0.52517671\n",
            "Iteration 146, loss = 0.52557505\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56846162\n",
            "Iteration 2, loss = 0.56463726\n",
            "Iteration 3, loss = 0.56530380\n",
            "Iteration 4, loss = 0.56464582\n",
            "Iteration 5, loss = 0.56571745\n",
            "Iteration 6, loss = 0.56590188\n",
            "Iteration 7, loss = 0.56559017\n",
            "Iteration 8, loss = 0.56455471\n",
            "Iteration 9, loss = 0.56495892\n",
            "Iteration 10, loss = 0.56527447\n",
            "Iteration 11, loss = 0.56420170\n",
            "Iteration 12, loss = 0.56503933\n",
            "Iteration 13, loss = 0.56481439\n",
            "Iteration 14, loss = 0.56450951\n",
            "Iteration 15, loss = 0.56578074\n",
            "Iteration 16, loss = 0.56447777\n",
            "Iteration 17, loss = 0.56414795\n",
            "Iteration 18, loss = 0.56447170\n",
            "Iteration 19, loss = 0.56421961\n",
            "Iteration 20, loss = 0.56361005\n",
            "Iteration 21, loss = 0.56432651\n",
            "Iteration 22, loss = 0.56410572\n",
            "Iteration 23, loss = 0.56360656\n",
            "Iteration 24, loss = 0.56414514\n",
            "Iteration 25, loss = 0.56432530\n",
            "Iteration 26, loss = 0.56268937\n",
            "Iteration 27, loss = 0.56359039\n",
            "Iteration 28, loss = 0.56360578\n",
            "Iteration 29, loss = 0.56428824\n",
            "Iteration 30, loss = 0.56408988\n",
            "Iteration 31, loss = 0.56361290\n",
            "Iteration 32, loss = 0.56379452\n",
            "Iteration 33, loss = 0.56350464\n",
            "Iteration 34, loss = 0.56350565\n",
            "Iteration 35, loss = 0.56264054\n",
            "Iteration 36, loss = 0.56334571\n",
            "Iteration 37, loss = 0.56344527\n",
            "Iteration 38, loss = 0.56336990\n",
            "Iteration 39, loss = 0.56265155\n",
            "Iteration 40, loss = 0.56304537\n",
            "Iteration 41, loss = 0.56245683\n",
            "Iteration 42, loss = 0.56240020\n",
            "Iteration 43, loss = 0.56176649\n",
            "Iteration 44, loss = 0.56260512\n",
            "Iteration 45, loss = 0.56169534\n",
            "Iteration 46, loss = 0.56116873\n",
            "Iteration 47, loss = 0.56158020\n",
            "Iteration 48, loss = 0.56000548\n",
            "Iteration 49, loss = 0.56094309\n",
            "Iteration 50, loss = 0.56021135\n",
            "Iteration 51, loss = 0.56112040\n",
            "Iteration 52, loss = 0.55972624\n",
            "Iteration 53, loss = 0.56074284\n",
            "Iteration 54, loss = 0.56060021\n",
            "Iteration 55, loss = 0.55946151\n",
            "Iteration 56, loss = 0.55996401\n",
            "Iteration 57, loss = 0.55980162\n",
            "Iteration 58, loss = 0.55898999\n",
            "Iteration 59, loss = 0.55877712\n",
            "Iteration 60, loss = 0.55832080\n",
            "Iteration 61, loss = 0.55839088\n",
            "Iteration 62, loss = 0.55827290\n",
            "Iteration 63, loss = 0.55711664\n",
            "Iteration 64, loss = 0.55692386\n",
            "Iteration 65, loss = 0.55628068\n",
            "Iteration 66, loss = 0.55574912\n",
            "Iteration 67, loss = 0.55519567\n",
            "Iteration 68, loss = 0.55554743\n",
            "Iteration 69, loss = 0.55497276\n",
            "Iteration 70, loss = 0.55431085\n",
            "Iteration 71, loss = 0.55328902\n",
            "Iteration 72, loss = 0.55257023\n",
            "Iteration 73, loss = 0.55243414\n",
            "Iteration 74, loss = 0.55141684\n",
            "Iteration 75, loss = 0.55092320\n",
            "Iteration 76, loss = 0.54918959\n",
            "Iteration 77, loss = 0.54961475\n",
            "Iteration 78, loss = 0.54892014\n",
            "Iteration 79, loss = 0.54815967\n",
            "Iteration 80, loss = 0.54597211\n",
            "Iteration 81, loss = 0.54603789\n",
            "Iteration 82, loss = 0.54588204\n",
            "Iteration 83, loss = 0.54546921\n",
            "Iteration 84, loss = 0.54332875\n",
            "Iteration 85, loss = 0.54399180\n",
            "Iteration 86, loss = 0.54205618\n",
            "Iteration 87, loss = 0.54175214\n",
            "Iteration 88, loss = 0.54116696\n",
            "Iteration 89, loss = 0.54121249\n",
            "Iteration 90, loss = 0.53956273\n",
            "Iteration 91, loss = 0.53841942\n",
            "Iteration 92, loss = 0.53873004\n",
            "Iteration 93, loss = 0.53777407\n",
            "Iteration 94, loss = 0.53642988\n",
            "Iteration 95, loss = 0.53592793\n",
            "Iteration 96, loss = 0.53587916\n",
            "Iteration 97, loss = 0.53461687\n",
            "Iteration 98, loss = 0.53391778\n",
            "Iteration 99, loss = 0.53393534\n",
            "Iteration 100, loss = 0.53318154\n",
            "Iteration 101, loss = 0.53320361\n",
            "Iteration 102, loss = 0.53275048\n",
            "Iteration 103, loss = 0.53203503\n",
            "Iteration 104, loss = 0.53121175\n",
            "Iteration 105, loss = 0.53199858\n",
            "Iteration 106, loss = 0.53095554\n",
            "Iteration 107, loss = 0.52999756\n",
            "Iteration 108, loss = 0.53050474\n",
            "Iteration 109, loss = 0.53017667\n",
            "Iteration 110, loss = 0.52928905\n",
            "Iteration 111, loss = 0.52819338\n",
            "Iteration 112, loss = 0.52810708\n",
            "Iteration 113, loss = 0.52852257\n",
            "Iteration 114, loss = 0.52913502\n",
            "Iteration 115, loss = 0.52774238\n",
            "Iteration 116, loss = 0.52770065\n",
            "Iteration 117, loss = 0.52722515\n",
            "Iteration 118, loss = 0.52785064\n",
            "Iteration 119, loss = 0.52710989\n",
            "Iteration 120, loss = 0.52696888\n",
            "Iteration 121, loss = 0.52720076\n",
            "Iteration 122, loss = 0.52625824\n",
            "Iteration 123, loss = 0.52678728\n",
            "Iteration 124, loss = 0.52676747\n",
            "Iteration 125, loss = 0.52585016\n",
            "Iteration 126, loss = 0.52616781\n",
            "Iteration 127, loss = 0.52582776\n",
            "Iteration 128, loss = 0.52501389\n",
            "Iteration 129, loss = 0.52602529\n",
            "Iteration 130, loss = 0.52372525\n",
            "Iteration 131, loss = 0.52491544\n",
            "Iteration 132, loss = 0.52507783\n",
            "Iteration 133, loss = 0.52458698\n",
            "Iteration 134, loss = 0.52572703\n",
            "Iteration 135, loss = 0.52467670\n",
            "Iteration 136, loss = 0.52389935\n",
            "Iteration 137, loss = 0.52418710\n",
            "Iteration 138, loss = 0.52396892\n",
            "Iteration 139, loss = 0.52459585\n",
            "Iteration 140, loss = 0.52457039\n",
            "Iteration 141, loss = 0.52379540\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56617118\n",
            "Iteration 2, loss = 0.55756775\n",
            "Iteration 3, loss = 0.55743872\n",
            "Iteration 4, loss = 0.55834460\n",
            "Iteration 5, loss = 0.55755020\n",
            "Iteration 6, loss = 0.55752238\n",
            "Iteration 7, loss = 0.55752509\n",
            "Iteration 8, loss = 0.55700807\n",
            "Iteration 9, loss = 0.55712242\n",
            "Iteration 10, loss = 0.55708959\n",
            "Iteration 11, loss = 0.55757330\n",
            "Iteration 12, loss = 0.55722778\n",
            "Iteration 13, loss = 0.55700120\n",
            "Iteration 14, loss = 0.55627418\n",
            "Iteration 15, loss = 0.55715673\n",
            "Iteration 16, loss = 0.55767967\n",
            "Iteration 17, loss = 0.55608887\n",
            "Iteration 18, loss = 0.55678267\n",
            "Iteration 19, loss = 0.55615300\n",
            "Iteration 20, loss = 0.55652520\n",
            "Iteration 21, loss = 0.55664002\n",
            "Iteration 22, loss = 0.55656899\n",
            "Iteration 23, loss = 0.55687093\n",
            "Iteration 24, loss = 0.55662040\n",
            "Iteration 25, loss = 0.55631120\n",
            "Iteration 26, loss = 0.55578372\n",
            "Iteration 27, loss = 0.55633977\n",
            "Iteration 28, loss = 0.55700336\n",
            "Iteration 29, loss = 0.55689121\n",
            "Iteration 30, loss = 0.55604027\n",
            "Iteration 31, loss = 0.55533244\n",
            "Iteration 32, loss = 0.55657867\n",
            "Iteration 33, loss = 0.55592588\n",
            "Iteration 34, loss = 0.55621657\n",
            "Iteration 35, loss = 0.55579949\n",
            "Iteration 36, loss = 0.55556001\n",
            "Iteration 37, loss = 0.55553537\n",
            "Iteration 38, loss = 0.55582800\n",
            "Iteration 39, loss = 0.55521361\n",
            "Iteration 40, loss = 0.55609100\n",
            "Iteration 41, loss = 0.55597377\n",
            "Iteration 42, loss = 0.55591641\n",
            "Iteration 43, loss = 0.55546799\n",
            "Iteration 44, loss = 0.55527253\n",
            "Iteration 45, loss = 0.55459903\n",
            "Iteration 46, loss = 0.55529169\n",
            "Iteration 47, loss = 0.55524743\n",
            "Iteration 48, loss = 0.55502884\n",
            "Iteration 49, loss = 0.55449089\n",
            "Iteration 50, loss = 0.55518753\n",
            "Iteration 51, loss = 0.55395410\n",
            "Iteration 52, loss = 0.55456336\n",
            "Iteration 53, loss = 0.55457394\n",
            "Iteration 54, loss = 0.55408057\n",
            "Iteration 55, loss = 0.55413771\n",
            "Iteration 56, loss = 0.55422128\n",
            "Iteration 57, loss = 0.55348616\n",
            "Iteration 58, loss = 0.55358774\n",
            "Iteration 59, loss = 0.55371114\n",
            "Iteration 60, loss = 0.55260102\n",
            "Iteration 61, loss = 0.55255120\n",
            "Iteration 62, loss = 0.55311889\n",
            "Iteration 63, loss = 0.55317753\n",
            "Iteration 64, loss = 0.55272736\n",
            "Iteration 65, loss = 0.55311563\n",
            "Iteration 66, loss = 0.55292623\n",
            "Iteration 67, loss = 0.55221137\n",
            "Iteration 68, loss = 0.55161954\n",
            "Iteration 69, loss = 0.54992721\n",
            "Iteration 70, loss = 0.55181354\n",
            "Iteration 71, loss = 0.55077102\n",
            "Iteration 72, loss = 0.55078602\n",
            "Iteration 73, loss = 0.55065943\n",
            "Iteration 74, loss = 0.54984541\n",
            "Iteration 75, loss = 0.55015352\n",
            "Iteration 76, loss = 0.54871972\n",
            "Iteration 77, loss = 0.54904035\n",
            "Iteration 78, loss = 0.54764705\n",
            "Iteration 79, loss = 0.54868672\n",
            "Iteration 80, loss = 0.54784029\n",
            "Iteration 81, loss = 0.54699278\n",
            "Iteration 82, loss = 0.54634150\n",
            "Iteration 83, loss = 0.54618000\n",
            "Iteration 84, loss = 0.54580662\n",
            "Iteration 85, loss = 0.54541755\n",
            "Iteration 86, loss = 0.54363189\n",
            "Iteration 87, loss = 0.54373781\n",
            "Iteration 88, loss = 0.54302420\n",
            "Iteration 89, loss = 0.54250578\n",
            "Iteration 90, loss = 0.54209843\n",
            "Iteration 91, loss = 0.54154926\n",
            "Iteration 92, loss = 0.54183486\n",
            "Iteration 93, loss = 0.53940885\n",
            "Iteration 94, loss = 0.53899222\n",
            "Iteration 95, loss = 0.53874021\n",
            "Iteration 96, loss = 0.53825410\n",
            "Iteration 97, loss = 0.53665653\n",
            "Iteration 98, loss = 0.53559108\n",
            "Iteration 99, loss = 0.53590429\n",
            "Iteration 100, loss = 0.53504258\n",
            "Iteration 101, loss = 0.53451682\n",
            "Iteration 102, loss = 0.53313950\n",
            "Iteration 103, loss = 0.53273856\n",
            "Iteration 104, loss = 0.53192389\n",
            "Iteration 105, loss = 0.53143320\n",
            "Iteration 106, loss = 0.53075949\n",
            "Iteration 107, loss = 0.53027769\n",
            "Iteration 108, loss = 0.52944568\n",
            "Iteration 109, loss = 0.52844727\n",
            "Iteration 110, loss = 0.52852233\n",
            "Iteration 111, loss = 0.52845808\n",
            "Iteration 112, loss = 0.52721798\n",
            "Iteration 113, loss = 0.52719349\n",
            "Iteration 114, loss = 0.52673032\n",
            "Iteration 115, loss = 0.52549391\n",
            "Iteration 116, loss = 0.52561153\n",
            "Iteration 117, loss = 0.52560976\n",
            "Iteration 118, loss = 0.52448751\n",
            "Iteration 119, loss = 0.52451195\n",
            "Iteration 120, loss = 0.52375824\n",
            "Iteration 121, loss = 0.52403405\n",
            "Iteration 122, loss = 0.52267953\n",
            "Iteration 123, loss = 0.52396697\n",
            "Iteration 124, loss = 0.52350797\n",
            "Iteration 125, loss = 0.52249250\n",
            "Iteration 126, loss = 0.52250774\n",
            "Iteration 127, loss = 0.52175036\n",
            "Iteration 128, loss = 0.52077666\n",
            "Iteration 129, loss = 0.52169523\n",
            "Iteration 130, loss = 0.52032187\n",
            "Iteration 131, loss = 0.52128889\n",
            "Iteration 132, loss = 0.52114643\n",
            "Iteration 133, loss = 0.52006299\n",
            "Iteration 134, loss = 0.52042794\n",
            "Iteration 135, loss = 0.52021646\n",
            "Iteration 136, loss = 0.51978853\n",
            "Iteration 137, loss = 0.51942818\n",
            "Iteration 138, loss = 0.51944660\n",
            "Iteration 139, loss = 0.51961706\n",
            "Iteration 140, loss = 0.51889911\n",
            "Iteration 141, loss = 0.51831978\n",
            "Iteration 142, loss = 0.51823120\n",
            "Iteration 143, loss = 0.51874365\n",
            "Iteration 144, loss = 0.51770672\n",
            "Iteration 145, loss = 0.51773555\n",
            "Iteration 146, loss = 0.51837863\n",
            "Iteration 147, loss = 0.51680778\n",
            "Iteration 148, loss = 0.51729259\n",
            "Iteration 149, loss = 0.51701689\n",
            "Iteration 150, loss = 0.51746145\n",
            "Iteration 151, loss = 0.51741955\n",
            "Iteration 152, loss = 0.51675926\n",
            "Iteration 153, loss = 0.51682905\n",
            "Iteration 154, loss = 0.51657954\n",
            "Iteration 155, loss = 0.51600445\n",
            "Iteration 156, loss = 0.51668791\n",
            "Iteration 157, loss = 0.51609560\n",
            "Iteration 158, loss = 0.51613198\n",
            "Iteration 159, loss = 0.51556647\n",
            "Iteration 160, loss = 0.51608207\n",
            "Iteration 161, loss = 0.51611051\n",
            "Iteration 162, loss = 0.51617199\n",
            "Iteration 163, loss = 0.51586870\n",
            "Iteration 164, loss = 0.51487791\n",
            "Iteration 165, loss = 0.51601351\n",
            "Iteration 166, loss = 0.51549318\n",
            "Iteration 167, loss = 0.51582214\n",
            "Iteration 168, loss = 0.51487517\n",
            "Iteration 169, loss = 0.51552021\n",
            "Iteration 170, loss = 0.51473407\n",
            "Iteration 171, loss = 0.51533718\n",
            "Iteration 172, loss = 0.51520467\n",
            "Iteration 173, loss = 0.51483829\n",
            "Iteration 174, loss = 0.51477473\n",
            "Iteration 175, loss = 0.51434407\n",
            "Iteration 176, loss = 0.51528208\n",
            "Iteration 177, loss = 0.51513236\n",
            "Iteration 178, loss = 0.51532328\n",
            "Iteration 179, loss = 0.51431388\n",
            "Iteration 180, loss = 0.51460103\n",
            "Iteration 181, loss = 0.51431432\n",
            "Iteration 182, loss = 0.51466187\n",
            "Iteration 183, loss = 0.51423425\n",
            "Iteration 184, loss = 0.51422640\n",
            "Iteration 185, loss = 0.51414373\n",
            "Iteration 186, loss = 0.51453048\n",
            "Iteration 187, loss = 0.51467364\n",
            "Iteration 188, loss = 0.51478450\n",
            "Iteration 189, loss = 0.51378358\n",
            "Iteration 190, loss = 0.51414786\n",
            "Iteration 191, loss = 0.51401167\n",
            "Iteration 192, loss = 0.51385856\n",
            "Iteration 193, loss = 0.51342396\n",
            "Iteration 194, loss = 0.51419468\n",
            "Iteration 195, loss = 0.51463254\n",
            "Iteration 196, loss = 0.51274861\n",
            "Iteration 197, loss = 0.51376807\n",
            "Iteration 198, loss = 0.51362583\n",
            "Iteration 199, loss = 0.51409904\n",
            "Iteration 200, loss = 0.51377295\n",
            "Iteration 201, loss = 0.51403264\n",
            "Iteration 202, loss = 0.51476037\n",
            "Iteration 203, loss = 0.51379373\n",
            "Iteration 204, loss = 0.51253484\n",
            "Iteration 205, loss = 0.51327857\n",
            "Iteration 206, loss = 0.51338597\n",
            "Iteration 207, loss = 0.51353187\n",
            "Iteration 208, loss = 0.51375812\n",
            "Iteration 209, loss = 0.51367646\n",
            "Iteration 210, loss = 0.51319755\n",
            "Iteration 211, loss = 0.51333694\n",
            "Iteration 212, loss = 0.51261287\n",
            "Iteration 213, loss = 0.51343665\n",
            "Iteration 214, loss = 0.51361633\n",
            "Iteration 215, loss = 0.51426873\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57595708\n",
            "Iteration 2, loss = 0.56207851\n",
            "Iteration 3, loss = 0.56279175\n",
            "Iteration 4, loss = 0.56213585\n",
            "Iteration 5, loss = 0.56247128\n",
            "Iteration 6, loss = 0.56203757\n",
            "Iteration 7, loss = 0.56196336\n",
            "Iteration 8, loss = 0.56162728\n",
            "Iteration 9, loss = 0.56300691\n",
            "Iteration 10, loss = 0.56218248\n",
            "Iteration 11, loss = 0.56277041\n",
            "Iteration 12, loss = 0.56163108\n",
            "Iteration 13, loss = 0.56207429\n",
            "Iteration 14, loss = 0.56181732\n",
            "Iteration 15, loss = 0.56130984\n",
            "Iteration 16, loss = 0.56081106\n",
            "Iteration 17, loss = 0.56176278\n",
            "Iteration 18, loss = 0.56072515\n",
            "Iteration 19, loss = 0.56171911\n",
            "Iteration 20, loss = 0.56185467\n",
            "Iteration 21, loss = 0.56165615\n",
            "Iteration 22, loss = 0.56210934\n",
            "Iteration 23, loss = 0.56182006\n",
            "Iteration 24, loss = 0.56122758\n",
            "Iteration 25, loss = 0.56158972\n",
            "Iteration 26, loss = 0.56128004\n",
            "Iteration 27, loss = 0.56105966\n",
            "Iteration 28, loss = 0.56140514\n",
            "Iteration 29, loss = 0.56112076\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56681396\n",
            "Iteration 2, loss = 0.55979336\n",
            "Iteration 3, loss = 0.56075174\n",
            "Iteration 4, loss = 0.55912944\n",
            "Iteration 5, loss = 0.55926917\n",
            "Iteration 6, loss = 0.55985097\n",
            "Iteration 7, loss = 0.55975165\n",
            "Iteration 8, loss = 0.55964891\n",
            "Iteration 9, loss = 0.55932121\n",
            "Iteration 10, loss = 0.55993309\n",
            "Iteration 11, loss = 0.55974747\n",
            "Iteration 12, loss = 0.56026044\n",
            "Iteration 13, loss = 0.55826048\n",
            "Iteration 14, loss = 0.55931323\n",
            "Iteration 15, loss = 0.55900048\n",
            "Iteration 16, loss = 0.55986048\n",
            "Iteration 17, loss = 0.55786259\n",
            "Iteration 18, loss = 0.56027250\n",
            "Iteration 19, loss = 0.55886127\n",
            "Iteration 20, loss = 0.55853597\n",
            "Iteration 21, loss = 0.55953787\n",
            "Iteration 22, loss = 0.55999683\n",
            "Iteration 23, loss = 0.55916426\n",
            "Iteration 24, loss = 0.55910282\n",
            "Iteration 25, loss = 0.55909547\n",
            "Iteration 26, loss = 0.55925925\n",
            "Iteration 27, loss = 0.55856447\n",
            "Iteration 28, loss = 0.55902667\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57964804\n",
            "Iteration 2, loss = 0.56285295\n",
            "Iteration 3, loss = 0.56314098\n",
            "Iteration 4, loss = 0.56265629\n",
            "Iteration 5, loss = 0.56308439\n",
            "Iteration 6, loss = 0.56335035\n",
            "Iteration 7, loss = 0.56287038\n",
            "Iteration 8, loss = 0.56320023\n",
            "Iteration 9, loss = 0.56257841\n",
            "Iteration 10, loss = 0.56226513\n",
            "Iteration 11, loss = 0.56239324\n",
            "Iteration 12, loss = 0.56313165\n",
            "Iteration 13, loss = 0.56324334\n",
            "Iteration 14, loss = 0.56242038\n",
            "Iteration 15, loss = 0.56229430\n",
            "Iteration 16, loss = 0.56223941\n",
            "Iteration 17, loss = 0.56245290\n",
            "Iteration 18, loss = 0.56161285\n",
            "Iteration 19, loss = 0.56204737\n",
            "Iteration 20, loss = 0.56222892\n",
            "Iteration 21, loss = 0.56193981\n",
            "Iteration 22, loss = 0.56185377\n",
            "Iteration 23, loss = 0.56238676\n",
            "Iteration 24, loss = 0.56166034\n",
            "Iteration 25, loss = 0.56160835\n",
            "Iteration 26, loss = 0.56148750\n",
            "Iteration 27, loss = 0.56178439\n",
            "Iteration 28, loss = 0.56153711\n",
            "Iteration 29, loss = 0.56104891\n",
            "Iteration 30, loss = 0.56148303\n",
            "Iteration 31, loss = 0.56209839\n",
            "Iteration 32, loss = 0.56100805\n",
            "Iteration 33, loss = 0.56114647\n",
            "Iteration 34, loss = 0.56038839\n",
            "Iteration 35, loss = 0.56130724\n",
            "Iteration 36, loss = 0.56055197\n",
            "Iteration 37, loss = 0.56137235\n",
            "Iteration 38, loss = 0.56053038\n",
            "Iteration 39, loss = 0.56076197\n",
            "Iteration 40, loss = 0.55995339\n",
            "Iteration 41, loss = 0.56031874\n",
            "Iteration 42, loss = 0.56036784\n",
            "Iteration 43, loss = 0.55995304\n",
            "Iteration 44, loss = 0.56045241\n",
            "Iteration 45, loss = 0.55934099\n",
            "Iteration 46, loss = 0.55974638\n",
            "Iteration 47, loss = 0.55936021\n",
            "Iteration 48, loss = 0.55924166\n",
            "Iteration 49, loss = 0.55923802\n",
            "Iteration 50, loss = 0.55942449\n",
            "Iteration 51, loss = 0.55841499\n",
            "Iteration 52, loss = 0.55815764\n",
            "Iteration 53, loss = 0.55791607\n",
            "Iteration 54, loss = 0.55820584\n",
            "Iteration 55, loss = 0.55656573\n",
            "Iteration 56, loss = 0.55721882\n",
            "Iteration 57, loss = 0.55639155\n",
            "Iteration 58, loss = 0.55757085\n",
            "Iteration 59, loss = 0.55692665\n",
            "Iteration 60, loss = 0.55671892\n",
            "Iteration 61, loss = 0.55597495\n",
            "Iteration 62, loss = 0.55634603\n",
            "Iteration 63, loss = 0.55618648\n",
            "Iteration 64, loss = 0.55500756\n",
            "Iteration 65, loss = 0.55387723\n",
            "Iteration 66, loss = 0.55437705\n",
            "Iteration 67, loss = 0.55388680\n",
            "Iteration 68, loss = 0.55281607\n",
            "Iteration 69, loss = 0.55357644\n",
            "Iteration 70, loss = 0.55222902\n",
            "Iteration 71, loss = 0.55213901\n",
            "Iteration 72, loss = 0.55118649\n",
            "Iteration 73, loss = 0.55100081\n",
            "Iteration 74, loss = 0.55018425\n",
            "Iteration 75, loss = 0.54895075\n",
            "Iteration 76, loss = 0.54862417\n",
            "Iteration 77, loss = 0.54835031\n",
            "Iteration 78, loss = 0.54744812\n",
            "Iteration 79, loss = 0.54620356\n",
            "Iteration 80, loss = 0.54654858\n",
            "Iteration 81, loss = 0.54545813\n",
            "Iteration 82, loss = 0.54500754\n",
            "Iteration 83, loss = 0.54396334\n",
            "Iteration 84, loss = 0.54222795\n",
            "Iteration 85, loss = 0.54280731\n",
            "Iteration 86, loss = 0.54125855\n",
            "Iteration 87, loss = 0.54169766\n",
            "Iteration 88, loss = 0.53996741\n",
            "Iteration 89, loss = 0.53915698\n",
            "Iteration 90, loss = 0.53894990\n",
            "Iteration 91, loss = 0.53759966\n",
            "Iteration 92, loss = 0.53711350\n",
            "Iteration 93, loss = 0.53642290\n",
            "Iteration 94, loss = 0.53608800\n",
            "Iteration 95, loss = 0.53528222\n",
            "Iteration 96, loss = 0.53442828\n",
            "Iteration 97, loss = 0.53416389\n",
            "Iteration 98, loss = 0.53278657\n",
            "Iteration 99, loss = 0.53262857\n",
            "Iteration 100, loss = 0.53168084\n",
            "Iteration 101, loss = 0.53137800\n",
            "Iteration 102, loss = 0.53083764\n",
            "Iteration 103, loss = 0.53109878\n",
            "Iteration 104, loss = 0.52949248\n",
            "Iteration 105, loss = 0.52962381\n",
            "Iteration 106, loss = 0.52920309\n",
            "Iteration 107, loss = 0.52796310\n",
            "Iteration 108, loss = 0.52888817\n",
            "Iteration 109, loss = 0.52805634\n",
            "Iteration 110, loss = 0.52738190\n",
            "Iteration 111, loss = 0.52727661\n",
            "Iteration 112, loss = 0.52667663\n",
            "Iteration 113, loss = 0.52669191\n",
            "Iteration 114, loss = 0.52745886\n",
            "Iteration 115, loss = 0.52636596\n",
            "Iteration 116, loss = 0.52534898\n",
            "Iteration 117, loss = 0.52434192\n",
            "Iteration 118, loss = 0.52568462\n",
            "Iteration 119, loss = 0.52611006\n",
            "Iteration 120, loss = 0.52484717\n",
            "Iteration 121, loss = 0.52516894\n",
            "Iteration 122, loss = 0.52445536\n",
            "Iteration 123, loss = 0.52397863\n",
            "Iteration 124, loss = 0.52405670\n",
            "Iteration 125, loss = 0.52347219\n",
            "Iteration 126, loss = 0.52312336\n",
            "Iteration 127, loss = 0.52379385\n",
            "Iteration 128, loss = 0.52275724\n",
            "Iteration 129, loss = 0.52291084\n",
            "Iteration 130, loss = 0.52257114\n",
            "Iteration 131, loss = 0.52217102\n",
            "Iteration 132, loss = 0.52205687\n",
            "Iteration 133, loss = 0.52162374\n",
            "Iteration 134, loss = 0.52120557\n",
            "Iteration 135, loss = 0.52145547\n",
            "Iteration 136, loss = 0.52185049\n",
            "Iteration 137, loss = 0.52046202\n",
            "Iteration 138, loss = 0.52105346\n",
            "Iteration 139, loss = 0.52113841\n",
            "Iteration 140, loss = 0.52234533\n",
            "Iteration 141, loss = 0.52066801\n",
            "Iteration 142, loss = 0.52074754\n",
            "Iteration 143, loss = 0.52092519\n",
            "Iteration 144, loss = 0.51938474\n",
            "Iteration 145, loss = 0.52042600\n",
            "Iteration 146, loss = 0.52089315\n",
            "Iteration 147, loss = 0.52030347\n",
            "Iteration 148, loss = 0.52019522\n",
            "Iteration 149, loss = 0.51998596\n",
            "Iteration 150, loss = 0.52048439\n",
            "Iteration 151, loss = 0.52001603\n",
            "Iteration 152, loss = 0.51839720\n",
            "Iteration 153, loss = 0.51942457\n",
            "Iteration 154, loss = 0.51898077\n",
            "Iteration 155, loss = 0.51905903\n",
            "Iteration 156, loss = 0.51928614\n",
            "Iteration 157, loss = 0.51863687\n",
            "Iteration 158, loss = 0.51892346\n",
            "Iteration 159, loss = 0.51842931\n",
            "Iteration 160, loss = 0.51897847\n",
            "Iteration 161, loss = 0.51851609\n",
            "Iteration 162, loss = 0.51881960\n",
            "Iteration 163, loss = 0.51907531\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "5\n",
            "Iteration 1, loss = 0.56440050\n",
            "Iteration 2, loss = 0.55854058\n",
            "Iteration 3, loss = 0.55808148\n",
            "Iteration 4, loss = 0.55841864\n",
            "Iteration 5, loss = 0.55842612\n",
            "Iteration 6, loss = 0.55893885\n",
            "Iteration 7, loss = 0.55776069\n",
            "Iteration 8, loss = 0.55826388\n",
            "Iteration 9, loss = 0.55763944\n",
            "Iteration 10, loss = 0.55854514\n",
            "Iteration 11, loss = 0.55828472\n",
            "Iteration 12, loss = 0.55792093\n",
            "Iteration 13, loss = 0.55769094\n",
            "Iteration 14, loss = 0.55846290\n",
            "Iteration 15, loss = 0.55810363\n",
            "Iteration 16, loss = 0.55707949\n",
            "Iteration 17, loss = 0.55710390\n",
            "Iteration 18, loss = 0.55698845\n",
            "Iteration 19, loss = 0.55769981\n",
            "Iteration 20, loss = 0.55737512\n",
            "Iteration 21, loss = 0.55681254\n",
            "Iteration 22, loss = 0.55709705\n",
            "Iteration 23, loss = 0.55675728\n",
            "Iteration 24, loss = 0.55678675\n",
            "Iteration 25, loss = 0.55637510\n",
            "Iteration 26, loss = 0.55762470\n",
            "Iteration 27, loss = 0.55692497\n",
            "Iteration 28, loss = 0.55668740\n",
            "Iteration 29, loss = 0.55626456\n",
            "Iteration 30, loss = 0.55689498\n",
            "Iteration 31, loss = 0.55642325\n",
            "Iteration 32, loss = 0.55627735\n",
            "Iteration 33, loss = 0.55593522\n",
            "Iteration 34, loss = 0.55555529\n",
            "Iteration 35, loss = 0.55566058\n",
            "Iteration 36, loss = 0.55593610\n",
            "Iteration 37, loss = 0.55510449\n",
            "Iteration 38, loss = 0.55518915\n",
            "Iteration 39, loss = 0.55461662\n",
            "Iteration 40, loss = 0.55541947\n",
            "Iteration 41, loss = 0.55427893\n",
            "Iteration 42, loss = 0.55514337\n",
            "Iteration 43, loss = 0.55427371\n",
            "Iteration 44, loss = 0.55418373\n",
            "Iteration 45, loss = 0.55398281\n",
            "Iteration 46, loss = 0.55393525\n",
            "Iteration 47, loss = 0.55331904\n",
            "Iteration 48, loss = 0.55351314\n",
            "Iteration 49, loss = 0.55343895\n",
            "Iteration 50, loss = 0.55287970\n",
            "Iteration 51, loss = 0.55245854\n",
            "Iteration 52, loss = 0.55221644\n",
            "Iteration 53, loss = 0.55235377\n",
            "Iteration 54, loss = 0.55086990\n",
            "Iteration 55, loss = 0.55129542\n",
            "Iteration 56, loss = 0.55123424\n",
            "Iteration 57, loss = 0.55059118\n",
            "Iteration 58, loss = 0.55085881\n",
            "Iteration 59, loss = 0.55035445\n",
            "Iteration 60, loss = 0.54893458\n",
            "Iteration 61, loss = 0.54947961\n",
            "Iteration 62, loss = 0.54758831\n",
            "Iteration 63, loss = 0.54806118\n",
            "Iteration 64, loss = 0.54855113\n",
            "Iteration 65, loss = 0.54705018\n",
            "Iteration 66, loss = 0.54776224\n",
            "Iteration 67, loss = 0.54610411\n",
            "Iteration 68, loss = 0.54523761\n",
            "Iteration 69, loss = 0.54548622\n",
            "Iteration 70, loss = 0.54380512\n",
            "Iteration 71, loss = 0.54384486\n",
            "Iteration 72, loss = 0.54308745\n",
            "Iteration 73, loss = 0.54273457\n",
            "Iteration 74, loss = 0.54127552\n",
            "Iteration 75, loss = 0.54123604\n",
            "Iteration 76, loss = 0.54028690\n",
            "Iteration 77, loss = 0.53950207\n",
            "Iteration 78, loss = 0.53861274\n",
            "Iteration 79, loss = 0.53872493\n",
            "Iteration 80, loss = 0.53782299\n",
            "Iteration 81, loss = 0.53633922\n",
            "Iteration 82, loss = 0.53624745\n",
            "Iteration 83, loss = 0.53476064\n",
            "Iteration 84, loss = 0.53431754\n",
            "Iteration 85, loss = 0.53429002\n",
            "Iteration 86, loss = 0.53251735\n",
            "Iteration 87, loss = 0.53189531\n",
            "Iteration 88, loss = 0.53190599\n",
            "Iteration 89, loss = 0.53163397\n",
            "Iteration 90, loss = 0.52974411\n",
            "Iteration 91, loss = 0.52925252\n",
            "Iteration 92, loss = 0.52993429\n",
            "Iteration 93, loss = 0.52861620\n",
            "Iteration 94, loss = 0.52866974\n",
            "Iteration 95, loss = 0.52781976\n",
            "Iteration 96, loss = 0.52785892\n",
            "Iteration 97, loss = 0.52734857\n",
            "Iteration 98, loss = 0.52599821\n",
            "Iteration 99, loss = 0.52701460\n",
            "Iteration 100, loss = 0.52591088\n",
            "Iteration 101, loss = 0.52459553\n",
            "Iteration 102, loss = 0.52559250\n",
            "Iteration 103, loss = 0.52509779\n",
            "Iteration 104, loss = 0.52428430\n",
            "Iteration 105, loss = 0.52427564\n",
            "Iteration 106, loss = 0.52479940\n",
            "Iteration 107, loss = 0.52308604\n",
            "Iteration 108, loss = 0.52276739\n",
            "Iteration 109, loss = 0.52255056\n",
            "Iteration 110, loss = 0.52273317\n",
            "Iteration 111, loss = 0.52195118\n",
            "Iteration 112, loss = 0.52176974\n",
            "Iteration 113, loss = 0.52279465\n",
            "Iteration 114, loss = 0.52188810\n",
            "Iteration 115, loss = 0.52068344\n",
            "Iteration 116, loss = 0.52166713\n",
            "Iteration 117, loss = 0.52094550\n",
            "Iteration 118, loss = 0.52054053\n",
            "Iteration 119, loss = 0.52087545\n",
            "Iteration 120, loss = 0.52050460\n",
            "Iteration 121, loss = 0.51923410\n",
            "Iteration 122, loss = 0.51984028\n",
            "Iteration 123, loss = 0.51975647\n",
            "Iteration 124, loss = 0.51916857\n",
            "Iteration 125, loss = 0.51948642\n",
            "Iteration 126, loss = 0.51864678\n",
            "Iteration 127, loss = 0.51827325\n",
            "Iteration 128, loss = 0.51953965\n",
            "Iteration 129, loss = 0.51846995\n",
            "Iteration 130, loss = 0.51903887\n",
            "Iteration 131, loss = 0.51838377\n",
            "Iteration 132, loss = 0.51753174\n",
            "Iteration 133, loss = 0.51843436\n",
            "Iteration 134, loss = 0.51778030\n",
            "Iteration 135, loss = 0.51796732\n",
            "Iteration 136, loss = 0.51762435\n",
            "Iteration 137, loss = 0.51722502\n",
            "Iteration 138, loss = 0.51682848\n",
            "Iteration 139, loss = 0.51701168\n",
            "Iteration 140, loss = 0.51707049\n",
            "Iteration 141, loss = 0.51582366\n",
            "Iteration 142, loss = 0.51651383\n",
            "Iteration 143, loss = 0.51610825\n",
            "Iteration 144, loss = 0.51636572\n",
            "Iteration 145, loss = 0.51733094\n",
            "Iteration 146, loss = 0.51598810\n",
            "Iteration 147, loss = 0.51684295\n",
            "Iteration 148, loss = 0.51670402\n",
            "Iteration 149, loss = 0.51635607\n",
            "Iteration 150, loss = 0.51637353\n",
            "Iteration 151, loss = 0.51671505\n",
            "Iteration 152, loss = 0.51596854\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56789533\n",
            "Iteration 2, loss = 0.56412955\n",
            "Iteration 3, loss = 0.56437205\n",
            "Iteration 4, loss = 0.56385831\n",
            "Iteration 5, loss = 0.56427018\n",
            "Iteration 6, loss = 0.56434138\n",
            "Iteration 7, loss = 0.56416003\n",
            "Iteration 8, loss = 0.56405822\n",
            "Iteration 9, loss = 0.56320750\n",
            "Iteration 10, loss = 0.56335280\n",
            "Iteration 11, loss = 0.56350304\n",
            "Iteration 12, loss = 0.56330698\n",
            "Iteration 13, loss = 0.56352300\n",
            "Iteration 14, loss = 0.56421297\n",
            "Iteration 15, loss = 0.56315597\n",
            "Iteration 16, loss = 0.56381084\n",
            "Iteration 17, loss = 0.56372291\n",
            "Iteration 18, loss = 0.56274075\n",
            "Iteration 19, loss = 0.56306678\n",
            "Iteration 20, loss = 0.56283942\n",
            "Iteration 21, loss = 0.56234325\n",
            "Iteration 22, loss = 0.56244858\n",
            "Iteration 23, loss = 0.56268817\n",
            "Iteration 24, loss = 0.56256362\n",
            "Iteration 25, loss = 0.56322604\n",
            "Iteration 26, loss = 0.56304952\n",
            "Iteration 27, loss = 0.56211471\n",
            "Iteration 28, loss = 0.56303832\n",
            "Iteration 29, loss = 0.56285204\n",
            "Iteration 30, loss = 0.56221417\n",
            "Iteration 31, loss = 0.56170224\n",
            "Iteration 32, loss = 0.56218911\n",
            "Iteration 33, loss = 0.56202810\n",
            "Iteration 34, loss = 0.56263633\n",
            "Iteration 35, loss = 0.56238989\n",
            "Iteration 36, loss = 0.56139110\n",
            "Iteration 37, loss = 0.56164075\n",
            "Iteration 38, loss = 0.56183156\n",
            "Iteration 39, loss = 0.56184375\n",
            "Iteration 40, loss = 0.56162759\n",
            "Iteration 41, loss = 0.56138515\n",
            "Iteration 42, loss = 0.56065602\n",
            "Iteration 43, loss = 0.55993961\n",
            "Iteration 44, loss = 0.55914568\n",
            "Iteration 45, loss = 0.56042738\n",
            "Iteration 46, loss = 0.55977769\n",
            "Iteration 47, loss = 0.56012235\n",
            "Iteration 48, loss = 0.56039395\n",
            "Iteration 49, loss = 0.55892078\n",
            "Iteration 50, loss = 0.55882733\n",
            "Iteration 51, loss = 0.55940764\n",
            "Iteration 52, loss = 0.55858121\n",
            "Iteration 53, loss = 0.55762110\n",
            "Iteration 54, loss = 0.55794840\n",
            "Iteration 55, loss = 0.55851932\n",
            "Iteration 56, loss = 0.55682059\n",
            "Iteration 57, loss = 0.55743925\n",
            "Iteration 58, loss = 0.55623916\n",
            "Iteration 59, loss = 0.55664854\n",
            "Iteration 60, loss = 0.55655518\n",
            "Iteration 61, loss = 0.55502626\n",
            "Iteration 62, loss = 0.55489065\n",
            "Iteration 63, loss = 0.55451418\n",
            "Iteration 64, loss = 0.55456810\n",
            "Iteration 65, loss = 0.55377322\n",
            "Iteration 66, loss = 0.55355903\n",
            "Iteration 67, loss = 0.55415079\n",
            "Iteration 68, loss = 0.55218649\n",
            "Iteration 69, loss = 0.55175140\n",
            "Iteration 70, loss = 0.55225131\n",
            "Iteration 71, loss = 0.55033745\n",
            "Iteration 72, loss = 0.55073806\n",
            "Iteration 73, loss = 0.54947062\n",
            "Iteration 74, loss = 0.54966203\n",
            "Iteration 75, loss = 0.54811039\n",
            "Iteration 76, loss = 0.54769738\n",
            "Iteration 77, loss = 0.54665837\n",
            "Iteration 78, loss = 0.54651432\n",
            "Iteration 79, loss = 0.54578371\n",
            "Iteration 80, loss = 0.54515389\n",
            "Iteration 81, loss = 0.54371015\n",
            "Iteration 82, loss = 0.54377548\n",
            "Iteration 83, loss = 0.54296548\n",
            "Iteration 84, loss = 0.54201662\n",
            "Iteration 85, loss = 0.54148711\n",
            "Iteration 86, loss = 0.54058399\n",
            "Iteration 87, loss = 0.53961961\n",
            "Iteration 88, loss = 0.53880976\n",
            "Iteration 89, loss = 0.53952752\n",
            "Iteration 90, loss = 0.53886034\n",
            "Iteration 91, loss = 0.53737663\n",
            "Iteration 92, loss = 0.53765081\n",
            "Iteration 93, loss = 0.53670699\n",
            "Iteration 94, loss = 0.53681232\n",
            "Iteration 95, loss = 0.53590341\n",
            "Iteration 96, loss = 0.53508015\n",
            "Iteration 97, loss = 0.53444513\n",
            "Iteration 98, loss = 0.53471265\n",
            "Iteration 99, loss = 0.53415193\n",
            "Iteration 100, loss = 0.53253994\n",
            "Iteration 101, loss = 0.53292207\n",
            "Iteration 102, loss = 0.53233354\n",
            "Iteration 103, loss = 0.53176991\n",
            "Iteration 104, loss = 0.53143122\n",
            "Iteration 105, loss = 0.53127392\n",
            "Iteration 106, loss = 0.53178936\n",
            "Iteration 107, loss = 0.53135017\n",
            "Iteration 108, loss = 0.53081791\n",
            "Iteration 109, loss = 0.53079306\n",
            "Iteration 110, loss = 0.52959258\n",
            "Iteration 111, loss = 0.52935626\n",
            "Iteration 112, loss = 0.52907600\n",
            "Iteration 113, loss = 0.52874107\n",
            "Iteration 114, loss = 0.52918141\n",
            "Iteration 115, loss = 0.52861790\n",
            "Iteration 116, loss = 0.52741185\n",
            "Iteration 117, loss = 0.52848461\n",
            "Iteration 118, loss = 0.52799104\n",
            "Iteration 119, loss = 0.52697613\n",
            "Iteration 120, loss = 0.52776960\n",
            "Iteration 121, loss = 0.52711371\n",
            "Iteration 122, loss = 0.52819612\n",
            "Iteration 123, loss = 0.52703219\n",
            "Iteration 124, loss = 0.52725300\n",
            "Iteration 125, loss = 0.52607469\n",
            "Iteration 126, loss = 0.52672125\n",
            "Iteration 127, loss = 0.52677505\n",
            "Iteration 128, loss = 0.52581381\n",
            "Iteration 129, loss = 0.52630277\n",
            "Iteration 130, loss = 0.52593396\n",
            "Iteration 131, loss = 0.52576154\n",
            "Iteration 132, loss = 0.52458813\n",
            "Iteration 133, loss = 0.52547389\n",
            "Iteration 134, loss = 0.52476080\n",
            "Iteration 135, loss = 0.52524996\n",
            "Iteration 136, loss = 0.52432472\n",
            "Iteration 137, loss = 0.52610305\n",
            "Iteration 138, loss = 0.52490498\n",
            "Iteration 139, loss = 0.52530873\n",
            "Iteration 140, loss = 0.52455217\n",
            "Iteration 141, loss = 0.52471688\n",
            "Iteration 142, loss = 0.52433048\n",
            "Iteration 143, loss = 0.52476041\n",
            "Iteration 144, loss = 0.52353629\n",
            "Iteration 145, loss = 0.52408205\n",
            "Iteration 146, loss = 0.52416482\n",
            "Iteration 147, loss = 0.52403869\n",
            "Iteration 148, loss = 0.52320224\n",
            "Iteration 149, loss = 0.52339185\n",
            "Iteration 150, loss = 0.52322538\n",
            "Iteration 151, loss = 0.52297416\n",
            "Iteration 152, loss = 0.52340847\n",
            "Iteration 153, loss = 0.52309100\n",
            "Iteration 154, loss = 0.52409580\n",
            "Iteration 155, loss = 0.52236994\n",
            "Iteration 156, loss = 0.52358921\n",
            "Iteration 157, loss = 0.52296545\n",
            "Iteration 158, loss = 0.52291262\n",
            "Iteration 159, loss = 0.52297646\n",
            "Iteration 160, loss = 0.52233714\n",
            "Iteration 161, loss = 0.52247054\n",
            "Iteration 162, loss = 0.52228117\n",
            "Iteration 163, loss = 0.52267853\n",
            "Iteration 164, loss = 0.52171338\n",
            "Iteration 165, loss = 0.52280222\n",
            "Iteration 166, loss = 0.52188492\n",
            "Iteration 167, loss = 0.52219772\n",
            "Iteration 168, loss = 0.52204836\n",
            "Iteration 169, loss = 0.52144859\n",
            "Iteration 170, loss = 0.52257364\n",
            "Iteration 171, loss = 0.52183543\n",
            "Iteration 172, loss = 0.52266368\n",
            "Iteration 173, loss = 0.52199728\n",
            "Iteration 174, loss = 0.52202889\n",
            "Iteration 175, loss = 0.52171983\n",
            "Iteration 176, loss = 0.52190146\n",
            "Iteration 177, loss = 0.52140952\n",
            "Iteration 178, loss = 0.52330688\n",
            "Iteration 179, loss = 0.52243411\n",
            "Iteration 180, loss = 0.52198557\n",
            "Iteration 181, loss = 0.52179289\n",
            "Iteration 182, loss = 0.52153515\n",
            "Iteration 183, loss = 0.52154902\n",
            "Iteration 184, loss = 0.52134383\n",
            "Iteration 185, loss = 0.52124936\n",
            "Iteration 186, loss = 0.52070361\n",
            "Iteration 187, loss = 0.52117035\n",
            "Iteration 188, loss = 0.52117162\n",
            "Iteration 189, loss = 0.52098598\n",
            "Iteration 190, loss = 0.52062830\n",
            "Iteration 191, loss = 0.52018576\n",
            "Iteration 192, loss = 0.52173665\n",
            "Iteration 193, loss = 0.52210867\n",
            "Iteration 194, loss = 0.52082133\n",
            "Iteration 195, loss = 0.52119447\n",
            "Iteration 196, loss = 0.52166150\n",
            "Iteration 197, loss = 0.52100510\n",
            "Iteration 198, loss = 0.52067197\n",
            "Iteration 199, loss = 0.52101275\n",
            "Iteration 200, loss = 0.52175565\n",
            "Iteration 201, loss = 0.52020403\n",
            "Iteration 202, loss = 0.52088287\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56343675\n",
            "Iteration 2, loss = 0.56344903\n",
            "Iteration 3, loss = 0.56289199\n",
            "Iteration 4, loss = 0.56286509\n",
            "Iteration 5, loss = 0.56123139\n",
            "Iteration 6, loss = 0.56296637\n",
            "Iteration 7, loss = 0.56224598\n",
            "Iteration 8, loss = 0.56227188\n",
            "Iteration 9, loss = 0.56210312\n",
            "Iteration 10, loss = 0.56241494\n",
            "Iteration 11, loss = 0.56235299\n",
            "Iteration 12, loss = 0.56148218\n",
            "Iteration 13, loss = 0.56215629\n",
            "Iteration 14, loss = 0.56249909\n",
            "Iteration 15, loss = 0.56195382\n",
            "Iteration 16, loss = 0.56200989\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57878645\n",
            "Iteration 2, loss = 0.56806951\n",
            "Iteration 3, loss = 0.56784768\n",
            "Iteration 4, loss = 0.56951749\n",
            "Iteration 5, loss = 0.56812667\n",
            "Iteration 6, loss = 0.56762872\n",
            "Iteration 7, loss = 0.56749367\n",
            "Iteration 8, loss = 0.56846953\n",
            "Iteration 9, loss = 0.56717259\n",
            "Iteration 10, loss = 0.56759480\n",
            "Iteration 11, loss = 0.56692987\n",
            "Iteration 12, loss = 0.56752216\n",
            "Iteration 13, loss = 0.56778161\n",
            "Iteration 14, loss = 0.56800087\n",
            "Iteration 15, loss = 0.56775440\n",
            "Iteration 16, loss = 0.56751915\n",
            "Iteration 17, loss = 0.56719361\n",
            "Iteration 18, loss = 0.56704605\n",
            "Iteration 19, loss = 0.56761121\n",
            "Iteration 20, loss = 0.56680004\n",
            "Iteration 21, loss = 0.56726313\n",
            "Iteration 22, loss = 0.56704365\n",
            "Iteration 23, loss = 0.56702779\n",
            "Iteration 24, loss = 0.56720189\n",
            "Iteration 25, loss = 0.56694662\n",
            "Iteration 26, loss = 0.56628681\n",
            "Iteration 27, loss = 0.56718856\n",
            "Iteration 28, loss = 0.56640236\n",
            "Iteration 29, loss = 0.56603184\n",
            "Iteration 30, loss = 0.56550285\n",
            "Iteration 31, loss = 0.56630347\n",
            "Iteration 32, loss = 0.56585579\n",
            "Iteration 33, loss = 0.56590105\n",
            "Iteration 34, loss = 0.56611577\n",
            "Iteration 35, loss = 0.56576883\n",
            "Iteration 36, loss = 0.56537527\n",
            "Iteration 37, loss = 0.56607473\n",
            "Iteration 38, loss = 0.56546607\n",
            "Iteration 39, loss = 0.56572613\n",
            "Iteration 40, loss = 0.56523860\n",
            "Iteration 41, loss = 0.56567322\n",
            "Iteration 42, loss = 0.56459529\n",
            "Iteration 43, loss = 0.56454118\n",
            "Iteration 44, loss = 0.56479598\n",
            "Iteration 45, loss = 0.56428958\n",
            "Iteration 46, loss = 0.56500601\n",
            "Iteration 47, loss = 0.56371768\n",
            "Iteration 48, loss = 0.56406203\n",
            "Iteration 49, loss = 0.56336875\n",
            "Iteration 50, loss = 0.56355395\n",
            "Iteration 51, loss = 0.56326338\n",
            "Iteration 52, loss = 0.56275055\n",
            "Iteration 53, loss = 0.56282188\n",
            "Iteration 54, loss = 0.56214028\n",
            "Iteration 55, loss = 0.56183634\n",
            "Iteration 56, loss = 0.56076879\n",
            "Iteration 57, loss = 0.56106713\n",
            "Iteration 58, loss = 0.56031703\n",
            "Iteration 59, loss = 0.56072929\n",
            "Iteration 60, loss = 0.56075495\n",
            "Iteration 61, loss = 0.56087910\n",
            "Iteration 62, loss = 0.55945094\n",
            "Iteration 63, loss = 0.55918057\n",
            "Iteration 64, loss = 0.55927052\n",
            "Iteration 65, loss = 0.55828152\n",
            "Iteration 66, loss = 0.55767777\n",
            "Iteration 67, loss = 0.55708259\n",
            "Iteration 68, loss = 0.55647227\n",
            "Iteration 69, loss = 0.55647931\n",
            "Iteration 70, loss = 0.55604441\n",
            "Iteration 71, loss = 0.55484468\n",
            "Iteration 72, loss = 0.55434678\n",
            "Iteration 73, loss = 0.55400023\n",
            "Iteration 74, loss = 0.55301793\n",
            "Iteration 75, loss = 0.55293730\n",
            "Iteration 76, loss = 0.55232255\n",
            "Iteration 77, loss = 0.55151375\n",
            "Iteration 78, loss = 0.55094670\n",
            "Iteration 79, loss = 0.55000504\n",
            "Iteration 80, loss = 0.54945346\n",
            "Iteration 81, loss = 0.54829979\n",
            "Iteration 82, loss = 0.54786423\n",
            "Iteration 83, loss = 0.54789724\n",
            "Iteration 84, loss = 0.54640192\n",
            "Iteration 85, loss = 0.54602424\n",
            "Iteration 86, loss = 0.54604332\n",
            "Iteration 87, loss = 0.54472756\n",
            "Iteration 88, loss = 0.54473532\n",
            "Iteration 89, loss = 0.54389753\n",
            "Iteration 90, loss = 0.54195525\n",
            "Iteration 91, loss = 0.54271146\n",
            "Iteration 92, loss = 0.54206378\n",
            "Iteration 93, loss = 0.54159583\n",
            "Iteration 94, loss = 0.54049442\n",
            "Iteration 95, loss = 0.54025836\n",
            "Iteration 96, loss = 0.53989553\n",
            "Iteration 97, loss = 0.53939102\n",
            "Iteration 98, loss = 0.53911769\n",
            "Iteration 99, loss = 0.53792263\n",
            "Iteration 100, loss = 0.53819722\n",
            "Iteration 101, loss = 0.53732247\n",
            "Iteration 102, loss = 0.53592193\n",
            "Iteration 103, loss = 0.53602357\n",
            "Iteration 104, loss = 0.53699113\n",
            "Iteration 105, loss = 0.53554863\n",
            "Iteration 106, loss = 0.53615655\n",
            "Iteration 107, loss = 0.53604869\n",
            "Iteration 108, loss = 0.53501732\n",
            "Iteration 109, loss = 0.53479094\n",
            "Iteration 110, loss = 0.53425809\n",
            "Iteration 111, loss = 0.53457372\n",
            "Iteration 112, loss = 0.53359645\n",
            "Iteration 113, loss = 0.53441457\n",
            "Iteration 114, loss = 0.53352726\n",
            "Iteration 115, loss = 0.53338021\n",
            "Iteration 116, loss = 0.53368722\n",
            "Iteration 117, loss = 0.53203650\n",
            "Iteration 118, loss = 0.53252533\n",
            "Iteration 119, loss = 0.53221838\n",
            "Iteration 120, loss = 0.53245136\n",
            "Iteration 121, loss = 0.53197640\n",
            "Iteration 122, loss = 0.53132702\n",
            "Iteration 123, loss = 0.53227212\n",
            "Iteration 124, loss = 0.53130057\n",
            "Iteration 125, loss = 0.53217227\n",
            "Iteration 126, loss = 0.53098550\n",
            "Iteration 127, loss = 0.53041394\n",
            "Iteration 128, loss = 0.53155891\n",
            "Iteration 129, loss = 0.53097294\n",
            "Iteration 130, loss = 0.52980952\n",
            "Iteration 131, loss = 0.52974844\n",
            "Iteration 132, loss = 0.52990667\n",
            "Iteration 133, loss = 0.52943928\n",
            "Iteration 134, loss = 0.52996766\n",
            "Iteration 135, loss = 0.52898097\n",
            "Iteration 136, loss = 0.52888355\n",
            "Iteration 137, loss = 0.52913054\n",
            "Iteration 138, loss = 0.52853855\n",
            "Iteration 139, loss = 0.52851123\n",
            "Iteration 140, loss = 0.52823497\n",
            "Iteration 141, loss = 0.52831978\n",
            "Iteration 142, loss = 0.52971823\n",
            "Iteration 143, loss = 0.52799233\n",
            "Iteration 144, loss = 0.52836139\n",
            "Iteration 145, loss = 0.52864645\n",
            "Iteration 146, loss = 0.52816890\n",
            "Iteration 147, loss = 0.52793212\n",
            "Iteration 148, loss = 0.52832272\n",
            "Iteration 149, loss = 0.52772942\n",
            "Iteration 150, loss = 0.52828114\n",
            "Iteration 151, loss = 0.52644872\n",
            "Iteration 152, loss = 0.52848670\n",
            "Iteration 153, loss = 0.52776606\n",
            "Iteration 154, loss = 0.52704362\n",
            "Iteration 155, loss = 0.52703602\n",
            "Iteration 156, loss = 0.52686347\n",
            "Iteration 157, loss = 0.52731171\n",
            "Iteration 158, loss = 0.52718118\n",
            "Iteration 159, loss = 0.52644165\n",
            "Iteration 160, loss = 0.52602337\n",
            "Iteration 161, loss = 0.52663623\n",
            "Iteration 162, loss = 0.52558762\n",
            "Iteration 163, loss = 0.52600447\n",
            "Iteration 164, loss = 0.52609116\n",
            "Iteration 165, loss = 0.52639500\n",
            "Iteration 166, loss = 0.52619285\n",
            "Iteration 167, loss = 0.52643966\n",
            "Iteration 168, loss = 0.52591860\n",
            "Iteration 169, loss = 0.52559024\n",
            "Iteration 170, loss = 0.52600193\n",
            "Iteration 171, loss = 0.52640883\n",
            "Iteration 172, loss = 0.52607888\n",
            "Iteration 173, loss = 0.52581780\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56522489\n",
            "Iteration 2, loss = 0.56279459\n",
            "Iteration 3, loss = 0.56285776\n",
            "Iteration 4, loss = 0.56224227\n",
            "Iteration 5, loss = 0.56213215\n",
            "Iteration 6, loss = 0.56303736\n",
            "Iteration 7, loss = 0.56239618\n",
            "Iteration 8, loss = 0.56293617\n",
            "Iteration 9, loss = 0.56302905\n",
            "Iteration 10, loss = 0.56330989\n",
            "Iteration 11, loss = 0.56279663\n",
            "Iteration 12, loss = 0.56261701\n",
            "Iteration 13, loss = 0.56225001\n",
            "Iteration 14, loss = 0.56245879\n",
            "Iteration 15, loss = 0.56236932\n",
            "Iteration 16, loss = 0.56270614\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56512936\n",
            "Iteration 2, loss = 0.56073242\n",
            "Iteration 3, loss = 0.56152713\n",
            "Iteration 4, loss = 0.56086829\n",
            "Iteration 5, loss = 0.56003706\n",
            "Iteration 6, loss = 0.56131078\n",
            "Iteration 7, loss = 0.56100306\n",
            "Iteration 8, loss = 0.56036012\n",
            "Iteration 9, loss = 0.56107493\n",
            "Iteration 10, loss = 0.56112100\n",
            "Iteration 11, loss = 0.56081262\n",
            "Iteration 12, loss = 0.56092517\n",
            "Iteration 13, loss = 0.56018355\n",
            "Iteration 14, loss = 0.56068856\n",
            "Iteration 15, loss = 0.56017469\n",
            "Iteration 16, loss = 0.56081805\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57299570\n",
            "Iteration 2, loss = 0.56517275\n",
            "Iteration 3, loss = 0.56416174\n",
            "Iteration 4, loss = 0.56499335\n",
            "Iteration 5, loss = 0.56542728\n",
            "Iteration 6, loss = 0.56460814\n",
            "Iteration 7, loss = 0.56508746\n",
            "Iteration 8, loss = 0.56479478\n",
            "Iteration 9, loss = 0.56498775\n",
            "Iteration 10, loss = 0.56459006\n",
            "Iteration 11, loss = 0.56472640\n",
            "Iteration 12, loss = 0.56438963\n",
            "Iteration 13, loss = 0.56450463\n",
            "Iteration 14, loss = 0.56421856\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57187556\n",
            "Iteration 2, loss = 0.56432930\n",
            "Iteration 3, loss = 0.56496355\n",
            "Iteration 4, loss = 0.56553349\n",
            "Iteration 5, loss = 0.56552992\n",
            "Iteration 6, loss = 0.56516382\n",
            "Iteration 7, loss = 0.56527651\n",
            "Iteration 8, loss = 0.56458525\n",
            "Iteration 9, loss = 0.56487547\n",
            "Iteration 10, loss = 0.56431778\n",
            "Iteration 11, loss = 0.56460007\n",
            "Iteration 12, loss = 0.56522874\n",
            "Iteration 13, loss = 0.56492851\n",
            "Iteration 14, loss = 0.56452909\n",
            "Iteration 15, loss = 0.56447659\n",
            "Iteration 16, loss = 0.56458626\n",
            "Iteration 17, loss = 0.56417619\n",
            "Iteration 18, loss = 0.56417562\n",
            "Iteration 19, loss = 0.56501498\n",
            "Iteration 20, loss = 0.56326660\n",
            "Iteration 21, loss = 0.56379421\n",
            "Iteration 22, loss = 0.56395226\n",
            "Iteration 23, loss = 0.56413024\n",
            "Iteration 24, loss = 0.56440227\n",
            "Iteration 25, loss = 0.56380750\n",
            "Iteration 26, loss = 0.56415657\n",
            "Iteration 27, loss = 0.56418694\n",
            "Iteration 28, loss = 0.56432120\n",
            "Iteration 29, loss = 0.56401728\n",
            "Iteration 30, loss = 0.56323682\n",
            "Iteration 31, loss = 0.56324801\n",
            "Iteration 32, loss = 0.56413873\n",
            "Iteration 33, loss = 0.56311007\n",
            "Iteration 34, loss = 0.56325955\n",
            "Iteration 35, loss = 0.56360475\n",
            "Iteration 36, loss = 0.56389992\n",
            "Iteration 37, loss = 0.56366638\n",
            "Iteration 38, loss = 0.56308915\n",
            "Iteration 39, loss = 0.56311953\n",
            "Iteration 40, loss = 0.56316825\n",
            "Iteration 41, loss = 0.56349531\n",
            "Iteration 42, loss = 0.56331658\n",
            "Iteration 43, loss = 0.56353686\n",
            "Iteration 44, loss = 0.56296290\n",
            "Iteration 45, loss = 0.56256824\n",
            "Iteration 46, loss = 0.56132770\n",
            "Iteration 47, loss = 0.56246901\n",
            "Iteration 48, loss = 0.56281273\n",
            "Iteration 49, loss = 0.56174968\n",
            "Iteration 50, loss = 0.56222111\n",
            "Iteration 51, loss = 0.56188791\n",
            "Iteration 52, loss = 0.56200237\n",
            "Iteration 53, loss = 0.56197368\n",
            "Iteration 54, loss = 0.56227006\n",
            "Iteration 55, loss = 0.56103191\n",
            "Iteration 56, loss = 0.56168889\n",
            "Iteration 57, loss = 0.56161987\n",
            "Iteration 58, loss = 0.56018912\n",
            "Iteration 59, loss = 0.56133939\n",
            "Iteration 60, loss = 0.56111047\n",
            "Iteration 61, loss = 0.55979282\n",
            "Iteration 62, loss = 0.56061093\n",
            "Iteration 63, loss = 0.55966143\n",
            "Iteration 64, loss = 0.55987255\n",
            "Iteration 65, loss = 0.55928146\n",
            "Iteration 66, loss = 0.55929344\n",
            "Iteration 67, loss = 0.55879410\n",
            "Iteration 68, loss = 0.55871563\n",
            "Iteration 69, loss = 0.55940168\n",
            "Iteration 70, loss = 0.55841648\n",
            "Iteration 71, loss = 0.55897280\n",
            "Iteration 72, loss = 0.55726972\n",
            "Iteration 73, loss = 0.55752457\n",
            "Iteration 74, loss = 0.55708453\n",
            "Iteration 75, loss = 0.55663476\n",
            "Iteration 76, loss = 0.55666181\n",
            "Iteration 77, loss = 0.55547460\n",
            "Iteration 78, loss = 0.55570437\n",
            "Iteration 79, loss = 0.55514817\n",
            "Iteration 80, loss = 0.55501940\n",
            "Iteration 81, loss = 0.55430651\n",
            "Iteration 82, loss = 0.55374707\n",
            "Iteration 83, loss = 0.55234062\n",
            "Iteration 84, loss = 0.55228681\n",
            "Iteration 85, loss = 0.55178467\n",
            "Iteration 86, loss = 0.55226257\n",
            "Iteration 87, loss = 0.54994016\n",
            "Iteration 88, loss = 0.55027119\n",
            "Iteration 89, loss = 0.54937265\n",
            "Iteration 90, loss = 0.54885199\n",
            "Iteration 91, loss = 0.54782209\n",
            "Iteration 92, loss = 0.54819623\n",
            "Iteration 93, loss = 0.54705465\n",
            "Iteration 94, loss = 0.54534784\n",
            "Iteration 95, loss = 0.54536324\n",
            "Iteration 96, loss = 0.54391065\n",
            "Iteration 97, loss = 0.54445136\n",
            "Iteration 98, loss = 0.54283804\n",
            "Iteration 99, loss = 0.54325710\n",
            "Iteration 100, loss = 0.54076191\n",
            "Iteration 101, loss = 0.54172724\n",
            "Iteration 102, loss = 0.54059128\n",
            "Iteration 103, loss = 0.53893091\n",
            "Iteration 104, loss = 0.53976654\n",
            "Iteration 105, loss = 0.53858315\n",
            "Iteration 106, loss = 0.53878666\n",
            "Iteration 107, loss = 0.53735587\n",
            "Iteration 108, loss = 0.53636067\n",
            "Iteration 109, loss = 0.53692890\n",
            "Iteration 110, loss = 0.53564032\n",
            "Iteration 111, loss = 0.53518237\n",
            "Iteration 112, loss = 0.53504911\n",
            "Iteration 113, loss = 0.53390755\n",
            "Iteration 114, loss = 0.53355047\n",
            "Iteration 115, loss = 0.53287245\n",
            "Iteration 116, loss = 0.53350187\n",
            "Iteration 117, loss = 0.53173038\n",
            "Iteration 118, loss = 0.53302055\n",
            "Iteration 119, loss = 0.53130947\n",
            "Iteration 120, loss = 0.53140720\n",
            "Iteration 121, loss = 0.53181208\n",
            "Iteration 122, loss = 0.53120331\n",
            "Iteration 123, loss = 0.53066751\n",
            "Iteration 124, loss = 0.53078211\n",
            "Iteration 125, loss = 0.53042490\n",
            "Iteration 126, loss = 0.52996493\n",
            "Iteration 127, loss = 0.52993247\n",
            "Iteration 128, loss = 0.52954545\n",
            "Iteration 129, loss = 0.52920330\n",
            "Iteration 130, loss = 0.52833914\n",
            "Iteration 131, loss = 0.52847711\n",
            "Iteration 132, loss = 0.52863946\n",
            "Iteration 133, loss = 0.52856534\n",
            "Iteration 134, loss = 0.52838960\n",
            "Iteration 135, loss = 0.52786532\n",
            "Iteration 136, loss = 0.52765524\n",
            "Iteration 137, loss = 0.52770101\n",
            "Iteration 138, loss = 0.52799269\n",
            "Iteration 139, loss = 0.52773702\n",
            "Iteration 140, loss = 0.52737060\n",
            "Iteration 141, loss = 0.52599283\n",
            "Iteration 142, loss = 0.52615737\n",
            "Iteration 143, loss = 0.52651640\n",
            "Iteration 144, loss = 0.52632795\n",
            "Iteration 145, loss = 0.52661578\n",
            "Iteration 146, loss = 0.52621107\n",
            "Iteration 147, loss = 0.52593069\n",
            "Iteration 148, loss = 0.52608782\n",
            "Iteration 149, loss = 0.52573869\n",
            "Iteration 150, loss = 0.52551487\n",
            "Iteration 151, loss = 0.52585174\n",
            "Iteration 152, loss = 0.52539775\n",
            "Iteration 153, loss = 0.52561712\n",
            "Iteration 154, loss = 0.52409548\n",
            "Iteration 155, loss = 0.52520377\n",
            "Iteration 156, loss = 0.52611905\n",
            "Iteration 157, loss = 0.52403833\n",
            "Iteration 158, loss = 0.52521828\n",
            "Iteration 159, loss = 0.52525542\n",
            "Iteration 160, loss = 0.52316684\n",
            "Iteration 161, loss = 0.52338870\n",
            "Iteration 162, loss = 0.52464402\n",
            "Iteration 163, loss = 0.52475265\n",
            "Iteration 164, loss = 0.52268765\n",
            "Iteration 165, loss = 0.52426775\n",
            "Iteration 166, loss = 0.52362675\n",
            "Iteration 167, loss = 0.52295182\n",
            "Iteration 168, loss = 0.52402532\n",
            "Iteration 169, loss = 0.52345753\n",
            "Iteration 170, loss = 0.52426223\n",
            "Iteration 171, loss = 0.52326657\n",
            "Iteration 172, loss = 0.52324043\n",
            "Iteration 173, loss = 0.52298065\n",
            "Iteration 174, loss = 0.52464796\n",
            "Iteration 175, loss = 0.52374710\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56476837\n",
            "Iteration 2, loss = 0.56000889\n",
            "Iteration 3, loss = 0.56067528\n",
            "Iteration 4, loss = 0.55957876\n",
            "Iteration 5, loss = 0.56031728\n",
            "Iteration 6, loss = 0.55991699\n",
            "Iteration 7, loss = 0.56050299\n",
            "Iteration 8, loss = 0.56054395\n",
            "Iteration 9, loss = 0.55973507\n",
            "Iteration 10, loss = 0.56033021\n",
            "Iteration 11, loss = 0.55975763\n",
            "Iteration 12, loss = 0.55979541\n",
            "Iteration 13, loss = 0.55912264\n",
            "Iteration 14, loss = 0.56001350\n",
            "Iteration 15, loss = 0.55932268\n",
            "Iteration 16, loss = 0.55951078\n",
            "Iteration 17, loss = 0.55941219\n",
            "Iteration 18, loss = 0.55972608\n",
            "Iteration 19, loss = 0.55888742\n",
            "Iteration 20, loss = 0.55892238\n",
            "Iteration 21, loss = 0.55938352\n",
            "Iteration 22, loss = 0.55861299\n",
            "Iteration 23, loss = 0.55878481\n",
            "Iteration 24, loss = 0.55939384\n",
            "Iteration 25, loss = 0.55856182\n",
            "Iteration 26, loss = 0.55923047\n",
            "Iteration 27, loss = 0.55871214\n",
            "Iteration 28, loss = 0.55859082\n",
            "Iteration 29, loss = 0.55856978\n",
            "Iteration 30, loss = 0.55883836\n",
            "Iteration 31, loss = 0.55833386\n",
            "Iteration 32, loss = 0.55858614\n",
            "Iteration 33, loss = 0.55887059\n",
            "Iteration 34, loss = 0.55870981\n",
            "Iteration 35, loss = 0.55827077\n",
            "Iteration 36, loss = 0.55834447\n",
            "Iteration 37, loss = 0.55806581\n",
            "Iteration 38, loss = 0.55783370\n",
            "Iteration 39, loss = 0.55823671\n",
            "Iteration 40, loss = 0.55673063\n",
            "Iteration 41, loss = 0.55715943\n",
            "Iteration 42, loss = 0.55779664\n",
            "Iteration 43, loss = 0.55674443\n",
            "Iteration 44, loss = 0.55654240\n",
            "Iteration 45, loss = 0.55699016\n",
            "Iteration 46, loss = 0.55675787\n",
            "Iteration 47, loss = 0.55684695\n",
            "Iteration 48, loss = 0.55575789\n",
            "Iteration 49, loss = 0.55534225\n",
            "Iteration 50, loss = 0.55538079\n",
            "Iteration 51, loss = 0.55621763\n",
            "Iteration 52, loss = 0.55452516\n",
            "Iteration 53, loss = 0.55486541\n",
            "Iteration 54, loss = 0.55516154\n",
            "Iteration 55, loss = 0.55480280\n",
            "Iteration 56, loss = 0.55434581\n",
            "Iteration 57, loss = 0.55387922\n",
            "Iteration 58, loss = 0.55352832\n",
            "Iteration 59, loss = 0.55355430\n",
            "Iteration 60, loss = 0.55325961\n",
            "Iteration 61, loss = 0.55267009\n",
            "Iteration 62, loss = 0.55170601\n",
            "Iteration 63, loss = 0.55126401\n",
            "Iteration 64, loss = 0.55170411\n",
            "Iteration 65, loss = 0.55206069\n",
            "Iteration 66, loss = 0.55051040\n",
            "Iteration 67, loss = 0.54891577\n",
            "Iteration 68, loss = 0.54982341\n",
            "Iteration 69, loss = 0.54945192\n",
            "Iteration 70, loss = 0.54909306\n",
            "Iteration 71, loss = 0.54850253\n",
            "Iteration 72, loss = 0.54840371\n",
            "Iteration 73, loss = 0.54717346\n",
            "Iteration 74, loss = 0.54589198\n",
            "Iteration 75, loss = 0.54530027\n",
            "Iteration 76, loss = 0.54522472\n",
            "Iteration 77, loss = 0.54486541\n",
            "Iteration 78, loss = 0.54429261\n",
            "Iteration 79, loss = 0.54301588\n",
            "Iteration 80, loss = 0.54303796\n",
            "Iteration 81, loss = 0.54213968\n",
            "Iteration 82, loss = 0.54147153\n",
            "Iteration 83, loss = 0.54081767\n",
            "Iteration 84, loss = 0.53932682\n",
            "Iteration 85, loss = 0.53691638\n",
            "Iteration 86, loss = 0.53809083\n",
            "Iteration 87, loss = 0.53783727\n",
            "Iteration 88, loss = 0.53744041\n",
            "Iteration 89, loss = 0.53646281\n",
            "Iteration 90, loss = 0.53555044\n",
            "Iteration 91, loss = 0.53542960\n",
            "Iteration 92, loss = 0.53496088\n",
            "Iteration 93, loss = 0.53345665\n",
            "Iteration 94, loss = 0.53281850\n",
            "Iteration 95, loss = 0.53300919\n",
            "Iteration 96, loss = 0.53218724\n",
            "Iteration 97, loss = 0.53149853\n",
            "Iteration 98, loss = 0.52983177\n",
            "Iteration 99, loss = 0.53064317\n",
            "Iteration 100, loss = 0.53046032\n",
            "Iteration 101, loss = 0.53015687\n",
            "Iteration 102, loss = 0.52865507\n",
            "Iteration 103, loss = 0.52954551\n",
            "Iteration 104, loss = 0.52873970\n",
            "Iteration 105, loss = 0.52732599\n",
            "Iteration 106, loss = 0.52734947\n",
            "Iteration 107, loss = 0.52828995\n",
            "Iteration 108, loss = 0.52754223\n",
            "Iteration 109, loss = 0.52768330\n",
            "Iteration 110, loss = 0.52674471\n",
            "Iteration 111, loss = 0.52691495\n",
            "Iteration 112, loss = 0.52584778\n",
            "Iteration 113, loss = 0.52615194\n",
            "Iteration 114, loss = 0.52613575\n",
            "Iteration 115, loss = 0.52570323\n",
            "Iteration 116, loss = 0.52409276\n",
            "Iteration 117, loss = 0.52512521\n",
            "Iteration 118, loss = 0.52512333\n",
            "Iteration 119, loss = 0.52543599\n",
            "Iteration 120, loss = 0.52467054\n",
            "Iteration 121, loss = 0.52443450\n",
            "Iteration 122, loss = 0.52345360\n",
            "Iteration 123, loss = 0.52357050\n",
            "Iteration 124, loss = 0.52415129\n",
            "Iteration 125, loss = 0.52335100\n",
            "Iteration 126, loss = 0.52305428\n",
            "Iteration 127, loss = 0.52278215\n",
            "Iteration 128, loss = 0.52354940\n",
            "Iteration 129, loss = 0.52221513\n",
            "Iteration 130, loss = 0.52170837\n",
            "Iteration 131, loss = 0.52308192\n",
            "Iteration 132, loss = 0.52247802\n",
            "Iteration 133, loss = 0.52246040\n",
            "Iteration 134, loss = 0.52103787\n",
            "Iteration 135, loss = 0.52170206\n",
            "Iteration 136, loss = 0.52240430\n",
            "Iteration 137, loss = 0.52143420\n",
            "Iteration 138, loss = 0.52102601\n",
            "Iteration 139, loss = 0.52225500\n",
            "Iteration 140, loss = 0.52179356\n",
            "Iteration 141, loss = 0.52124991\n",
            "Iteration 142, loss = 0.52179000\n",
            "Iteration 143, loss = 0.52021392\n",
            "Iteration 144, loss = 0.52078705\n",
            "Iteration 145, loss = 0.52031673\n",
            "Iteration 146, loss = 0.52064091\n",
            "Iteration 147, loss = 0.52089820\n",
            "Iteration 148, loss = 0.52049805\n",
            "Iteration 149, loss = 0.52021878\n",
            "Iteration 150, loss = 0.52006008\n",
            "Iteration 151, loss = 0.52075066\n",
            "Iteration 152, loss = 0.52007844\n",
            "Iteration 153, loss = 0.52070440\n",
            "Iteration 154, loss = 0.51988106\n",
            "Iteration 155, loss = 0.52056131\n",
            "Iteration 156, loss = 0.51971877\n",
            "Iteration 157, loss = 0.52051545\n",
            "Iteration 158, loss = 0.51885717\n",
            "Iteration 159, loss = 0.51899710\n",
            "Iteration 160, loss = 0.51970891\n",
            "Iteration 161, loss = 0.51935423\n",
            "Iteration 162, loss = 0.51907057\n",
            "Iteration 163, loss = 0.51953236\n",
            "Iteration 164, loss = 0.51923435\n",
            "Iteration 165, loss = 0.51954190\n",
            "Iteration 166, loss = 0.51965380\n",
            "Iteration 167, loss = 0.51941721\n",
            "Iteration 168, loss = 0.51957669\n",
            "Iteration 169, loss = 0.51899123\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57420894\n",
            "Iteration 2, loss = 0.56169733\n",
            "Iteration 3, loss = 0.56111537\n",
            "Iteration 4, loss = 0.56134583\n",
            "Iteration 5, loss = 0.56156319\n",
            "Iteration 6, loss = 0.56183485\n",
            "Iteration 7, loss = 0.56133460\n",
            "Iteration 8, loss = 0.56138389\n",
            "Iteration 9, loss = 0.56070468\n",
            "Iteration 10, loss = 0.56177061\n",
            "Iteration 11, loss = 0.56164519\n",
            "Iteration 12, loss = 0.56093757\n",
            "Iteration 13, loss = 0.56078201\n",
            "Iteration 14, loss = 0.56100245\n",
            "Iteration 15, loss = 0.56094120\n",
            "Iteration 16, loss = 0.56120360\n",
            "Iteration 17, loss = 0.56165165\n",
            "Iteration 18, loss = 0.56047370\n",
            "Iteration 19, loss = 0.55990101\n",
            "Iteration 20, loss = 0.56109159\n",
            "Iteration 21, loss = 0.56051823\n",
            "Iteration 22, loss = 0.56053818\n",
            "Iteration 23, loss = 0.56046975\n",
            "Iteration 24, loss = 0.56106912\n",
            "Iteration 25, loss = 0.56038780\n",
            "Iteration 26, loss = 0.56015519\n",
            "Iteration 27, loss = 0.56049717\n",
            "Iteration 28, loss = 0.55930250\n",
            "Iteration 29, loss = 0.55975316\n",
            "Iteration 30, loss = 0.55923753\n",
            "Iteration 31, loss = 0.55959389\n",
            "Iteration 32, loss = 0.56007537\n",
            "Iteration 33, loss = 0.56012047\n",
            "Iteration 34, loss = 0.55885382\n",
            "Iteration 35, loss = 0.55948179\n",
            "Iteration 36, loss = 0.55930294\n",
            "Iteration 37, loss = 0.55918495\n",
            "Iteration 38, loss = 0.55895717\n",
            "Iteration 39, loss = 0.55922196\n",
            "Iteration 40, loss = 0.55798977\n",
            "Iteration 41, loss = 0.55839759\n",
            "Iteration 42, loss = 0.55879070\n",
            "Iteration 43, loss = 0.55820409\n",
            "Iteration 44, loss = 0.55858870\n",
            "Iteration 45, loss = 0.55810809\n",
            "Iteration 46, loss = 0.55793416\n",
            "Iteration 47, loss = 0.55785965\n",
            "Iteration 48, loss = 0.55793528\n",
            "Iteration 49, loss = 0.55780987\n",
            "Iteration 50, loss = 0.55770199\n",
            "Iteration 51, loss = 0.55740511\n",
            "Iteration 52, loss = 0.55782981\n",
            "Iteration 53, loss = 0.55770666\n",
            "Iteration 54, loss = 0.55644812\n",
            "Iteration 55, loss = 0.55644235\n",
            "Iteration 56, loss = 0.55549618\n",
            "Iteration 57, loss = 0.55517150\n",
            "Iteration 58, loss = 0.55575036\n",
            "Iteration 59, loss = 0.55478407\n",
            "Iteration 60, loss = 0.55551264\n",
            "Iteration 61, loss = 0.55574508\n",
            "Iteration 62, loss = 0.55445286\n",
            "Iteration 63, loss = 0.55384476\n",
            "Iteration 64, loss = 0.55343735\n",
            "Iteration 65, loss = 0.55389881\n",
            "Iteration 66, loss = 0.55337610\n",
            "Iteration 67, loss = 0.55314426\n",
            "Iteration 68, loss = 0.55238397\n",
            "Iteration 69, loss = 0.55183231\n",
            "Iteration 70, loss = 0.55237101\n",
            "Iteration 71, loss = 0.55100516\n",
            "Iteration 72, loss = 0.55060713\n",
            "Iteration 73, loss = 0.55019026\n",
            "Iteration 74, loss = 0.54936176\n",
            "Iteration 75, loss = 0.54924395\n",
            "Iteration 76, loss = 0.54820094\n",
            "Iteration 77, loss = 0.54796548\n",
            "Iteration 78, loss = 0.54693382\n",
            "Iteration 79, loss = 0.54614585\n",
            "Iteration 80, loss = 0.54529263\n",
            "Iteration 81, loss = 0.54528536\n",
            "Iteration 82, loss = 0.54482482\n",
            "Iteration 83, loss = 0.54390861\n",
            "Iteration 84, loss = 0.54431252\n",
            "Iteration 85, loss = 0.54307236\n",
            "Iteration 86, loss = 0.54181943\n",
            "Iteration 87, loss = 0.54178182\n",
            "Iteration 88, loss = 0.54048904\n",
            "Iteration 89, loss = 0.53930966\n",
            "Iteration 90, loss = 0.53886393\n",
            "Iteration 91, loss = 0.53796908\n",
            "Iteration 92, loss = 0.53738918\n",
            "Iteration 93, loss = 0.53707140\n",
            "Iteration 94, loss = 0.53614765\n",
            "Iteration 95, loss = 0.53540166\n",
            "Iteration 96, loss = 0.53506542\n",
            "Iteration 97, loss = 0.53430273\n",
            "Iteration 98, loss = 0.53312474\n",
            "Iteration 99, loss = 0.53287115\n",
            "Iteration 100, loss = 0.53271095\n",
            "Iteration 101, loss = 0.53172402\n",
            "Iteration 102, loss = 0.53186333\n",
            "Iteration 103, loss = 0.53104917\n",
            "Iteration 104, loss = 0.53000172\n",
            "Iteration 105, loss = 0.53052403\n",
            "Iteration 106, loss = 0.52863160\n",
            "Iteration 107, loss = 0.52792157\n",
            "Iteration 108, loss = 0.53000546\n",
            "Iteration 109, loss = 0.52838957\n",
            "Iteration 110, loss = 0.52765733\n",
            "Iteration 111, loss = 0.52782033\n",
            "Iteration 112, loss = 0.52749682\n",
            "Iteration 113, loss = 0.52737750\n",
            "Iteration 114, loss = 0.52656085\n",
            "Iteration 115, loss = 0.52566449\n",
            "Iteration 116, loss = 0.52612262\n",
            "Iteration 117, loss = 0.52587399\n",
            "Iteration 118, loss = 0.52424643\n",
            "Iteration 119, loss = 0.52510778\n",
            "Iteration 120, loss = 0.52604112\n",
            "Iteration 121, loss = 0.52460733\n",
            "Iteration 122, loss = 0.52554697\n",
            "Iteration 123, loss = 0.52426430\n",
            "Iteration 124, loss = 0.52409576\n",
            "Iteration 125, loss = 0.52398591\n",
            "Iteration 126, loss = 0.52393475\n",
            "Iteration 127, loss = 0.52227568\n",
            "Iteration 128, loss = 0.52294462\n",
            "Iteration 129, loss = 0.52298305\n",
            "Iteration 130, loss = 0.52344115\n",
            "Iteration 131, loss = 0.52248205\n",
            "Iteration 132, loss = 0.52286146\n",
            "Iteration 133, loss = 0.52332145\n",
            "Iteration 134, loss = 0.52229518\n",
            "Iteration 135, loss = 0.52231283\n",
            "Iteration 136, loss = 0.52136416\n",
            "Iteration 137, loss = 0.52228046\n",
            "Iteration 138, loss = 0.52169396\n",
            "Iteration 139, loss = 0.52151693\n",
            "Iteration 140, loss = 0.52092338\n",
            "Iteration 141, loss = 0.52155958\n",
            "Iteration 142, loss = 0.52085403\n",
            "Iteration 143, loss = 0.52116177\n",
            "Iteration 144, loss = 0.52026165\n",
            "Iteration 145, loss = 0.52104377\n",
            "Iteration 146, loss = 0.52027249\n",
            "Iteration 147, loss = 0.52062701\n",
            "Iteration 148, loss = 0.52143652\n",
            "Iteration 149, loss = 0.52029564\n",
            "Iteration 150, loss = 0.51959322\n",
            "Iteration 151, loss = 0.52033269\n",
            "Iteration 152, loss = 0.52084381\n",
            "Iteration 153, loss = 0.51907582\n",
            "Iteration 154, loss = 0.51929571\n",
            "Iteration 155, loss = 0.51969908\n",
            "Iteration 156, loss = 0.51815962\n",
            "Iteration 157, loss = 0.51936925\n",
            "Iteration 158, loss = 0.51970609\n",
            "Iteration 159, loss = 0.51838074\n",
            "Iteration 160, loss = 0.51930297\n",
            "Iteration 161, loss = 0.51811888\n",
            "Iteration 162, loss = 0.51849722\n",
            "Iteration 163, loss = 0.51899751\n",
            "Iteration 164, loss = 0.51867246\n",
            "Iteration 165, loss = 0.51896688\n",
            "Iteration 166, loss = 0.51922261\n",
            "Iteration 167, loss = 0.51904523\n",
            "Iteration 168, loss = 0.51828500\n",
            "Iteration 169, loss = 0.51757154\n",
            "Iteration 170, loss = 0.51885294\n",
            "Iteration 171, loss = 0.51826472\n",
            "Iteration 172, loss = 0.51879060\n",
            "Iteration 173, loss = 0.51852972\n",
            "Iteration 174, loss = 0.51810797\n",
            "Iteration 175, loss = 0.51769672\n",
            "Iteration 176, loss = 0.51922027\n",
            "Iteration 177, loss = 0.51831253\n",
            "Iteration 178, loss = 0.51858020\n",
            "Iteration 179, loss = 0.51764619\n",
            "Iteration 180, loss = 0.51869813\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "6\n",
            "Iteration 1, loss = 0.57122054\n",
            "Iteration 2, loss = 0.56347488\n",
            "Iteration 3, loss = 0.56351129\n",
            "Iteration 4, loss = 0.56362948\n",
            "Iteration 5, loss = 0.56265615\n",
            "Iteration 6, loss = 0.56345027\n",
            "Iteration 7, loss = 0.56289233\n",
            "Iteration 8, loss = 0.56282796\n",
            "Iteration 9, loss = 0.56394589\n",
            "Iteration 10, loss = 0.56321179\n",
            "Iteration 11, loss = 0.56326631\n",
            "Iteration 12, loss = 0.56290711\n",
            "Iteration 13, loss = 0.56297596\n",
            "Iteration 14, loss = 0.56309859\n",
            "Iteration 15, loss = 0.56238738\n",
            "Iteration 16, loss = 0.56300277\n",
            "Iteration 17, loss = 0.56300426\n",
            "Iteration 18, loss = 0.56312105\n",
            "Iteration 19, loss = 0.56271975\n",
            "Iteration 20, loss = 0.56273289\n",
            "Iteration 21, loss = 0.56234046\n",
            "Iteration 22, loss = 0.56329700\n",
            "Iteration 23, loss = 0.56230179\n",
            "Iteration 24, loss = 0.56251814\n",
            "Iteration 25, loss = 0.56312662\n",
            "Iteration 26, loss = 0.56218931\n",
            "Iteration 27, loss = 0.56169009\n",
            "Iteration 28, loss = 0.56137546\n",
            "Iteration 29, loss = 0.56251538\n",
            "Iteration 30, loss = 0.56227388\n",
            "Iteration 31, loss = 0.56220160\n",
            "Iteration 32, loss = 0.56181227\n",
            "Iteration 33, loss = 0.56139491\n",
            "Iteration 34, loss = 0.56125637\n",
            "Iteration 35, loss = 0.56055838\n",
            "Iteration 36, loss = 0.56097173\n",
            "Iteration 37, loss = 0.56147538\n",
            "Iteration 38, loss = 0.56139529\n",
            "Iteration 39, loss = 0.56088711\n",
            "Iteration 40, loss = 0.56122401\n",
            "Iteration 41, loss = 0.56071465\n",
            "Iteration 42, loss = 0.56078610\n",
            "Iteration 43, loss = 0.56003142\n",
            "Iteration 44, loss = 0.56069610\n",
            "Iteration 45, loss = 0.56020062\n",
            "Iteration 46, loss = 0.56017141\n",
            "Iteration 47, loss = 0.56036677\n",
            "Iteration 48, loss = 0.55945235\n",
            "Iteration 49, loss = 0.55970564\n",
            "Iteration 50, loss = 0.55950541\n",
            "Iteration 51, loss = 0.55790426\n",
            "Iteration 52, loss = 0.55913305\n",
            "Iteration 53, loss = 0.55938029\n",
            "Iteration 54, loss = 0.55864583\n",
            "Iteration 55, loss = 0.55833118\n",
            "Iteration 56, loss = 0.55890727\n",
            "Iteration 57, loss = 0.55802656\n",
            "Iteration 58, loss = 0.55735399\n",
            "Iteration 59, loss = 0.55762415\n",
            "Iteration 60, loss = 0.55698931\n",
            "Iteration 61, loss = 0.55561906\n",
            "Iteration 62, loss = 0.55621809\n",
            "Iteration 63, loss = 0.55599577\n",
            "Iteration 64, loss = 0.55509098\n",
            "Iteration 65, loss = 0.55472650\n",
            "Iteration 66, loss = 0.55511222\n",
            "Iteration 67, loss = 0.55425451\n",
            "Iteration 68, loss = 0.55359922\n",
            "Iteration 69, loss = 0.55328873\n",
            "Iteration 70, loss = 0.55231789\n",
            "Iteration 71, loss = 0.55215790\n",
            "Iteration 72, loss = 0.55261773\n",
            "Iteration 73, loss = 0.55111101\n",
            "Iteration 74, loss = 0.54973176\n",
            "Iteration 75, loss = 0.55023503\n",
            "Iteration 76, loss = 0.54877556\n",
            "Iteration 77, loss = 0.54977787\n",
            "Iteration 78, loss = 0.54846533\n",
            "Iteration 79, loss = 0.54677201\n",
            "Iteration 80, loss = 0.54687267\n",
            "Iteration 81, loss = 0.54649807\n",
            "Iteration 82, loss = 0.54552948\n",
            "Iteration 83, loss = 0.54435491\n",
            "Iteration 84, loss = 0.54353441\n",
            "Iteration 85, loss = 0.54268468\n",
            "Iteration 86, loss = 0.54254940\n",
            "Iteration 87, loss = 0.54098529\n",
            "Iteration 88, loss = 0.54041030\n",
            "Iteration 89, loss = 0.54045139\n",
            "Iteration 90, loss = 0.54023220\n",
            "Iteration 91, loss = 0.53846795\n",
            "Iteration 92, loss = 0.53759676\n",
            "Iteration 93, loss = 0.53715316\n",
            "Iteration 94, loss = 0.53653086\n",
            "Iteration 95, loss = 0.53626915\n",
            "Iteration 96, loss = 0.53496924\n",
            "Iteration 97, loss = 0.53537320\n",
            "Iteration 98, loss = 0.53365469\n",
            "Iteration 99, loss = 0.53270146\n",
            "Iteration 100, loss = 0.53340743\n",
            "Iteration 101, loss = 0.53329315\n",
            "Iteration 102, loss = 0.53182416\n",
            "Iteration 103, loss = 0.53182640\n",
            "Iteration 104, loss = 0.53074350\n",
            "Iteration 105, loss = 0.53101034\n",
            "Iteration 106, loss = 0.53099972\n",
            "Iteration 107, loss = 0.53036084\n",
            "Iteration 108, loss = 0.52930992\n",
            "Iteration 109, loss = 0.52945154\n",
            "Iteration 110, loss = 0.52936798\n",
            "Iteration 111, loss = 0.52916657\n",
            "Iteration 112, loss = 0.52898370\n",
            "Iteration 113, loss = 0.52890223\n",
            "Iteration 114, loss = 0.52814728\n",
            "Iteration 115, loss = 0.52842987\n",
            "Iteration 116, loss = 0.52833511\n",
            "Iteration 117, loss = 0.52784023\n",
            "Iteration 118, loss = 0.52708636\n",
            "Iteration 119, loss = 0.52632194\n",
            "Iteration 120, loss = 0.52707411\n",
            "Iteration 121, loss = 0.52712098\n",
            "Iteration 122, loss = 0.52656796\n",
            "Iteration 123, loss = 0.52606795\n",
            "Iteration 124, loss = 0.52611664\n",
            "Iteration 125, loss = 0.52719130\n",
            "Iteration 126, loss = 0.52595542\n",
            "Iteration 127, loss = 0.52586383\n",
            "Iteration 128, loss = 0.52496697\n",
            "Iteration 129, loss = 0.52548563\n",
            "Iteration 130, loss = 0.52495128\n",
            "Iteration 131, loss = 0.52510199\n",
            "Iteration 132, loss = 0.52498388\n",
            "Iteration 133, loss = 0.52384367\n",
            "Iteration 134, loss = 0.52407485\n",
            "Iteration 135, loss = 0.52435209\n",
            "Iteration 136, loss = 0.52337408\n",
            "Iteration 137, loss = 0.52338773\n",
            "Iteration 138, loss = 0.52474035\n",
            "Iteration 139, loss = 0.52491240\n",
            "Iteration 140, loss = 0.52353567\n",
            "Iteration 141, loss = 0.52296248\n",
            "Iteration 142, loss = 0.52314680\n",
            "Iteration 143, loss = 0.52410411\n",
            "Iteration 144, loss = 0.52326328\n",
            "Iteration 145, loss = 0.52297046\n",
            "Iteration 146, loss = 0.52283228\n",
            "Iteration 147, loss = 0.52246986\n",
            "Iteration 148, loss = 0.52237128\n",
            "Iteration 149, loss = 0.52285861\n",
            "Iteration 150, loss = 0.52221128\n",
            "Iteration 151, loss = 0.52212090\n",
            "Iteration 152, loss = 0.52330683\n",
            "Iteration 153, loss = 0.52240040\n",
            "Iteration 154, loss = 0.52270963\n",
            "Iteration 155, loss = 0.52204717\n",
            "Iteration 156, loss = 0.52293436\n",
            "Iteration 157, loss = 0.52091050\n",
            "Iteration 158, loss = 0.52234934\n",
            "Iteration 159, loss = 0.52118428\n",
            "Iteration 160, loss = 0.52160948\n",
            "Iteration 161, loss = 0.52101440\n",
            "Iteration 162, loss = 0.52193316\n",
            "Iteration 163, loss = 0.52184881\n",
            "Iteration 164, loss = 0.52000832\n",
            "Iteration 165, loss = 0.52206953\n",
            "Iteration 166, loss = 0.52218431\n",
            "Iteration 167, loss = 0.52170895\n",
            "Iteration 168, loss = 0.52195383\n",
            "Iteration 169, loss = 0.52165506\n",
            "Iteration 170, loss = 0.52173046\n",
            "Iteration 171, loss = 0.52101050\n",
            "Iteration 172, loss = 0.52151442\n",
            "Iteration 173, loss = 0.52094035\n",
            "Iteration 174, loss = 0.52075032\n",
            "Iteration 175, loss = 0.52057710\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56704829\n",
            "Iteration 2, loss = 0.56334140\n",
            "Iteration 3, loss = 0.56307920\n",
            "Iteration 4, loss = 0.56298013\n",
            "Iteration 5, loss = 0.56287521\n",
            "Iteration 6, loss = 0.56274604\n",
            "Iteration 7, loss = 0.56318751\n",
            "Iteration 8, loss = 0.56234156\n",
            "Iteration 9, loss = 0.56334070\n",
            "Iteration 10, loss = 0.56288619\n",
            "Iteration 11, loss = 0.56266546\n",
            "Iteration 12, loss = 0.56317417\n",
            "Iteration 13, loss = 0.56212049\n",
            "Iteration 14, loss = 0.56259563\n",
            "Iteration 15, loss = 0.56208567\n",
            "Iteration 16, loss = 0.56249182\n",
            "Iteration 17, loss = 0.56262029\n",
            "Iteration 18, loss = 0.56325803\n",
            "Iteration 19, loss = 0.56206507\n",
            "Iteration 20, loss = 0.56240657\n",
            "Iteration 21, loss = 0.56222886\n",
            "Iteration 22, loss = 0.56263733\n",
            "Iteration 23, loss = 0.56206534\n",
            "Iteration 24, loss = 0.56208018\n",
            "Iteration 25, loss = 0.56125256\n",
            "Iteration 26, loss = 0.56246322\n",
            "Iteration 27, loss = 0.56167681\n",
            "Iteration 28, loss = 0.56238620\n",
            "Iteration 29, loss = 0.56185080\n",
            "Iteration 30, loss = 0.56137236\n",
            "Iteration 31, loss = 0.56169760\n",
            "Iteration 32, loss = 0.56203897\n",
            "Iteration 33, loss = 0.56031975\n",
            "Iteration 34, loss = 0.56220252\n",
            "Iteration 35, loss = 0.56130949\n",
            "Iteration 36, loss = 0.56143957\n",
            "Iteration 37, loss = 0.56161947\n",
            "Iteration 38, loss = 0.56160128\n",
            "Iteration 39, loss = 0.56101943\n",
            "Iteration 40, loss = 0.56049542\n",
            "Iteration 41, loss = 0.56114875\n",
            "Iteration 42, loss = 0.56049661\n",
            "Iteration 43, loss = 0.56119708\n",
            "Iteration 44, loss = 0.55947821\n",
            "Iteration 45, loss = 0.56001230\n",
            "Iteration 46, loss = 0.56010287\n",
            "Iteration 47, loss = 0.55999416\n",
            "Iteration 48, loss = 0.55909054\n",
            "Iteration 49, loss = 0.55976362\n",
            "Iteration 50, loss = 0.55935859\n",
            "Iteration 51, loss = 0.55907559\n",
            "Iteration 52, loss = 0.55916725\n",
            "Iteration 53, loss = 0.55892372\n",
            "Iteration 54, loss = 0.55854775\n",
            "Iteration 55, loss = 0.55811158\n",
            "Iteration 56, loss = 0.55720055\n",
            "Iteration 57, loss = 0.55743002\n",
            "Iteration 58, loss = 0.55749837\n",
            "Iteration 59, loss = 0.55662159\n",
            "Iteration 60, loss = 0.55772798\n",
            "Iteration 61, loss = 0.55738482\n",
            "Iteration 62, loss = 0.55675106\n",
            "Iteration 63, loss = 0.55648368\n",
            "Iteration 64, loss = 0.55614637\n",
            "Iteration 65, loss = 0.55518463\n",
            "Iteration 66, loss = 0.55484684\n",
            "Iteration 67, loss = 0.55513942\n",
            "Iteration 68, loss = 0.55413876\n",
            "Iteration 69, loss = 0.55409041\n",
            "Iteration 70, loss = 0.55379939\n",
            "Iteration 71, loss = 0.55310063\n",
            "Iteration 72, loss = 0.55294796\n",
            "Iteration 73, loss = 0.55134568\n",
            "Iteration 74, loss = 0.55154015\n",
            "Iteration 75, loss = 0.55139925\n",
            "Iteration 76, loss = 0.55041969\n",
            "Iteration 77, loss = 0.54945460\n",
            "Iteration 78, loss = 0.54935934\n",
            "Iteration 79, loss = 0.54863460\n",
            "Iteration 80, loss = 0.54842467\n",
            "Iteration 81, loss = 0.54707900\n",
            "Iteration 82, loss = 0.54616764\n",
            "Iteration 83, loss = 0.54601591\n",
            "Iteration 84, loss = 0.54460205\n",
            "Iteration 85, loss = 0.54361965\n",
            "Iteration 86, loss = 0.54372744\n",
            "Iteration 87, loss = 0.54246258\n",
            "Iteration 88, loss = 0.54116982\n",
            "Iteration 89, loss = 0.54151789\n",
            "Iteration 90, loss = 0.54095422\n",
            "Iteration 91, loss = 0.53947330\n",
            "Iteration 92, loss = 0.53996059\n",
            "Iteration 93, loss = 0.53926304\n",
            "Iteration 94, loss = 0.53726682\n",
            "Iteration 95, loss = 0.53736217\n",
            "Iteration 96, loss = 0.53686304\n",
            "Iteration 97, loss = 0.53562833\n",
            "Iteration 98, loss = 0.53547274\n",
            "Iteration 99, loss = 0.53596750\n",
            "Iteration 100, loss = 0.53468340\n",
            "Iteration 101, loss = 0.53451014\n",
            "Iteration 102, loss = 0.53349534\n",
            "Iteration 103, loss = 0.53329575\n",
            "Iteration 104, loss = 0.53230070\n",
            "Iteration 105, loss = 0.53245828\n",
            "Iteration 106, loss = 0.53203613\n",
            "Iteration 107, loss = 0.53097605\n",
            "Iteration 108, loss = 0.53127341\n",
            "Iteration 109, loss = 0.53129705\n",
            "Iteration 110, loss = 0.53066103\n",
            "Iteration 111, loss = 0.52973372\n",
            "Iteration 112, loss = 0.53017573\n",
            "Iteration 113, loss = 0.53018819\n",
            "Iteration 114, loss = 0.52842197\n",
            "Iteration 115, loss = 0.52875035\n",
            "Iteration 116, loss = 0.52838612\n",
            "Iteration 117, loss = 0.52783222\n",
            "Iteration 118, loss = 0.52823855\n",
            "Iteration 119, loss = 0.52766601\n",
            "Iteration 120, loss = 0.52825875\n",
            "Iteration 121, loss = 0.52706563\n",
            "Iteration 122, loss = 0.52749947\n",
            "Iteration 123, loss = 0.52595940\n",
            "Iteration 124, loss = 0.52723320\n",
            "Iteration 125, loss = 0.52607793\n",
            "Iteration 126, loss = 0.52688125\n",
            "Iteration 127, loss = 0.52640256\n",
            "Iteration 128, loss = 0.52603720\n",
            "Iteration 129, loss = 0.52598245\n",
            "Iteration 130, loss = 0.52581978\n",
            "Iteration 131, loss = 0.52501813\n",
            "Iteration 132, loss = 0.52539721\n",
            "Iteration 133, loss = 0.52526233\n",
            "Iteration 134, loss = 0.52491505\n",
            "Iteration 135, loss = 0.52512833\n",
            "Iteration 136, loss = 0.52429278\n",
            "Iteration 137, loss = 0.52399001\n",
            "Iteration 138, loss = 0.52460192\n",
            "Iteration 139, loss = 0.52416407\n",
            "Iteration 140, loss = 0.52480052\n",
            "Iteration 141, loss = 0.52420005\n",
            "Iteration 142, loss = 0.52443033\n",
            "Iteration 143, loss = 0.52406308\n",
            "Iteration 144, loss = 0.52380381\n",
            "Iteration 145, loss = 0.52431152\n",
            "Iteration 146, loss = 0.52370417\n",
            "Iteration 147, loss = 0.52475584\n",
            "Iteration 148, loss = 0.52319124\n",
            "Iteration 149, loss = 0.52399394\n",
            "Iteration 150, loss = 0.52324192\n",
            "Iteration 151, loss = 0.52300642\n",
            "Iteration 152, loss = 0.52328584\n",
            "Iteration 153, loss = 0.52310340\n",
            "Iteration 154, loss = 0.52225237\n",
            "Iteration 155, loss = 0.52360266\n",
            "Iteration 156, loss = 0.52209309\n",
            "Iteration 157, loss = 0.52304299\n",
            "Iteration 158, loss = 0.52219515\n",
            "Iteration 159, loss = 0.52318623\n",
            "Iteration 160, loss = 0.52246749\n",
            "Iteration 161, loss = 0.52334783\n",
            "Iteration 162, loss = 0.52237221\n",
            "Iteration 163, loss = 0.52218529\n",
            "Iteration 164, loss = 0.52177685\n",
            "Iteration 165, loss = 0.52173237\n",
            "Iteration 166, loss = 0.52220358\n",
            "Iteration 167, loss = 0.52290767\n",
            "Iteration 168, loss = 0.52302150\n",
            "Iteration 169, loss = 0.52150702\n",
            "Iteration 170, loss = 0.52180974\n",
            "Iteration 171, loss = 0.52210874\n",
            "Iteration 172, loss = 0.52108926\n",
            "Iteration 173, loss = 0.52202389\n",
            "Iteration 174, loss = 0.52211312\n",
            "Iteration 175, loss = 0.52190402\n",
            "Iteration 176, loss = 0.52165146\n",
            "Iteration 177, loss = 0.52200883\n",
            "Iteration 178, loss = 0.52157676\n",
            "Iteration 179, loss = 0.52259299\n",
            "Iteration 180, loss = 0.52141464\n",
            "Iteration 181, loss = 0.52110417\n",
            "Iteration 182, loss = 0.52171967\n",
            "Iteration 183, loss = 0.52220091\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56894017\n",
            "Iteration 2, loss = 0.56227104\n",
            "Iteration 3, loss = 0.56309014\n",
            "Iteration 4, loss = 0.56194241\n",
            "Iteration 5, loss = 0.56177201\n",
            "Iteration 6, loss = 0.56236955\n",
            "Iteration 7, loss = 0.56214327\n",
            "Iteration 8, loss = 0.56190601\n",
            "Iteration 9, loss = 0.56179253\n",
            "Iteration 10, loss = 0.56182765\n",
            "Iteration 11, loss = 0.56227166\n",
            "Iteration 12, loss = 0.56145892\n",
            "Iteration 13, loss = 0.56186197\n",
            "Iteration 14, loss = 0.56182060\n",
            "Iteration 15, loss = 0.56095947\n",
            "Iteration 16, loss = 0.56168812\n",
            "Iteration 17, loss = 0.56128797\n",
            "Iteration 18, loss = 0.56137758\n",
            "Iteration 19, loss = 0.56124017\n",
            "Iteration 20, loss = 0.56163111\n",
            "Iteration 21, loss = 0.56236161\n",
            "Iteration 22, loss = 0.56151320\n",
            "Iteration 23, loss = 0.56150613\n",
            "Iteration 24, loss = 0.56118733\n",
            "Iteration 25, loss = 0.56144893\n",
            "Iteration 26, loss = 0.56164736\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56459392\n",
            "Iteration 2, loss = 0.56312932\n",
            "Iteration 3, loss = 0.56390082\n",
            "Iteration 4, loss = 0.56334398\n",
            "Iteration 5, loss = 0.56419230\n",
            "Iteration 6, loss = 0.56337996\n",
            "Iteration 7, loss = 0.56362718\n",
            "Iteration 8, loss = 0.56317655\n",
            "Iteration 9, loss = 0.56413695\n",
            "Iteration 10, loss = 0.56314548\n",
            "Iteration 11, loss = 0.56303231\n",
            "Iteration 12, loss = 0.56368502\n",
            "Iteration 13, loss = 0.56320606\n",
            "Iteration 14, loss = 0.56366380\n",
            "Iteration 15, loss = 0.56290646\n",
            "Iteration 16, loss = 0.56262558\n",
            "Iteration 17, loss = 0.56287782\n",
            "Iteration 18, loss = 0.56272398\n",
            "Iteration 19, loss = 0.56329243\n",
            "Iteration 20, loss = 0.56370599\n",
            "Iteration 21, loss = 0.56385577\n",
            "Iteration 22, loss = 0.56344367\n",
            "Iteration 23, loss = 0.56309063\n",
            "Iteration 24, loss = 0.56254091\n",
            "Iteration 25, loss = 0.56285273\n",
            "Iteration 26, loss = 0.56243981\n",
            "Iteration 27, loss = 0.56274188\n",
            "Iteration 28, loss = 0.56192972\n",
            "Iteration 29, loss = 0.56266398\n",
            "Iteration 30, loss = 0.56255855\n",
            "Iteration 31, loss = 0.56267966\n",
            "Iteration 32, loss = 0.56291459\n",
            "Iteration 33, loss = 0.56206290\n",
            "Iteration 34, loss = 0.56193823\n",
            "Iteration 35, loss = 0.56204698\n",
            "Iteration 36, loss = 0.56231759\n",
            "Iteration 37, loss = 0.56195316\n",
            "Iteration 38, loss = 0.56204843\n",
            "Iteration 39, loss = 0.56162111\n",
            "Iteration 40, loss = 0.56168005\n",
            "Iteration 41, loss = 0.56129959\n",
            "Iteration 42, loss = 0.56105114\n",
            "Iteration 43, loss = 0.56089253\n",
            "Iteration 44, loss = 0.56182142\n",
            "Iteration 45, loss = 0.56169523\n",
            "Iteration 46, loss = 0.56078164\n",
            "Iteration 47, loss = 0.56113663\n",
            "Iteration 48, loss = 0.56056326\n",
            "Iteration 49, loss = 0.56045846\n",
            "Iteration 50, loss = 0.56078119\n",
            "Iteration 51, loss = 0.55992858\n",
            "Iteration 52, loss = 0.56021931\n",
            "Iteration 53, loss = 0.56009943\n",
            "Iteration 54, loss = 0.55902025\n",
            "Iteration 55, loss = 0.55932273\n",
            "Iteration 56, loss = 0.55881243\n",
            "Iteration 57, loss = 0.55932876\n",
            "Iteration 58, loss = 0.55807946\n",
            "Iteration 59, loss = 0.55818512\n",
            "Iteration 60, loss = 0.55790351\n",
            "Iteration 61, loss = 0.55822087\n",
            "Iteration 62, loss = 0.55706907\n",
            "Iteration 63, loss = 0.55800452\n",
            "Iteration 64, loss = 0.55672196\n",
            "Iteration 65, loss = 0.55636466\n",
            "Iteration 66, loss = 0.55624586\n",
            "Iteration 67, loss = 0.55566106\n",
            "Iteration 68, loss = 0.55555043\n",
            "Iteration 69, loss = 0.55513453\n",
            "Iteration 70, loss = 0.55546408\n",
            "Iteration 71, loss = 0.55495543\n",
            "Iteration 72, loss = 0.55396493\n",
            "Iteration 73, loss = 0.55409035\n",
            "Iteration 74, loss = 0.55303761\n",
            "Iteration 75, loss = 0.55227658\n",
            "Iteration 76, loss = 0.55210412\n",
            "Iteration 77, loss = 0.55189829\n",
            "Iteration 78, loss = 0.55028885\n",
            "Iteration 79, loss = 0.55020822\n",
            "Iteration 80, loss = 0.54941072\n",
            "Iteration 81, loss = 0.54879094\n",
            "Iteration 82, loss = 0.54829425\n",
            "Iteration 83, loss = 0.54737007\n",
            "Iteration 84, loss = 0.54721909\n",
            "Iteration 85, loss = 0.54622102\n",
            "Iteration 86, loss = 0.54584202\n",
            "Iteration 87, loss = 0.54402239\n",
            "Iteration 88, loss = 0.54429683\n",
            "Iteration 89, loss = 0.54437996\n",
            "Iteration 90, loss = 0.54259092\n",
            "Iteration 91, loss = 0.54230467\n",
            "Iteration 92, loss = 0.54153290\n",
            "Iteration 93, loss = 0.53965855\n",
            "Iteration 94, loss = 0.54072658\n",
            "Iteration 95, loss = 0.53888301\n",
            "Iteration 96, loss = 0.53813570\n",
            "Iteration 97, loss = 0.53775173\n",
            "Iteration 98, loss = 0.53715154\n",
            "Iteration 99, loss = 0.53632037\n",
            "Iteration 100, loss = 0.53548619\n",
            "Iteration 101, loss = 0.53513736\n",
            "Iteration 102, loss = 0.53484244\n",
            "Iteration 103, loss = 0.53366447\n",
            "Iteration 104, loss = 0.53358036\n",
            "Iteration 105, loss = 0.53272202\n",
            "Iteration 106, loss = 0.53311669\n",
            "Iteration 107, loss = 0.53205621\n",
            "Iteration 108, loss = 0.53224208\n",
            "Iteration 109, loss = 0.53067321\n",
            "Iteration 110, loss = 0.52876687\n",
            "Iteration 111, loss = 0.53004659\n",
            "Iteration 112, loss = 0.52981198\n",
            "Iteration 113, loss = 0.53022146\n",
            "Iteration 114, loss = 0.52839798\n",
            "Iteration 115, loss = 0.52927089\n",
            "Iteration 116, loss = 0.52801300\n",
            "Iteration 117, loss = 0.52829136\n",
            "Iteration 118, loss = 0.52734938\n",
            "Iteration 119, loss = 0.52715165\n",
            "Iteration 120, loss = 0.52728929\n",
            "Iteration 121, loss = 0.52619066\n",
            "Iteration 122, loss = 0.52640235\n",
            "Iteration 123, loss = 0.52597041\n",
            "Iteration 124, loss = 0.52571056\n",
            "Iteration 125, loss = 0.52515860\n",
            "Iteration 126, loss = 0.52573352\n",
            "Iteration 127, loss = 0.52601163\n",
            "Iteration 128, loss = 0.52373039\n",
            "Iteration 129, loss = 0.52479831\n",
            "Iteration 130, loss = 0.52396771\n",
            "Iteration 131, loss = 0.52403270\n",
            "Iteration 132, loss = 0.52357599\n",
            "Iteration 133, loss = 0.52329514\n",
            "Iteration 134, loss = 0.52292280\n",
            "Iteration 135, loss = 0.52264531\n",
            "Iteration 136, loss = 0.52355424\n",
            "Iteration 137, loss = 0.52279832\n",
            "Iteration 138, loss = 0.52258641\n",
            "Iteration 139, loss = 0.52242613\n",
            "Iteration 140, loss = 0.52214944\n",
            "Iteration 141, loss = 0.52341482\n",
            "Iteration 142, loss = 0.52232297\n",
            "Iteration 143, loss = 0.52229341\n",
            "Iteration 144, loss = 0.52198964\n",
            "Iteration 145, loss = 0.52285092\n",
            "Iteration 146, loss = 0.52204282\n",
            "Iteration 147, loss = 0.52109916\n",
            "Iteration 148, loss = 0.52109408\n",
            "Iteration 149, loss = 0.52056531\n",
            "Iteration 150, loss = 0.52078716\n",
            "Iteration 151, loss = 0.52091221\n",
            "Iteration 152, loss = 0.52006564\n",
            "Iteration 153, loss = 0.52003140\n",
            "Iteration 154, loss = 0.52033974\n",
            "Iteration 155, loss = 0.52053755\n",
            "Iteration 156, loss = 0.52043176\n",
            "Iteration 157, loss = 0.51991078\n",
            "Iteration 158, loss = 0.52024858\n",
            "Iteration 159, loss = 0.52028527\n",
            "Iteration 160, loss = 0.51890228\n",
            "Iteration 161, loss = 0.51970566\n",
            "Iteration 162, loss = 0.51874669\n",
            "Iteration 163, loss = 0.52054330\n",
            "Iteration 164, loss = 0.51880314\n",
            "Iteration 165, loss = 0.51930590\n",
            "Iteration 166, loss = 0.51930865\n",
            "Iteration 167, loss = 0.51901923\n",
            "Iteration 168, loss = 0.51955205\n",
            "Iteration 169, loss = 0.51927281\n",
            "Iteration 170, loss = 0.52143806\n",
            "Iteration 171, loss = 0.51799345\n",
            "Iteration 172, loss = 0.51841424\n",
            "Iteration 173, loss = 0.51899904\n",
            "Iteration 174, loss = 0.51859325\n",
            "Iteration 175, loss = 0.51944956\n",
            "Iteration 176, loss = 0.51829548\n",
            "Iteration 177, loss = 0.51827461\n",
            "Iteration 178, loss = 0.51822407\n",
            "Iteration 179, loss = 0.51891281\n",
            "Iteration 180, loss = 0.51961805\n",
            "Iteration 181, loss = 0.51830667\n",
            "Iteration 182, loss = 0.51793248\n",
            "Iteration 183, loss = 0.51895701\n",
            "Iteration 184, loss = 0.51850252\n",
            "Iteration 185, loss = 0.51810511\n",
            "Iteration 186, loss = 0.51918641\n",
            "Iteration 187, loss = 0.51782367\n",
            "Iteration 188, loss = 0.51780833\n",
            "Iteration 189, loss = 0.51781034\n",
            "Iteration 190, loss = 0.51858512\n",
            "Iteration 191, loss = 0.51867230\n",
            "Iteration 192, loss = 0.51858682\n",
            "Iteration 193, loss = 0.51848472\n",
            "Iteration 194, loss = 0.51825464\n",
            "Iteration 195, loss = 0.51764565\n",
            "Iteration 196, loss = 0.51731081\n",
            "Iteration 197, loss = 0.51771223\n",
            "Iteration 198, loss = 0.51733863\n",
            "Iteration 199, loss = 0.51784502\n",
            "Iteration 200, loss = 0.51964668\n",
            "Iteration 201, loss = 0.51771288\n",
            "Iteration 202, loss = 0.51767130\n",
            "Iteration 203, loss = 0.51773296\n",
            "Iteration 204, loss = 0.51783868\n",
            "Iteration 205, loss = 0.51748861\n",
            "Iteration 206, loss = 0.51780132\n",
            "Iteration 207, loss = 0.51776083\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56133439\n",
            "Iteration 2, loss = 0.56043633\n",
            "Iteration 3, loss = 0.55940880\n",
            "Iteration 4, loss = 0.55981431\n",
            "Iteration 5, loss = 0.55977212\n",
            "Iteration 6, loss = 0.55998985\n",
            "Iteration 7, loss = 0.55967484\n",
            "Iteration 8, loss = 0.55957321\n",
            "Iteration 9, loss = 0.56017165\n",
            "Iteration 10, loss = 0.55947779\n",
            "Iteration 11, loss = 0.55955995\n",
            "Iteration 12, loss = 0.55893441\n",
            "Iteration 13, loss = 0.55929178\n",
            "Iteration 14, loss = 0.55959575\n",
            "Iteration 15, loss = 0.55914163\n",
            "Iteration 16, loss = 0.55903882\n",
            "Iteration 17, loss = 0.55919971\n",
            "Iteration 18, loss = 0.55947596\n",
            "Iteration 19, loss = 0.55857831\n",
            "Iteration 20, loss = 0.55957994\n",
            "Iteration 21, loss = 0.55955783\n",
            "Iteration 22, loss = 0.55847631\n",
            "Iteration 23, loss = 0.55949657\n",
            "Iteration 24, loss = 0.55894316\n",
            "Iteration 25, loss = 0.55858355\n",
            "Iteration 26, loss = 0.55889753\n",
            "Iteration 27, loss = 0.55889090\n",
            "Iteration 28, loss = 0.55828325\n",
            "Iteration 29, loss = 0.55868945\n",
            "Iteration 30, loss = 0.55853340\n",
            "Iteration 31, loss = 0.55802780\n",
            "Iteration 32, loss = 0.55786182\n",
            "Iteration 33, loss = 0.55829535\n",
            "Iteration 34, loss = 0.55738296\n",
            "Iteration 35, loss = 0.55795875\n",
            "Iteration 36, loss = 0.55750605\n",
            "Iteration 37, loss = 0.55818640\n",
            "Iteration 38, loss = 0.55788638\n",
            "Iteration 39, loss = 0.55767017\n",
            "Iteration 40, loss = 0.55790042\n",
            "Iteration 41, loss = 0.55760122\n",
            "Iteration 42, loss = 0.55721180\n",
            "Iteration 43, loss = 0.55640689\n",
            "Iteration 44, loss = 0.55712110\n",
            "Iteration 45, loss = 0.55653364\n",
            "Iteration 46, loss = 0.55662110\n",
            "Iteration 47, loss = 0.55650944\n",
            "Iteration 48, loss = 0.55597817\n",
            "Iteration 49, loss = 0.55565570\n",
            "Iteration 50, loss = 0.55562084\n",
            "Iteration 51, loss = 0.55524446\n",
            "Iteration 52, loss = 0.55609886\n",
            "Iteration 53, loss = 0.55508973\n",
            "Iteration 54, loss = 0.55467455\n",
            "Iteration 55, loss = 0.55417806\n",
            "Iteration 56, loss = 0.55417260\n",
            "Iteration 57, loss = 0.55290101\n",
            "Iteration 58, loss = 0.55357568\n",
            "Iteration 59, loss = 0.55365490\n",
            "Iteration 60, loss = 0.55351965\n",
            "Iteration 61, loss = 0.55205701\n",
            "Iteration 62, loss = 0.55266043\n",
            "Iteration 63, loss = 0.55174560\n",
            "Iteration 64, loss = 0.55143542\n",
            "Iteration 65, loss = 0.55109121\n",
            "Iteration 66, loss = 0.55073634\n",
            "Iteration 67, loss = 0.55060396\n",
            "Iteration 68, loss = 0.54944116\n",
            "Iteration 69, loss = 0.54992489\n",
            "Iteration 70, loss = 0.54848224\n",
            "Iteration 71, loss = 0.54815499\n",
            "Iteration 72, loss = 0.54736300\n",
            "Iteration 73, loss = 0.54660186\n",
            "Iteration 74, loss = 0.54610973\n",
            "Iteration 75, loss = 0.54538595\n",
            "Iteration 76, loss = 0.54547241\n",
            "Iteration 77, loss = 0.54400059\n",
            "Iteration 78, loss = 0.54393456\n",
            "Iteration 79, loss = 0.54363876\n",
            "Iteration 80, loss = 0.54220364\n",
            "Iteration 81, loss = 0.54158265\n",
            "Iteration 82, loss = 0.54058386\n",
            "Iteration 83, loss = 0.54087076\n",
            "Iteration 84, loss = 0.53872479\n",
            "Iteration 85, loss = 0.53863509\n",
            "Iteration 86, loss = 0.53703839\n",
            "Iteration 87, loss = 0.53717558\n",
            "Iteration 88, loss = 0.53589466\n",
            "Iteration 89, loss = 0.53477971\n",
            "Iteration 90, loss = 0.53374157\n",
            "Iteration 91, loss = 0.53439317\n",
            "Iteration 92, loss = 0.53302076\n",
            "Iteration 93, loss = 0.53290232\n",
            "Iteration 94, loss = 0.53120137\n",
            "Iteration 95, loss = 0.52992184\n",
            "Iteration 96, loss = 0.53089974\n",
            "Iteration 97, loss = 0.52885375\n",
            "Iteration 98, loss = 0.52899897\n",
            "Iteration 99, loss = 0.52841128\n",
            "Iteration 100, loss = 0.52756915\n",
            "Iteration 101, loss = 0.52624553\n",
            "Iteration 102, loss = 0.52721299\n",
            "Iteration 103, loss = 0.52605170\n",
            "Iteration 104, loss = 0.52667405\n",
            "Iteration 105, loss = 0.52577546\n",
            "Iteration 106, loss = 0.52559817\n",
            "Iteration 107, loss = 0.52525088\n",
            "Iteration 108, loss = 0.52508462\n",
            "Iteration 109, loss = 0.52426758\n",
            "Iteration 110, loss = 0.52315909\n",
            "Iteration 111, loss = 0.52337000\n",
            "Iteration 112, loss = 0.52371315\n",
            "Iteration 113, loss = 0.52240981\n",
            "Iteration 114, loss = 0.52258173\n",
            "Iteration 115, loss = 0.52279590\n",
            "Iteration 116, loss = 0.52166849\n",
            "Iteration 117, loss = 0.52301142\n",
            "Iteration 118, loss = 0.52172374\n",
            "Iteration 119, loss = 0.52120698\n",
            "Iteration 120, loss = 0.52059163\n",
            "Iteration 121, loss = 0.52129088\n",
            "Iteration 122, loss = 0.52025097\n",
            "Iteration 123, loss = 0.52017200\n",
            "Iteration 124, loss = 0.52035624\n",
            "Iteration 125, loss = 0.51984631\n",
            "Iteration 126, loss = 0.52012037\n",
            "Iteration 127, loss = 0.51958883\n",
            "Iteration 128, loss = 0.51858274\n",
            "Iteration 129, loss = 0.51902050\n",
            "Iteration 130, loss = 0.51879617\n",
            "Iteration 131, loss = 0.52041050\n",
            "Iteration 132, loss = 0.51928036\n",
            "Iteration 133, loss = 0.51867159\n",
            "Iteration 134, loss = 0.51860221\n",
            "Iteration 135, loss = 0.51839405\n",
            "Iteration 136, loss = 0.51864456\n",
            "Iteration 137, loss = 0.51851658\n",
            "Iteration 138, loss = 0.51836626\n",
            "Iteration 139, loss = 0.51859073\n",
            "Iteration 140, loss = 0.51729725\n",
            "Iteration 141, loss = 0.51753066\n",
            "Iteration 142, loss = 0.51775483\n",
            "Iteration 143, loss = 0.51779408\n",
            "Iteration 144, loss = 0.51728312\n",
            "Iteration 145, loss = 0.51693462\n",
            "Iteration 146, loss = 0.51759358\n",
            "Iteration 147, loss = 0.51698129\n",
            "Iteration 148, loss = 0.51701155\n",
            "Iteration 149, loss = 0.51709171\n",
            "Iteration 150, loss = 0.51674750\n",
            "Iteration 151, loss = 0.51715654\n",
            "Iteration 152, loss = 0.51693017\n",
            "Iteration 153, loss = 0.51680119\n",
            "Iteration 154, loss = 0.51711493\n",
            "Iteration 155, loss = 0.51584933\n",
            "Iteration 156, loss = 0.51643351\n",
            "Iteration 157, loss = 0.51670140\n",
            "Iteration 158, loss = 0.51588607\n",
            "Iteration 159, loss = 0.51647491\n",
            "Iteration 160, loss = 0.51627241\n",
            "Iteration 161, loss = 0.51629502\n",
            "Iteration 162, loss = 0.51659228\n",
            "Iteration 163, loss = 0.51584299\n",
            "Iteration 164, loss = 0.51561130\n",
            "Iteration 165, loss = 0.51664678\n",
            "Iteration 166, loss = 0.51512729\n",
            "Iteration 167, loss = 0.51617215\n",
            "Iteration 168, loss = 0.51608360\n",
            "Iteration 169, loss = 0.51505396\n",
            "Iteration 170, loss = 0.51503540\n",
            "Iteration 171, loss = 0.51625938\n",
            "Iteration 172, loss = 0.51544152\n",
            "Iteration 173, loss = 0.51614756\n",
            "Iteration 174, loss = 0.51553543\n",
            "Iteration 175, loss = 0.51470154\n",
            "Iteration 176, loss = 0.51577344\n",
            "Iteration 177, loss = 0.51530414\n",
            "Iteration 178, loss = 0.51559527\n",
            "Iteration 179, loss = 0.51372453\n",
            "Iteration 180, loss = 0.51570159\n",
            "Iteration 181, loss = 0.51523072\n",
            "Iteration 182, loss = 0.51546732\n",
            "Iteration 183, loss = 0.51528708\n",
            "Iteration 184, loss = 0.51548960\n",
            "Iteration 185, loss = 0.51519177\n",
            "Iteration 186, loss = 0.51496528\n",
            "Iteration 187, loss = 0.51579234\n",
            "Iteration 188, loss = 0.51546521\n",
            "Iteration 189, loss = 0.51596144\n",
            "Iteration 190, loss = 0.51665983\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57104437\n",
            "Iteration 2, loss = 0.56644318\n",
            "Iteration 3, loss = 0.56567586\n",
            "Iteration 4, loss = 0.56600579\n",
            "Iteration 5, loss = 0.56622905\n",
            "Iteration 6, loss = 0.56651812\n",
            "Iteration 7, loss = 0.56602591\n",
            "Iteration 8, loss = 0.56595158\n",
            "Iteration 9, loss = 0.56631205\n",
            "Iteration 10, loss = 0.56573641\n",
            "Iteration 11, loss = 0.56608459\n",
            "Iteration 12, loss = 0.56589170\n",
            "Iteration 13, loss = 0.56575098\n",
            "Iteration 14, loss = 0.56679918\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56721870\n",
            "Iteration 2, loss = 0.56242812\n",
            "Iteration 3, loss = 0.56210740\n",
            "Iteration 4, loss = 0.56209419\n",
            "Iteration 5, loss = 0.56262925\n",
            "Iteration 6, loss = 0.56143702\n",
            "Iteration 7, loss = 0.56100709\n",
            "Iteration 8, loss = 0.56191495\n",
            "Iteration 9, loss = 0.56180997\n",
            "Iteration 10, loss = 0.56222859\n",
            "Iteration 11, loss = 0.56128839\n",
            "Iteration 12, loss = 0.56176549\n",
            "Iteration 13, loss = 0.56171507\n",
            "Iteration 14, loss = 0.56040460\n",
            "Iteration 15, loss = 0.56230490\n",
            "Iteration 16, loss = 0.56174953\n",
            "Iteration 17, loss = 0.56110379\n",
            "Iteration 18, loss = 0.56137993\n",
            "Iteration 19, loss = 0.56194446\n",
            "Iteration 20, loss = 0.56128339\n",
            "Iteration 21, loss = 0.56092203\n",
            "Iteration 22, loss = 0.56082728\n",
            "Iteration 23, loss = 0.56089245\n",
            "Iteration 24, loss = 0.56121586\n",
            "Iteration 25, loss = 0.56105982\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56361364\n",
            "Iteration 2, loss = 0.56393211\n",
            "Iteration 3, loss = 0.56394829\n",
            "Iteration 4, loss = 0.56338062\n",
            "Iteration 5, loss = 0.56369434\n",
            "Iteration 6, loss = 0.56392081\n",
            "Iteration 7, loss = 0.56343827\n",
            "Iteration 8, loss = 0.56386098\n",
            "Iteration 9, loss = 0.56356525\n",
            "Iteration 10, loss = 0.56354905\n",
            "Iteration 11, loss = 0.56438462\n",
            "Iteration 12, loss = 0.56316567\n",
            "Iteration 13, loss = 0.56313532\n",
            "Iteration 14, loss = 0.56318359\n",
            "Iteration 15, loss = 0.56358752\n",
            "Iteration 16, loss = 0.56354304\n",
            "Iteration 17, loss = 0.56445356\n",
            "Iteration 18, loss = 0.56400381\n",
            "Iteration 19, loss = 0.56246087\n",
            "Iteration 20, loss = 0.56316935\n",
            "Iteration 21, loss = 0.56314495\n",
            "Iteration 22, loss = 0.56293147\n",
            "Iteration 23, loss = 0.56335746\n",
            "Iteration 24, loss = 0.56299589\n",
            "Iteration 25, loss = 0.56235885\n",
            "Iteration 26, loss = 0.56267129\n",
            "Iteration 27, loss = 0.56263305\n",
            "Iteration 28, loss = 0.56222804\n",
            "Iteration 29, loss = 0.56237215\n",
            "Iteration 30, loss = 0.56284387\n",
            "Iteration 31, loss = 0.56365700\n",
            "Iteration 32, loss = 0.56203815\n",
            "Iteration 33, loss = 0.56199029\n",
            "Iteration 34, loss = 0.56215630\n",
            "Iteration 35, loss = 0.56183843\n",
            "Iteration 36, loss = 0.56274429\n",
            "Iteration 37, loss = 0.56165907\n",
            "Iteration 38, loss = 0.56210699\n",
            "Iteration 39, loss = 0.56224562\n",
            "Iteration 40, loss = 0.56225498\n",
            "Iteration 41, loss = 0.56181431\n",
            "Iteration 42, loss = 0.56160454\n",
            "Iteration 43, loss = 0.56106901\n",
            "Iteration 44, loss = 0.56048827\n",
            "Iteration 45, loss = 0.56058309\n",
            "Iteration 46, loss = 0.56046979\n",
            "Iteration 47, loss = 0.56131482\n",
            "Iteration 48, loss = 0.56039923\n",
            "Iteration 49, loss = 0.56204712\n",
            "Iteration 50, loss = 0.56100523\n",
            "Iteration 51, loss = 0.55987597\n",
            "Iteration 52, loss = 0.56096583\n",
            "Iteration 53, loss = 0.56028289\n",
            "Iteration 54, loss = 0.55962636\n",
            "Iteration 55, loss = 0.55866752\n",
            "Iteration 56, loss = 0.55974024\n",
            "Iteration 57, loss = 0.55950034\n",
            "Iteration 58, loss = 0.55851515\n",
            "Iteration 59, loss = 0.55917223\n",
            "Iteration 60, loss = 0.55878135\n",
            "Iteration 61, loss = 0.55785359\n",
            "Iteration 62, loss = 0.55810888\n",
            "Iteration 63, loss = 0.55757488\n",
            "Iteration 64, loss = 0.55694110\n",
            "Iteration 65, loss = 0.55718804\n",
            "Iteration 66, loss = 0.55683383\n",
            "Iteration 67, loss = 0.55664423\n",
            "Iteration 68, loss = 0.55586559\n",
            "Iteration 69, loss = 0.55590808\n",
            "Iteration 70, loss = 0.55473388\n",
            "Iteration 71, loss = 0.55508031\n",
            "Iteration 72, loss = 0.55513274\n",
            "Iteration 73, loss = 0.55466162\n",
            "Iteration 74, loss = 0.55322073\n",
            "Iteration 75, loss = 0.55284975\n",
            "Iteration 76, loss = 0.55240411\n",
            "Iteration 77, loss = 0.55146945\n",
            "Iteration 78, loss = 0.55112819\n",
            "Iteration 79, loss = 0.55114034\n",
            "Iteration 80, loss = 0.55002156\n",
            "Iteration 81, loss = 0.54938405\n",
            "Iteration 82, loss = 0.54885160\n",
            "Iteration 83, loss = 0.54829419\n",
            "Iteration 84, loss = 0.54737423\n",
            "Iteration 85, loss = 0.54709199\n",
            "Iteration 86, loss = 0.54576553\n",
            "Iteration 87, loss = 0.54564950\n",
            "Iteration 88, loss = 0.54595614\n",
            "Iteration 89, loss = 0.54448690\n",
            "Iteration 90, loss = 0.54337952\n",
            "Iteration 91, loss = 0.54218698\n",
            "Iteration 92, loss = 0.54150017\n",
            "Iteration 93, loss = 0.54112003\n",
            "Iteration 94, loss = 0.54028447\n",
            "Iteration 95, loss = 0.53919556\n",
            "Iteration 96, loss = 0.53848366\n",
            "Iteration 97, loss = 0.53838541\n",
            "Iteration 98, loss = 0.53775017\n",
            "Iteration 99, loss = 0.53646308\n",
            "Iteration 100, loss = 0.53553857\n",
            "Iteration 101, loss = 0.53565699\n",
            "Iteration 102, loss = 0.53398834\n",
            "Iteration 103, loss = 0.53393511\n",
            "Iteration 104, loss = 0.53365320\n",
            "Iteration 105, loss = 0.53273034\n",
            "Iteration 106, loss = 0.53227474\n",
            "Iteration 107, loss = 0.53174473\n",
            "Iteration 108, loss = 0.53143140\n",
            "Iteration 109, loss = 0.53007886\n",
            "Iteration 110, loss = 0.53102811\n",
            "Iteration 111, loss = 0.53080880\n",
            "Iteration 112, loss = 0.52998742\n",
            "Iteration 113, loss = 0.52967288\n",
            "Iteration 114, loss = 0.52870983\n",
            "Iteration 115, loss = 0.52856833\n",
            "Iteration 116, loss = 0.52754891\n",
            "Iteration 117, loss = 0.52780748\n",
            "Iteration 118, loss = 0.52748258\n",
            "Iteration 119, loss = 0.52738753\n",
            "Iteration 120, loss = 0.52730798\n",
            "Iteration 121, loss = 0.52690682\n",
            "Iteration 122, loss = 0.52677435\n",
            "Iteration 123, loss = 0.52623997\n",
            "Iteration 124, loss = 0.52624256\n",
            "Iteration 125, loss = 0.52563905\n",
            "Iteration 126, loss = 0.52504252\n",
            "Iteration 127, loss = 0.52533837\n",
            "Iteration 128, loss = 0.52465583\n",
            "Iteration 129, loss = 0.52484236\n",
            "Iteration 130, loss = 0.52464798\n",
            "Iteration 131, loss = 0.52454578\n",
            "Iteration 132, loss = 0.52451080\n",
            "Iteration 133, loss = 0.52475003\n",
            "Iteration 134, loss = 0.52388805\n",
            "Iteration 135, loss = 0.52358726\n",
            "Iteration 136, loss = 0.52445247\n",
            "Iteration 137, loss = 0.52351406\n",
            "Iteration 138, loss = 0.52374875\n",
            "Iteration 139, loss = 0.52359390\n",
            "Iteration 140, loss = 0.52277315\n",
            "Iteration 141, loss = 0.52294772\n",
            "Iteration 142, loss = 0.52218433\n",
            "Iteration 143, loss = 0.52289864\n",
            "Iteration 144, loss = 0.52245091\n",
            "Iteration 145, loss = 0.52180320\n",
            "Iteration 146, loss = 0.52348919\n",
            "Iteration 147, loss = 0.52155564\n",
            "Iteration 148, loss = 0.52229101\n",
            "Iteration 149, loss = 0.52272029\n",
            "Iteration 150, loss = 0.52348017\n",
            "Iteration 151, loss = 0.52178613\n",
            "Iteration 152, loss = 0.52207473\n",
            "Iteration 153, loss = 0.52170997\n",
            "Iteration 154, loss = 0.52181108\n",
            "Iteration 155, loss = 0.52162011\n",
            "Iteration 156, loss = 0.52105918\n",
            "Iteration 157, loss = 0.52050276\n",
            "Iteration 158, loss = 0.52189292\n",
            "Iteration 159, loss = 0.52097554\n",
            "Iteration 160, loss = 0.52072236\n",
            "Iteration 161, loss = 0.52092629\n",
            "Iteration 162, loss = 0.52156208\n",
            "Iteration 163, loss = 0.52127371\n",
            "Iteration 164, loss = 0.52036556\n",
            "Iteration 165, loss = 0.52070858\n",
            "Iteration 166, loss = 0.52036403\n",
            "Iteration 167, loss = 0.52066821\n",
            "Iteration 168, loss = 0.51966777\n",
            "Iteration 169, loss = 0.52041946\n",
            "Iteration 170, loss = 0.52038503\n",
            "Iteration 171, loss = 0.52027042\n",
            "Iteration 172, loss = 0.51975006\n",
            "Iteration 173, loss = 0.52045414\n",
            "Iteration 174, loss = 0.52049082\n",
            "Iteration 175, loss = 0.52063472\n",
            "Iteration 176, loss = 0.51994514\n",
            "Iteration 177, loss = 0.52055000\n",
            "Iteration 178, loss = 0.52016534\n",
            "Iteration 179, loss = 0.51993098\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57266770\n",
            "Iteration 2, loss = 0.56593860\n",
            "Iteration 3, loss = 0.56679419\n",
            "Iteration 4, loss = 0.56600496\n",
            "Iteration 5, loss = 0.56618599\n",
            "Iteration 6, loss = 0.56630739\n",
            "Iteration 7, loss = 0.56504469\n",
            "Iteration 8, loss = 0.56554151\n",
            "Iteration 9, loss = 0.56589135\n",
            "Iteration 10, loss = 0.56599168\n",
            "Iteration 11, loss = 0.56649756\n",
            "Iteration 12, loss = 0.56609327\n",
            "Iteration 13, loss = 0.56651796\n",
            "Iteration 14, loss = 0.56516435\n",
            "Iteration 15, loss = 0.56689579\n",
            "Iteration 16, loss = 0.56546947\n",
            "Iteration 17, loss = 0.56501572\n",
            "Iteration 18, loss = 0.56579671\n",
            "Iteration 19, loss = 0.56602962\n",
            "Iteration 20, loss = 0.56563204\n",
            "Iteration 21, loss = 0.56551172\n",
            "Iteration 22, loss = 0.56537154\n",
            "Iteration 23, loss = 0.56594482\n",
            "Iteration 24, loss = 0.56547183\n",
            "Iteration 25, loss = 0.56522824\n",
            "Iteration 26, loss = 0.56491888\n",
            "Iteration 27, loss = 0.56502144\n",
            "Iteration 28, loss = 0.56481726\n",
            "Iteration 29, loss = 0.56448736\n",
            "Iteration 30, loss = 0.56462992\n",
            "Iteration 31, loss = 0.56400788\n",
            "Iteration 32, loss = 0.56498570\n",
            "Iteration 33, loss = 0.56372310\n",
            "Iteration 34, loss = 0.56471419\n",
            "Iteration 35, loss = 0.56405234\n",
            "Iteration 36, loss = 0.56419508\n",
            "Iteration 37, loss = 0.56438887\n",
            "Iteration 38, loss = 0.56352483\n",
            "Iteration 39, loss = 0.56372156\n",
            "Iteration 40, loss = 0.56370382\n",
            "Iteration 41, loss = 0.56342300\n",
            "Iteration 42, loss = 0.56368116\n",
            "Iteration 43, loss = 0.56272397\n",
            "Iteration 44, loss = 0.56380013\n",
            "Iteration 45, loss = 0.56272339\n",
            "Iteration 46, loss = 0.56243631\n",
            "Iteration 47, loss = 0.56283242\n",
            "Iteration 48, loss = 0.56214346\n",
            "Iteration 49, loss = 0.56262431\n",
            "Iteration 50, loss = 0.56177614\n",
            "Iteration 51, loss = 0.56201946\n",
            "Iteration 52, loss = 0.56218181\n",
            "Iteration 53, loss = 0.56202275\n",
            "Iteration 54, loss = 0.56124054\n",
            "Iteration 55, loss = 0.56121381\n",
            "Iteration 56, loss = 0.56128314\n",
            "Iteration 57, loss = 0.56058658\n",
            "Iteration 58, loss = 0.55993568\n",
            "Iteration 59, loss = 0.55982515\n",
            "Iteration 60, loss = 0.55876515\n",
            "Iteration 61, loss = 0.55832313\n",
            "Iteration 62, loss = 0.55901162\n",
            "Iteration 63, loss = 0.55833940\n",
            "Iteration 64, loss = 0.55754635\n",
            "Iteration 65, loss = 0.55810827\n",
            "Iteration 66, loss = 0.55715012\n",
            "Iteration 67, loss = 0.55700092\n",
            "Iteration 68, loss = 0.55574793\n",
            "Iteration 69, loss = 0.55466853\n",
            "Iteration 70, loss = 0.55529837\n",
            "Iteration 71, loss = 0.55504399\n",
            "Iteration 72, loss = 0.55443709\n",
            "Iteration 73, loss = 0.55334791\n",
            "Iteration 74, loss = 0.55288651\n",
            "Iteration 75, loss = 0.55283900\n",
            "Iteration 76, loss = 0.55219174\n",
            "Iteration 77, loss = 0.55091716\n",
            "Iteration 78, loss = 0.55064625\n",
            "Iteration 79, loss = 0.54998015\n",
            "Iteration 80, loss = 0.54842200\n",
            "Iteration 81, loss = 0.54874121\n",
            "Iteration 82, loss = 0.54796706\n",
            "Iteration 83, loss = 0.54710995\n",
            "Iteration 84, loss = 0.54611422\n",
            "Iteration 85, loss = 0.54583967\n",
            "Iteration 86, loss = 0.54469377\n",
            "Iteration 87, loss = 0.54420863\n",
            "Iteration 88, loss = 0.54336892\n",
            "Iteration 89, loss = 0.54197419\n",
            "Iteration 90, loss = 0.54193186\n",
            "Iteration 91, loss = 0.54096700\n",
            "Iteration 92, loss = 0.53996265\n",
            "Iteration 93, loss = 0.54000479\n",
            "Iteration 94, loss = 0.53901682\n",
            "Iteration 95, loss = 0.53887039\n",
            "Iteration 96, loss = 0.53825119\n",
            "Iteration 97, loss = 0.53704601\n",
            "Iteration 98, loss = 0.53574963\n",
            "Iteration 99, loss = 0.53584993\n",
            "Iteration 100, loss = 0.53586789\n",
            "Iteration 101, loss = 0.53571062\n",
            "Iteration 102, loss = 0.53537762\n",
            "Iteration 103, loss = 0.53368148\n",
            "Iteration 104, loss = 0.53384515\n",
            "Iteration 105, loss = 0.53345127\n",
            "Iteration 106, loss = 0.53337263\n",
            "Iteration 107, loss = 0.53186787\n",
            "Iteration 108, loss = 0.53193948\n",
            "Iteration 109, loss = 0.53233609\n",
            "Iteration 110, loss = 0.53119496\n",
            "Iteration 111, loss = 0.53147040\n",
            "Iteration 112, loss = 0.53110796\n",
            "Iteration 113, loss = 0.53116750\n",
            "Iteration 114, loss = 0.53018948\n",
            "Iteration 115, loss = 0.52968833\n",
            "Iteration 116, loss = 0.53007925\n",
            "Iteration 117, loss = 0.52996971\n",
            "Iteration 118, loss = 0.52960611\n",
            "Iteration 119, loss = 0.53005638\n",
            "Iteration 120, loss = 0.52921683\n",
            "Iteration 121, loss = 0.52782249\n",
            "Iteration 122, loss = 0.52888296\n",
            "Iteration 123, loss = 0.52822819\n",
            "Iteration 124, loss = 0.52795769\n",
            "Iteration 125, loss = 0.52839516\n",
            "Iteration 126, loss = 0.52825752\n",
            "Iteration 127, loss = 0.52685022\n",
            "Iteration 128, loss = 0.52756008\n",
            "Iteration 129, loss = 0.52779909\n",
            "Iteration 130, loss = 0.52734646\n",
            "Iteration 131, loss = 0.52706698\n",
            "Iteration 132, loss = 0.52592267\n",
            "Iteration 133, loss = 0.52692749\n",
            "Iteration 134, loss = 0.52619533\n",
            "Iteration 135, loss = 0.52640124\n",
            "Iteration 136, loss = 0.52678925\n",
            "Iteration 137, loss = 0.52697824\n",
            "Iteration 138, loss = 0.52595977\n",
            "Iteration 139, loss = 0.52556129\n",
            "Iteration 140, loss = 0.52508815\n",
            "Iteration 141, loss = 0.52628242\n",
            "Iteration 142, loss = 0.52549519\n",
            "Iteration 143, loss = 0.52508893\n",
            "Iteration 144, loss = 0.52580499\n",
            "Iteration 145, loss = 0.52549235\n",
            "Iteration 146, loss = 0.52537670\n",
            "Iteration 147, loss = 0.52424082\n",
            "Iteration 148, loss = 0.52595446\n",
            "Iteration 149, loss = 0.52370899\n",
            "Iteration 150, loss = 0.52440352\n",
            "Iteration 151, loss = 0.52513465\n",
            "Iteration 152, loss = 0.52449860\n",
            "Iteration 153, loss = 0.52420581\n",
            "Iteration 154, loss = 0.52378656\n",
            "Iteration 155, loss = 0.52434260\n",
            "Iteration 156, loss = 0.52417418\n",
            "Iteration 157, loss = 0.52393836\n",
            "Iteration 158, loss = 0.52440536\n",
            "Iteration 159, loss = 0.52421843\n",
            "Iteration 160, loss = 0.52375214\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57194553\n",
            "Iteration 2, loss = 0.55861712\n",
            "Iteration 3, loss = 0.55866456\n",
            "Iteration 4, loss = 0.55879809\n",
            "Iteration 5, loss = 0.55841410\n",
            "Iteration 6, loss = 0.55830735\n",
            "Iteration 7, loss = 0.55827194\n",
            "Iteration 8, loss = 0.55901227\n",
            "Iteration 9, loss = 0.55908279\n",
            "Iteration 10, loss = 0.55897867\n",
            "Iteration 11, loss = 0.55864640\n",
            "Iteration 12, loss = 0.55896794\n",
            "Iteration 13, loss = 0.55791314\n",
            "Iteration 14, loss = 0.55898396\n",
            "Iteration 15, loss = 0.55872677\n",
            "Iteration 16, loss = 0.55816101\n",
            "Iteration 17, loss = 0.55914655\n",
            "Iteration 18, loss = 0.55861307\n",
            "Iteration 19, loss = 0.55846232\n",
            "Iteration 20, loss = 0.55894189\n",
            "Iteration 21, loss = 0.55862807\n",
            "Iteration 22, loss = 0.55887080\n",
            "Iteration 23, loss = 0.55815254\n",
            "Iteration 24, loss = 0.55839603\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "7\n",
            "Iteration 1, loss = 0.57055620\n",
            "Iteration 2, loss = 0.56473747\n",
            "Iteration 3, loss = 0.56357212\n",
            "Iteration 4, loss = 0.56369306\n",
            "Iteration 5, loss = 0.56375144\n",
            "Iteration 6, loss = 0.56398962\n",
            "Iteration 7, loss = 0.56317650\n",
            "Iteration 8, loss = 0.56347636\n",
            "Iteration 9, loss = 0.56332225\n",
            "Iteration 10, loss = 0.56341068\n",
            "Iteration 11, loss = 0.56296329\n",
            "Iteration 12, loss = 0.56284717\n",
            "Iteration 13, loss = 0.56249056\n",
            "Iteration 14, loss = 0.56334491\n",
            "Iteration 15, loss = 0.56274641\n",
            "Iteration 16, loss = 0.56332291\n",
            "Iteration 17, loss = 0.56342801\n",
            "Iteration 18, loss = 0.56221119\n",
            "Iteration 19, loss = 0.56358265\n",
            "Iteration 20, loss = 0.56288504\n",
            "Iteration 21, loss = 0.56346414\n",
            "Iteration 22, loss = 0.56230410\n",
            "Iteration 23, loss = 0.56282870\n",
            "Iteration 24, loss = 0.56285105\n",
            "Iteration 25, loss = 0.56270460\n",
            "Iteration 26, loss = 0.56201834\n",
            "Iteration 27, loss = 0.56284035\n",
            "Iteration 28, loss = 0.56144268\n",
            "Iteration 29, loss = 0.56274179\n",
            "Iteration 30, loss = 0.56196440\n",
            "Iteration 31, loss = 0.56185804\n",
            "Iteration 32, loss = 0.56170671\n",
            "Iteration 33, loss = 0.56203358\n",
            "Iteration 34, loss = 0.56207225\n",
            "Iteration 35, loss = 0.56224828\n",
            "Iteration 36, loss = 0.56249523\n",
            "Iteration 37, loss = 0.56239427\n",
            "Iteration 38, loss = 0.56198364\n",
            "Iteration 39, loss = 0.56179120\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56837411\n",
            "Iteration 2, loss = 0.56493697\n",
            "Iteration 3, loss = 0.56413405\n",
            "Iteration 4, loss = 0.56495567\n",
            "Iteration 5, loss = 0.56464490\n",
            "Iteration 6, loss = 0.56386690\n",
            "Iteration 7, loss = 0.56428090\n",
            "Iteration 8, loss = 0.56326039\n",
            "Iteration 9, loss = 0.56456175\n",
            "Iteration 10, loss = 0.56408754\n",
            "Iteration 11, loss = 0.56431018\n",
            "Iteration 12, loss = 0.56371014\n",
            "Iteration 13, loss = 0.56393225\n",
            "Iteration 14, loss = 0.56447346\n",
            "Iteration 15, loss = 0.56422171\n",
            "Iteration 16, loss = 0.56406400\n",
            "Iteration 17, loss = 0.56405569\n",
            "Iteration 18, loss = 0.56432532\n",
            "Iteration 19, loss = 0.56380930\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57626126\n",
            "Iteration 2, loss = 0.56510648\n",
            "Iteration 3, loss = 0.56496017\n",
            "Iteration 4, loss = 0.56557949\n",
            "Iteration 5, loss = 0.56442985\n",
            "Iteration 6, loss = 0.56467417\n",
            "Iteration 7, loss = 0.56503469\n",
            "Iteration 8, loss = 0.56468971\n",
            "Iteration 9, loss = 0.56473877\n",
            "Iteration 10, loss = 0.56445911\n",
            "Iteration 11, loss = 0.56373490\n",
            "Iteration 12, loss = 0.56456283\n",
            "Iteration 13, loss = 0.56469480\n",
            "Iteration 14, loss = 0.56416886\n",
            "Iteration 15, loss = 0.56441367\n",
            "Iteration 16, loss = 0.56398993\n",
            "Iteration 17, loss = 0.56457899\n",
            "Iteration 18, loss = 0.56344428\n",
            "Iteration 19, loss = 0.56424562\n",
            "Iteration 20, loss = 0.56423144\n",
            "Iteration 21, loss = 0.56420661\n",
            "Iteration 22, loss = 0.56490716\n",
            "Iteration 23, loss = 0.56396255\n",
            "Iteration 24, loss = 0.56350664\n",
            "Iteration 25, loss = 0.56390602\n",
            "Iteration 26, loss = 0.56295777\n",
            "Iteration 27, loss = 0.56394456\n",
            "Iteration 28, loss = 0.56290048\n",
            "Iteration 29, loss = 0.56273648\n",
            "Iteration 30, loss = 0.56363045\n",
            "Iteration 31, loss = 0.56389730\n",
            "Iteration 32, loss = 0.56219826\n",
            "Iteration 33, loss = 0.56306160\n",
            "Iteration 34, loss = 0.56308831\n",
            "Iteration 35, loss = 0.56274763\n",
            "Iteration 36, loss = 0.56322429\n",
            "Iteration 37, loss = 0.56263499\n",
            "Iteration 38, loss = 0.56202709\n",
            "Iteration 39, loss = 0.56223717\n",
            "Iteration 40, loss = 0.56296155\n",
            "Iteration 41, loss = 0.56090359\n",
            "Iteration 42, loss = 0.56200211\n",
            "Iteration 43, loss = 0.56198932\n",
            "Iteration 44, loss = 0.56160234\n",
            "Iteration 45, loss = 0.56234341\n",
            "Iteration 46, loss = 0.56149431\n",
            "Iteration 47, loss = 0.56065929\n",
            "Iteration 48, loss = 0.56041729\n",
            "Iteration 49, loss = 0.56103991\n",
            "Iteration 50, loss = 0.56082322\n",
            "Iteration 51, loss = 0.56100622\n",
            "Iteration 52, loss = 0.56013011\n",
            "Iteration 53, loss = 0.55958015\n",
            "Iteration 54, loss = 0.55889279\n",
            "Iteration 55, loss = 0.55860424\n",
            "Iteration 56, loss = 0.55903056\n",
            "Iteration 57, loss = 0.55821037\n",
            "Iteration 58, loss = 0.55797514\n",
            "Iteration 59, loss = 0.55767209\n",
            "Iteration 60, loss = 0.55719245\n",
            "Iteration 61, loss = 0.55687280\n",
            "Iteration 62, loss = 0.55667679\n",
            "Iteration 63, loss = 0.55635670\n",
            "Iteration 64, loss = 0.55695901\n",
            "Iteration 65, loss = 0.55605010\n",
            "Iteration 66, loss = 0.55451338\n",
            "Iteration 67, loss = 0.55455657\n",
            "Iteration 68, loss = 0.55421682\n",
            "Iteration 69, loss = 0.55319398\n",
            "Iteration 70, loss = 0.55283743\n",
            "Iteration 71, loss = 0.55096322\n",
            "Iteration 72, loss = 0.55193469\n",
            "Iteration 73, loss = 0.55085913\n",
            "Iteration 74, loss = 0.55058985\n",
            "Iteration 75, loss = 0.54965022\n",
            "Iteration 76, loss = 0.54917149\n",
            "Iteration 77, loss = 0.54892552\n",
            "Iteration 78, loss = 0.54772779\n",
            "Iteration 79, loss = 0.54719168\n",
            "Iteration 80, loss = 0.54653299\n",
            "Iteration 81, loss = 0.54468505\n",
            "Iteration 82, loss = 0.54488799\n",
            "Iteration 83, loss = 0.54379493\n",
            "Iteration 84, loss = 0.54322450\n",
            "Iteration 85, loss = 0.54263792\n",
            "Iteration 86, loss = 0.54166629\n",
            "Iteration 87, loss = 0.54060030\n",
            "Iteration 88, loss = 0.54083947\n",
            "Iteration 89, loss = 0.54093816\n",
            "Iteration 90, loss = 0.53789687\n",
            "Iteration 91, loss = 0.53924878\n",
            "Iteration 92, loss = 0.53765757\n",
            "Iteration 93, loss = 0.53714829\n",
            "Iteration 94, loss = 0.53537194\n",
            "Iteration 95, loss = 0.53646780\n",
            "Iteration 96, loss = 0.53494812\n",
            "Iteration 97, loss = 0.53544441\n",
            "Iteration 98, loss = 0.53415114\n",
            "Iteration 99, loss = 0.53483262\n",
            "Iteration 100, loss = 0.53175545\n",
            "Iteration 101, loss = 0.53267741\n",
            "Iteration 102, loss = 0.53277920\n",
            "Iteration 103, loss = 0.53248796\n",
            "Iteration 104, loss = 0.53217412\n",
            "Iteration 105, loss = 0.53092280\n",
            "Iteration 106, loss = 0.53143158\n",
            "Iteration 107, loss = 0.53037664\n",
            "Iteration 108, loss = 0.53007537\n",
            "Iteration 109, loss = 0.53070018\n",
            "Iteration 110, loss = 0.52858131\n",
            "Iteration 111, loss = 0.52899968\n",
            "Iteration 112, loss = 0.52868630\n",
            "Iteration 113, loss = 0.52963726\n",
            "Iteration 114, loss = 0.52914944\n",
            "Iteration 115, loss = 0.52868119\n",
            "Iteration 116, loss = 0.52792747\n",
            "Iteration 117, loss = 0.52819657\n",
            "Iteration 118, loss = 0.52777801\n",
            "Iteration 119, loss = 0.52707124\n",
            "Iteration 120, loss = 0.52756765\n",
            "Iteration 121, loss = 0.52692189\n",
            "Iteration 122, loss = 0.52606821\n",
            "Iteration 123, loss = 0.52685063\n",
            "Iteration 124, loss = 0.52708062\n",
            "Iteration 125, loss = 0.52669791\n",
            "Iteration 126, loss = 0.52679385\n",
            "Iteration 127, loss = 0.52616266\n",
            "Iteration 128, loss = 0.52484175\n",
            "Iteration 129, loss = 0.52574808\n",
            "Iteration 130, loss = 0.52555056\n",
            "Iteration 131, loss = 0.52531825\n",
            "Iteration 132, loss = 0.52557164\n",
            "Iteration 133, loss = 0.52534061\n",
            "Iteration 134, loss = 0.52468747\n",
            "Iteration 135, loss = 0.52480179\n",
            "Iteration 136, loss = 0.52447533\n",
            "Iteration 137, loss = 0.52506455\n",
            "Iteration 138, loss = 0.52439827\n",
            "Iteration 139, loss = 0.52428491\n",
            "Iteration 140, loss = 0.52419967\n",
            "Iteration 141, loss = 0.52403548\n",
            "Iteration 142, loss = 0.52436657\n",
            "Iteration 143, loss = 0.52357523\n",
            "Iteration 144, loss = 0.52421431\n",
            "Iteration 145, loss = 0.52309884\n",
            "Iteration 146, loss = 0.52318661\n",
            "Iteration 147, loss = 0.52402719\n",
            "Iteration 148, loss = 0.52355098\n",
            "Iteration 149, loss = 0.52339425\n",
            "Iteration 150, loss = 0.52403311\n",
            "Iteration 151, loss = 0.52278524\n",
            "Iteration 152, loss = 0.52298919\n",
            "Iteration 153, loss = 0.52301989\n",
            "Iteration 154, loss = 0.52371422\n",
            "Iteration 155, loss = 0.52242693\n",
            "Iteration 156, loss = 0.52218096\n",
            "Iteration 157, loss = 0.52237619\n",
            "Iteration 158, loss = 0.52230034\n",
            "Iteration 159, loss = 0.52191528\n",
            "Iteration 160, loss = 0.52185273\n",
            "Iteration 161, loss = 0.52287217\n",
            "Iteration 162, loss = 0.52282813\n",
            "Iteration 163, loss = 0.52275880\n",
            "Iteration 164, loss = 0.52074572\n",
            "Iteration 165, loss = 0.52151407\n",
            "Iteration 166, loss = 0.52256866\n",
            "Iteration 167, loss = 0.52125504\n",
            "Iteration 168, loss = 0.52182953\n",
            "Iteration 169, loss = 0.52197715\n",
            "Iteration 170, loss = 0.52225405\n",
            "Iteration 171, loss = 0.52070720\n",
            "Iteration 172, loss = 0.52181261\n",
            "Iteration 173, loss = 0.52041553\n",
            "Iteration 174, loss = 0.52178024\n",
            "Iteration 175, loss = 0.52153071\n",
            "Iteration 176, loss = 0.52159587\n",
            "Iteration 177, loss = 0.52145111\n",
            "Iteration 178, loss = 0.52078173\n",
            "Iteration 179, loss = 0.52009334\n",
            "Iteration 180, loss = 0.52154113\n",
            "Iteration 181, loss = 0.52207882\n",
            "Iteration 182, loss = 0.52135385\n",
            "Iteration 183, loss = 0.52134674\n",
            "Iteration 184, loss = 0.52116095\n",
            "Iteration 185, loss = 0.52170683\n",
            "Iteration 186, loss = 0.52179584\n",
            "Iteration 187, loss = 0.52156336\n",
            "Iteration 188, loss = 0.52189391\n",
            "Iteration 189, loss = 0.52081509\n",
            "Iteration 190, loss = 0.52090814\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56345734\n",
            "Iteration 2, loss = 0.56286620\n",
            "Iteration 3, loss = 0.56339871\n",
            "Iteration 4, loss = 0.56308304\n",
            "Iteration 5, loss = 0.56303075\n",
            "Iteration 6, loss = 0.56322013\n",
            "Iteration 7, loss = 0.56373182\n",
            "Iteration 8, loss = 0.56324707\n",
            "Iteration 9, loss = 0.56365414\n",
            "Iteration 10, loss = 0.56306655\n",
            "Iteration 11, loss = 0.56308009\n",
            "Iteration 12, loss = 0.56262082\n",
            "Iteration 13, loss = 0.56328592\n",
            "Iteration 14, loss = 0.56316487\n",
            "Iteration 15, loss = 0.56305069\n",
            "Iteration 16, loss = 0.56292112\n",
            "Iteration 17, loss = 0.56258766\n",
            "Iteration 18, loss = 0.56234532\n",
            "Iteration 19, loss = 0.56315790\n",
            "Iteration 20, loss = 0.56213754\n",
            "Iteration 21, loss = 0.56187812\n",
            "Iteration 22, loss = 0.56240573\n",
            "Iteration 23, loss = 0.56161786\n",
            "Iteration 24, loss = 0.56204345\n",
            "Iteration 25, loss = 0.56214627\n",
            "Iteration 26, loss = 0.56205786\n",
            "Iteration 27, loss = 0.56189646\n",
            "Iteration 28, loss = 0.56232647\n",
            "Iteration 29, loss = 0.56180965\n",
            "Iteration 30, loss = 0.56136294\n",
            "Iteration 31, loss = 0.56154084\n",
            "Iteration 32, loss = 0.56205170\n",
            "Iteration 33, loss = 0.56148264\n",
            "Iteration 34, loss = 0.56075832\n",
            "Iteration 35, loss = 0.56184548\n",
            "Iteration 36, loss = 0.56132429\n",
            "Iteration 37, loss = 0.56115711\n",
            "Iteration 38, loss = 0.56073401\n",
            "Iteration 39, loss = 0.56090825\n",
            "Iteration 40, loss = 0.56128665\n",
            "Iteration 41, loss = 0.56037802\n",
            "Iteration 42, loss = 0.56019861\n",
            "Iteration 43, loss = 0.56051637\n",
            "Iteration 44, loss = 0.56060533\n",
            "Iteration 45, loss = 0.56047213\n",
            "Iteration 46, loss = 0.56124737\n",
            "Iteration 47, loss = 0.55928635\n",
            "Iteration 48, loss = 0.55967404\n",
            "Iteration 49, loss = 0.55943027\n",
            "Iteration 50, loss = 0.55949186\n",
            "Iteration 51, loss = 0.55934007\n",
            "Iteration 52, loss = 0.55896747\n",
            "Iteration 53, loss = 0.55896080\n",
            "Iteration 54, loss = 0.55934469\n",
            "Iteration 55, loss = 0.55828887\n",
            "Iteration 56, loss = 0.55828825\n",
            "Iteration 57, loss = 0.55804356\n",
            "Iteration 58, loss = 0.55765872\n",
            "Iteration 59, loss = 0.55697831\n",
            "Iteration 60, loss = 0.55601425\n",
            "Iteration 61, loss = 0.55660569\n",
            "Iteration 62, loss = 0.55598541\n",
            "Iteration 63, loss = 0.55626070\n",
            "Iteration 64, loss = 0.55616652\n",
            "Iteration 65, loss = 0.55518246\n",
            "Iteration 66, loss = 0.55468386\n",
            "Iteration 67, loss = 0.55464457\n",
            "Iteration 68, loss = 0.55463516\n",
            "Iteration 69, loss = 0.55378553\n",
            "Iteration 70, loss = 0.55323644\n",
            "Iteration 71, loss = 0.55249032\n",
            "Iteration 72, loss = 0.55179511\n",
            "Iteration 73, loss = 0.55182795\n",
            "Iteration 74, loss = 0.55081134\n",
            "Iteration 75, loss = 0.55031852\n",
            "Iteration 76, loss = 0.54963961\n",
            "Iteration 77, loss = 0.54902587\n",
            "Iteration 78, loss = 0.54826517\n",
            "Iteration 79, loss = 0.54824080\n",
            "Iteration 80, loss = 0.54730395\n",
            "Iteration 81, loss = 0.54648101\n",
            "Iteration 82, loss = 0.54451139\n",
            "Iteration 83, loss = 0.54477337\n",
            "Iteration 84, loss = 0.54397869\n",
            "Iteration 85, loss = 0.54348427\n",
            "Iteration 86, loss = 0.54232011\n",
            "Iteration 87, loss = 0.54058591\n",
            "Iteration 88, loss = 0.54124375\n",
            "Iteration 89, loss = 0.54066106\n",
            "Iteration 90, loss = 0.53907575\n",
            "Iteration 91, loss = 0.53964845\n",
            "Iteration 92, loss = 0.53757642\n",
            "Iteration 93, loss = 0.53711383\n",
            "Iteration 94, loss = 0.53600995\n",
            "Iteration 95, loss = 0.53578783\n",
            "Iteration 96, loss = 0.53573322\n",
            "Iteration 97, loss = 0.53450827\n",
            "Iteration 98, loss = 0.53398351\n",
            "Iteration 99, loss = 0.53385630\n",
            "Iteration 100, loss = 0.53288718\n",
            "Iteration 101, loss = 0.53320924\n",
            "Iteration 102, loss = 0.53178674\n",
            "Iteration 103, loss = 0.53117377\n",
            "Iteration 104, loss = 0.53083743\n",
            "Iteration 105, loss = 0.53022652\n",
            "Iteration 106, loss = 0.52991254\n",
            "Iteration 107, loss = 0.53007994\n",
            "Iteration 108, loss = 0.52953084\n",
            "Iteration 109, loss = 0.52906528\n",
            "Iteration 110, loss = 0.52901239\n",
            "Iteration 111, loss = 0.52881686\n",
            "Iteration 112, loss = 0.52858042\n",
            "Iteration 113, loss = 0.52824647\n",
            "Iteration 114, loss = 0.52713327\n",
            "Iteration 115, loss = 0.52732466\n",
            "Iteration 116, loss = 0.52632880\n",
            "Iteration 117, loss = 0.52610569\n",
            "Iteration 118, loss = 0.52653902\n",
            "Iteration 119, loss = 0.52579579\n",
            "Iteration 120, loss = 0.52600467\n",
            "Iteration 121, loss = 0.52597329\n",
            "Iteration 122, loss = 0.52571946\n",
            "Iteration 123, loss = 0.52583548\n",
            "Iteration 124, loss = 0.52483218\n",
            "Iteration 125, loss = 0.52473367\n",
            "Iteration 126, loss = 0.52379147\n",
            "Iteration 127, loss = 0.52400202\n",
            "Iteration 128, loss = 0.52460360\n",
            "Iteration 129, loss = 0.52319436\n",
            "Iteration 130, loss = 0.52476809\n",
            "Iteration 131, loss = 0.52377283\n",
            "Iteration 132, loss = 0.52421006\n",
            "Iteration 133, loss = 0.52316172\n",
            "Iteration 134, loss = 0.52340660\n",
            "Iteration 135, loss = 0.52356707\n",
            "Iteration 136, loss = 0.52256762\n",
            "Iteration 137, loss = 0.52210435\n",
            "Iteration 138, loss = 0.52350633\n",
            "Iteration 139, loss = 0.52269816\n",
            "Iteration 140, loss = 0.52285603\n",
            "Iteration 141, loss = 0.52252164\n",
            "Iteration 142, loss = 0.52246925\n",
            "Iteration 143, loss = 0.52268509\n",
            "Iteration 144, loss = 0.52205225\n",
            "Iteration 145, loss = 0.52196007\n",
            "Iteration 146, loss = 0.52250480\n",
            "Iteration 147, loss = 0.52276229\n",
            "Iteration 148, loss = 0.52138112\n",
            "Iteration 149, loss = 0.52139733\n",
            "Iteration 150, loss = 0.52040480\n",
            "Iteration 151, loss = 0.52157816\n",
            "Iteration 152, loss = 0.52090925\n",
            "Iteration 153, loss = 0.52059413\n",
            "Iteration 154, loss = 0.52057419\n",
            "Iteration 155, loss = 0.52095454\n",
            "Iteration 156, loss = 0.52090774\n",
            "Iteration 157, loss = 0.52065823\n",
            "Iteration 158, loss = 0.52073321\n",
            "Iteration 159, loss = 0.52083099\n",
            "Iteration 160, loss = 0.52168478\n",
            "Iteration 161, loss = 0.52039923\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56048829\n",
            "Iteration 2, loss = 0.56076363\n",
            "Iteration 3, loss = 0.56046150\n",
            "Iteration 4, loss = 0.56038196\n",
            "Iteration 5, loss = 0.56016261\n",
            "Iteration 6, loss = 0.56087001\n",
            "Iteration 7, loss = 0.56015873\n",
            "Iteration 8, loss = 0.56028861\n",
            "Iteration 9, loss = 0.56040525\n",
            "Iteration 10, loss = 0.55994025\n",
            "Iteration 11, loss = 0.55928898\n",
            "Iteration 12, loss = 0.56013658\n",
            "Iteration 13, loss = 0.56012834\n",
            "Iteration 14, loss = 0.56079270\n",
            "Iteration 15, loss = 0.56014989\n",
            "Iteration 16, loss = 0.55905025\n",
            "Iteration 17, loss = 0.55994823\n",
            "Iteration 18, loss = 0.55957080\n",
            "Iteration 19, loss = 0.56004762\n",
            "Iteration 20, loss = 0.56016561\n",
            "Iteration 21, loss = 0.55933062\n",
            "Iteration 22, loss = 0.55903019\n",
            "Iteration 23, loss = 0.55922422\n",
            "Iteration 24, loss = 0.55845451\n",
            "Iteration 25, loss = 0.55949668\n",
            "Iteration 26, loss = 0.55911428\n",
            "Iteration 27, loss = 0.55911871\n",
            "Iteration 28, loss = 0.55827246\n",
            "Iteration 29, loss = 0.55973450\n",
            "Iteration 30, loss = 0.55968582\n",
            "Iteration 31, loss = 0.55893563\n",
            "Iteration 32, loss = 0.55843166\n",
            "Iteration 33, loss = 0.55870319\n",
            "Iteration 34, loss = 0.55895668\n",
            "Iteration 35, loss = 0.55804244\n",
            "Iteration 36, loss = 0.55878599\n",
            "Iteration 37, loss = 0.55785256\n",
            "Iteration 38, loss = 0.55795248\n",
            "Iteration 39, loss = 0.55871995\n",
            "Iteration 40, loss = 0.55783009\n",
            "Iteration 41, loss = 0.55852549\n",
            "Iteration 42, loss = 0.55780684\n",
            "Iteration 43, loss = 0.55805827\n",
            "Iteration 44, loss = 0.55743056\n",
            "Iteration 45, loss = 0.55656508\n",
            "Iteration 46, loss = 0.55681327\n",
            "Iteration 47, loss = 0.55764292\n",
            "Iteration 48, loss = 0.55630799\n",
            "Iteration 49, loss = 0.55692008\n",
            "Iteration 50, loss = 0.55464837\n",
            "Iteration 51, loss = 0.55527145\n",
            "Iteration 52, loss = 0.55542838\n",
            "Iteration 53, loss = 0.55613646\n",
            "Iteration 54, loss = 0.55538983\n",
            "Iteration 55, loss = 0.55499422\n",
            "Iteration 56, loss = 0.55469195\n",
            "Iteration 57, loss = 0.55363404\n",
            "Iteration 58, loss = 0.55426994\n",
            "Iteration 59, loss = 0.55374819\n",
            "Iteration 60, loss = 0.55435710\n",
            "Iteration 61, loss = 0.55320386\n",
            "Iteration 62, loss = 0.55410509\n",
            "Iteration 63, loss = 0.55234696\n",
            "Iteration 64, loss = 0.55180509\n",
            "Iteration 65, loss = 0.55181096\n",
            "Iteration 66, loss = 0.55131804\n",
            "Iteration 67, loss = 0.55040832\n",
            "Iteration 68, loss = 0.55101283\n",
            "Iteration 69, loss = 0.55019183\n",
            "Iteration 70, loss = 0.54876639\n",
            "Iteration 71, loss = 0.54932554\n",
            "Iteration 72, loss = 0.54831522\n",
            "Iteration 73, loss = 0.54772650\n",
            "Iteration 74, loss = 0.54715365\n",
            "Iteration 75, loss = 0.54714066\n",
            "Iteration 76, loss = 0.54576201\n",
            "Iteration 77, loss = 0.54591277\n",
            "Iteration 78, loss = 0.54411704\n",
            "Iteration 79, loss = 0.54445969\n",
            "Iteration 80, loss = 0.54313314\n",
            "Iteration 81, loss = 0.54234932\n",
            "Iteration 82, loss = 0.54202863\n",
            "Iteration 83, loss = 0.54126839\n",
            "Iteration 84, loss = 0.54042527\n",
            "Iteration 85, loss = 0.54009375\n",
            "Iteration 86, loss = 0.53908246\n",
            "Iteration 87, loss = 0.53882549\n",
            "Iteration 88, loss = 0.53765705\n",
            "Iteration 89, loss = 0.53785071\n",
            "Iteration 90, loss = 0.53597643\n",
            "Iteration 91, loss = 0.53617919\n",
            "Iteration 92, loss = 0.53492497\n",
            "Iteration 93, loss = 0.53397761\n",
            "Iteration 94, loss = 0.53378483\n",
            "Iteration 95, loss = 0.53377549\n",
            "Iteration 96, loss = 0.53270561\n",
            "Iteration 97, loss = 0.53180927\n",
            "Iteration 98, loss = 0.53254332\n",
            "Iteration 99, loss = 0.53032284\n",
            "Iteration 100, loss = 0.53082231\n",
            "Iteration 101, loss = 0.53132623\n",
            "Iteration 102, loss = 0.52991164\n",
            "Iteration 103, loss = 0.52927849\n",
            "Iteration 104, loss = 0.52982022\n",
            "Iteration 105, loss = 0.52913906\n",
            "Iteration 106, loss = 0.52821800\n",
            "Iteration 107, loss = 0.52593813\n",
            "Iteration 108, loss = 0.52802404\n",
            "Iteration 109, loss = 0.52867762\n",
            "Iteration 110, loss = 0.52686613\n",
            "Iteration 111, loss = 0.52585714\n",
            "Iteration 112, loss = 0.52695739\n",
            "Iteration 113, loss = 0.52613852\n",
            "Iteration 114, loss = 0.52569092\n",
            "Iteration 115, loss = 0.52428637\n",
            "Iteration 116, loss = 0.52733966\n",
            "Iteration 117, loss = 0.52601899\n",
            "Iteration 118, loss = 0.52578994\n",
            "Iteration 119, loss = 0.52571594\n",
            "Iteration 120, loss = 0.52517525\n",
            "Iteration 121, loss = 0.52451184\n",
            "Iteration 122, loss = 0.52466139\n",
            "Iteration 123, loss = 0.52388768\n",
            "Iteration 124, loss = 0.52484488\n",
            "Iteration 125, loss = 0.52439061\n",
            "Iteration 126, loss = 0.52391659\n",
            "Iteration 127, loss = 0.52369865\n",
            "Iteration 128, loss = 0.52395501\n",
            "Iteration 129, loss = 0.52401935\n",
            "Iteration 130, loss = 0.52292625\n",
            "Iteration 131, loss = 0.52291612\n",
            "Iteration 132, loss = 0.52263430\n",
            "Iteration 133, loss = 0.52360499\n",
            "Iteration 134, loss = 0.52281178\n",
            "Iteration 135, loss = 0.52304600\n",
            "Iteration 136, loss = 0.52301562\n",
            "Iteration 137, loss = 0.52328459\n",
            "Iteration 138, loss = 0.52269311\n",
            "Iteration 139, loss = 0.52273666\n",
            "Iteration 140, loss = 0.52127193\n",
            "Iteration 141, loss = 0.52224162\n",
            "Iteration 142, loss = 0.52164150\n",
            "Iteration 143, loss = 0.52227359\n",
            "Iteration 144, loss = 0.52237548\n",
            "Iteration 145, loss = 0.52179397\n",
            "Iteration 146, loss = 0.52171817\n",
            "Iteration 147, loss = 0.52116800\n",
            "Iteration 148, loss = 0.52182188\n",
            "Iteration 149, loss = 0.52162581\n",
            "Iteration 150, loss = 0.52129397\n",
            "Iteration 151, loss = 0.52114358\n",
            "Iteration 152, loss = 0.51993114\n",
            "Iteration 153, loss = 0.52127717\n",
            "Iteration 154, loss = 0.52055140\n",
            "Iteration 155, loss = 0.52086664\n",
            "Iteration 156, loss = 0.51948456\n",
            "Iteration 157, loss = 0.52116154\n",
            "Iteration 158, loss = 0.52226336\n",
            "Iteration 159, loss = 0.52033071\n",
            "Iteration 160, loss = 0.52088000\n",
            "Iteration 161, loss = 0.51963248\n",
            "Iteration 162, loss = 0.52087252\n",
            "Iteration 163, loss = 0.52056980\n",
            "Iteration 164, loss = 0.52030729\n",
            "Iteration 165, loss = 0.52059856\n",
            "Iteration 166, loss = 0.51951697\n",
            "Iteration 167, loss = 0.51959482\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56426728\n",
            "Iteration 2, loss = 0.56270238\n",
            "Iteration 3, loss = 0.56338626\n",
            "Iteration 4, loss = 0.56254890\n",
            "Iteration 5, loss = 0.56217289\n",
            "Iteration 6, loss = 0.56336990\n",
            "Iteration 7, loss = 0.56245514\n",
            "Iteration 8, loss = 0.56262207\n",
            "Iteration 9, loss = 0.56259673\n",
            "Iteration 10, loss = 0.56204909\n",
            "Iteration 11, loss = 0.56268433\n",
            "Iteration 12, loss = 0.56267626\n",
            "Iteration 13, loss = 0.56265503\n",
            "Iteration 14, loss = 0.56243945\n",
            "Iteration 15, loss = 0.56163546\n",
            "Iteration 16, loss = 0.56258026\n",
            "Iteration 17, loss = 0.56165324\n",
            "Iteration 18, loss = 0.56148256\n",
            "Iteration 19, loss = 0.56179175\n",
            "Iteration 20, loss = 0.56191930\n",
            "Iteration 21, loss = 0.56205162\n",
            "Iteration 22, loss = 0.56195764\n",
            "Iteration 23, loss = 0.56145112\n",
            "Iteration 24, loss = 0.56204786\n",
            "Iteration 25, loss = 0.56148004\n",
            "Iteration 26, loss = 0.56159671\n",
            "Iteration 27, loss = 0.56092453\n",
            "Iteration 28, loss = 0.56054425\n",
            "Iteration 29, loss = 0.56147388\n",
            "Iteration 30, loss = 0.56003843\n",
            "Iteration 31, loss = 0.56078964\n",
            "Iteration 32, loss = 0.56134442\n",
            "Iteration 33, loss = 0.56011666\n",
            "Iteration 34, loss = 0.56014521\n",
            "Iteration 35, loss = 0.56154426\n",
            "Iteration 36, loss = 0.56055512\n",
            "Iteration 37, loss = 0.56016462\n",
            "Iteration 38, loss = 0.55993550\n",
            "Iteration 39, loss = 0.56040267\n",
            "Iteration 40, loss = 0.56050206\n",
            "Iteration 41, loss = 0.55876867\n",
            "Iteration 42, loss = 0.55967073\n",
            "Iteration 43, loss = 0.55966222\n",
            "Iteration 44, loss = 0.55920902\n",
            "Iteration 45, loss = 0.55852816\n",
            "Iteration 46, loss = 0.55854509\n",
            "Iteration 47, loss = 0.55938819\n",
            "Iteration 48, loss = 0.55856261\n",
            "Iteration 49, loss = 0.55787654\n",
            "Iteration 50, loss = 0.55820130\n",
            "Iteration 51, loss = 0.55756981\n",
            "Iteration 52, loss = 0.55741633\n",
            "Iteration 53, loss = 0.55724334\n",
            "Iteration 54, loss = 0.55671728\n",
            "Iteration 55, loss = 0.55662454\n",
            "Iteration 56, loss = 0.55639625\n",
            "Iteration 57, loss = 0.55644892\n",
            "Iteration 58, loss = 0.55567208\n",
            "Iteration 59, loss = 0.55562504\n",
            "Iteration 60, loss = 0.55466800\n",
            "Iteration 61, loss = 0.55372635\n",
            "Iteration 62, loss = 0.55418043\n",
            "Iteration 63, loss = 0.55369976\n",
            "Iteration 64, loss = 0.55332156\n",
            "Iteration 65, loss = 0.55277966\n",
            "Iteration 66, loss = 0.55121887\n",
            "Iteration 67, loss = 0.55224707\n",
            "Iteration 68, loss = 0.55160573\n",
            "Iteration 69, loss = 0.55031969\n",
            "Iteration 70, loss = 0.55043278\n",
            "Iteration 71, loss = 0.54879570\n",
            "Iteration 72, loss = 0.54900533\n",
            "Iteration 73, loss = 0.54851797\n",
            "Iteration 74, loss = 0.54813620\n",
            "Iteration 75, loss = 0.54709705\n",
            "Iteration 76, loss = 0.54536246\n",
            "Iteration 77, loss = 0.54598662\n",
            "Iteration 78, loss = 0.54449996\n",
            "Iteration 79, loss = 0.54338998\n",
            "Iteration 80, loss = 0.54282990\n",
            "Iteration 81, loss = 0.54146685\n",
            "Iteration 82, loss = 0.54145935\n",
            "Iteration 83, loss = 0.54166786\n",
            "Iteration 84, loss = 0.54033015\n",
            "Iteration 85, loss = 0.53970558\n",
            "Iteration 86, loss = 0.53882172\n",
            "Iteration 87, loss = 0.53738047\n",
            "Iteration 88, loss = 0.53716179\n",
            "Iteration 89, loss = 0.53625356\n",
            "Iteration 90, loss = 0.53594536\n",
            "Iteration 91, loss = 0.53539087\n",
            "Iteration 92, loss = 0.53481016\n",
            "Iteration 93, loss = 0.53379913\n",
            "Iteration 94, loss = 0.53395265\n",
            "Iteration 95, loss = 0.53261484\n",
            "Iteration 96, loss = 0.53260376\n",
            "Iteration 97, loss = 0.53134620\n",
            "Iteration 98, loss = 0.53026173\n",
            "Iteration 99, loss = 0.53005397\n",
            "Iteration 100, loss = 0.52935743\n",
            "Iteration 101, loss = 0.53078513\n",
            "Iteration 102, loss = 0.52864674\n",
            "Iteration 103, loss = 0.52948207\n",
            "Iteration 104, loss = 0.52860871\n",
            "Iteration 105, loss = 0.52798326\n",
            "Iteration 106, loss = 0.52776682\n",
            "Iteration 107, loss = 0.52765892\n",
            "Iteration 108, loss = 0.52667136\n",
            "Iteration 109, loss = 0.52574655\n",
            "Iteration 110, loss = 0.52656160\n",
            "Iteration 111, loss = 0.52662011\n",
            "Iteration 112, loss = 0.52655287\n",
            "Iteration 113, loss = 0.52624384\n",
            "Iteration 114, loss = 0.52545394\n",
            "Iteration 115, loss = 0.52531110\n",
            "Iteration 116, loss = 0.52530798\n",
            "Iteration 117, loss = 0.52461858\n",
            "Iteration 118, loss = 0.52445517\n",
            "Iteration 119, loss = 0.52494962\n",
            "Iteration 120, loss = 0.52497169\n",
            "Iteration 121, loss = 0.52315887\n",
            "Iteration 122, loss = 0.52350966\n",
            "Iteration 123, loss = 0.52384939\n",
            "Iteration 124, loss = 0.52310334\n",
            "Iteration 125, loss = 0.52325158\n",
            "Iteration 126, loss = 0.52293599\n",
            "Iteration 127, loss = 0.52280899\n",
            "Iteration 128, loss = 0.52221535\n",
            "Iteration 129, loss = 0.52100053\n",
            "Iteration 130, loss = 0.52274619\n",
            "Iteration 131, loss = 0.52189150\n",
            "Iteration 132, loss = 0.52224984\n",
            "Iteration 133, loss = 0.52163573\n",
            "Iteration 134, loss = 0.52142220\n",
            "Iteration 135, loss = 0.52176768\n",
            "Iteration 136, loss = 0.52198278\n",
            "Iteration 137, loss = 0.52071198\n",
            "Iteration 138, loss = 0.52084841\n",
            "Iteration 139, loss = 0.52098786\n",
            "Iteration 140, loss = 0.52067609\n",
            "Iteration 141, loss = 0.52022286\n",
            "Iteration 142, loss = 0.52044761\n",
            "Iteration 143, loss = 0.52051335\n",
            "Iteration 144, loss = 0.52051285\n",
            "Iteration 145, loss = 0.52069290\n",
            "Iteration 146, loss = 0.52001359\n",
            "Iteration 147, loss = 0.52024197\n",
            "Iteration 148, loss = 0.52010384\n",
            "Iteration 149, loss = 0.51983409\n",
            "Iteration 150, loss = 0.51895489\n",
            "Iteration 151, loss = 0.51950131\n",
            "Iteration 152, loss = 0.51917542\n",
            "Iteration 153, loss = 0.52000538\n",
            "Iteration 154, loss = 0.52016749\n",
            "Iteration 155, loss = 0.51920061\n",
            "Iteration 156, loss = 0.51877797\n",
            "Iteration 157, loss = 0.51939806\n",
            "Iteration 158, loss = 0.51997644\n",
            "Iteration 159, loss = 0.51923617\n",
            "Iteration 160, loss = 0.51894623\n",
            "Iteration 161, loss = 0.51882185\n",
            "Iteration 162, loss = 0.51922985\n",
            "Iteration 163, loss = 0.51917588\n",
            "Iteration 164, loss = 0.51880751\n",
            "Iteration 165, loss = 0.51883266\n",
            "Iteration 166, loss = 0.51834327\n",
            "Iteration 167, loss = 0.51871046\n",
            "Iteration 168, loss = 0.51854429\n",
            "Iteration 169, loss = 0.51973114\n",
            "Iteration 170, loss = 0.51846860\n",
            "Iteration 171, loss = 0.51800888\n",
            "Iteration 172, loss = 0.51893749\n",
            "Iteration 173, loss = 0.51793348\n",
            "Iteration 174, loss = 0.51802124\n",
            "Iteration 175, loss = 0.51744367\n",
            "Iteration 176, loss = 0.51822222\n",
            "Iteration 177, loss = 0.51837020\n",
            "Iteration 178, loss = 0.51768850\n",
            "Iteration 179, loss = 0.51792420\n",
            "Iteration 180, loss = 0.51791133\n",
            "Iteration 181, loss = 0.51802276\n",
            "Iteration 182, loss = 0.51773665\n",
            "Iteration 183, loss = 0.51848493\n",
            "Iteration 184, loss = 0.51796580\n",
            "Iteration 185, loss = 0.51837386\n",
            "Iteration 186, loss = 0.51862115\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57019427\n",
            "Iteration 2, loss = 0.56225764\n",
            "Iteration 3, loss = 0.56204646\n",
            "Iteration 4, loss = 0.56223594\n",
            "Iteration 5, loss = 0.56184378\n",
            "Iteration 6, loss = 0.56248719\n",
            "Iteration 7, loss = 0.56239151\n",
            "Iteration 8, loss = 0.56175510\n",
            "Iteration 9, loss = 0.56222687\n",
            "Iteration 10, loss = 0.56142688\n",
            "Iteration 11, loss = 0.56141106\n",
            "Iteration 12, loss = 0.56148688\n",
            "Iteration 13, loss = 0.56167215\n",
            "Iteration 14, loss = 0.56186044\n",
            "Iteration 15, loss = 0.56133246\n",
            "Iteration 16, loss = 0.56193885\n",
            "Iteration 17, loss = 0.56106298\n",
            "Iteration 18, loss = 0.56116438\n",
            "Iteration 19, loss = 0.56183851\n",
            "Iteration 20, loss = 0.56100110\n",
            "Iteration 21, loss = 0.56103356\n",
            "Iteration 22, loss = 0.56119076\n",
            "Iteration 23, loss = 0.56107795\n",
            "Iteration 24, loss = 0.56142487\n",
            "Iteration 25, loss = 0.56200907\n",
            "Iteration 26, loss = 0.56070942\n",
            "Iteration 27, loss = 0.56122724\n",
            "Iteration 28, loss = 0.56060282\n",
            "Iteration 29, loss = 0.56022664\n",
            "Iteration 30, loss = 0.56034757\n",
            "Iteration 31, loss = 0.56033771\n",
            "Iteration 32, loss = 0.56038160\n",
            "Iteration 33, loss = 0.55976502\n",
            "Iteration 34, loss = 0.55960693\n",
            "Iteration 35, loss = 0.55956303\n",
            "Iteration 36, loss = 0.55980129\n",
            "Iteration 37, loss = 0.56013541\n",
            "Iteration 38, loss = 0.55957742\n",
            "Iteration 39, loss = 0.56003033\n",
            "Iteration 40, loss = 0.55967280\n",
            "Iteration 41, loss = 0.55967817\n",
            "Iteration 42, loss = 0.55920955\n",
            "Iteration 43, loss = 0.55880751\n",
            "Iteration 44, loss = 0.55876768\n",
            "Iteration 45, loss = 0.55850714\n",
            "Iteration 46, loss = 0.55941575\n",
            "Iteration 47, loss = 0.55814442\n",
            "Iteration 48, loss = 0.55832780\n",
            "Iteration 49, loss = 0.55817417\n",
            "Iteration 50, loss = 0.55773225\n",
            "Iteration 51, loss = 0.55755464\n",
            "Iteration 52, loss = 0.55725393\n",
            "Iteration 53, loss = 0.55683055\n",
            "Iteration 54, loss = 0.55701897\n",
            "Iteration 55, loss = 0.55672240\n",
            "Iteration 56, loss = 0.55628582\n",
            "Iteration 57, loss = 0.55579666\n",
            "Iteration 58, loss = 0.55656136\n",
            "Iteration 59, loss = 0.55599444\n",
            "Iteration 60, loss = 0.55572717\n",
            "Iteration 61, loss = 0.55457569\n",
            "Iteration 62, loss = 0.55453400\n",
            "Iteration 63, loss = 0.55400277\n",
            "Iteration 64, loss = 0.55344564\n",
            "Iteration 65, loss = 0.55379158\n",
            "Iteration 66, loss = 0.55302410\n",
            "Iteration 67, loss = 0.55217327\n",
            "Iteration 68, loss = 0.55247859\n",
            "Iteration 69, loss = 0.55230393\n",
            "Iteration 70, loss = 0.55034255\n",
            "Iteration 71, loss = 0.54971976\n",
            "Iteration 72, loss = 0.54961478\n",
            "Iteration 73, loss = 0.55000929\n",
            "Iteration 74, loss = 0.54943874\n",
            "Iteration 75, loss = 0.54891454\n",
            "Iteration 76, loss = 0.54875828\n",
            "Iteration 77, loss = 0.54714833\n",
            "Iteration 78, loss = 0.54675033\n",
            "Iteration 79, loss = 0.54655456\n",
            "Iteration 80, loss = 0.54576143\n",
            "Iteration 81, loss = 0.54553631\n",
            "Iteration 82, loss = 0.54501095\n",
            "Iteration 83, loss = 0.54463632\n",
            "Iteration 84, loss = 0.54287130\n",
            "Iteration 85, loss = 0.54135891\n",
            "Iteration 86, loss = 0.54099468\n",
            "Iteration 87, loss = 0.54070616\n",
            "Iteration 88, loss = 0.54055423\n",
            "Iteration 89, loss = 0.53989340\n",
            "Iteration 90, loss = 0.53915055\n",
            "Iteration 91, loss = 0.53924645\n",
            "Iteration 92, loss = 0.53851138\n",
            "Iteration 93, loss = 0.53715829\n",
            "Iteration 94, loss = 0.53704246\n",
            "Iteration 95, loss = 0.53580426\n",
            "Iteration 96, loss = 0.53628063\n",
            "Iteration 97, loss = 0.53531254\n",
            "Iteration 98, loss = 0.53331496\n",
            "Iteration 99, loss = 0.53472471\n",
            "Iteration 100, loss = 0.53310811\n",
            "Iteration 101, loss = 0.53322544\n",
            "Iteration 102, loss = 0.53278726\n",
            "Iteration 103, loss = 0.53188923\n",
            "Iteration 104, loss = 0.53249776\n",
            "Iteration 105, loss = 0.53083735\n",
            "Iteration 106, loss = 0.53109029\n",
            "Iteration 107, loss = 0.52977474\n",
            "Iteration 108, loss = 0.53037184\n",
            "Iteration 109, loss = 0.53039846\n",
            "Iteration 110, loss = 0.53035841\n",
            "Iteration 111, loss = 0.52941052\n",
            "Iteration 112, loss = 0.52888748\n",
            "Iteration 113, loss = 0.52952319\n",
            "Iteration 114, loss = 0.52740015\n",
            "Iteration 115, loss = 0.52849385\n",
            "Iteration 116, loss = 0.52907884\n",
            "Iteration 117, loss = 0.52769000\n",
            "Iteration 118, loss = 0.52836250\n",
            "Iteration 119, loss = 0.52755139\n",
            "Iteration 120, loss = 0.52702590\n",
            "Iteration 121, loss = 0.52703514\n",
            "Iteration 122, loss = 0.52675840\n",
            "Iteration 123, loss = 0.52772011\n",
            "Iteration 124, loss = 0.52709731\n",
            "Iteration 125, loss = 0.52534545\n",
            "Iteration 126, loss = 0.52611922\n",
            "Iteration 127, loss = 0.52629366\n",
            "Iteration 128, loss = 0.52670748\n",
            "Iteration 129, loss = 0.52552480\n",
            "Iteration 130, loss = 0.52500041\n",
            "Iteration 131, loss = 0.52510672\n",
            "Iteration 132, loss = 0.52589161\n",
            "Iteration 133, loss = 0.52503076\n",
            "Iteration 134, loss = 0.52420540\n",
            "Iteration 135, loss = 0.52447360\n",
            "Iteration 136, loss = 0.52433974\n",
            "Iteration 137, loss = 0.52455483\n",
            "Iteration 138, loss = 0.52518029\n",
            "Iteration 139, loss = 0.52431131\n",
            "Iteration 140, loss = 0.52389762\n",
            "Iteration 141, loss = 0.52360889\n",
            "Iteration 142, loss = 0.52272332\n",
            "Iteration 143, loss = 0.52362759\n",
            "Iteration 144, loss = 0.52346050\n",
            "Iteration 145, loss = 0.52349743\n",
            "Iteration 146, loss = 0.52351138\n",
            "Iteration 147, loss = 0.52355380\n",
            "Iteration 148, loss = 0.52259067\n",
            "Iteration 149, loss = 0.52304454\n",
            "Iteration 150, loss = 0.52290444\n",
            "Iteration 151, loss = 0.52256551\n",
            "Iteration 152, loss = 0.52307484\n",
            "Iteration 153, loss = 0.52276295\n",
            "Iteration 154, loss = 0.52323302\n",
            "Iteration 155, loss = 0.52216747\n",
            "Iteration 156, loss = 0.52236239\n",
            "Iteration 157, loss = 0.52189612\n",
            "Iteration 158, loss = 0.52090314\n",
            "Iteration 159, loss = 0.52086265\n",
            "Iteration 160, loss = 0.52196443\n",
            "Iteration 161, loss = 0.52251617\n",
            "Iteration 162, loss = 0.52098232\n",
            "Iteration 163, loss = 0.52233883\n",
            "Iteration 164, loss = 0.52181381\n",
            "Iteration 165, loss = 0.52218185\n",
            "Iteration 166, loss = 0.52160642\n",
            "Iteration 167, loss = 0.52185609\n",
            "Iteration 168, loss = 0.52143932\n",
            "Iteration 169, loss = 0.52145192\n",
            "Iteration 170, loss = 0.52072150\n",
            "Iteration 171, loss = 0.52118628\n",
            "Iteration 172, loss = 0.52099471\n",
            "Iteration 173, loss = 0.52070428\n",
            "Iteration 174, loss = 0.52152002\n",
            "Iteration 175, loss = 0.52225142\n",
            "Iteration 176, loss = 0.52157128\n",
            "Iteration 177, loss = 0.52176211\n",
            "Iteration 178, loss = 0.52151558\n",
            "Iteration 179, loss = 0.52069054\n",
            "Iteration 180, loss = 0.52083794\n",
            "Iteration 181, loss = 0.52047591\n",
            "Iteration 182, loss = 0.52048376\n",
            "Iteration 183, loss = 0.52108367\n",
            "Iteration 184, loss = 0.52096454\n",
            "Iteration 185, loss = 0.52191875\n",
            "Iteration 186, loss = 0.52073715\n",
            "Iteration 187, loss = 0.52069065\n",
            "Iteration 188, loss = 0.51968694\n",
            "Iteration 189, loss = 0.52121350\n",
            "Iteration 190, loss = 0.52054646\n",
            "Iteration 191, loss = 0.52043899\n",
            "Iteration 192, loss = 0.51957618\n",
            "Iteration 193, loss = 0.52051221\n",
            "Iteration 194, loss = 0.52012491\n",
            "Iteration 195, loss = 0.52030377\n",
            "Iteration 196, loss = 0.52063992\n",
            "Iteration 197, loss = 0.52071826\n",
            "Iteration 198, loss = 0.52008065\n",
            "Iteration 199, loss = 0.51992810\n",
            "Iteration 200, loss = 0.52088922\n",
            "Iteration 201, loss = 0.52087775\n",
            "Iteration 202, loss = 0.52054743\n",
            "Iteration 203, loss = 0.52017739\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56995628\n",
            "Iteration 2, loss = 0.56418770\n",
            "Iteration 3, loss = 0.56434611\n",
            "Iteration 4, loss = 0.56471266\n",
            "Iteration 5, loss = 0.56416956\n",
            "Iteration 6, loss = 0.56359866\n",
            "Iteration 7, loss = 0.56477105\n",
            "Iteration 8, loss = 0.56456996\n",
            "Iteration 9, loss = 0.56483041\n",
            "Iteration 10, loss = 0.56429552\n",
            "Iteration 11, loss = 0.56328554\n",
            "Iteration 12, loss = 0.56381700\n",
            "Iteration 13, loss = 0.56289976\n",
            "Iteration 14, loss = 0.56392592\n",
            "Iteration 15, loss = 0.56349560\n",
            "Iteration 16, loss = 0.56335612\n",
            "Iteration 17, loss = 0.56364432\n",
            "Iteration 18, loss = 0.56415644\n",
            "Iteration 19, loss = 0.56401588\n",
            "Iteration 20, loss = 0.56372191\n",
            "Iteration 21, loss = 0.56348985\n",
            "Iteration 22, loss = 0.56375566\n",
            "Iteration 23, loss = 0.56331913\n",
            "Iteration 24, loss = 0.56252179\n",
            "Iteration 25, loss = 0.56376267\n",
            "Iteration 26, loss = 0.56341488\n",
            "Iteration 27, loss = 0.56231305\n",
            "Iteration 28, loss = 0.56315688\n",
            "Iteration 29, loss = 0.56282544\n",
            "Iteration 30, loss = 0.56263921\n",
            "Iteration 31, loss = 0.56305872\n",
            "Iteration 32, loss = 0.56263920\n",
            "Iteration 33, loss = 0.56295738\n",
            "Iteration 34, loss = 0.56310108\n",
            "Iteration 35, loss = 0.56300599\n",
            "Iteration 36, loss = 0.56207599\n",
            "Iteration 37, loss = 0.56277739\n",
            "Iteration 38, loss = 0.56247670\n",
            "Iteration 39, loss = 0.56123482\n",
            "Iteration 40, loss = 0.56267814\n",
            "Iteration 41, loss = 0.56274175\n",
            "Iteration 42, loss = 0.56234427\n",
            "Iteration 43, loss = 0.56200946\n",
            "Iteration 44, loss = 0.56185696\n",
            "Iteration 45, loss = 0.56096343\n",
            "Iteration 46, loss = 0.56151071\n",
            "Iteration 47, loss = 0.56195052\n",
            "Iteration 48, loss = 0.56177317\n",
            "Iteration 49, loss = 0.56147791\n",
            "Iteration 50, loss = 0.56149757\n",
            "Iteration 51, loss = 0.56143757\n",
            "Iteration 52, loss = 0.56134712\n",
            "Iteration 53, loss = 0.56061232\n",
            "Iteration 54, loss = 0.56048909\n",
            "Iteration 55, loss = 0.56080122\n",
            "Iteration 56, loss = 0.56016187\n",
            "Iteration 57, loss = 0.56017077\n",
            "Iteration 58, loss = 0.55953374\n",
            "Iteration 59, loss = 0.55951850\n",
            "Iteration 60, loss = 0.56032238\n",
            "Iteration 61, loss = 0.55921145\n",
            "Iteration 62, loss = 0.55908842\n",
            "Iteration 63, loss = 0.55939823\n",
            "Iteration 64, loss = 0.55854955\n",
            "Iteration 65, loss = 0.55913619\n",
            "Iteration 66, loss = 0.55902044\n",
            "Iteration 67, loss = 0.55849365\n",
            "Iteration 68, loss = 0.55793292\n",
            "Iteration 69, loss = 0.55889511\n",
            "Iteration 70, loss = 0.55808267\n",
            "Iteration 71, loss = 0.55780837\n",
            "Iteration 72, loss = 0.55595406\n",
            "Iteration 73, loss = 0.55520092\n",
            "Iteration 74, loss = 0.55672033\n",
            "Iteration 75, loss = 0.55594434\n",
            "Iteration 76, loss = 0.55529142\n",
            "Iteration 77, loss = 0.55525656\n",
            "Iteration 78, loss = 0.55464868\n",
            "Iteration 79, loss = 0.55390146\n",
            "Iteration 80, loss = 0.55342982\n",
            "Iteration 81, loss = 0.55314895\n",
            "Iteration 82, loss = 0.55350594\n",
            "Iteration 83, loss = 0.55176160\n",
            "Iteration 84, loss = 0.55153603\n",
            "Iteration 85, loss = 0.55073529\n",
            "Iteration 86, loss = 0.55063910\n",
            "Iteration 87, loss = 0.54958542\n",
            "Iteration 88, loss = 0.54920151\n",
            "Iteration 89, loss = 0.54824918\n",
            "Iteration 90, loss = 0.54822761\n",
            "Iteration 91, loss = 0.54694525\n",
            "Iteration 92, loss = 0.54713716\n",
            "Iteration 93, loss = 0.54576469\n",
            "Iteration 94, loss = 0.54516048\n",
            "Iteration 95, loss = 0.54542892\n",
            "Iteration 96, loss = 0.54460715\n",
            "Iteration 97, loss = 0.54390147\n",
            "Iteration 98, loss = 0.54076569\n",
            "Iteration 99, loss = 0.54250347\n",
            "Iteration 100, loss = 0.54141281\n",
            "Iteration 101, loss = 0.54105088\n",
            "Iteration 102, loss = 0.54043993\n",
            "Iteration 103, loss = 0.54049625\n",
            "Iteration 104, loss = 0.53960412\n",
            "Iteration 105, loss = 0.53837658\n",
            "Iteration 106, loss = 0.53779735\n",
            "Iteration 107, loss = 0.53833784\n",
            "Iteration 108, loss = 0.53645971\n",
            "Iteration 109, loss = 0.53649578\n",
            "Iteration 110, loss = 0.53628159\n",
            "Iteration 111, loss = 0.53589000\n",
            "Iteration 112, loss = 0.53512094\n",
            "Iteration 113, loss = 0.53416948\n",
            "Iteration 114, loss = 0.53447445\n",
            "Iteration 115, loss = 0.53384150\n",
            "Iteration 116, loss = 0.53430121\n",
            "Iteration 117, loss = 0.53360330\n",
            "Iteration 118, loss = 0.53170108\n",
            "Iteration 119, loss = 0.53254047\n",
            "Iteration 120, loss = 0.53169681\n",
            "Iteration 121, loss = 0.53134644\n",
            "Iteration 122, loss = 0.53130964\n",
            "Iteration 123, loss = 0.53168393\n",
            "Iteration 124, loss = 0.53194130\n",
            "Iteration 125, loss = 0.53167826\n",
            "Iteration 126, loss = 0.53079140\n",
            "Iteration 127, loss = 0.53035966\n",
            "Iteration 128, loss = 0.53024667\n",
            "Iteration 129, loss = 0.53009469\n",
            "Iteration 130, loss = 0.52927316\n",
            "Iteration 131, loss = 0.52741247\n",
            "Iteration 132, loss = 0.52966350\n",
            "Iteration 133, loss = 0.52852026\n",
            "Iteration 134, loss = 0.52899282\n",
            "Iteration 135, loss = 0.52840620\n",
            "Iteration 136, loss = 0.52891411\n",
            "Iteration 137, loss = 0.52828934\n",
            "Iteration 138, loss = 0.52768260\n",
            "Iteration 139, loss = 0.52821592\n",
            "Iteration 140, loss = 0.52729896\n",
            "Iteration 141, loss = 0.52720742\n",
            "Iteration 142, loss = 0.52633246\n",
            "Iteration 143, loss = 0.52578613\n",
            "Iteration 144, loss = 0.52698914\n",
            "Iteration 145, loss = 0.52707057\n",
            "Iteration 146, loss = 0.52643890\n",
            "Iteration 147, loss = 0.52608920\n",
            "Iteration 148, loss = 0.52657096\n",
            "Iteration 149, loss = 0.52724825\n",
            "Iteration 150, loss = 0.52579441\n",
            "Iteration 151, loss = 0.52560245\n",
            "Iteration 152, loss = 0.52577200\n",
            "Iteration 153, loss = 0.52581492\n",
            "Iteration 154, loss = 0.52515167\n",
            "Iteration 155, loss = 0.52551956\n",
            "Iteration 156, loss = 0.52510005\n",
            "Iteration 157, loss = 0.52446379\n",
            "Iteration 158, loss = 0.52571651\n",
            "Iteration 159, loss = 0.52329089\n",
            "Iteration 160, loss = 0.52536599\n",
            "Iteration 161, loss = 0.52449142\n",
            "Iteration 162, loss = 0.52506150\n",
            "Iteration 163, loss = 0.52446371\n",
            "Iteration 164, loss = 0.52372918\n",
            "Iteration 165, loss = 0.52395165\n",
            "Iteration 166, loss = 0.52415529\n",
            "Iteration 167, loss = 0.52506666\n",
            "Iteration 168, loss = 0.52394231\n",
            "Iteration 169, loss = 0.52302236\n",
            "Iteration 170, loss = 0.52443835\n",
            "Iteration 171, loss = 0.52344066\n",
            "Iteration 172, loss = 0.52339559\n",
            "Iteration 173, loss = 0.52373259\n",
            "Iteration 174, loss = 0.52372771\n",
            "Iteration 175, loss = 0.52389581\n",
            "Iteration 176, loss = 0.52305819\n",
            "Iteration 177, loss = 0.52241116\n",
            "Iteration 178, loss = 0.52240783\n",
            "Iteration 179, loss = 0.52348919\n",
            "Iteration 180, loss = 0.52359739\n",
            "Iteration 181, loss = 0.52364472\n",
            "Iteration 182, loss = 0.52316385\n",
            "Iteration 183, loss = 0.52339890\n",
            "Iteration 184, loss = 0.52327098\n",
            "Iteration 185, loss = 0.52273333\n",
            "Iteration 186, loss = 0.52269933\n",
            "Iteration 187, loss = 0.52311391\n",
            "Iteration 188, loss = 0.52191591\n",
            "Iteration 189, loss = 0.52297075\n",
            "Iteration 190, loss = 0.52222196\n",
            "Iteration 191, loss = 0.52245095\n",
            "Iteration 192, loss = 0.52405828\n",
            "Iteration 193, loss = 0.52235743\n",
            "Iteration 194, loss = 0.52230479\n",
            "Iteration 195, loss = 0.52289330\n",
            "Iteration 196, loss = 0.52239901\n",
            "Iteration 197, loss = 0.52351122\n",
            "Iteration 198, loss = 0.52163599\n",
            "Iteration 199, loss = 0.52243309\n",
            "Iteration 200, loss = 0.52282798\n",
            "Iteration 201, loss = 0.52217567\n",
            "Iteration 202, loss = 0.52301493\n",
            "Iteration 203, loss = 0.52259835\n",
            "Iteration 204, loss = 0.52188922\n",
            "Iteration 205, loss = 0.52269726\n",
            "Iteration 206, loss = 0.52320395\n",
            "Iteration 207, loss = 0.52237831\n",
            "Iteration 208, loss = 0.52162957\n",
            "Iteration 209, loss = 0.52266658\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56350057\n",
            "Iteration 2, loss = 0.56026214\n",
            "Iteration 3, loss = 0.56161028\n",
            "Iteration 4, loss = 0.56056776\n",
            "Iteration 5, loss = 0.56073544\n",
            "Iteration 6, loss = 0.56087673\n",
            "Iteration 7, loss = 0.56043661\n",
            "Iteration 8, loss = 0.56058475\n",
            "Iteration 9, loss = 0.56083385\n",
            "Iteration 10, loss = 0.55979459\n",
            "Iteration 11, loss = 0.56038884\n",
            "Iteration 12, loss = 0.56077594\n",
            "Iteration 13, loss = 0.56069106\n",
            "Iteration 14, loss = 0.56063674\n",
            "Iteration 15, loss = 0.55998292\n",
            "Iteration 16, loss = 0.56047777\n",
            "Iteration 17, loss = 0.55946175\n",
            "Iteration 18, loss = 0.56055456\n",
            "Iteration 19, loss = 0.56025440\n",
            "Iteration 20, loss = 0.55939739\n",
            "Iteration 21, loss = 0.56023196\n",
            "Iteration 22, loss = 0.55900168\n",
            "Iteration 23, loss = 0.56011476\n",
            "Iteration 24, loss = 0.56026328\n",
            "Iteration 25, loss = 0.55984089\n",
            "Iteration 26, loss = 0.55953778\n",
            "Iteration 27, loss = 0.56010441\n",
            "Iteration 28, loss = 0.55918195\n",
            "Iteration 29, loss = 0.55908233\n",
            "Iteration 30, loss = 0.55902555\n",
            "Iteration 31, loss = 0.55815514\n",
            "Iteration 32, loss = 0.55946657\n",
            "Iteration 33, loss = 0.55909715\n",
            "Iteration 34, loss = 0.55833304\n",
            "Iteration 35, loss = 0.55877813\n",
            "Iteration 36, loss = 0.55899656\n",
            "Iteration 37, loss = 0.55904443\n",
            "Iteration 38, loss = 0.55870463\n",
            "Iteration 39, loss = 0.55922880\n",
            "Iteration 40, loss = 0.55766837\n",
            "Iteration 41, loss = 0.55950651\n",
            "Iteration 42, loss = 0.55855323\n",
            "Iteration 43, loss = 0.55908643\n",
            "Iteration 44, loss = 0.55756384\n",
            "Iteration 45, loss = 0.55743039\n",
            "Iteration 46, loss = 0.55880391\n",
            "Iteration 47, loss = 0.55742890\n",
            "Iteration 48, loss = 0.55710746\n",
            "Iteration 49, loss = 0.55772921\n",
            "Iteration 50, loss = 0.55688707\n",
            "Iteration 51, loss = 0.55680346\n",
            "Iteration 52, loss = 0.55779752\n",
            "Iteration 53, loss = 0.55713448\n",
            "Iteration 54, loss = 0.55680374\n",
            "Iteration 55, loss = 0.55687889\n",
            "Iteration 56, loss = 0.55574803\n",
            "Iteration 57, loss = 0.55576581\n",
            "Iteration 58, loss = 0.55598296\n",
            "Iteration 59, loss = 0.55546602\n",
            "Iteration 60, loss = 0.55547452\n",
            "Iteration 61, loss = 0.55521880\n",
            "Iteration 62, loss = 0.55515563\n",
            "Iteration 63, loss = 0.55469432\n",
            "Iteration 64, loss = 0.55492616\n",
            "Iteration 65, loss = 0.55387708\n",
            "Iteration 66, loss = 0.55305484\n",
            "Iteration 67, loss = 0.55333935\n",
            "Iteration 68, loss = 0.55295177\n",
            "Iteration 69, loss = 0.55386893\n",
            "Iteration 70, loss = 0.55213771\n",
            "Iteration 71, loss = 0.55143004\n",
            "Iteration 72, loss = 0.55153606\n",
            "Iteration 73, loss = 0.55092850\n",
            "Iteration 74, loss = 0.55054930\n",
            "Iteration 75, loss = 0.55015373\n",
            "Iteration 76, loss = 0.54991046\n",
            "Iteration 77, loss = 0.54904093\n",
            "Iteration 78, loss = 0.54890965\n",
            "Iteration 79, loss = 0.54894293\n",
            "Iteration 80, loss = 0.54733569\n",
            "Iteration 81, loss = 0.54653878\n",
            "Iteration 82, loss = 0.54680672\n",
            "Iteration 83, loss = 0.54548885\n",
            "Iteration 84, loss = 0.54472583\n",
            "Iteration 85, loss = 0.54488050\n",
            "Iteration 86, loss = 0.54353473\n",
            "Iteration 87, loss = 0.54299447\n",
            "Iteration 88, loss = 0.54229278\n",
            "Iteration 89, loss = 0.54188835\n",
            "Iteration 90, loss = 0.54120054\n",
            "Iteration 91, loss = 0.53950736\n",
            "Iteration 92, loss = 0.54029985\n",
            "Iteration 93, loss = 0.53857148\n",
            "Iteration 94, loss = 0.53897036\n",
            "Iteration 95, loss = 0.53846815\n",
            "Iteration 96, loss = 0.53669127\n",
            "Iteration 97, loss = 0.53604940\n",
            "Iteration 98, loss = 0.53502376\n",
            "Iteration 99, loss = 0.53470594\n",
            "Iteration 100, loss = 0.53493180\n",
            "Iteration 101, loss = 0.53428933\n",
            "Iteration 102, loss = 0.53318554\n",
            "Iteration 103, loss = 0.53180208\n",
            "Iteration 104, loss = 0.53210578\n",
            "Iteration 105, loss = 0.53105688\n",
            "Iteration 106, loss = 0.53128700\n",
            "Iteration 107, loss = 0.53031691\n",
            "Iteration 108, loss = 0.53120978\n",
            "Iteration 109, loss = 0.52972908\n",
            "Iteration 110, loss = 0.52934364\n",
            "Iteration 111, loss = 0.52827248\n",
            "Iteration 112, loss = 0.52845936\n",
            "Iteration 113, loss = 0.52825192\n",
            "Iteration 114, loss = 0.52803553\n",
            "Iteration 115, loss = 0.52848138\n",
            "Iteration 116, loss = 0.52782988\n",
            "Iteration 117, loss = 0.52694674\n",
            "Iteration 118, loss = 0.52710109\n",
            "Iteration 119, loss = 0.52637527\n",
            "Iteration 120, loss = 0.52603843\n",
            "Iteration 121, loss = 0.52593627\n",
            "Iteration 122, loss = 0.52597544\n",
            "Iteration 123, loss = 0.52544877\n",
            "Iteration 124, loss = 0.52514783\n",
            "Iteration 125, loss = 0.52504751\n",
            "Iteration 126, loss = 0.52584387\n",
            "Iteration 127, loss = 0.52470424\n",
            "Iteration 128, loss = 0.52427153\n",
            "Iteration 129, loss = 0.52377917\n",
            "Iteration 130, loss = 0.52469379\n",
            "Iteration 131, loss = 0.52396845\n",
            "Iteration 132, loss = 0.52420522\n",
            "Iteration 133, loss = 0.52340085\n",
            "Iteration 134, loss = 0.52317770\n",
            "Iteration 135, loss = 0.52260775\n",
            "Iteration 136, loss = 0.52351560\n",
            "Iteration 137, loss = 0.52330344\n",
            "Iteration 138, loss = 0.52272239\n",
            "Iteration 139, loss = 0.52241322\n",
            "Iteration 140, loss = 0.52191143\n",
            "Iteration 141, loss = 0.52242126\n",
            "Iteration 142, loss = 0.52207965\n",
            "Iteration 143, loss = 0.52301513\n",
            "Iteration 144, loss = 0.52093312\n",
            "Iteration 145, loss = 0.52120983\n",
            "Iteration 146, loss = 0.52165206\n",
            "Iteration 147, loss = 0.52089629\n",
            "Iteration 148, loss = 0.52157095\n",
            "Iteration 149, loss = 0.52162003\n",
            "Iteration 150, loss = 0.52085462\n",
            "Iteration 151, loss = 0.52084105\n",
            "Iteration 152, loss = 0.52144248\n",
            "Iteration 153, loss = 0.52084519\n",
            "Iteration 154, loss = 0.51892715\n",
            "Iteration 155, loss = 0.52159517\n",
            "Iteration 156, loss = 0.52108061\n",
            "Iteration 157, loss = 0.52042294\n",
            "Iteration 158, loss = 0.52002080\n",
            "Iteration 159, loss = 0.52063049\n",
            "Iteration 160, loss = 0.51978745\n",
            "Iteration 161, loss = 0.52016464\n",
            "Iteration 162, loss = 0.52005352\n",
            "Iteration 163, loss = 0.52075799\n",
            "Iteration 164, loss = 0.51998170\n",
            "Iteration 165, loss = 0.52058893\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56577566\n",
            "Iteration 2, loss = 0.56295603\n",
            "Iteration 3, loss = 0.56396801\n",
            "Iteration 4, loss = 0.56287305\n",
            "Iteration 5, loss = 0.56304756\n",
            "Iteration 6, loss = 0.56256799\n",
            "Iteration 7, loss = 0.56333686\n",
            "Iteration 8, loss = 0.56273835\n",
            "Iteration 9, loss = 0.56327966\n",
            "Iteration 10, loss = 0.56283093\n",
            "Iteration 11, loss = 0.56247982\n",
            "Iteration 12, loss = 0.56302585\n",
            "Iteration 13, loss = 0.56271257\n",
            "Iteration 14, loss = 0.56207611\n",
            "Iteration 15, loss = 0.56248283\n",
            "Iteration 16, loss = 0.56240975\n",
            "Iteration 17, loss = 0.56260228\n",
            "Iteration 18, loss = 0.56238161\n",
            "Iteration 19, loss = 0.56238562\n",
            "Iteration 20, loss = 0.56282067\n",
            "Iteration 21, loss = 0.56274689\n",
            "Iteration 22, loss = 0.56201302\n",
            "Iteration 23, loss = 0.56218920\n",
            "Iteration 24, loss = 0.56192815\n",
            "Iteration 25, loss = 0.56065882\n",
            "Iteration 26, loss = 0.56169998\n",
            "Iteration 27, loss = 0.56202666\n",
            "Iteration 28, loss = 0.56131698\n",
            "Iteration 29, loss = 0.56143766\n",
            "Iteration 30, loss = 0.56178077\n",
            "Iteration 31, loss = 0.56126586\n",
            "Iteration 32, loss = 0.56206130\n",
            "Iteration 33, loss = 0.56106612\n",
            "Iteration 34, loss = 0.56134693\n",
            "Iteration 35, loss = 0.56103025\n",
            "Iteration 36, loss = 0.55999720\n",
            "Iteration 37, loss = 0.56121917\n",
            "Iteration 38, loss = 0.56074484\n",
            "Iteration 39, loss = 0.56078481\n",
            "Iteration 40, loss = 0.56091095\n",
            "Iteration 41, loss = 0.56004995\n",
            "Iteration 42, loss = 0.56025083\n",
            "Iteration 43, loss = 0.55921906\n",
            "Iteration 44, loss = 0.56020010\n",
            "Iteration 45, loss = 0.55987048\n",
            "Iteration 46, loss = 0.55938271\n",
            "Iteration 47, loss = 0.55926049\n",
            "Iteration 48, loss = 0.55883978\n",
            "Iteration 49, loss = 0.55848283\n",
            "Iteration 50, loss = 0.55800517\n",
            "Iteration 51, loss = 0.55830520\n",
            "Iteration 52, loss = 0.55816030\n",
            "Iteration 53, loss = 0.55791769\n",
            "Iteration 54, loss = 0.55670609\n",
            "Iteration 55, loss = 0.55719719\n",
            "Iteration 56, loss = 0.55632582\n",
            "Iteration 57, loss = 0.55668639\n",
            "Iteration 58, loss = 0.55539422\n",
            "Iteration 59, loss = 0.55559199\n",
            "Iteration 60, loss = 0.55519458\n",
            "Iteration 61, loss = 0.55459075\n",
            "Iteration 62, loss = 0.55394231\n",
            "Iteration 63, loss = 0.55421297\n",
            "Iteration 64, loss = 0.55268144\n",
            "Iteration 65, loss = 0.55313222\n",
            "Iteration 66, loss = 0.55267609\n",
            "Iteration 67, loss = 0.55250466\n",
            "Iteration 68, loss = 0.55165929\n",
            "Iteration 69, loss = 0.55046967\n",
            "Iteration 70, loss = 0.55002814\n",
            "Iteration 71, loss = 0.54927544\n",
            "Iteration 72, loss = 0.54911358\n",
            "Iteration 73, loss = 0.54818266\n",
            "Iteration 74, loss = 0.54743963\n",
            "Iteration 75, loss = 0.54688318\n",
            "Iteration 76, loss = 0.54420239\n",
            "Iteration 77, loss = 0.54691551\n",
            "Iteration 78, loss = 0.54489270\n",
            "Iteration 79, loss = 0.54424549\n",
            "Iteration 80, loss = 0.54334484\n",
            "Iteration 81, loss = 0.54238946\n",
            "Iteration 82, loss = 0.54230502\n",
            "Iteration 83, loss = 0.54011829\n",
            "Iteration 84, loss = 0.53993297\n",
            "Iteration 85, loss = 0.54025162\n",
            "Iteration 86, loss = 0.53827404\n",
            "Iteration 87, loss = 0.53758335\n",
            "Iteration 88, loss = 0.53798753\n",
            "Iteration 89, loss = 0.53630256\n",
            "Iteration 90, loss = 0.53662758\n",
            "Iteration 91, loss = 0.53356020\n",
            "Iteration 92, loss = 0.53462825\n",
            "Iteration 93, loss = 0.53326348\n",
            "Iteration 94, loss = 0.53236750\n",
            "Iteration 95, loss = 0.53238091\n",
            "Iteration 96, loss = 0.53225948\n",
            "Iteration 97, loss = 0.53072091\n",
            "Iteration 98, loss = 0.53118789\n",
            "Iteration 99, loss = 0.53089077\n",
            "Iteration 100, loss = 0.52987389\n",
            "Iteration 101, loss = 0.52926016\n",
            "Iteration 102, loss = 0.52905929\n",
            "Iteration 103, loss = 0.52845477\n",
            "Iteration 104, loss = 0.52768078\n",
            "Iteration 105, loss = 0.52797076\n",
            "Iteration 106, loss = 0.52751979\n",
            "Iteration 107, loss = 0.52725089\n",
            "Iteration 108, loss = 0.52741705\n",
            "Iteration 109, loss = 0.52671127\n",
            "Iteration 110, loss = 0.52591919\n",
            "Iteration 111, loss = 0.52641127\n",
            "Iteration 112, loss = 0.52615380\n",
            "Iteration 113, loss = 0.52561788\n",
            "Iteration 114, loss = 0.52503757\n",
            "Iteration 115, loss = 0.52528099\n",
            "Iteration 116, loss = 0.52496785\n",
            "Iteration 117, loss = 0.52478205\n",
            "Iteration 118, loss = 0.52434236\n",
            "Iteration 119, loss = 0.52476843\n",
            "Iteration 120, loss = 0.52322089\n",
            "Iteration 121, loss = 0.52338317\n",
            "Iteration 122, loss = 0.52323865\n",
            "Iteration 123, loss = 0.52337259\n",
            "Iteration 124, loss = 0.52378722\n",
            "Iteration 125, loss = 0.52302705\n",
            "Iteration 126, loss = 0.52327310\n",
            "Iteration 127, loss = 0.52197873\n",
            "Iteration 128, loss = 0.52143917\n",
            "Iteration 129, loss = 0.52204844\n",
            "Iteration 130, loss = 0.52239122\n",
            "Iteration 131, loss = 0.52135137\n",
            "Iteration 132, loss = 0.52184361\n",
            "Iteration 133, loss = 0.52218488\n",
            "Iteration 134, loss = 0.52167994\n",
            "Iteration 135, loss = 0.52097312\n",
            "Iteration 136, loss = 0.52184445\n",
            "Iteration 137, loss = 0.52160886\n",
            "Iteration 138, loss = 0.52082216\n",
            "Iteration 139, loss = 0.52122641\n",
            "Iteration 140, loss = 0.52031929\n",
            "Iteration 141, loss = 0.52004128\n",
            "Iteration 142, loss = 0.52085903\n",
            "Iteration 143, loss = 0.52036222\n",
            "Iteration 144, loss = 0.52081610\n",
            "Iteration 145, loss = 0.52032238\n",
            "Iteration 146, loss = 0.52039168\n",
            "Iteration 147, loss = 0.52068668\n",
            "Iteration 148, loss = 0.52056694\n",
            "Iteration 149, loss = 0.51976044\n",
            "Iteration 150, loss = 0.52008386\n",
            "Iteration 151, loss = 0.51967767\n",
            "Iteration 152, loss = 0.51946376\n",
            "Iteration 153, loss = 0.51910082\n",
            "Iteration 154, loss = 0.51898433\n",
            "Iteration 155, loss = 0.51969232\n",
            "Iteration 156, loss = 0.51868962\n",
            "Iteration 157, loss = 0.51893502\n",
            "Iteration 158, loss = 0.51939920\n",
            "Iteration 159, loss = 0.51880829\n",
            "Iteration 160, loss = 0.51932033\n",
            "Iteration 161, loss = 0.51924673\n",
            "Iteration 162, loss = 0.51896154\n",
            "Iteration 163, loss = 0.51971713\n",
            "Iteration 164, loss = 0.51871888\n",
            "Iteration 165, loss = 0.51745594\n",
            "Iteration 166, loss = 0.51864307\n",
            "Iteration 167, loss = 0.51838862\n",
            "Iteration 168, loss = 0.51782385\n",
            "Iteration 169, loss = 0.51767261\n",
            "Iteration 170, loss = 0.51715255\n",
            "Iteration 171, loss = 0.51788231\n",
            "Iteration 172, loss = 0.51883366\n",
            "Iteration 173, loss = 0.51778995\n",
            "Iteration 174, loss = 0.51815358\n",
            "Iteration 175, loss = 0.51890088\n",
            "Iteration 176, loss = 0.51835305\n",
            "Iteration 177, loss = 0.51795215\n",
            "Iteration 178, loss = 0.51750101\n",
            "Iteration 179, loss = 0.51807112\n",
            "Iteration 180, loss = 0.51788713\n",
            "Iteration 181, loss = 0.51721758\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "8\n",
            "Iteration 1, loss = 0.56764460\n",
            "Iteration 2, loss = 0.56553511\n",
            "Iteration 3, loss = 0.56540946\n",
            "Iteration 4, loss = 0.56538169\n",
            "Iteration 5, loss = 0.56473784\n",
            "Iteration 6, loss = 0.56485621\n",
            "Iteration 7, loss = 0.56431648\n",
            "Iteration 8, loss = 0.56455225\n",
            "Iteration 9, loss = 0.56483975\n",
            "Iteration 10, loss = 0.56488980\n",
            "Iteration 11, loss = 0.56460927\n",
            "Iteration 12, loss = 0.56515692\n",
            "Iteration 13, loss = 0.56544871\n",
            "Iteration 14, loss = 0.56461061\n",
            "Iteration 15, loss = 0.56443508\n",
            "Iteration 16, loss = 0.56451854\n",
            "Iteration 17, loss = 0.56389550\n",
            "Iteration 18, loss = 0.56413219\n",
            "Iteration 19, loss = 0.56464229\n",
            "Iteration 20, loss = 0.56483038\n",
            "Iteration 21, loss = 0.56405492\n",
            "Iteration 22, loss = 0.56437710\n",
            "Iteration 23, loss = 0.56409317\n",
            "Iteration 24, loss = 0.56456538\n",
            "Iteration 25, loss = 0.56366623\n",
            "Iteration 26, loss = 0.56369434\n",
            "Iteration 27, loss = 0.56506548\n",
            "Iteration 28, loss = 0.56384289\n",
            "Iteration 29, loss = 0.56424362\n",
            "Iteration 30, loss = 0.56444352\n",
            "Iteration 31, loss = 0.56322136\n",
            "Iteration 32, loss = 0.56264657\n",
            "Iteration 33, loss = 0.56372272\n",
            "Iteration 34, loss = 0.56330332\n",
            "Iteration 35, loss = 0.56325974\n",
            "Iteration 36, loss = 0.56417778\n",
            "Iteration 37, loss = 0.56355348\n",
            "Iteration 38, loss = 0.56304403\n",
            "Iteration 39, loss = 0.56220383\n",
            "Iteration 40, loss = 0.56296624\n",
            "Iteration 41, loss = 0.56278783\n",
            "Iteration 42, loss = 0.56292071\n",
            "Iteration 43, loss = 0.56224190\n",
            "Iteration 44, loss = 0.56243887\n",
            "Iteration 45, loss = 0.56263201\n",
            "Iteration 46, loss = 0.56370087\n",
            "Iteration 47, loss = 0.56277527\n",
            "Iteration 48, loss = 0.56232323\n",
            "Iteration 49, loss = 0.56257741\n",
            "Iteration 50, loss = 0.56191303\n",
            "Iteration 51, loss = 0.56252113\n",
            "Iteration 52, loss = 0.56231206\n",
            "Iteration 53, loss = 0.56234484\n",
            "Iteration 54, loss = 0.56177038\n",
            "Iteration 55, loss = 0.56150560\n",
            "Iteration 56, loss = 0.56054000\n",
            "Iteration 57, loss = 0.56121072\n",
            "Iteration 58, loss = 0.56088284\n",
            "Iteration 59, loss = 0.55947074\n",
            "Iteration 60, loss = 0.56096998\n",
            "Iteration 61, loss = 0.56147043\n",
            "Iteration 62, loss = 0.55996293\n",
            "Iteration 63, loss = 0.55991688\n",
            "Iteration 64, loss = 0.55935356\n",
            "Iteration 65, loss = 0.55962471\n",
            "Iteration 66, loss = 0.55983798\n",
            "Iteration 67, loss = 0.55893975\n",
            "Iteration 68, loss = 0.55867480\n",
            "Iteration 69, loss = 0.55794543\n",
            "Iteration 70, loss = 0.55819695\n",
            "Iteration 71, loss = 0.55799049\n",
            "Iteration 72, loss = 0.55807646\n",
            "Iteration 73, loss = 0.55679026\n",
            "Iteration 74, loss = 0.55655439\n",
            "Iteration 75, loss = 0.55618176\n",
            "Iteration 76, loss = 0.55614919\n",
            "Iteration 77, loss = 0.55625055\n",
            "Iteration 78, loss = 0.55510642\n",
            "Iteration 79, loss = 0.55435731\n",
            "Iteration 80, loss = 0.55410139\n",
            "Iteration 81, loss = 0.55333655\n",
            "Iteration 82, loss = 0.55229330\n",
            "Iteration 83, loss = 0.55286214\n",
            "Iteration 84, loss = 0.55233314\n",
            "Iteration 85, loss = 0.55074037\n",
            "Iteration 86, loss = 0.55078933\n",
            "Iteration 87, loss = 0.54944766\n",
            "Iteration 88, loss = 0.54941894\n",
            "Iteration 89, loss = 0.54831409\n",
            "Iteration 90, loss = 0.54724718\n",
            "Iteration 91, loss = 0.54685662\n",
            "Iteration 92, loss = 0.54688463\n",
            "Iteration 93, loss = 0.54514354\n",
            "Iteration 94, loss = 0.54474699\n",
            "Iteration 95, loss = 0.54441105\n",
            "Iteration 96, loss = 0.54368959\n",
            "Iteration 97, loss = 0.54259452\n",
            "Iteration 98, loss = 0.54161320\n",
            "Iteration 99, loss = 0.54140744\n",
            "Iteration 100, loss = 0.54008848\n",
            "Iteration 101, loss = 0.53856888\n",
            "Iteration 102, loss = 0.53929365\n",
            "Iteration 103, loss = 0.53864197\n",
            "Iteration 104, loss = 0.53777848\n",
            "Iteration 105, loss = 0.53683204\n",
            "Iteration 106, loss = 0.53569730\n",
            "Iteration 107, loss = 0.53505778\n",
            "Iteration 108, loss = 0.53483694\n",
            "Iteration 109, loss = 0.53389683\n",
            "Iteration 110, loss = 0.53391325\n",
            "Iteration 111, loss = 0.53349785\n",
            "Iteration 112, loss = 0.53287409\n",
            "Iteration 113, loss = 0.53241243\n",
            "Iteration 114, loss = 0.53221091\n",
            "Iteration 115, loss = 0.53144229\n",
            "Iteration 116, loss = 0.53189670\n",
            "Iteration 117, loss = 0.53117658\n",
            "Iteration 118, loss = 0.52996261\n",
            "Iteration 119, loss = 0.52987326\n",
            "Iteration 120, loss = 0.52884511\n",
            "Iteration 121, loss = 0.52912991\n",
            "Iteration 122, loss = 0.52880641\n",
            "Iteration 123, loss = 0.52865804\n",
            "Iteration 124, loss = 0.52921648\n",
            "Iteration 125, loss = 0.52797796\n",
            "Iteration 126, loss = 0.52806017\n",
            "Iteration 127, loss = 0.52873134\n",
            "Iteration 128, loss = 0.52738325\n",
            "Iteration 129, loss = 0.52856756\n",
            "Iteration 130, loss = 0.52740907\n",
            "Iteration 131, loss = 0.52812976\n",
            "Iteration 132, loss = 0.52647856\n",
            "Iteration 133, loss = 0.52756424\n",
            "Iteration 134, loss = 0.52692674\n",
            "Iteration 135, loss = 0.52605688\n",
            "Iteration 136, loss = 0.52553645\n",
            "Iteration 137, loss = 0.52560381\n",
            "Iteration 138, loss = 0.52517404\n",
            "Iteration 139, loss = 0.52576453\n",
            "Iteration 140, loss = 0.52579666\n",
            "Iteration 141, loss = 0.52536612\n",
            "Iteration 142, loss = 0.52515426\n",
            "Iteration 143, loss = 0.52535635\n",
            "Iteration 144, loss = 0.52406904\n",
            "Iteration 145, loss = 0.52514396\n",
            "Iteration 146, loss = 0.52481501\n",
            "Iteration 147, loss = 0.52485993\n",
            "Iteration 148, loss = 0.52405780\n",
            "Iteration 149, loss = 0.52309594\n",
            "Iteration 150, loss = 0.52388659\n",
            "Iteration 151, loss = 0.52422157\n",
            "Iteration 152, loss = 0.52321892\n",
            "Iteration 153, loss = 0.52438244\n",
            "Iteration 154, loss = 0.52341427\n",
            "Iteration 155, loss = 0.52375384\n",
            "Iteration 156, loss = 0.52221788\n",
            "Iteration 157, loss = 0.52323959\n",
            "Iteration 158, loss = 0.52263105\n",
            "Iteration 159, loss = 0.52344558\n",
            "Iteration 160, loss = 0.52269660\n",
            "Iteration 161, loss = 0.52198616\n",
            "Iteration 162, loss = 0.52251928\n",
            "Iteration 163, loss = 0.52368475\n",
            "Iteration 164, loss = 0.52233154\n",
            "Iteration 165, loss = 0.52258731\n",
            "Iteration 166, loss = 0.52310432\n",
            "Iteration 167, loss = 0.52250271\n",
            "Iteration 168, loss = 0.52227698\n",
            "Iteration 169, loss = 0.52330554\n",
            "Iteration 170, loss = 0.52193970\n",
            "Iteration 171, loss = 0.52240556\n",
            "Iteration 172, loss = 0.52202631\n",
            "Iteration 173, loss = 0.52187491\n",
            "Iteration 174, loss = 0.52121169\n",
            "Iteration 175, loss = 0.52163340\n",
            "Iteration 176, loss = 0.52201552\n",
            "Iteration 177, loss = 0.52228026\n",
            "Iteration 178, loss = 0.52152512\n",
            "Iteration 179, loss = 0.52116687\n",
            "Iteration 180, loss = 0.52206694\n",
            "Iteration 181, loss = 0.52239953\n",
            "Iteration 182, loss = 0.52092868\n",
            "Iteration 183, loss = 0.52111766\n",
            "Iteration 184, loss = 0.52128191\n",
            "Iteration 185, loss = 0.52122736\n",
            "Iteration 186, loss = 0.52181307\n",
            "Iteration 187, loss = 0.52076655\n",
            "Iteration 188, loss = 0.52109963\n",
            "Iteration 189, loss = 0.52184608\n",
            "Iteration 190, loss = 0.52083649\n",
            "Iteration 191, loss = 0.52128761\n",
            "Iteration 192, loss = 0.52057623\n",
            "Iteration 193, loss = 0.52070362\n",
            "Iteration 194, loss = 0.52089964\n",
            "Iteration 195, loss = 0.52110312\n",
            "Iteration 196, loss = 0.52188765\n",
            "Iteration 197, loss = 0.52103200\n",
            "Iteration 198, loss = 0.52139468\n",
            "Iteration 199, loss = 0.52065306\n",
            "Iteration 200, loss = 0.52125373\n",
            "Iteration 201, loss = 0.52062815\n",
            "Iteration 202, loss = 0.52104386\n",
            "Iteration 203, loss = 0.52084841\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57039633\n",
            "Iteration 2, loss = 0.56218971\n",
            "Iteration 3, loss = 0.56247922\n",
            "Iteration 4, loss = 0.56232491\n",
            "Iteration 5, loss = 0.56282225\n",
            "Iteration 6, loss = 0.56230971\n",
            "Iteration 7, loss = 0.56190295\n",
            "Iteration 8, loss = 0.56260421\n",
            "Iteration 9, loss = 0.56242224\n",
            "Iteration 10, loss = 0.56263835\n",
            "Iteration 11, loss = 0.56186404\n",
            "Iteration 12, loss = 0.56214959\n",
            "Iteration 13, loss = 0.56109833\n",
            "Iteration 14, loss = 0.56219474\n",
            "Iteration 15, loss = 0.56219143\n",
            "Iteration 16, loss = 0.56220229\n",
            "Iteration 17, loss = 0.56163070\n",
            "Iteration 18, loss = 0.56215701\n",
            "Iteration 19, loss = 0.56152417\n",
            "Iteration 20, loss = 0.56169796\n",
            "Iteration 21, loss = 0.56151631\n",
            "Iteration 22, loss = 0.56158013\n",
            "Iteration 23, loss = 0.56093972\n",
            "Iteration 24, loss = 0.56168705\n",
            "Iteration 25, loss = 0.56113496\n",
            "Iteration 26, loss = 0.56163280\n",
            "Iteration 27, loss = 0.56082375\n",
            "Iteration 28, loss = 0.56072300\n",
            "Iteration 29, loss = 0.56083079\n",
            "Iteration 30, loss = 0.56059572\n",
            "Iteration 31, loss = 0.56082874\n",
            "Iteration 32, loss = 0.56118962\n",
            "Iteration 33, loss = 0.55975465\n",
            "Iteration 34, loss = 0.56089419\n",
            "Iteration 35, loss = 0.56051543\n",
            "Iteration 36, loss = 0.56040936\n",
            "Iteration 37, loss = 0.56064148\n",
            "Iteration 38, loss = 0.55942516\n",
            "Iteration 39, loss = 0.55960180\n",
            "Iteration 40, loss = 0.55937758\n",
            "Iteration 41, loss = 0.55977182\n",
            "Iteration 42, loss = 0.55988971\n",
            "Iteration 43, loss = 0.55977921\n",
            "Iteration 44, loss = 0.55855402\n",
            "Iteration 45, loss = 0.55983628\n",
            "Iteration 46, loss = 0.55880862\n",
            "Iteration 47, loss = 0.55892784\n",
            "Iteration 48, loss = 0.55816851\n",
            "Iteration 49, loss = 0.55892766\n",
            "Iteration 50, loss = 0.55872547\n",
            "Iteration 51, loss = 0.55806938\n",
            "Iteration 52, loss = 0.55797462\n",
            "Iteration 53, loss = 0.55793231\n",
            "Iteration 54, loss = 0.55668411\n",
            "Iteration 55, loss = 0.55817664\n",
            "Iteration 56, loss = 0.55682802\n",
            "Iteration 57, loss = 0.55640914\n",
            "Iteration 58, loss = 0.55625217\n",
            "Iteration 59, loss = 0.55620018\n",
            "Iteration 60, loss = 0.55614893\n",
            "Iteration 61, loss = 0.55574714\n",
            "Iteration 62, loss = 0.55462089\n",
            "Iteration 63, loss = 0.55585482\n",
            "Iteration 64, loss = 0.55418071\n",
            "Iteration 65, loss = 0.55433808\n",
            "Iteration 66, loss = 0.55380596\n",
            "Iteration 67, loss = 0.55335259\n",
            "Iteration 68, loss = 0.55308246\n",
            "Iteration 69, loss = 0.55266455\n",
            "Iteration 70, loss = 0.55312940\n",
            "Iteration 71, loss = 0.55162853\n",
            "Iteration 72, loss = 0.55155041\n",
            "Iteration 73, loss = 0.55050337\n",
            "Iteration 74, loss = 0.55006252\n",
            "Iteration 75, loss = 0.54791484\n",
            "Iteration 76, loss = 0.54893304\n",
            "Iteration 77, loss = 0.54783973\n",
            "Iteration 78, loss = 0.54771443\n",
            "Iteration 79, loss = 0.54711324\n",
            "Iteration 80, loss = 0.54594729\n",
            "Iteration 81, loss = 0.54547986\n",
            "Iteration 82, loss = 0.54453396\n",
            "Iteration 83, loss = 0.54464993\n",
            "Iteration 84, loss = 0.54289693\n",
            "Iteration 85, loss = 0.54237880\n",
            "Iteration 86, loss = 0.54270096\n",
            "Iteration 87, loss = 0.54097324\n",
            "Iteration 88, loss = 0.54089198\n",
            "Iteration 89, loss = 0.53942316\n",
            "Iteration 90, loss = 0.53901144\n",
            "Iteration 91, loss = 0.53754261\n",
            "Iteration 92, loss = 0.53679207\n",
            "Iteration 93, loss = 0.53714568\n",
            "Iteration 94, loss = 0.53554076\n",
            "Iteration 95, loss = 0.53583891\n",
            "Iteration 96, loss = 0.53495859\n",
            "Iteration 97, loss = 0.53478818\n",
            "Iteration 98, loss = 0.53370765\n",
            "Iteration 99, loss = 0.53295273\n",
            "Iteration 100, loss = 0.53354489\n",
            "Iteration 101, loss = 0.53258154\n",
            "Iteration 102, loss = 0.53234576\n",
            "Iteration 103, loss = 0.53085576\n",
            "Iteration 104, loss = 0.53085142\n",
            "Iteration 105, loss = 0.52996347\n",
            "Iteration 106, loss = 0.53022549\n",
            "Iteration 107, loss = 0.53007796\n",
            "Iteration 108, loss = 0.52880053\n",
            "Iteration 109, loss = 0.52958469\n",
            "Iteration 110, loss = 0.52870378\n",
            "Iteration 111, loss = 0.52766342\n",
            "Iteration 112, loss = 0.52770048\n",
            "Iteration 113, loss = 0.52745379\n",
            "Iteration 114, loss = 0.52580504\n",
            "Iteration 115, loss = 0.52732710\n",
            "Iteration 116, loss = 0.52676803\n",
            "Iteration 117, loss = 0.52669659\n",
            "Iteration 118, loss = 0.52743756\n",
            "Iteration 119, loss = 0.52575447\n",
            "Iteration 120, loss = 0.52496550\n",
            "Iteration 121, loss = 0.52521026\n",
            "Iteration 122, loss = 0.52504737\n",
            "Iteration 123, loss = 0.52585016\n",
            "Iteration 124, loss = 0.52436400\n",
            "Iteration 125, loss = 0.52488274\n",
            "Iteration 126, loss = 0.52477545\n",
            "Iteration 127, loss = 0.52384700\n",
            "Iteration 128, loss = 0.52416858\n",
            "Iteration 129, loss = 0.52351141\n",
            "Iteration 130, loss = 0.52420771\n",
            "Iteration 131, loss = 0.52331555\n",
            "Iteration 132, loss = 0.52350203\n",
            "Iteration 133, loss = 0.52302868\n",
            "Iteration 134, loss = 0.52295777\n",
            "Iteration 135, loss = 0.52313123\n",
            "Iteration 136, loss = 0.52330578\n",
            "Iteration 137, loss = 0.52254551\n",
            "Iteration 138, loss = 0.52253267\n",
            "Iteration 139, loss = 0.52156238\n",
            "Iteration 140, loss = 0.52327147\n",
            "Iteration 141, loss = 0.52275359\n",
            "Iteration 142, loss = 0.52214242\n",
            "Iteration 143, loss = 0.52265424\n",
            "Iteration 144, loss = 0.52160983\n",
            "Iteration 145, loss = 0.52156273\n",
            "Iteration 146, loss = 0.52189539\n",
            "Iteration 147, loss = 0.52147557\n",
            "Iteration 148, loss = 0.52138816\n",
            "Iteration 149, loss = 0.52123716\n",
            "Iteration 150, loss = 0.52074318\n",
            "Iteration 151, loss = 0.52117576\n",
            "Iteration 152, loss = 0.52061421\n",
            "Iteration 153, loss = 0.52086497\n",
            "Iteration 154, loss = 0.52099257\n",
            "Iteration 155, loss = 0.51969285\n",
            "Iteration 156, loss = 0.52074442\n",
            "Iteration 157, loss = 0.52012307\n",
            "Iteration 158, loss = 0.52041615\n",
            "Iteration 159, loss = 0.52039948\n",
            "Iteration 160, loss = 0.52027281\n",
            "Iteration 161, loss = 0.52062252\n",
            "Iteration 162, loss = 0.52138233\n",
            "Iteration 163, loss = 0.51999998\n",
            "Iteration 164, loss = 0.52022793\n",
            "Iteration 165, loss = 0.52092151\n",
            "Iteration 166, loss = 0.52052714\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56735216\n",
            "Iteration 2, loss = 0.56152132\n",
            "Iteration 3, loss = 0.56086731\n",
            "Iteration 4, loss = 0.56139054\n",
            "Iteration 5, loss = 0.56085550\n",
            "Iteration 6, loss = 0.56121515\n",
            "Iteration 7, loss = 0.56132985\n",
            "Iteration 8, loss = 0.56057824\n",
            "Iteration 9, loss = 0.56114780\n",
            "Iteration 10, loss = 0.56102315\n",
            "Iteration 11, loss = 0.56104384\n",
            "Iteration 12, loss = 0.56024540\n",
            "Iteration 13, loss = 0.56068919\n",
            "Iteration 14, loss = 0.56057308\n",
            "Iteration 15, loss = 0.56041282\n",
            "Iteration 16, loss = 0.56065114\n",
            "Iteration 17, loss = 0.56039720\n",
            "Iteration 18, loss = 0.55951318\n",
            "Iteration 19, loss = 0.56016732\n",
            "Iteration 20, loss = 0.56037208\n",
            "Iteration 21, loss = 0.56043144\n",
            "Iteration 22, loss = 0.56067382\n",
            "Iteration 23, loss = 0.56033405\n",
            "Iteration 24, loss = 0.55987268\n",
            "Iteration 25, loss = 0.55992838\n",
            "Iteration 26, loss = 0.56010365\n",
            "Iteration 27, loss = 0.56006490\n",
            "Iteration 28, loss = 0.55937639\n",
            "Iteration 29, loss = 0.55968394\n",
            "Iteration 30, loss = 0.55989333\n",
            "Iteration 31, loss = 0.55925749\n",
            "Iteration 32, loss = 0.55968339\n",
            "Iteration 33, loss = 0.55939907\n",
            "Iteration 34, loss = 0.55918068\n",
            "Iteration 35, loss = 0.55920831\n",
            "Iteration 36, loss = 0.55870532\n",
            "Iteration 37, loss = 0.55862253\n",
            "Iteration 38, loss = 0.55929556\n",
            "Iteration 39, loss = 0.55730766\n",
            "Iteration 40, loss = 0.55813553\n",
            "Iteration 41, loss = 0.55863792\n",
            "Iteration 42, loss = 0.55847940\n",
            "Iteration 43, loss = 0.55838616\n",
            "Iteration 44, loss = 0.55822769\n",
            "Iteration 45, loss = 0.55730607\n",
            "Iteration 46, loss = 0.55881003\n",
            "Iteration 47, loss = 0.55751008\n",
            "Iteration 48, loss = 0.55685975\n",
            "Iteration 49, loss = 0.55737932\n",
            "Iteration 50, loss = 0.55739502\n",
            "Iteration 51, loss = 0.55691688\n",
            "Iteration 52, loss = 0.55689050\n",
            "Iteration 53, loss = 0.55603911\n",
            "Iteration 54, loss = 0.55625774\n",
            "Iteration 55, loss = 0.55660080\n",
            "Iteration 56, loss = 0.55593792\n",
            "Iteration 57, loss = 0.55626564\n",
            "Iteration 58, loss = 0.55517000\n",
            "Iteration 59, loss = 0.55602013\n",
            "Iteration 60, loss = 0.55480167\n",
            "Iteration 61, loss = 0.55386728\n",
            "Iteration 62, loss = 0.55419959\n",
            "Iteration 63, loss = 0.55469802\n",
            "Iteration 64, loss = 0.55385372\n",
            "Iteration 65, loss = 0.55323426\n",
            "Iteration 66, loss = 0.55348708\n",
            "Iteration 67, loss = 0.55229203\n",
            "Iteration 68, loss = 0.55282036\n",
            "Iteration 69, loss = 0.55148420\n",
            "Iteration 70, loss = 0.55085731\n",
            "Iteration 71, loss = 0.55074291\n",
            "Iteration 72, loss = 0.55016671\n",
            "Iteration 73, loss = 0.54824117\n",
            "Iteration 74, loss = 0.54931545\n",
            "Iteration 75, loss = 0.54862993\n",
            "Iteration 76, loss = 0.54768265\n",
            "Iteration 77, loss = 0.54740516\n",
            "Iteration 78, loss = 0.54706421\n",
            "Iteration 79, loss = 0.54717592\n",
            "Iteration 80, loss = 0.54569213\n",
            "Iteration 81, loss = 0.54477025\n",
            "Iteration 82, loss = 0.54451906\n",
            "Iteration 83, loss = 0.54284670\n",
            "Iteration 84, loss = 0.54320822\n",
            "Iteration 85, loss = 0.54168592\n",
            "Iteration 86, loss = 0.54131763\n",
            "Iteration 87, loss = 0.54067797\n",
            "Iteration 88, loss = 0.53981758\n",
            "Iteration 89, loss = 0.53904946\n",
            "Iteration 90, loss = 0.53819789\n",
            "Iteration 91, loss = 0.53770638\n",
            "Iteration 92, loss = 0.53705045\n",
            "Iteration 93, loss = 0.53618508\n",
            "Iteration 94, loss = 0.53579527\n",
            "Iteration 95, loss = 0.53489286\n",
            "Iteration 96, loss = 0.53414196\n",
            "Iteration 97, loss = 0.53356476\n",
            "Iteration 98, loss = 0.53329313\n",
            "Iteration 99, loss = 0.53233723\n",
            "Iteration 100, loss = 0.53214283\n",
            "Iteration 101, loss = 0.53133441\n",
            "Iteration 102, loss = 0.53081838\n",
            "Iteration 103, loss = 0.53062489\n",
            "Iteration 104, loss = 0.53033723\n",
            "Iteration 105, loss = 0.52944596\n",
            "Iteration 106, loss = 0.52948552\n",
            "Iteration 107, loss = 0.52915352\n",
            "Iteration 108, loss = 0.52866325\n",
            "Iteration 109, loss = 0.52821085\n",
            "Iteration 110, loss = 0.52795480\n",
            "Iteration 111, loss = 0.52790442\n",
            "Iteration 112, loss = 0.52781619\n",
            "Iteration 113, loss = 0.52724982\n",
            "Iteration 114, loss = 0.52741456\n",
            "Iteration 115, loss = 0.52592337\n",
            "Iteration 116, loss = 0.52546566\n",
            "Iteration 117, loss = 0.52668616\n",
            "Iteration 118, loss = 0.52458067\n",
            "Iteration 119, loss = 0.52500323\n",
            "Iteration 120, loss = 0.52429049\n",
            "Iteration 121, loss = 0.52470452\n",
            "Iteration 122, loss = 0.52513173\n",
            "Iteration 123, loss = 0.52425235\n",
            "Iteration 124, loss = 0.52437564\n",
            "Iteration 125, loss = 0.52397028\n",
            "Iteration 126, loss = 0.52378543\n",
            "Iteration 127, loss = 0.52346975\n",
            "Iteration 128, loss = 0.52370943\n",
            "Iteration 129, loss = 0.52378580\n",
            "Iteration 130, loss = 0.52294070\n",
            "Iteration 131, loss = 0.52270333\n",
            "Iteration 132, loss = 0.52249984\n",
            "Iteration 133, loss = 0.52236657\n",
            "Iteration 134, loss = 0.52260664\n",
            "Iteration 135, loss = 0.52200090\n",
            "Iteration 136, loss = 0.52162922\n",
            "Iteration 137, loss = 0.52094582\n",
            "Iteration 138, loss = 0.52167475\n",
            "Iteration 139, loss = 0.52160645\n",
            "Iteration 140, loss = 0.52170672\n",
            "Iteration 141, loss = 0.52110433\n",
            "Iteration 142, loss = 0.52172731\n",
            "Iteration 143, loss = 0.52004534\n",
            "Iteration 144, loss = 0.52066814\n",
            "Iteration 145, loss = 0.52117800\n",
            "Iteration 146, loss = 0.52032052\n",
            "Iteration 147, loss = 0.52072868\n",
            "Iteration 148, loss = 0.52052308\n",
            "Iteration 149, loss = 0.52031459\n",
            "Iteration 150, loss = 0.52039277\n",
            "Iteration 151, loss = 0.51972788\n",
            "Iteration 152, loss = 0.51946196\n",
            "Iteration 153, loss = 0.51892411\n",
            "Iteration 154, loss = 0.51949929\n",
            "Iteration 155, loss = 0.51946194\n",
            "Iteration 156, loss = 0.52017907\n",
            "Iteration 157, loss = 0.51874773\n",
            "Iteration 158, loss = 0.51962458\n",
            "Iteration 159, loss = 0.51973145\n",
            "Iteration 160, loss = 0.51886990\n",
            "Iteration 161, loss = 0.51885271\n",
            "Iteration 162, loss = 0.51855509\n",
            "Iteration 163, loss = 0.51888155\n",
            "Iteration 164, loss = 0.51871270\n",
            "Iteration 165, loss = 0.51862661\n",
            "Iteration 166, loss = 0.51916634\n",
            "Iteration 167, loss = 0.51917344\n",
            "Iteration 168, loss = 0.51750575\n",
            "Iteration 169, loss = 0.51891572\n",
            "Iteration 170, loss = 0.51859254\n",
            "Iteration 171, loss = 0.51899337\n",
            "Iteration 172, loss = 0.51880351\n",
            "Iteration 173, loss = 0.51847145\n",
            "Iteration 174, loss = 0.51782328\n",
            "Iteration 175, loss = 0.51726885\n",
            "Iteration 176, loss = 0.51803213\n",
            "Iteration 177, loss = 0.51800718\n",
            "Iteration 178, loss = 0.51741983\n",
            "Iteration 179, loss = 0.51793069\n",
            "Iteration 180, loss = 0.51801313\n",
            "Iteration 181, loss = 0.51852623\n",
            "Iteration 182, loss = 0.51782742\n",
            "Iteration 183, loss = 0.51815528\n",
            "Iteration 184, loss = 0.51832995\n",
            "Iteration 185, loss = 0.51755773\n",
            "Iteration 186, loss = 0.51760464\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56744226\n",
            "Iteration 2, loss = 0.56227991\n",
            "Iteration 3, loss = 0.56216413\n",
            "Iteration 4, loss = 0.56222322\n",
            "Iteration 5, loss = 0.56208000\n",
            "Iteration 6, loss = 0.56188642\n",
            "Iteration 7, loss = 0.56174507\n",
            "Iteration 8, loss = 0.56216608\n",
            "Iteration 9, loss = 0.56271976\n",
            "Iteration 10, loss = 0.56144257\n",
            "Iteration 11, loss = 0.56194432\n",
            "Iteration 12, loss = 0.56154296\n",
            "Iteration 13, loss = 0.56129464\n",
            "Iteration 14, loss = 0.56208031\n",
            "Iteration 15, loss = 0.56174069\n",
            "Iteration 16, loss = 0.56164078\n",
            "Iteration 17, loss = 0.56103493\n",
            "Iteration 18, loss = 0.56171851\n",
            "Iteration 19, loss = 0.56049649\n",
            "Iteration 20, loss = 0.56153080\n",
            "Iteration 21, loss = 0.56113716\n",
            "Iteration 22, loss = 0.56110051\n",
            "Iteration 23, loss = 0.56127636\n",
            "Iteration 24, loss = 0.56099512\n",
            "Iteration 25, loss = 0.56000785\n",
            "Iteration 26, loss = 0.56166392\n",
            "Iteration 27, loss = 0.56078652\n",
            "Iteration 28, loss = 0.56046527\n",
            "Iteration 29, loss = 0.56121748\n",
            "Iteration 30, loss = 0.56060396\n",
            "Iteration 31, loss = 0.56024368\n",
            "Iteration 32, loss = 0.56088959\n",
            "Iteration 33, loss = 0.56043216\n",
            "Iteration 34, loss = 0.55993863\n",
            "Iteration 35, loss = 0.56036971\n",
            "Iteration 36, loss = 0.55949393\n",
            "Iteration 37, loss = 0.55960563\n",
            "Iteration 38, loss = 0.55981084\n",
            "Iteration 39, loss = 0.55781191\n",
            "Iteration 40, loss = 0.55962971\n",
            "Iteration 41, loss = 0.55871021\n",
            "Iteration 42, loss = 0.55904020\n",
            "Iteration 43, loss = 0.55939709\n",
            "Iteration 44, loss = 0.55838133\n",
            "Iteration 45, loss = 0.55840355\n",
            "Iteration 46, loss = 0.55825531\n",
            "Iteration 47, loss = 0.55730703\n",
            "Iteration 48, loss = 0.55720840\n",
            "Iteration 49, loss = 0.55769021\n",
            "Iteration 50, loss = 0.55676785\n",
            "Iteration 51, loss = 0.55677093\n",
            "Iteration 52, loss = 0.55691448\n",
            "Iteration 53, loss = 0.55655949\n",
            "Iteration 54, loss = 0.55624880\n",
            "Iteration 55, loss = 0.55556748\n",
            "Iteration 56, loss = 0.55614596\n",
            "Iteration 57, loss = 0.55427161\n",
            "Iteration 58, loss = 0.55475850\n",
            "Iteration 59, loss = 0.55535152\n",
            "Iteration 60, loss = 0.55460453\n",
            "Iteration 61, loss = 0.55381415\n",
            "Iteration 62, loss = 0.55311476\n",
            "Iteration 63, loss = 0.55291060\n",
            "Iteration 64, loss = 0.55202893\n",
            "Iteration 65, loss = 0.55245744\n",
            "Iteration 66, loss = 0.55103905\n",
            "Iteration 67, loss = 0.55194185\n",
            "Iteration 68, loss = 0.55040805\n",
            "Iteration 69, loss = 0.54981994\n",
            "Iteration 70, loss = 0.54920352\n",
            "Iteration 71, loss = 0.54821749\n",
            "Iteration 72, loss = 0.54835333\n",
            "Iteration 73, loss = 0.54684704\n",
            "Iteration 74, loss = 0.54687827\n",
            "Iteration 75, loss = 0.54627714\n",
            "Iteration 76, loss = 0.54622418\n",
            "Iteration 77, loss = 0.54321418\n",
            "Iteration 78, loss = 0.54386921\n",
            "Iteration 79, loss = 0.54313158\n",
            "Iteration 80, loss = 0.54330403\n",
            "Iteration 81, loss = 0.54152991\n",
            "Iteration 82, loss = 0.54120763\n",
            "Iteration 83, loss = 0.54045877\n",
            "Iteration 84, loss = 0.53913957\n",
            "Iteration 85, loss = 0.53918458\n",
            "Iteration 86, loss = 0.53840061\n",
            "Iteration 87, loss = 0.53734377\n",
            "Iteration 88, loss = 0.53593159\n",
            "Iteration 89, loss = 0.53592481\n",
            "Iteration 90, loss = 0.53614949\n",
            "Iteration 91, loss = 0.53551156\n",
            "Iteration 92, loss = 0.53456681\n",
            "Iteration 93, loss = 0.53383003\n",
            "Iteration 94, loss = 0.53313328\n",
            "Iteration 95, loss = 0.53283742\n",
            "Iteration 96, loss = 0.53263242\n",
            "Iteration 97, loss = 0.53144530\n",
            "Iteration 98, loss = 0.53176323\n",
            "Iteration 99, loss = 0.53030394\n",
            "Iteration 100, loss = 0.53103763\n",
            "Iteration 101, loss = 0.53112858\n",
            "Iteration 102, loss = 0.53017521\n",
            "Iteration 103, loss = 0.52943801\n",
            "Iteration 104, loss = 0.52904705\n",
            "Iteration 105, loss = 0.52836189\n",
            "Iteration 106, loss = 0.52845897\n",
            "Iteration 107, loss = 0.52832862\n",
            "Iteration 108, loss = 0.52760697\n",
            "Iteration 109, loss = 0.52796393\n",
            "Iteration 110, loss = 0.52710096\n",
            "Iteration 111, loss = 0.52667986\n",
            "Iteration 112, loss = 0.52617678\n",
            "Iteration 113, loss = 0.52509331\n",
            "Iteration 114, loss = 0.52678937\n",
            "Iteration 115, loss = 0.52588565\n",
            "Iteration 116, loss = 0.52526294\n",
            "Iteration 117, loss = 0.52544109\n",
            "Iteration 118, loss = 0.52543801\n",
            "Iteration 119, loss = 0.52506747\n",
            "Iteration 120, loss = 0.52484359\n",
            "Iteration 121, loss = 0.52490957\n",
            "Iteration 122, loss = 0.52485356\n",
            "Iteration 123, loss = 0.52426623\n",
            "Iteration 124, loss = 0.52405937\n",
            "Iteration 125, loss = 0.52327220\n",
            "Iteration 126, loss = 0.52404703\n",
            "Iteration 127, loss = 0.52352260\n",
            "Iteration 128, loss = 0.52350215\n",
            "Iteration 129, loss = 0.52291327\n",
            "Iteration 130, loss = 0.52337476\n",
            "Iteration 131, loss = 0.52244576\n",
            "Iteration 132, loss = 0.52278109\n",
            "Iteration 133, loss = 0.52300062\n",
            "Iteration 134, loss = 0.52234807\n",
            "Iteration 135, loss = 0.52302105\n",
            "Iteration 136, loss = 0.52201005\n",
            "Iteration 137, loss = 0.52196747\n",
            "Iteration 138, loss = 0.52237249\n",
            "Iteration 139, loss = 0.52116926\n",
            "Iteration 140, loss = 0.52018913\n",
            "Iteration 141, loss = 0.52127256\n",
            "Iteration 142, loss = 0.52205320\n",
            "Iteration 143, loss = 0.52157467\n",
            "Iteration 144, loss = 0.52181176\n",
            "Iteration 145, loss = 0.52101357\n",
            "Iteration 146, loss = 0.52168996\n",
            "Iteration 147, loss = 0.52123170\n",
            "Iteration 148, loss = 0.52057207\n",
            "Iteration 149, loss = 0.52118722\n",
            "Iteration 150, loss = 0.52086302\n",
            "Iteration 151, loss = 0.52063804\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57080723\n",
            "Iteration 2, loss = 0.56226364\n",
            "Iteration 3, loss = 0.56308109\n",
            "Iteration 4, loss = 0.56269341\n",
            "Iteration 5, loss = 0.56263237\n",
            "Iteration 6, loss = 0.56236897\n",
            "Iteration 7, loss = 0.56282944\n",
            "Iteration 8, loss = 0.56223186\n",
            "Iteration 9, loss = 0.56270699\n",
            "Iteration 10, loss = 0.56212632\n",
            "Iteration 11, loss = 0.56278735\n",
            "Iteration 12, loss = 0.56171944\n",
            "Iteration 13, loss = 0.56154715\n",
            "Iteration 14, loss = 0.56190891\n",
            "Iteration 15, loss = 0.56201393\n",
            "Iteration 16, loss = 0.56240015\n",
            "Iteration 17, loss = 0.56193256\n",
            "Iteration 18, loss = 0.56161106\n",
            "Iteration 19, loss = 0.56181562\n",
            "Iteration 20, loss = 0.56219299\n",
            "Iteration 21, loss = 0.56141281\n",
            "Iteration 22, loss = 0.56229474\n",
            "Iteration 23, loss = 0.56217363\n",
            "Iteration 24, loss = 0.56160263\n",
            "Iteration 25, loss = 0.56126786\n",
            "Iteration 26, loss = 0.56209071\n",
            "Iteration 27, loss = 0.56180701\n",
            "Iteration 28, loss = 0.56136445\n",
            "Iteration 29, loss = 0.56156283\n",
            "Iteration 30, loss = 0.56153780\n",
            "Iteration 31, loss = 0.56146209\n",
            "Iteration 32, loss = 0.56154816\n",
            "Iteration 33, loss = 0.56130607\n",
            "Iteration 34, loss = 0.56071878\n",
            "Iteration 35, loss = 0.56103814\n",
            "Iteration 36, loss = 0.56021696\n",
            "Iteration 37, loss = 0.56153477\n",
            "Iteration 38, loss = 0.56083641\n",
            "Iteration 39, loss = 0.56078148\n",
            "Iteration 40, loss = 0.56047416\n",
            "Iteration 41, loss = 0.56040474\n",
            "Iteration 42, loss = 0.56026464\n",
            "Iteration 43, loss = 0.56041375\n",
            "Iteration 44, loss = 0.56048569\n",
            "Iteration 45, loss = 0.56033328\n",
            "Iteration 46, loss = 0.56084551\n",
            "Iteration 47, loss = 0.55936690\n",
            "Iteration 48, loss = 0.55964252\n",
            "Iteration 49, loss = 0.55974445\n",
            "Iteration 50, loss = 0.56028081\n",
            "Iteration 51, loss = 0.55979040\n",
            "Iteration 52, loss = 0.55971792\n",
            "Iteration 53, loss = 0.55996527\n",
            "Iteration 54, loss = 0.55868754\n",
            "Iteration 55, loss = 0.55897538\n",
            "Iteration 56, loss = 0.55942485\n",
            "Iteration 57, loss = 0.55924189\n",
            "Iteration 58, loss = 0.55821095\n",
            "Iteration 59, loss = 0.55863050\n",
            "Iteration 60, loss = 0.55856697\n",
            "Iteration 61, loss = 0.55815051\n",
            "Iteration 62, loss = 0.55817257\n",
            "Iteration 63, loss = 0.55777360\n",
            "Iteration 64, loss = 0.55764113\n",
            "Iteration 65, loss = 0.55793603\n",
            "Iteration 66, loss = 0.55682245\n",
            "Iteration 67, loss = 0.55693762\n",
            "Iteration 68, loss = 0.55667408\n",
            "Iteration 69, loss = 0.55621071\n",
            "Iteration 70, loss = 0.55647247\n",
            "Iteration 71, loss = 0.55579675\n",
            "Iteration 72, loss = 0.55523593\n",
            "Iteration 73, loss = 0.55482309\n",
            "Iteration 74, loss = 0.55410173\n",
            "Iteration 75, loss = 0.55368058\n",
            "Iteration 76, loss = 0.55352095\n",
            "Iteration 77, loss = 0.55408707\n",
            "Iteration 78, loss = 0.55404153\n",
            "Iteration 79, loss = 0.55268137\n",
            "Iteration 80, loss = 0.55298449\n",
            "Iteration 81, loss = 0.55194379\n",
            "Iteration 82, loss = 0.55192443\n",
            "Iteration 83, loss = 0.55095337\n",
            "Iteration 84, loss = 0.55051757\n",
            "Iteration 85, loss = 0.55048590\n",
            "Iteration 86, loss = 0.54955124\n",
            "Iteration 87, loss = 0.54923173\n",
            "Iteration 88, loss = 0.54810859\n",
            "Iteration 89, loss = 0.54835033\n",
            "Iteration 90, loss = 0.54706475\n",
            "Iteration 91, loss = 0.54720735\n",
            "Iteration 92, loss = 0.54592366\n",
            "Iteration 93, loss = 0.54579752\n",
            "Iteration 94, loss = 0.54500080\n",
            "Iteration 95, loss = 0.54472732\n",
            "Iteration 96, loss = 0.54407403\n",
            "Iteration 97, loss = 0.54378640\n",
            "Iteration 98, loss = 0.54201602\n",
            "Iteration 99, loss = 0.54222689\n",
            "Iteration 100, loss = 0.54211470\n",
            "Iteration 101, loss = 0.54038927\n",
            "Iteration 102, loss = 0.54073669\n",
            "Iteration 103, loss = 0.53899577\n",
            "Iteration 104, loss = 0.53958156\n",
            "Iteration 105, loss = 0.53966826\n",
            "Iteration 106, loss = 0.53698500\n",
            "Iteration 107, loss = 0.53802103\n",
            "Iteration 108, loss = 0.53707862\n",
            "Iteration 109, loss = 0.53672020\n",
            "Iteration 110, loss = 0.53567092\n",
            "Iteration 111, loss = 0.53564809\n",
            "Iteration 112, loss = 0.53476593\n",
            "Iteration 113, loss = 0.53450816\n",
            "Iteration 114, loss = 0.53406595\n",
            "Iteration 115, loss = 0.53402396\n",
            "Iteration 116, loss = 0.53365696\n",
            "Iteration 117, loss = 0.53319616\n",
            "Iteration 118, loss = 0.53222080\n",
            "Iteration 119, loss = 0.53271067\n",
            "Iteration 120, loss = 0.53227198\n",
            "Iteration 121, loss = 0.53289134\n",
            "Iteration 122, loss = 0.53161546\n",
            "Iteration 123, loss = 0.53135249\n",
            "Iteration 124, loss = 0.53127756\n",
            "Iteration 125, loss = 0.53005129\n",
            "Iteration 126, loss = 0.53111865\n",
            "Iteration 127, loss = 0.53097344\n",
            "Iteration 128, loss = 0.52987017\n",
            "Iteration 129, loss = 0.52951076\n",
            "Iteration 130, loss = 0.53056765\n",
            "Iteration 131, loss = 0.52933502\n",
            "Iteration 132, loss = 0.52811357\n",
            "Iteration 133, loss = 0.52896661\n",
            "Iteration 134, loss = 0.52885492\n",
            "Iteration 135, loss = 0.52813395\n",
            "Iteration 136, loss = 0.52821455\n",
            "Iteration 137, loss = 0.52839333\n",
            "Iteration 138, loss = 0.52752074\n",
            "Iteration 139, loss = 0.52732648\n",
            "Iteration 140, loss = 0.52667250\n",
            "Iteration 141, loss = 0.52678322\n",
            "Iteration 142, loss = 0.52725755\n",
            "Iteration 143, loss = 0.52710068\n",
            "Iteration 144, loss = 0.52542791\n",
            "Iteration 145, loss = 0.52612666\n",
            "Iteration 146, loss = 0.52687427\n",
            "Iteration 147, loss = 0.52681624\n",
            "Iteration 148, loss = 0.52567942\n",
            "Iteration 149, loss = 0.52651866\n",
            "Iteration 150, loss = 0.52559208\n",
            "Iteration 151, loss = 0.52549116\n",
            "Iteration 152, loss = 0.52591437\n",
            "Iteration 153, loss = 0.52561970\n",
            "Iteration 154, loss = 0.52421908\n",
            "Iteration 155, loss = 0.52476346\n",
            "Iteration 156, loss = 0.52521448\n",
            "Iteration 157, loss = 0.52446614\n",
            "Iteration 158, loss = 0.52481967\n",
            "Iteration 159, loss = 0.52476941\n",
            "Iteration 160, loss = 0.52417699\n",
            "Iteration 161, loss = 0.52387120\n",
            "Iteration 162, loss = 0.52367951\n",
            "Iteration 163, loss = 0.52415054\n",
            "Iteration 164, loss = 0.52391728\n",
            "Iteration 165, loss = 0.52319594\n",
            "Iteration 166, loss = 0.52441592\n",
            "Iteration 167, loss = 0.52402791\n",
            "Iteration 168, loss = 0.52410556\n",
            "Iteration 169, loss = 0.52327610\n",
            "Iteration 170, loss = 0.52376433\n",
            "Iteration 171, loss = 0.52369254\n",
            "Iteration 172, loss = 0.52262495\n",
            "Iteration 173, loss = 0.52377502\n",
            "Iteration 174, loss = 0.52381471\n",
            "Iteration 175, loss = 0.52277905\n",
            "Iteration 176, loss = 0.52333018\n",
            "Iteration 177, loss = 0.52245761\n",
            "Iteration 178, loss = 0.52293485\n",
            "Iteration 179, loss = 0.52188686\n",
            "Iteration 180, loss = 0.52403441\n",
            "Iteration 181, loss = 0.52337090\n",
            "Iteration 182, loss = 0.52293759\n",
            "Iteration 183, loss = 0.52284141\n",
            "Iteration 184, loss = 0.52265121\n",
            "Iteration 185, loss = 0.52187177\n",
            "Iteration 186, loss = 0.52239818\n",
            "Iteration 187, loss = 0.52223206\n",
            "Iteration 188, loss = 0.52243083\n",
            "Iteration 189, loss = 0.52210678\n",
            "Iteration 190, loss = 0.52254512\n",
            "Iteration 191, loss = 0.52241750\n",
            "Iteration 192, loss = 0.52177001\n",
            "Iteration 193, loss = 0.52267351\n",
            "Iteration 194, loss = 0.52157107\n",
            "Iteration 195, loss = 0.52241042\n",
            "Iteration 196, loss = 0.52149919\n",
            "Iteration 197, loss = 0.52066303\n",
            "Iteration 198, loss = 0.52143500\n",
            "Iteration 199, loss = 0.52179790\n",
            "Iteration 200, loss = 0.52156447\n",
            "Iteration 201, loss = 0.52144988\n",
            "Iteration 202, loss = 0.52233327\n",
            "Iteration 203, loss = 0.52231513\n",
            "Iteration 204, loss = 0.52194957\n",
            "Iteration 205, loss = 0.52126119\n",
            "Iteration 206, loss = 0.52173879\n",
            "Iteration 207, loss = 0.52172652\n",
            "Iteration 208, loss = 0.52111131\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56752100\n",
            "Iteration 2, loss = 0.56146637\n",
            "Iteration 3, loss = 0.56033824\n",
            "Iteration 4, loss = 0.56281947\n",
            "Iteration 5, loss = 0.56215966\n",
            "Iteration 6, loss = 0.56310237\n",
            "Iteration 7, loss = 0.56201229\n",
            "Iteration 8, loss = 0.56202944\n",
            "Iteration 9, loss = 0.56209595\n",
            "Iteration 10, loss = 0.56212665\n",
            "Iteration 11, loss = 0.56159827\n",
            "Iteration 12, loss = 0.56230529\n",
            "Iteration 13, loss = 0.56189520\n",
            "Iteration 14, loss = 0.56184674\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56154759\n",
            "Iteration 2, loss = 0.56019110\n",
            "Iteration 3, loss = 0.55972062\n",
            "Iteration 4, loss = 0.55978214\n",
            "Iteration 5, loss = 0.55944814\n",
            "Iteration 6, loss = 0.56031746\n",
            "Iteration 7, loss = 0.56023972\n",
            "Iteration 8, loss = 0.55984677\n",
            "Iteration 9, loss = 0.56010084\n",
            "Iteration 10, loss = 0.55977429\n",
            "Iteration 11, loss = 0.55843712\n",
            "Iteration 12, loss = 0.55960592\n",
            "Iteration 13, loss = 0.55980464\n",
            "Iteration 14, loss = 0.55919797\n",
            "Iteration 15, loss = 0.55920869\n",
            "Iteration 16, loss = 0.55886765\n",
            "Iteration 17, loss = 0.55815514\n",
            "Iteration 18, loss = 0.55974647\n",
            "Iteration 19, loss = 0.55865748\n",
            "Iteration 20, loss = 0.55874828\n",
            "Iteration 21, loss = 0.55927342\n",
            "Iteration 22, loss = 0.55834578\n",
            "Iteration 23, loss = 0.55858575\n",
            "Iteration 24, loss = 0.55884160\n",
            "Iteration 25, loss = 0.55879480\n",
            "Iteration 26, loss = 0.55790478\n",
            "Iteration 27, loss = 0.55781100\n",
            "Iteration 28, loss = 0.55847688\n",
            "Iteration 29, loss = 0.55843046\n",
            "Iteration 30, loss = 0.55798873\n",
            "Iteration 31, loss = 0.55708587\n",
            "Iteration 32, loss = 0.55792179\n",
            "Iteration 33, loss = 0.55785203\n",
            "Iteration 34, loss = 0.55780762\n",
            "Iteration 35, loss = 0.55793025\n",
            "Iteration 36, loss = 0.55689896\n",
            "Iteration 37, loss = 0.55777018\n",
            "Iteration 38, loss = 0.55751014\n",
            "Iteration 39, loss = 0.55651549\n",
            "Iteration 40, loss = 0.55679792\n",
            "Iteration 41, loss = 0.55720423\n",
            "Iteration 42, loss = 0.55660855\n",
            "Iteration 43, loss = 0.55666931\n",
            "Iteration 44, loss = 0.55665663\n",
            "Iteration 45, loss = 0.55550072\n",
            "Iteration 46, loss = 0.55550095\n",
            "Iteration 47, loss = 0.55535519\n",
            "Iteration 48, loss = 0.55511317\n",
            "Iteration 49, loss = 0.55514641\n",
            "Iteration 50, loss = 0.55506691\n",
            "Iteration 51, loss = 0.55431173\n",
            "Iteration 52, loss = 0.55453910\n",
            "Iteration 53, loss = 0.55429904\n",
            "Iteration 54, loss = 0.55396473\n",
            "Iteration 55, loss = 0.55399253\n",
            "Iteration 56, loss = 0.55357358\n",
            "Iteration 57, loss = 0.55286159\n",
            "Iteration 58, loss = 0.55286255\n",
            "Iteration 59, loss = 0.55229299\n",
            "Iteration 60, loss = 0.55178495\n",
            "Iteration 61, loss = 0.55161445\n",
            "Iteration 62, loss = 0.55137877\n",
            "Iteration 63, loss = 0.55130473\n",
            "Iteration 64, loss = 0.54988648\n",
            "Iteration 65, loss = 0.54973740\n",
            "Iteration 66, loss = 0.55023258\n",
            "Iteration 67, loss = 0.54936866\n",
            "Iteration 68, loss = 0.54866395\n",
            "Iteration 69, loss = 0.54813836\n",
            "Iteration 70, loss = 0.54661903\n",
            "Iteration 71, loss = 0.54713603\n",
            "Iteration 72, loss = 0.54632350\n",
            "Iteration 73, loss = 0.54573557\n",
            "Iteration 74, loss = 0.54409880\n",
            "Iteration 75, loss = 0.54409802\n",
            "Iteration 76, loss = 0.54402024\n",
            "Iteration 77, loss = 0.54250988\n",
            "Iteration 78, loss = 0.54212273\n",
            "Iteration 79, loss = 0.54194527\n",
            "Iteration 80, loss = 0.54052165\n",
            "Iteration 81, loss = 0.54013332\n",
            "Iteration 82, loss = 0.53911207\n",
            "Iteration 83, loss = 0.53905371\n",
            "Iteration 84, loss = 0.53842472\n",
            "Iteration 85, loss = 0.53723187\n",
            "Iteration 86, loss = 0.53704236\n",
            "Iteration 87, loss = 0.53600195\n",
            "Iteration 88, loss = 0.53577189\n",
            "Iteration 89, loss = 0.53566797\n",
            "Iteration 90, loss = 0.53455936\n",
            "Iteration 91, loss = 0.53408036\n",
            "Iteration 92, loss = 0.53363285\n",
            "Iteration 93, loss = 0.53140075\n",
            "Iteration 94, loss = 0.53239285\n",
            "Iteration 95, loss = 0.53186552\n",
            "Iteration 96, loss = 0.53191306\n",
            "Iteration 97, loss = 0.53040005\n",
            "Iteration 98, loss = 0.52997497\n",
            "Iteration 99, loss = 0.52979711\n",
            "Iteration 100, loss = 0.53071257\n",
            "Iteration 101, loss = 0.52854576\n",
            "Iteration 102, loss = 0.52907789\n",
            "Iteration 103, loss = 0.52814224\n",
            "Iteration 104, loss = 0.52743161\n",
            "Iteration 105, loss = 0.52775383\n",
            "Iteration 106, loss = 0.52683420\n",
            "Iteration 107, loss = 0.52695247\n",
            "Iteration 108, loss = 0.52696778\n",
            "Iteration 109, loss = 0.52591183\n",
            "Iteration 110, loss = 0.52503255\n",
            "Iteration 111, loss = 0.52531985\n",
            "Iteration 112, loss = 0.52510928\n",
            "Iteration 113, loss = 0.52407519\n",
            "Iteration 114, loss = 0.52447062\n",
            "Iteration 115, loss = 0.52559759\n",
            "Iteration 116, loss = 0.52441951\n",
            "Iteration 117, loss = 0.52402692\n",
            "Iteration 118, loss = 0.52443802\n",
            "Iteration 119, loss = 0.52377964\n",
            "Iteration 120, loss = 0.52340049\n",
            "Iteration 121, loss = 0.52368412\n",
            "Iteration 122, loss = 0.52335067\n",
            "Iteration 123, loss = 0.52285285\n",
            "Iteration 124, loss = 0.52217622\n",
            "Iteration 125, loss = 0.52302432\n",
            "Iteration 126, loss = 0.52288338\n",
            "Iteration 127, loss = 0.52269828\n",
            "Iteration 128, loss = 0.52182280\n",
            "Iteration 129, loss = 0.52208081\n",
            "Iteration 130, loss = 0.52155556\n",
            "Iteration 131, loss = 0.52169699\n",
            "Iteration 132, loss = 0.52128980\n",
            "Iteration 133, loss = 0.52137946\n",
            "Iteration 134, loss = 0.52135669\n",
            "Iteration 135, loss = 0.52029501\n",
            "Iteration 136, loss = 0.52056800\n",
            "Iteration 137, loss = 0.52041952\n",
            "Iteration 138, loss = 0.52129021\n",
            "Iteration 139, loss = 0.52034167\n",
            "Iteration 140, loss = 0.52078610\n",
            "Iteration 141, loss = 0.51935477\n",
            "Iteration 142, loss = 0.51992472\n",
            "Iteration 143, loss = 0.52132578\n",
            "Iteration 144, loss = 0.51987741\n",
            "Iteration 145, loss = 0.51995785\n",
            "Iteration 146, loss = 0.51953447\n",
            "Iteration 147, loss = 0.52000300\n",
            "Iteration 148, loss = 0.51968054\n",
            "Iteration 149, loss = 0.51912828\n",
            "Iteration 150, loss = 0.52056790\n",
            "Iteration 151, loss = 0.51967286\n",
            "Iteration 152, loss = 0.51918901\n",
            "Iteration 153, loss = 0.51960688\n",
            "Iteration 154, loss = 0.51863321\n",
            "Iteration 155, loss = 0.51854821\n",
            "Iteration 156, loss = 0.51867948\n",
            "Iteration 157, loss = 0.51858177\n",
            "Iteration 158, loss = 0.51806810\n",
            "Iteration 159, loss = 0.51803678\n",
            "Iteration 160, loss = 0.51893491\n",
            "Iteration 161, loss = 0.51867562\n",
            "Iteration 162, loss = 0.51878532\n",
            "Iteration 163, loss = 0.51821657\n",
            "Iteration 164, loss = 0.51811457\n",
            "Iteration 165, loss = 0.51834305\n",
            "Iteration 166, loss = 0.51823688\n",
            "Iteration 167, loss = 0.51889665\n",
            "Iteration 168, loss = 0.51938151\n",
            "Iteration 169, loss = 0.51788271\n",
            "Iteration 170, loss = 0.51733024\n",
            "Iteration 171, loss = 0.51781807\n",
            "Iteration 172, loss = 0.51728458\n",
            "Iteration 173, loss = 0.51795490\n",
            "Iteration 174, loss = 0.51798704\n",
            "Iteration 175, loss = 0.51817892\n",
            "Iteration 176, loss = 0.51704180\n",
            "Iteration 177, loss = 0.51800536\n",
            "Iteration 178, loss = 0.51711238\n",
            "Iteration 179, loss = 0.51807198\n",
            "Iteration 180, loss = 0.51733248\n",
            "Iteration 181, loss = 0.51816432\n",
            "Iteration 182, loss = 0.51713714\n",
            "Iteration 183, loss = 0.51805732\n",
            "Iteration 184, loss = 0.51720601\n",
            "Iteration 185, loss = 0.51694927\n",
            "Iteration 186, loss = 0.51765419\n",
            "Iteration 187, loss = 0.51769822\n",
            "Iteration 188, loss = 0.51679178\n",
            "Iteration 189, loss = 0.51767789\n",
            "Iteration 190, loss = 0.51658023\n",
            "Iteration 191, loss = 0.51760691\n",
            "Iteration 192, loss = 0.51703340\n",
            "Iteration 193, loss = 0.51665222\n",
            "Iteration 194, loss = 0.51699738\n",
            "Iteration 195, loss = 0.51774942\n",
            "Iteration 196, loss = 0.51653978\n",
            "Iteration 197, loss = 0.51716538\n",
            "Iteration 198, loss = 0.51663184\n",
            "Iteration 199, loss = 0.51630424\n",
            "Iteration 200, loss = 0.51700467\n",
            "Iteration 201, loss = 0.51671396\n",
            "Iteration 202, loss = 0.51712370\n",
            "Iteration 203, loss = 0.51750726\n",
            "Iteration 204, loss = 0.51706792\n",
            "Iteration 205, loss = 0.51700255\n",
            "Iteration 206, loss = 0.51688649\n",
            "Iteration 207, loss = 0.51717436\n",
            "Iteration 208, loss = 0.51743540\n",
            "Iteration 209, loss = 0.51662471\n",
            "Iteration 210, loss = 0.51609221\n",
            "Iteration 211, loss = 0.51610531\n",
            "Iteration 212, loss = 0.51602979\n",
            "Iteration 213, loss = 0.51697863\n",
            "Iteration 214, loss = 0.51609350\n",
            "Iteration 215, loss = 0.51695488\n",
            "Iteration 216, loss = 0.51747716\n",
            "Iteration 217, loss = 0.51587107\n",
            "Iteration 218, loss = 0.51626367\n",
            "Iteration 219, loss = 0.51723167\n",
            "Iteration 220, loss = 0.51700290\n",
            "Iteration 221, loss = 0.51653088\n",
            "Iteration 222, loss = 0.51578166\n",
            "Iteration 223, loss = 0.51583811\n",
            "Iteration 224, loss = 0.51656036\n",
            "Iteration 225, loss = 0.51674960\n",
            "Iteration 226, loss = 0.51677759\n",
            "Iteration 227, loss = 0.51597087\n",
            "Iteration 228, loss = 0.51679735\n",
            "Iteration 229, loss = 0.51681508\n",
            "Iteration 230, loss = 0.51636724\n",
            "Iteration 231, loss = 0.51566284\n",
            "Iteration 232, loss = 0.51650223\n",
            "Iteration 233, loss = 0.51773174\n",
            "Iteration 234, loss = 0.51721879\n",
            "Iteration 235, loss = 0.51636762\n",
            "Iteration 236, loss = 0.51704996\n",
            "Iteration 237, loss = 0.51636921\n",
            "Iteration 238, loss = 0.51546284\n",
            "Iteration 239, loss = 0.51637348\n",
            "Iteration 240, loss = 0.51620934\n",
            "Iteration 241, loss = 0.51614561\n",
            "Iteration 242, loss = 0.51714886\n",
            "Iteration 243, loss = 0.51617804\n",
            "Iteration 244, loss = 0.51689903\n",
            "Iteration 245, loss = 0.51595304\n",
            "Iteration 246, loss = 0.51561615\n",
            "Iteration 247, loss = 0.51631227\n",
            "Iteration 248, loss = 0.51502573\n",
            "Iteration 249, loss = 0.51617733\n",
            "Iteration 250, loss = 0.51664486\n",
            "Iteration 251, loss = 0.51576624\n",
            "Iteration 252, loss = 0.51596914\n",
            "Iteration 253, loss = 0.51689309\n",
            "Iteration 254, loss = 0.51707780\n",
            "Iteration 255, loss = 0.51689300\n",
            "Iteration 256, loss = 0.51617865\n",
            "Iteration 257, loss = 0.51597946\n",
            "Iteration 258, loss = 0.51591240\n",
            "Iteration 259, loss = 0.51602739\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56381190\n",
            "Iteration 2, loss = 0.56252605\n",
            "Iteration 3, loss = 0.56185398\n",
            "Iteration 4, loss = 0.56159427\n",
            "Iteration 5, loss = 0.56188055\n",
            "Iteration 6, loss = 0.56180480\n",
            "Iteration 7, loss = 0.56129749\n",
            "Iteration 8, loss = 0.56151552\n",
            "Iteration 9, loss = 0.56255856\n",
            "Iteration 10, loss = 0.56166796\n",
            "Iteration 11, loss = 0.56187318\n",
            "Iteration 12, loss = 0.56148481\n",
            "Iteration 13, loss = 0.56178716\n",
            "Iteration 14, loss = 0.56145286\n",
            "Iteration 15, loss = 0.56134151\n",
            "Iteration 16, loss = 0.56009893\n",
            "Iteration 17, loss = 0.56202713\n",
            "Iteration 18, loss = 0.56138703\n",
            "Iteration 19, loss = 0.56093259\n",
            "Iteration 20, loss = 0.56087571\n",
            "Iteration 21, loss = 0.56145842\n",
            "Iteration 22, loss = 0.56118706\n",
            "Iteration 23, loss = 0.56074335\n",
            "Iteration 24, loss = 0.56141692\n",
            "Iteration 25, loss = 0.56078623\n",
            "Iteration 26, loss = 0.56085491\n",
            "Iteration 27, loss = 0.56093508\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56992197\n",
            "Iteration 2, loss = 0.56728087\n",
            "Iteration 3, loss = 0.56716448\n",
            "Iteration 4, loss = 0.56590099\n",
            "Iteration 5, loss = 0.56680523\n",
            "Iteration 6, loss = 0.56606819\n",
            "Iteration 7, loss = 0.56593315\n",
            "Iteration 8, loss = 0.56657753\n",
            "Iteration 9, loss = 0.56623325\n",
            "Iteration 10, loss = 0.56589259\n",
            "Iteration 11, loss = 0.56608196\n",
            "Iteration 12, loss = 0.56679940\n",
            "Iteration 13, loss = 0.56577916\n",
            "Iteration 14, loss = 0.56641988\n",
            "Iteration 15, loss = 0.56576116\n",
            "Iteration 16, loss = 0.56566796\n",
            "Iteration 17, loss = 0.56529446\n",
            "Iteration 18, loss = 0.56617391\n",
            "Iteration 19, loss = 0.56592258\n",
            "Iteration 20, loss = 0.56555517\n",
            "Iteration 21, loss = 0.56528595\n",
            "Iteration 22, loss = 0.56587925\n",
            "Iteration 23, loss = 0.56548635\n",
            "Iteration 24, loss = 0.56528842\n",
            "Iteration 25, loss = 0.56571196\n",
            "Iteration 26, loss = 0.56541811\n",
            "Iteration 27, loss = 0.56506087\n",
            "Iteration 28, loss = 0.56431572\n",
            "Iteration 29, loss = 0.56509853\n",
            "Iteration 30, loss = 0.56479301\n",
            "Iteration 31, loss = 0.56524142\n",
            "Iteration 32, loss = 0.56554862\n",
            "Iteration 33, loss = 0.56472307\n",
            "Iteration 34, loss = 0.56452135\n",
            "Iteration 35, loss = 0.56512435\n",
            "Iteration 36, loss = 0.56436633\n",
            "Iteration 37, loss = 0.56481425\n",
            "Iteration 38, loss = 0.56428913\n",
            "Iteration 39, loss = 0.56469951\n",
            "Iteration 40, loss = 0.56414182\n",
            "Iteration 41, loss = 0.56398376\n",
            "Iteration 42, loss = 0.56413286\n",
            "Iteration 43, loss = 0.56350514\n",
            "Iteration 44, loss = 0.56378235\n",
            "Iteration 45, loss = 0.56270181\n",
            "Iteration 46, loss = 0.56346120\n",
            "Iteration 47, loss = 0.56311065\n",
            "Iteration 48, loss = 0.56332014\n",
            "Iteration 49, loss = 0.56346304\n",
            "Iteration 50, loss = 0.56304108\n",
            "Iteration 51, loss = 0.56241603\n",
            "Iteration 52, loss = 0.56270318\n",
            "Iteration 53, loss = 0.56229683\n",
            "Iteration 54, loss = 0.56319678\n",
            "Iteration 55, loss = 0.56193087\n",
            "Iteration 56, loss = 0.56169508\n",
            "Iteration 57, loss = 0.56216324\n",
            "Iteration 58, loss = 0.56137131\n",
            "Iteration 59, loss = 0.56146588\n",
            "Iteration 60, loss = 0.56134212\n",
            "Iteration 61, loss = 0.56069934\n",
            "Iteration 62, loss = 0.56002311\n",
            "Iteration 63, loss = 0.55961795\n",
            "Iteration 64, loss = 0.55952479\n",
            "Iteration 65, loss = 0.55947807\n",
            "Iteration 66, loss = 0.55904383\n",
            "Iteration 67, loss = 0.55882937\n",
            "Iteration 68, loss = 0.55833568\n",
            "Iteration 69, loss = 0.55767359\n",
            "Iteration 70, loss = 0.55765485\n",
            "Iteration 71, loss = 0.55664440\n",
            "Iteration 72, loss = 0.55670405\n",
            "Iteration 73, loss = 0.55619593\n",
            "Iteration 74, loss = 0.55581899\n",
            "Iteration 75, loss = 0.55478847\n",
            "Iteration 76, loss = 0.55431078\n",
            "Iteration 77, loss = 0.55456763\n",
            "Iteration 78, loss = 0.55268876\n",
            "Iteration 79, loss = 0.55255726\n",
            "Iteration 80, loss = 0.55206902\n",
            "Iteration 81, loss = 0.55119875\n",
            "Iteration 82, loss = 0.55042896\n",
            "Iteration 83, loss = 0.54890373\n",
            "Iteration 84, loss = 0.54932614\n",
            "Iteration 85, loss = 0.54860001\n",
            "Iteration 86, loss = 0.54757651\n",
            "Iteration 87, loss = 0.54644589\n",
            "Iteration 88, loss = 0.54654167\n",
            "Iteration 89, loss = 0.54512980\n",
            "Iteration 90, loss = 0.54431562\n",
            "Iteration 91, loss = 0.54371758\n",
            "Iteration 92, loss = 0.54259041\n",
            "Iteration 93, loss = 0.54250317\n",
            "Iteration 94, loss = 0.54037671\n",
            "Iteration 95, loss = 0.54046392\n",
            "Iteration 96, loss = 0.53910142\n",
            "Iteration 97, loss = 0.53875318\n",
            "Iteration 98, loss = 0.53813610\n",
            "Iteration 99, loss = 0.53736508\n",
            "Iteration 100, loss = 0.53614597\n",
            "Iteration 101, loss = 0.53608296\n",
            "Iteration 102, loss = 0.53570646\n",
            "Iteration 103, loss = 0.53495110\n",
            "Iteration 104, loss = 0.53468572\n",
            "Iteration 105, loss = 0.53321320\n",
            "Iteration 106, loss = 0.53274895\n",
            "Iteration 107, loss = 0.53231245\n",
            "Iteration 108, loss = 0.53283371\n",
            "Iteration 109, loss = 0.53207729\n",
            "Iteration 110, loss = 0.53021071\n",
            "Iteration 111, loss = 0.53143969\n",
            "Iteration 112, loss = 0.52853488\n",
            "Iteration 113, loss = 0.53014205\n",
            "Iteration 114, loss = 0.52962348\n",
            "Iteration 115, loss = 0.52964069\n",
            "Iteration 116, loss = 0.52953478\n",
            "Iteration 117, loss = 0.52934360\n",
            "Iteration 118, loss = 0.52903349\n",
            "Iteration 119, loss = 0.52822003\n",
            "Iteration 120, loss = 0.52798869\n",
            "Iteration 121, loss = 0.52770387\n",
            "Iteration 122, loss = 0.52707823\n",
            "Iteration 123, loss = 0.52811788\n",
            "Iteration 124, loss = 0.52771019\n",
            "Iteration 125, loss = 0.52683999\n",
            "Iteration 126, loss = 0.52667435\n",
            "Iteration 127, loss = 0.52580444\n",
            "Iteration 128, loss = 0.52550336\n",
            "Iteration 129, loss = 0.52634225\n",
            "Iteration 130, loss = 0.52628448\n",
            "Iteration 131, loss = 0.52601404\n",
            "Iteration 132, loss = 0.52574476\n",
            "Iteration 133, loss = 0.52527775\n",
            "Iteration 134, loss = 0.52577637\n",
            "Iteration 135, loss = 0.52584631\n",
            "Iteration 136, loss = 0.52503439\n",
            "Iteration 137, loss = 0.52468297\n",
            "Iteration 138, loss = 0.52473347\n",
            "Iteration 139, loss = 0.52483707\n",
            "Iteration 140, loss = 0.52317514\n",
            "Iteration 141, loss = 0.52480833\n",
            "Iteration 142, loss = 0.52441398\n",
            "Iteration 143, loss = 0.52324669\n",
            "Iteration 144, loss = 0.52308014\n",
            "Iteration 145, loss = 0.52432298\n",
            "Iteration 146, loss = 0.52281350\n",
            "Iteration 147, loss = 0.52463064\n",
            "Iteration 148, loss = 0.52209497\n",
            "Iteration 149, loss = 0.52353762\n",
            "Iteration 150, loss = 0.52304775\n",
            "Iteration 151, loss = 0.52381706\n",
            "Iteration 152, loss = 0.52336888\n",
            "Iteration 153, loss = 0.52304836\n",
            "Iteration 154, loss = 0.52237223\n",
            "Iteration 155, loss = 0.52312780\n",
            "Iteration 156, loss = 0.52240432\n",
            "Iteration 157, loss = 0.52291983\n",
            "Iteration 158, loss = 0.52207589\n",
            "Iteration 159, loss = 0.52181786\n",
            "Iteration 160, loss = 0.52191002\n",
            "Iteration 161, loss = 0.52241729\n",
            "Iteration 162, loss = 0.52189193\n",
            "Iteration 163, loss = 0.52137608\n",
            "Iteration 164, loss = 0.52272900\n",
            "Iteration 165, loss = 0.52180127\n",
            "Iteration 166, loss = 0.52129067\n",
            "Iteration 167, loss = 0.52244794\n",
            "Iteration 168, loss = 0.52165798\n",
            "Iteration 169, loss = 0.52227600\n",
            "Iteration 170, loss = 0.52171852\n",
            "Iteration 171, loss = 0.52145381\n",
            "Iteration 172, loss = 0.52191299\n",
            "Iteration 173, loss = 0.52154887\n",
            "Iteration 174, loss = 0.52037010\n",
            "Iteration 175, loss = 0.52098431\n",
            "Iteration 176, loss = 0.52170565\n",
            "Iteration 177, loss = 0.52088776\n",
            "Iteration 178, loss = 0.52173686\n",
            "Iteration 179, loss = 0.52073768\n",
            "Iteration 180, loss = 0.52128201\n",
            "Iteration 181, loss = 0.52075207\n",
            "Iteration 182, loss = 0.52114853\n",
            "Iteration 183, loss = 0.52129368\n",
            "Iteration 184, loss = 0.52067795\n",
            "Iteration 185, loss = 0.52098206\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57042144\n",
            "Iteration 2, loss = 0.56523720\n",
            "Iteration 3, loss = 0.56524064\n",
            "Iteration 4, loss = 0.56557641\n",
            "Iteration 5, loss = 0.56585853\n",
            "Iteration 6, loss = 0.56577234\n",
            "Iteration 7, loss = 0.56558110\n",
            "Iteration 8, loss = 0.56531078\n",
            "Iteration 9, loss = 0.56533730\n",
            "Iteration 10, loss = 0.56460993\n",
            "Iteration 11, loss = 0.56481801\n",
            "Iteration 12, loss = 0.56527125\n",
            "Iteration 13, loss = 0.56584529\n",
            "Iteration 14, loss = 0.56497020\n",
            "Iteration 15, loss = 0.56505000\n",
            "Iteration 16, loss = 0.56387927\n",
            "Iteration 17, loss = 0.56396213\n",
            "Iteration 18, loss = 0.56537904\n",
            "Iteration 19, loss = 0.56555576\n",
            "Iteration 20, loss = 0.56497064\n",
            "Iteration 21, loss = 0.56398384\n",
            "Iteration 22, loss = 0.56499041\n",
            "Iteration 23, loss = 0.56485353\n",
            "Iteration 24, loss = 0.56408747\n",
            "Iteration 25, loss = 0.56415898\n",
            "Iteration 26, loss = 0.56385206\n",
            "Iteration 27, loss = 0.56500836\n",
            "Iteration 28, loss = 0.56430714\n",
            "Iteration 29, loss = 0.56393166\n",
            "Iteration 30, loss = 0.56409090\n",
            "Iteration 31, loss = 0.56414931\n",
            "Iteration 32, loss = 0.56407677\n",
            "Iteration 33, loss = 0.56349481\n",
            "Iteration 34, loss = 0.56384865\n",
            "Iteration 35, loss = 0.56366290\n",
            "Iteration 36, loss = 0.56394817\n",
            "Iteration 37, loss = 0.56387458\n",
            "Iteration 38, loss = 0.56360946\n",
            "Iteration 39, loss = 0.56286421\n",
            "Iteration 40, loss = 0.56201294\n",
            "Iteration 41, loss = 0.56297833\n",
            "Iteration 42, loss = 0.56239092\n",
            "Iteration 43, loss = 0.56205617\n",
            "Iteration 44, loss = 0.56201653\n",
            "Iteration 45, loss = 0.56271535\n",
            "Iteration 46, loss = 0.56181780\n",
            "Iteration 47, loss = 0.56205368\n",
            "Iteration 48, loss = 0.56142145\n",
            "Iteration 49, loss = 0.56173308\n",
            "Iteration 50, loss = 0.56145289\n",
            "Iteration 51, loss = 0.56066984\n",
            "Iteration 52, loss = 0.56089657\n",
            "Iteration 53, loss = 0.56113142\n",
            "Iteration 54, loss = 0.56127733\n",
            "Iteration 55, loss = 0.56081754\n",
            "Iteration 56, loss = 0.56002552\n",
            "Iteration 57, loss = 0.56022949\n",
            "Iteration 58, loss = 0.56042930\n",
            "Iteration 59, loss = 0.55946558\n",
            "Iteration 60, loss = 0.55934355\n",
            "Iteration 61, loss = 0.55921108\n",
            "Iteration 62, loss = 0.55870202\n",
            "Iteration 63, loss = 0.55803328\n",
            "Iteration 64, loss = 0.55810751\n",
            "Iteration 65, loss = 0.55763807\n",
            "Iteration 66, loss = 0.55648831\n",
            "Iteration 67, loss = 0.55536385\n",
            "Iteration 68, loss = 0.55629448\n",
            "Iteration 69, loss = 0.55595499\n",
            "Iteration 70, loss = 0.55499154\n",
            "Iteration 71, loss = 0.55504205\n",
            "Iteration 72, loss = 0.55471087\n",
            "Iteration 73, loss = 0.55390278\n",
            "Iteration 74, loss = 0.55361533\n",
            "Iteration 75, loss = 0.55241826\n",
            "Iteration 76, loss = 0.55196103\n",
            "Iteration 77, loss = 0.55141769\n",
            "Iteration 78, loss = 0.55128921\n",
            "Iteration 79, loss = 0.55044638\n",
            "Iteration 80, loss = 0.54982233\n",
            "Iteration 81, loss = 0.54928333\n",
            "Iteration 82, loss = 0.54901250\n",
            "Iteration 83, loss = 0.54726359\n",
            "Iteration 84, loss = 0.54732625\n",
            "Iteration 85, loss = 0.54685525\n",
            "Iteration 86, loss = 0.54598753\n",
            "Iteration 87, loss = 0.54596812\n",
            "Iteration 88, loss = 0.54483346\n",
            "Iteration 89, loss = 0.54409890\n",
            "Iteration 90, loss = 0.54354163\n",
            "Iteration 91, loss = 0.54367679\n",
            "Iteration 92, loss = 0.54220770\n",
            "Iteration 93, loss = 0.54193118\n",
            "Iteration 94, loss = 0.54166386\n",
            "Iteration 95, loss = 0.54033479\n",
            "Iteration 96, loss = 0.54013125\n",
            "Iteration 97, loss = 0.54005740\n",
            "Iteration 98, loss = 0.54030128\n",
            "Iteration 99, loss = 0.53821123\n",
            "Iteration 100, loss = 0.53815588\n",
            "Iteration 101, loss = 0.53787318\n",
            "Iteration 102, loss = 0.53744819\n",
            "Iteration 103, loss = 0.53693649\n",
            "Iteration 104, loss = 0.53680814\n",
            "Iteration 105, loss = 0.53630640\n",
            "Iteration 106, loss = 0.53489133\n",
            "Iteration 107, loss = 0.53439906\n",
            "Iteration 108, loss = 0.53607443\n",
            "Iteration 109, loss = 0.53474050\n",
            "Iteration 110, loss = 0.53459075\n",
            "Iteration 111, loss = 0.53407771\n",
            "Iteration 112, loss = 0.53392254\n",
            "Iteration 113, loss = 0.53356533\n",
            "Iteration 114, loss = 0.53312521\n",
            "Iteration 115, loss = 0.53349356\n",
            "Iteration 116, loss = 0.53394641\n",
            "Iteration 117, loss = 0.53174262\n",
            "Iteration 118, loss = 0.53391635\n",
            "Iteration 119, loss = 0.53175687\n",
            "Iteration 120, loss = 0.53155360\n",
            "Iteration 121, loss = 0.53119275\n",
            "Iteration 122, loss = 0.53137093\n",
            "Iteration 123, loss = 0.53119919\n",
            "Iteration 124, loss = 0.53103535\n",
            "Iteration 125, loss = 0.53099854\n",
            "Iteration 126, loss = 0.53063890\n",
            "Iteration 127, loss = 0.53095710\n",
            "Iteration 128, loss = 0.52978885\n",
            "Iteration 129, loss = 0.53121719\n",
            "Iteration 130, loss = 0.52980770\n",
            "Iteration 131, loss = 0.53009233\n",
            "Iteration 132, loss = 0.52871074\n",
            "Iteration 133, loss = 0.52951543\n",
            "Iteration 134, loss = 0.52890396\n",
            "Iteration 135, loss = 0.52966577\n",
            "Iteration 136, loss = 0.52963694\n",
            "Iteration 137, loss = 0.52941040\n",
            "Iteration 138, loss = 0.52918659\n",
            "Iteration 139, loss = 0.52868088\n",
            "Iteration 140, loss = 0.52833047\n",
            "Iteration 141, loss = 0.52865174\n",
            "Iteration 142, loss = 0.52828925\n",
            "Iteration 143, loss = 0.52875929\n",
            "Iteration 144, loss = 0.52749717\n",
            "Iteration 145, loss = 0.52872613\n",
            "Iteration 146, loss = 0.52817173\n",
            "Iteration 147, loss = 0.52727140\n",
            "Iteration 148, loss = 0.52732361\n",
            "Iteration 149, loss = 0.52782457\n",
            "Iteration 150, loss = 0.52802131\n",
            "Iteration 151, loss = 0.52735382\n",
            "Iteration 152, loss = 0.52708193\n",
            "Iteration 153, loss = 0.52781363\n",
            "Iteration 154, loss = 0.52823745\n",
            "Iteration 155, loss = 0.52698318\n",
            "Iteration 156, loss = 0.52720666\n",
            "Iteration 157, loss = 0.52699489\n",
            "Iteration 158, loss = 0.52692064\n",
            "Iteration 159, loss = 0.52657182\n",
            "Iteration 160, loss = 0.52644245\n",
            "Iteration 161, loss = 0.52650105\n",
            "Iteration 162, loss = 0.52691637\n",
            "Iteration 163, loss = 0.52610787\n",
            "Iteration 164, loss = 0.52637167\n",
            "Iteration 165, loss = 0.52680296\n",
            "Iteration 166, loss = 0.52650564\n",
            "Iteration 167, loss = 0.52570009\n",
            "Iteration 168, loss = 0.52653368\n",
            "Iteration 169, loss = 0.52545029\n",
            "Iteration 170, loss = 0.52748190\n",
            "Iteration 171, loss = 0.52591437\n",
            "Iteration 172, loss = 0.52612718\n",
            "Iteration 173, loss = 0.52520900\n",
            "Iteration 174, loss = 0.52546728\n",
            "Iteration 175, loss = 0.52465361\n",
            "Iteration 176, loss = 0.52601013\n",
            "Iteration 177, loss = 0.52566945\n",
            "Iteration 178, loss = 0.52630673\n",
            "Iteration 179, loss = 0.52598230\n",
            "Iteration 180, loss = 0.52529607\n",
            "Iteration 181, loss = 0.52556870\n",
            "Iteration 182, loss = 0.52582821\n",
            "Iteration 183, loss = 0.52469407\n",
            "Iteration 184, loss = 0.52577548\n",
            "Iteration 185, loss = 0.52520569\n",
            "Iteration 186, loss = 0.52444545\n",
            "Iteration 187, loss = 0.52483644\n",
            "Iteration 188, loss = 0.52565654\n",
            "Iteration 189, loss = 0.52555163\n",
            "Iteration 190, loss = 0.52454493\n",
            "Iteration 191, loss = 0.52524325\n",
            "Iteration 192, loss = 0.52540341\n",
            "Iteration 193, loss = 0.52468713\n",
            "Iteration 194, loss = 0.52606522\n",
            "Iteration 195, loss = 0.52610842\n",
            "Iteration 196, loss = 0.52508929\n",
            "Iteration 197, loss = 0.52586407\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "9\n",
            "Iteration 1, loss = 0.56643225\n",
            "Iteration 2, loss = 0.56010821\n",
            "Iteration 3, loss = 0.55908249\n",
            "Iteration 4, loss = 0.55951575\n",
            "Iteration 5, loss = 0.55881458\n",
            "Iteration 6, loss = 0.55938224\n",
            "Iteration 7, loss = 0.55981453\n",
            "Iteration 8, loss = 0.55975390\n",
            "Iteration 9, loss = 0.55970474\n",
            "Iteration 10, loss = 0.55956191\n",
            "Iteration 11, loss = 0.55926111\n",
            "Iteration 12, loss = 0.55966334\n",
            "Iteration 13, loss = 0.55921887\n",
            "Iteration 14, loss = 0.55959435\n",
            "Iteration 15, loss = 0.55953498\n",
            "Iteration 16, loss = 0.55872924\n",
            "Iteration 17, loss = 0.55908701\n",
            "Iteration 18, loss = 0.55859808\n",
            "Iteration 19, loss = 0.55926992\n",
            "Iteration 20, loss = 0.55948111\n",
            "Iteration 21, loss = 0.55844438\n",
            "Iteration 22, loss = 0.55789091\n",
            "Iteration 23, loss = 0.55887765\n",
            "Iteration 24, loss = 0.55938962\n",
            "Iteration 25, loss = 0.55898848\n",
            "Iteration 26, loss = 0.55807891\n",
            "Iteration 27, loss = 0.55849535\n",
            "Iteration 28, loss = 0.55876765\n",
            "Iteration 29, loss = 0.55862291\n",
            "Iteration 30, loss = 0.55831324\n",
            "Iteration 31, loss = 0.55871168\n",
            "Iteration 32, loss = 0.55801993\n",
            "Iteration 33, loss = 0.55787702\n",
            "Iteration 34, loss = 0.55821396\n",
            "Iteration 35, loss = 0.55875433\n",
            "Iteration 36, loss = 0.55727843\n",
            "Iteration 37, loss = 0.55835451\n",
            "Iteration 38, loss = 0.55778891\n",
            "Iteration 39, loss = 0.55799236\n",
            "Iteration 40, loss = 0.55752907\n",
            "Iteration 41, loss = 0.55702901\n",
            "Iteration 42, loss = 0.55795672\n",
            "Iteration 43, loss = 0.55659444\n",
            "Iteration 44, loss = 0.55715140\n",
            "Iteration 45, loss = 0.55714528\n",
            "Iteration 46, loss = 0.55682525\n",
            "Iteration 47, loss = 0.55731090\n",
            "Iteration 48, loss = 0.55701717\n",
            "Iteration 49, loss = 0.55552560\n",
            "Iteration 50, loss = 0.55666287\n",
            "Iteration 51, loss = 0.55699079\n",
            "Iteration 52, loss = 0.55615054\n",
            "Iteration 53, loss = 0.55575112\n",
            "Iteration 54, loss = 0.55622515\n",
            "Iteration 55, loss = 0.55555048\n",
            "Iteration 56, loss = 0.55432542\n",
            "Iteration 57, loss = 0.55519450\n",
            "Iteration 58, loss = 0.55489325\n",
            "Iteration 59, loss = 0.55485149\n",
            "Iteration 60, loss = 0.55510805\n",
            "Iteration 61, loss = 0.55315260\n",
            "Iteration 62, loss = 0.55391756\n",
            "Iteration 63, loss = 0.55342850\n",
            "Iteration 64, loss = 0.55397325\n",
            "Iteration 65, loss = 0.55253550\n",
            "Iteration 66, loss = 0.55228227\n",
            "Iteration 67, loss = 0.55200606\n",
            "Iteration 68, loss = 0.55164245\n",
            "Iteration 69, loss = 0.55153986\n",
            "Iteration 70, loss = 0.55168686\n",
            "Iteration 71, loss = 0.55068827\n",
            "Iteration 72, loss = 0.54985626\n",
            "Iteration 73, loss = 0.54974360\n",
            "Iteration 74, loss = 0.54926814\n",
            "Iteration 75, loss = 0.54898441\n",
            "Iteration 76, loss = 0.54807645\n",
            "Iteration 77, loss = 0.54846464\n",
            "Iteration 78, loss = 0.54657952\n",
            "Iteration 79, loss = 0.54660068\n",
            "Iteration 80, loss = 0.54656679\n",
            "Iteration 81, loss = 0.54580273\n",
            "Iteration 82, loss = 0.54487831\n",
            "Iteration 83, loss = 0.54409779\n",
            "Iteration 84, loss = 0.54398524\n",
            "Iteration 85, loss = 0.54282568\n",
            "Iteration 86, loss = 0.54301708\n",
            "Iteration 87, loss = 0.54199666\n",
            "Iteration 88, loss = 0.54022611\n",
            "Iteration 89, loss = 0.54015995\n",
            "Iteration 90, loss = 0.53982503\n",
            "Iteration 91, loss = 0.53942144\n",
            "Iteration 92, loss = 0.53827071\n",
            "Iteration 93, loss = 0.53729788\n",
            "Iteration 94, loss = 0.53668565\n",
            "Iteration 95, loss = 0.53579087\n",
            "Iteration 96, loss = 0.53557735\n",
            "Iteration 97, loss = 0.53489930\n",
            "Iteration 98, loss = 0.53453395\n",
            "Iteration 99, loss = 0.53367207\n",
            "Iteration 100, loss = 0.53271612\n",
            "Iteration 101, loss = 0.53350164\n",
            "Iteration 102, loss = 0.53214456\n",
            "Iteration 103, loss = 0.53216117\n",
            "Iteration 104, loss = 0.53136902\n",
            "Iteration 105, loss = 0.53037275\n",
            "Iteration 106, loss = 0.53065912\n",
            "Iteration 107, loss = 0.52936355\n",
            "Iteration 108, loss = 0.53053615\n",
            "Iteration 109, loss = 0.52948295\n",
            "Iteration 110, loss = 0.52818982\n",
            "Iteration 111, loss = 0.52839379\n",
            "Iteration 112, loss = 0.52787089\n",
            "Iteration 113, loss = 0.52782445\n",
            "Iteration 114, loss = 0.52683830\n",
            "Iteration 115, loss = 0.52629431\n",
            "Iteration 116, loss = 0.52698820\n",
            "Iteration 117, loss = 0.52568628\n",
            "Iteration 118, loss = 0.52656599\n",
            "Iteration 119, loss = 0.52610585\n",
            "Iteration 120, loss = 0.52597321\n",
            "Iteration 121, loss = 0.52500266\n",
            "Iteration 122, loss = 0.52532794\n",
            "Iteration 123, loss = 0.52502154\n",
            "Iteration 124, loss = 0.52549902\n",
            "Iteration 125, loss = 0.52432351\n",
            "Iteration 126, loss = 0.52430254\n",
            "Iteration 127, loss = 0.52410300\n",
            "Iteration 128, loss = 0.52444203\n",
            "Iteration 129, loss = 0.52492003\n",
            "Iteration 130, loss = 0.52382416\n",
            "Iteration 131, loss = 0.52419813\n",
            "Iteration 132, loss = 0.52295288\n",
            "Iteration 133, loss = 0.52315413\n",
            "Iteration 134, loss = 0.52338450\n",
            "Iteration 135, loss = 0.52171415\n",
            "Iteration 136, loss = 0.52345906\n",
            "Iteration 137, loss = 0.52262527\n",
            "Iteration 138, loss = 0.52209457\n",
            "Iteration 139, loss = 0.52325675\n",
            "Iteration 140, loss = 0.52187981\n",
            "Iteration 141, loss = 0.52283720\n",
            "Iteration 142, loss = 0.52253845\n",
            "Iteration 143, loss = 0.52184193\n",
            "Iteration 144, loss = 0.52182522\n",
            "Iteration 145, loss = 0.52161840\n",
            "Iteration 146, loss = 0.52161368\n",
            "Iteration 147, loss = 0.52151945\n",
            "Iteration 148, loss = 0.52076289\n",
            "Iteration 149, loss = 0.52200846\n",
            "Iteration 150, loss = 0.52149583\n",
            "Iteration 151, loss = 0.52080218\n",
            "Iteration 152, loss = 0.52046248\n",
            "Iteration 153, loss = 0.52156203\n",
            "Iteration 154, loss = 0.52120228\n",
            "Iteration 155, loss = 0.52097826\n",
            "Iteration 156, loss = 0.52077204\n",
            "Iteration 157, loss = 0.52002104\n",
            "Iteration 158, loss = 0.52149306\n",
            "Iteration 159, loss = 0.52010383\n",
            "Iteration 160, loss = 0.52026306\n",
            "Iteration 161, loss = 0.52051398\n",
            "Iteration 162, loss = 0.52023685\n",
            "Iteration 163, loss = 0.52070986\n",
            "Iteration 164, loss = 0.52050356\n",
            "Iteration 165, loss = 0.52048008\n",
            "Iteration 166, loss = 0.52015088\n",
            "Iteration 167, loss = 0.51991519\n",
            "Iteration 168, loss = 0.51963354\n",
            "Iteration 169, loss = 0.51944413\n",
            "Iteration 170, loss = 0.51945888\n",
            "Iteration 171, loss = 0.51993910\n",
            "Iteration 172, loss = 0.52039751\n",
            "Iteration 173, loss = 0.51921903\n",
            "Iteration 174, loss = 0.52099751\n",
            "Iteration 175, loss = 0.52031941\n",
            "Iteration 176, loss = 0.51965169\n",
            "Iteration 177, loss = 0.52024520\n",
            "Iteration 178, loss = 0.51928978\n",
            "Iteration 179, loss = 0.51934301\n",
            "Iteration 180, loss = 0.51976806\n",
            "Iteration 181, loss = 0.51964155\n",
            "Iteration 182, loss = 0.51992981\n",
            "Iteration 183, loss = 0.51894719\n",
            "Iteration 184, loss = 0.51806705\n",
            "Iteration 185, loss = 0.51942268\n",
            "Iteration 186, loss = 0.51890317\n",
            "Iteration 187, loss = 0.51854734\n",
            "Iteration 188, loss = 0.51927768\n",
            "Iteration 189, loss = 0.51995773\n",
            "Iteration 190, loss = 0.51848939\n",
            "Iteration 191, loss = 0.51830814\n",
            "Iteration 192, loss = 0.51884168\n",
            "Iteration 193, loss = 0.51938653\n",
            "Iteration 194, loss = 0.51854727\n",
            "Iteration 195, loss = 0.51938421\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56879508\n",
            "Iteration 2, loss = 0.56427885\n",
            "Iteration 3, loss = 0.56463230\n",
            "Iteration 4, loss = 0.56399425\n",
            "Iteration 5, loss = 0.56290845\n",
            "Iteration 6, loss = 0.56413971\n",
            "Iteration 7, loss = 0.56485154\n",
            "Iteration 8, loss = 0.56364645\n",
            "Iteration 9, loss = 0.56454244\n",
            "Iteration 10, loss = 0.56393392\n",
            "Iteration 11, loss = 0.56416455\n",
            "Iteration 12, loss = 0.56428822\n",
            "Iteration 13, loss = 0.56427255\n",
            "Iteration 14, loss = 0.56398141\n",
            "Iteration 15, loss = 0.56382706\n",
            "Iteration 16, loss = 0.56357600\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56427460\n",
            "Iteration 2, loss = 0.56310132\n",
            "Iteration 3, loss = 0.56276638\n",
            "Iteration 4, loss = 0.56300316\n",
            "Iteration 5, loss = 0.56259050\n",
            "Iteration 6, loss = 0.56220913\n",
            "Iteration 7, loss = 0.56290167\n",
            "Iteration 8, loss = 0.56326437\n",
            "Iteration 9, loss = 0.56306804\n",
            "Iteration 10, loss = 0.56328943\n",
            "Iteration 11, loss = 0.56279179\n",
            "Iteration 12, loss = 0.56236805\n",
            "Iteration 13, loss = 0.56230453\n",
            "Iteration 14, loss = 0.56273400\n",
            "Iteration 15, loss = 0.56216436\n",
            "Iteration 16, loss = 0.56261058\n",
            "Iteration 17, loss = 0.56177836\n",
            "Iteration 18, loss = 0.56119074\n",
            "Iteration 19, loss = 0.56266994\n",
            "Iteration 20, loss = 0.56183583\n",
            "Iteration 21, loss = 0.56176759\n",
            "Iteration 22, loss = 0.56219233\n",
            "Iteration 23, loss = 0.56199512\n",
            "Iteration 24, loss = 0.56227685\n",
            "Iteration 25, loss = 0.56205381\n",
            "Iteration 26, loss = 0.56189602\n",
            "Iteration 27, loss = 0.56249548\n",
            "Iteration 28, loss = 0.56219017\n",
            "Iteration 29, loss = 0.56147158\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57018105\n",
            "Iteration 2, loss = 0.56601607\n",
            "Iteration 3, loss = 0.56557153\n",
            "Iteration 4, loss = 0.56493564\n",
            "Iteration 5, loss = 0.56505571\n",
            "Iteration 6, loss = 0.56479373\n",
            "Iteration 7, loss = 0.56510820\n",
            "Iteration 8, loss = 0.56535894\n",
            "Iteration 9, loss = 0.56583825\n",
            "Iteration 10, loss = 0.56506330\n",
            "Iteration 11, loss = 0.56499960\n",
            "Iteration 12, loss = 0.56499851\n",
            "Iteration 13, loss = 0.56490203\n",
            "Iteration 14, loss = 0.56482233\n",
            "Iteration 15, loss = 0.56490459\n",
            "Iteration 16, loss = 0.56447825\n",
            "Iteration 17, loss = 0.56507136\n",
            "Iteration 18, loss = 0.56554640\n",
            "Iteration 19, loss = 0.56431748\n",
            "Iteration 20, loss = 0.56426277\n",
            "Iteration 21, loss = 0.56357520\n",
            "Iteration 22, loss = 0.56404716\n",
            "Iteration 23, loss = 0.56446171\n",
            "Iteration 24, loss = 0.56403778\n",
            "Iteration 25, loss = 0.56414502\n",
            "Iteration 26, loss = 0.56349088\n",
            "Iteration 27, loss = 0.56499900\n",
            "Iteration 28, loss = 0.56381524\n",
            "Iteration 29, loss = 0.56309262\n",
            "Iteration 30, loss = 0.56361286\n",
            "Iteration 31, loss = 0.56430263\n",
            "Iteration 32, loss = 0.56366268\n",
            "Iteration 33, loss = 0.56261342\n",
            "Iteration 34, loss = 0.56340166\n",
            "Iteration 35, loss = 0.56247112\n",
            "Iteration 36, loss = 0.56346190\n",
            "Iteration 37, loss = 0.56180542\n",
            "Iteration 38, loss = 0.56245360\n",
            "Iteration 39, loss = 0.56270906\n",
            "Iteration 40, loss = 0.56204022\n",
            "Iteration 41, loss = 0.56267261\n",
            "Iteration 42, loss = 0.56234404\n",
            "Iteration 43, loss = 0.56219053\n",
            "Iteration 44, loss = 0.56244082\n",
            "Iteration 45, loss = 0.56089430\n",
            "Iteration 46, loss = 0.56186384\n",
            "Iteration 47, loss = 0.56228077\n",
            "Iteration 48, loss = 0.56130082\n",
            "Iteration 49, loss = 0.56146855\n",
            "Iteration 50, loss = 0.56070998\n",
            "Iteration 51, loss = 0.56099765\n",
            "Iteration 52, loss = 0.56080259\n",
            "Iteration 53, loss = 0.55899818\n",
            "Iteration 54, loss = 0.56046163\n",
            "Iteration 55, loss = 0.56008223\n",
            "Iteration 56, loss = 0.56056162\n",
            "Iteration 57, loss = 0.55967178\n",
            "Iteration 58, loss = 0.55859971\n",
            "Iteration 59, loss = 0.55920064\n",
            "Iteration 60, loss = 0.55814189\n",
            "Iteration 61, loss = 0.55807297\n",
            "Iteration 62, loss = 0.55783912\n",
            "Iteration 63, loss = 0.55692997\n",
            "Iteration 64, loss = 0.55771769\n",
            "Iteration 65, loss = 0.55663458\n",
            "Iteration 66, loss = 0.55751867\n",
            "Iteration 67, loss = 0.55645916\n",
            "Iteration 68, loss = 0.55488355\n",
            "Iteration 69, loss = 0.55537729\n",
            "Iteration 70, loss = 0.55530526\n",
            "Iteration 71, loss = 0.55373831\n",
            "Iteration 72, loss = 0.55411012\n",
            "Iteration 73, loss = 0.55340070\n",
            "Iteration 74, loss = 0.55280780\n",
            "Iteration 75, loss = 0.55274115\n",
            "Iteration 76, loss = 0.55188366\n",
            "Iteration 77, loss = 0.55103683\n",
            "Iteration 78, loss = 0.55077415\n",
            "Iteration 79, loss = 0.55002551\n",
            "Iteration 80, loss = 0.54968712\n",
            "Iteration 81, loss = 0.54815722\n",
            "Iteration 82, loss = 0.54773787\n",
            "Iteration 83, loss = 0.54697048\n",
            "Iteration 84, loss = 0.54692510\n",
            "Iteration 85, loss = 0.54594338\n",
            "Iteration 86, loss = 0.54509436\n",
            "Iteration 87, loss = 0.54454077\n",
            "Iteration 88, loss = 0.54473322\n",
            "Iteration 89, loss = 0.54345804\n",
            "Iteration 90, loss = 0.54167110\n",
            "Iteration 91, loss = 0.54211385\n",
            "Iteration 92, loss = 0.54151040\n",
            "Iteration 93, loss = 0.54165718\n",
            "Iteration 94, loss = 0.54069400\n",
            "Iteration 95, loss = 0.53946951\n",
            "Iteration 96, loss = 0.53904749\n",
            "Iteration 97, loss = 0.53889271\n",
            "Iteration 98, loss = 0.53791152\n",
            "Iteration 99, loss = 0.53828066\n",
            "Iteration 100, loss = 0.53744393\n",
            "Iteration 101, loss = 0.53656445\n",
            "Iteration 102, loss = 0.53654859\n",
            "Iteration 103, loss = 0.53611352\n",
            "Iteration 104, loss = 0.53566648\n",
            "Iteration 105, loss = 0.53475747\n",
            "Iteration 106, loss = 0.53324357\n",
            "Iteration 107, loss = 0.53395337\n",
            "Iteration 108, loss = 0.53414122\n",
            "Iteration 109, loss = 0.53361246\n",
            "Iteration 110, loss = 0.53368272\n",
            "Iteration 111, loss = 0.53290778\n",
            "Iteration 112, loss = 0.53378490\n",
            "Iteration 113, loss = 0.53240486\n",
            "Iteration 114, loss = 0.53232174\n",
            "Iteration 115, loss = 0.53124239\n",
            "Iteration 116, loss = 0.53157492\n",
            "Iteration 117, loss = 0.53193093\n",
            "Iteration 118, loss = 0.53120993\n",
            "Iteration 119, loss = 0.53028783\n",
            "Iteration 120, loss = 0.53058649\n",
            "Iteration 121, loss = 0.52895851\n",
            "Iteration 122, loss = 0.52967694\n",
            "Iteration 123, loss = 0.52941549\n",
            "Iteration 124, loss = 0.52930452\n",
            "Iteration 125, loss = 0.52745853\n",
            "Iteration 126, loss = 0.52901240\n",
            "Iteration 127, loss = 0.52851289\n",
            "Iteration 128, loss = 0.52877777\n",
            "Iteration 129, loss = 0.52837177\n",
            "Iteration 130, loss = 0.52779102\n",
            "Iteration 131, loss = 0.52871317\n",
            "Iteration 132, loss = 0.52769073\n",
            "Iteration 133, loss = 0.52785373\n",
            "Iteration 134, loss = 0.52715179\n",
            "Iteration 135, loss = 0.52813711\n",
            "Iteration 136, loss = 0.52789127\n",
            "Iteration 137, loss = 0.52746142\n",
            "Iteration 138, loss = 0.52719181\n",
            "Iteration 139, loss = 0.52659436\n",
            "Iteration 140, loss = 0.52709991\n",
            "Iteration 141, loss = 0.52607330\n",
            "Iteration 142, loss = 0.52674631\n",
            "Iteration 143, loss = 0.52576109\n",
            "Iteration 144, loss = 0.52663143\n",
            "Iteration 145, loss = 0.52645180\n",
            "Iteration 146, loss = 0.52477936\n",
            "Iteration 147, loss = 0.52653450\n",
            "Iteration 148, loss = 0.52519198\n",
            "Iteration 149, loss = 0.52639754\n",
            "Iteration 150, loss = 0.52632851\n",
            "Iteration 151, loss = 0.52573189\n",
            "Iteration 152, loss = 0.52598311\n",
            "Iteration 153, loss = 0.52431415\n",
            "Iteration 154, loss = 0.52537595\n",
            "Iteration 155, loss = 0.52445288\n",
            "Iteration 156, loss = 0.52428695\n",
            "Iteration 157, loss = 0.52530864\n",
            "Iteration 158, loss = 0.52507836\n",
            "Iteration 159, loss = 0.52491193\n",
            "Iteration 160, loss = 0.52434668\n",
            "Iteration 161, loss = 0.52406594\n",
            "Iteration 162, loss = 0.52473978\n",
            "Iteration 163, loss = 0.52410385\n",
            "Iteration 164, loss = 0.52566836\n",
            "Iteration 165, loss = 0.52476873\n",
            "Iteration 166, loss = 0.52386756\n",
            "Iteration 167, loss = 0.52413877\n",
            "Iteration 168, loss = 0.52433458\n",
            "Iteration 169, loss = 0.52364857\n",
            "Iteration 170, loss = 0.52423911\n",
            "Iteration 171, loss = 0.52353900\n",
            "Iteration 172, loss = 0.52341335\n",
            "Iteration 173, loss = 0.52367395\n",
            "Iteration 174, loss = 0.52406606\n",
            "Iteration 175, loss = 0.52405469\n",
            "Iteration 176, loss = 0.52387600\n",
            "Iteration 177, loss = 0.52379924\n",
            "Iteration 178, loss = 0.52365484\n",
            "Iteration 179, loss = 0.52375684\n",
            "Iteration 180, loss = 0.52329518\n",
            "Iteration 181, loss = 0.52424476\n",
            "Iteration 182, loss = 0.52403975\n",
            "Iteration 183, loss = 0.52277525\n",
            "Iteration 184, loss = 0.52397521\n",
            "Iteration 185, loss = 0.52363140\n",
            "Iteration 186, loss = 0.52329561\n",
            "Iteration 187, loss = 0.52372066\n",
            "Iteration 188, loss = 0.52312584\n",
            "Iteration 189, loss = 0.52295019\n",
            "Iteration 190, loss = 0.52371398\n",
            "Iteration 191, loss = 0.52373118\n",
            "Iteration 192, loss = 0.52270612\n",
            "Iteration 193, loss = 0.52313200\n",
            "Iteration 194, loss = 0.52296908\n",
            "Iteration 195, loss = 0.52304165\n",
            "Iteration 196, loss = 0.52393637\n",
            "Iteration 197, loss = 0.52324083\n",
            "Iteration 198, loss = 0.52310747\n",
            "Iteration 199, loss = 0.52302829\n",
            "Iteration 200, loss = 0.52365727\n",
            "Iteration 201, loss = 0.52402143\n",
            "Iteration 202, loss = 0.52347144\n",
            "Iteration 203, loss = 0.52321940\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56732084\n",
            "Iteration 2, loss = 0.56289451\n",
            "Iteration 3, loss = 0.56225170\n",
            "Iteration 4, loss = 0.56335927\n",
            "Iteration 5, loss = 0.56373615\n",
            "Iteration 6, loss = 0.56298199\n",
            "Iteration 7, loss = 0.56276908\n",
            "Iteration 8, loss = 0.56340124\n",
            "Iteration 9, loss = 0.56338342\n",
            "Iteration 10, loss = 0.56269002\n",
            "Iteration 11, loss = 0.56257978\n",
            "Iteration 12, loss = 0.56108410\n",
            "Iteration 13, loss = 0.56358157\n",
            "Iteration 14, loss = 0.56310557\n",
            "Iteration 15, loss = 0.56286733\n",
            "Iteration 16, loss = 0.56309297\n",
            "Iteration 17, loss = 0.56308028\n",
            "Iteration 18, loss = 0.56269813\n",
            "Iteration 19, loss = 0.56176556\n",
            "Iteration 20, loss = 0.56205188\n",
            "Iteration 21, loss = 0.56245003\n",
            "Iteration 22, loss = 0.56135155\n",
            "Iteration 23, loss = 0.56191078\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56656996\n",
            "Iteration 2, loss = 0.56287918\n",
            "Iteration 3, loss = 0.56253261\n",
            "Iteration 4, loss = 0.56237991\n",
            "Iteration 5, loss = 0.56310128\n",
            "Iteration 6, loss = 0.56291906\n",
            "Iteration 7, loss = 0.56275039\n",
            "Iteration 8, loss = 0.56259338\n",
            "Iteration 9, loss = 0.56241961\n",
            "Iteration 10, loss = 0.56291936\n",
            "Iteration 11, loss = 0.56330836\n",
            "Iteration 12, loss = 0.56254652\n",
            "Iteration 13, loss = 0.56246714\n",
            "Iteration 14, loss = 0.56199527\n",
            "Iteration 15, loss = 0.56276380\n",
            "Iteration 16, loss = 0.56193751\n",
            "Iteration 17, loss = 0.56261840\n",
            "Iteration 18, loss = 0.56218698\n",
            "Iteration 19, loss = 0.56245222\n",
            "Iteration 20, loss = 0.56182458\n",
            "Iteration 21, loss = 0.56234091\n",
            "Iteration 22, loss = 0.56213738\n",
            "Iteration 23, loss = 0.56197535\n",
            "Iteration 24, loss = 0.56157635\n",
            "Iteration 25, loss = 0.56199336\n",
            "Iteration 26, loss = 0.56198329\n",
            "Iteration 27, loss = 0.56203838\n",
            "Iteration 28, loss = 0.56218500\n",
            "Iteration 29, loss = 0.56222447\n",
            "Iteration 30, loss = 0.56117122\n",
            "Iteration 31, loss = 0.56201772\n",
            "Iteration 32, loss = 0.56200164\n",
            "Iteration 33, loss = 0.56152847\n",
            "Iteration 34, loss = 0.56069062\n",
            "Iteration 35, loss = 0.56097157\n",
            "Iteration 36, loss = 0.56142823\n",
            "Iteration 37, loss = 0.56086601\n",
            "Iteration 38, loss = 0.56073774\n",
            "Iteration 39, loss = 0.56064038\n",
            "Iteration 40, loss = 0.56109563\n",
            "Iteration 41, loss = 0.56060462\n",
            "Iteration 42, loss = 0.56109752\n",
            "Iteration 43, loss = 0.56109487\n",
            "Iteration 44, loss = 0.56062362\n",
            "Iteration 45, loss = 0.56018443\n",
            "Iteration 46, loss = 0.56001000\n",
            "Iteration 47, loss = 0.55995454\n",
            "Iteration 48, loss = 0.55960785\n",
            "Iteration 49, loss = 0.56008356\n",
            "Iteration 50, loss = 0.55981865\n",
            "Iteration 51, loss = 0.55970449\n",
            "Iteration 52, loss = 0.55952156\n",
            "Iteration 53, loss = 0.55906781\n",
            "Iteration 54, loss = 0.55880398\n",
            "Iteration 55, loss = 0.55915775\n",
            "Iteration 56, loss = 0.55885007\n",
            "Iteration 57, loss = 0.55847764\n",
            "Iteration 58, loss = 0.55851736\n",
            "Iteration 59, loss = 0.55852170\n",
            "Iteration 60, loss = 0.55841289\n",
            "Iteration 61, loss = 0.55797069\n",
            "Iteration 62, loss = 0.55690844\n",
            "Iteration 63, loss = 0.55786014\n",
            "Iteration 64, loss = 0.55761663\n",
            "Iteration 65, loss = 0.55625051\n",
            "Iteration 66, loss = 0.55611259\n",
            "Iteration 67, loss = 0.55649393\n",
            "Iteration 68, loss = 0.55579608\n",
            "Iteration 69, loss = 0.55515123\n",
            "Iteration 70, loss = 0.55532000\n",
            "Iteration 71, loss = 0.55489903\n",
            "Iteration 72, loss = 0.55460168\n",
            "Iteration 73, loss = 0.55360593\n",
            "Iteration 74, loss = 0.55371619\n",
            "Iteration 75, loss = 0.55372373\n",
            "Iteration 76, loss = 0.55257175\n",
            "Iteration 77, loss = 0.55216020\n",
            "Iteration 78, loss = 0.55197636\n",
            "Iteration 79, loss = 0.55116010\n",
            "Iteration 80, loss = 0.55122263\n",
            "Iteration 81, loss = 0.55049176\n",
            "Iteration 82, loss = 0.54963699\n",
            "Iteration 83, loss = 0.54977225\n",
            "Iteration 84, loss = 0.54939747\n",
            "Iteration 85, loss = 0.54869165\n",
            "Iteration 86, loss = 0.54839858\n",
            "Iteration 87, loss = 0.54673136\n",
            "Iteration 88, loss = 0.54657170\n",
            "Iteration 89, loss = 0.54518126\n",
            "Iteration 90, loss = 0.54478246\n",
            "Iteration 91, loss = 0.54364165\n",
            "Iteration 92, loss = 0.54435556\n",
            "Iteration 93, loss = 0.54359806\n",
            "Iteration 94, loss = 0.54275820\n",
            "Iteration 95, loss = 0.54139745\n",
            "Iteration 96, loss = 0.54122776\n",
            "Iteration 97, loss = 0.54015938\n",
            "Iteration 98, loss = 0.54019221\n",
            "Iteration 99, loss = 0.53989942\n",
            "Iteration 100, loss = 0.53923392\n",
            "Iteration 101, loss = 0.53845635\n",
            "Iteration 102, loss = 0.53818377\n",
            "Iteration 103, loss = 0.53647270\n",
            "Iteration 104, loss = 0.53702511\n",
            "Iteration 105, loss = 0.53736298\n",
            "Iteration 106, loss = 0.53615081\n",
            "Iteration 107, loss = 0.53514191\n",
            "Iteration 108, loss = 0.53591866\n",
            "Iteration 109, loss = 0.53585250\n",
            "Iteration 110, loss = 0.53375225\n",
            "Iteration 111, loss = 0.53365037\n",
            "Iteration 112, loss = 0.53342956\n",
            "Iteration 113, loss = 0.53374180\n",
            "Iteration 114, loss = 0.53248559\n",
            "Iteration 115, loss = 0.53269358\n",
            "Iteration 116, loss = 0.53282724\n",
            "Iteration 117, loss = 0.53294579\n",
            "Iteration 118, loss = 0.53150279\n",
            "Iteration 119, loss = 0.53218751\n",
            "Iteration 120, loss = 0.53005093\n",
            "Iteration 121, loss = 0.53201833\n",
            "Iteration 122, loss = 0.53047099\n",
            "Iteration 123, loss = 0.53022911\n",
            "Iteration 124, loss = 0.53007161\n",
            "Iteration 125, loss = 0.52987061\n",
            "Iteration 126, loss = 0.53013621\n",
            "Iteration 127, loss = 0.52984682\n",
            "Iteration 128, loss = 0.52937206\n",
            "Iteration 129, loss = 0.52869723\n",
            "Iteration 130, loss = 0.52953575\n",
            "Iteration 131, loss = 0.52844454\n",
            "Iteration 132, loss = 0.52800216\n",
            "Iteration 133, loss = 0.52825473\n",
            "Iteration 134, loss = 0.52760217\n",
            "Iteration 135, loss = 0.52874114\n",
            "Iteration 136, loss = 0.52814835\n",
            "Iteration 137, loss = 0.52711435\n",
            "Iteration 138, loss = 0.52796533\n",
            "Iteration 139, loss = 0.52717628\n",
            "Iteration 140, loss = 0.52762764\n",
            "Iteration 141, loss = 0.52737572\n",
            "Iteration 142, loss = 0.52729830\n",
            "Iteration 143, loss = 0.52599321\n",
            "Iteration 144, loss = 0.52612042\n",
            "Iteration 145, loss = 0.52700808\n",
            "Iteration 146, loss = 0.52593156\n",
            "Iteration 147, loss = 0.52484039\n",
            "Iteration 148, loss = 0.52655952\n",
            "Iteration 149, loss = 0.52485729\n",
            "Iteration 150, loss = 0.52600810\n",
            "Iteration 151, loss = 0.52626365\n",
            "Iteration 152, loss = 0.52577726\n",
            "Iteration 153, loss = 0.52495824\n",
            "Iteration 154, loss = 0.52492808\n",
            "Iteration 155, loss = 0.52559997\n",
            "Iteration 156, loss = 0.52455078\n",
            "Iteration 157, loss = 0.52538670\n",
            "Iteration 158, loss = 0.52559927\n",
            "Iteration 159, loss = 0.52574528\n",
            "Iteration 160, loss = 0.52450407\n",
            "Iteration 161, loss = 0.52604496\n",
            "Iteration 162, loss = 0.52458365\n",
            "Iteration 163, loss = 0.52434690\n",
            "Iteration 164, loss = 0.52527317\n",
            "Iteration 165, loss = 0.52532086\n",
            "Iteration 166, loss = 0.52414833\n",
            "Iteration 167, loss = 0.52505845\n",
            "Iteration 168, loss = 0.52406264\n",
            "Iteration 169, loss = 0.52397120\n",
            "Iteration 170, loss = 0.52404367\n",
            "Iteration 171, loss = 0.52285511\n",
            "Iteration 172, loss = 0.52285305\n",
            "Iteration 173, loss = 0.52441779\n",
            "Iteration 174, loss = 0.52381726\n",
            "Iteration 175, loss = 0.52345508\n",
            "Iteration 176, loss = 0.52420166\n",
            "Iteration 177, loss = 0.52406405\n",
            "Iteration 178, loss = 0.52418022\n",
            "Iteration 179, loss = 0.52381027\n",
            "Iteration 180, loss = 0.52292423\n",
            "Iteration 181, loss = 0.52323817\n",
            "Iteration 182, loss = 0.52314342\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56618337\n",
            "Iteration 2, loss = 0.56036534\n",
            "Iteration 3, loss = 0.55963237\n",
            "Iteration 4, loss = 0.56046935\n",
            "Iteration 5, loss = 0.56004242\n",
            "Iteration 6, loss = 0.56055019\n",
            "Iteration 7, loss = 0.56021030\n",
            "Iteration 8, loss = 0.56020106\n",
            "Iteration 9, loss = 0.55988929\n",
            "Iteration 10, loss = 0.55994022\n",
            "Iteration 11, loss = 0.55943370\n",
            "Iteration 12, loss = 0.56013603\n",
            "Iteration 13, loss = 0.55948394\n",
            "Iteration 14, loss = 0.55963635\n",
            "Iteration 15, loss = 0.56002807\n",
            "Iteration 16, loss = 0.55971547\n",
            "Iteration 17, loss = 0.56003979\n",
            "Iteration 18, loss = 0.56021898\n",
            "Iteration 19, loss = 0.55955376\n",
            "Iteration 20, loss = 0.55887011\n",
            "Iteration 21, loss = 0.55979711\n",
            "Iteration 22, loss = 0.55939601\n",
            "Iteration 23, loss = 0.55967226\n",
            "Iteration 24, loss = 0.55976197\n",
            "Iteration 25, loss = 0.55852657\n",
            "Iteration 26, loss = 0.56003930\n",
            "Iteration 27, loss = 0.55834368\n",
            "Iteration 28, loss = 0.55939719\n",
            "Iteration 29, loss = 0.55926388\n",
            "Iteration 30, loss = 0.55842544\n",
            "Iteration 31, loss = 0.55894019\n",
            "Iteration 32, loss = 0.55760598\n",
            "Iteration 33, loss = 0.55884436\n",
            "Iteration 34, loss = 0.55913578\n",
            "Iteration 35, loss = 0.55841127\n",
            "Iteration 36, loss = 0.55752133\n",
            "Iteration 37, loss = 0.55812211\n",
            "Iteration 38, loss = 0.55899914\n",
            "Iteration 39, loss = 0.55797764\n",
            "Iteration 40, loss = 0.55794015\n",
            "Iteration 41, loss = 0.55751477\n",
            "Iteration 42, loss = 0.55773136\n",
            "Iteration 43, loss = 0.55779687\n",
            "Iteration 44, loss = 0.55719488\n",
            "Iteration 45, loss = 0.55726493\n",
            "Iteration 46, loss = 0.55746683\n",
            "Iteration 47, loss = 0.55729355\n",
            "Iteration 48, loss = 0.55670157\n",
            "Iteration 49, loss = 0.55559059\n",
            "Iteration 50, loss = 0.55666083\n",
            "Iteration 51, loss = 0.55622746\n",
            "Iteration 52, loss = 0.55557886\n",
            "Iteration 53, loss = 0.55610019\n",
            "Iteration 54, loss = 0.55603197\n",
            "Iteration 55, loss = 0.55511025\n",
            "Iteration 56, loss = 0.55639572\n",
            "Iteration 57, loss = 0.55481260\n",
            "Iteration 58, loss = 0.55610777\n",
            "Iteration 59, loss = 0.55416023\n",
            "Iteration 60, loss = 0.55468137\n",
            "Iteration 61, loss = 0.55448869\n",
            "Iteration 62, loss = 0.55390886\n",
            "Iteration 63, loss = 0.55414233\n",
            "Iteration 64, loss = 0.55398824\n",
            "Iteration 65, loss = 0.55307061\n",
            "Iteration 66, loss = 0.55328962\n",
            "Iteration 67, loss = 0.55211991\n",
            "Iteration 68, loss = 0.55135391\n",
            "Iteration 69, loss = 0.55158714\n",
            "Iteration 70, loss = 0.55077816\n",
            "Iteration 71, loss = 0.55000542\n",
            "Iteration 72, loss = 0.55023860\n",
            "Iteration 73, loss = 0.54926054\n",
            "Iteration 74, loss = 0.54839797\n",
            "Iteration 75, loss = 0.54945061\n",
            "Iteration 76, loss = 0.54767566\n",
            "Iteration 77, loss = 0.54694395\n",
            "Iteration 78, loss = 0.54642339\n",
            "Iteration 79, loss = 0.54592232\n",
            "Iteration 80, loss = 0.54512784\n",
            "Iteration 81, loss = 0.54434526\n",
            "Iteration 82, loss = 0.54426940\n",
            "Iteration 83, loss = 0.54325636\n",
            "Iteration 84, loss = 0.54278322\n",
            "Iteration 85, loss = 0.54238662\n",
            "Iteration 86, loss = 0.54099229\n",
            "Iteration 87, loss = 0.54061831\n",
            "Iteration 88, loss = 0.53951043\n",
            "Iteration 89, loss = 0.53920743\n",
            "Iteration 90, loss = 0.53814197\n",
            "Iteration 91, loss = 0.53790127\n",
            "Iteration 92, loss = 0.53667632\n",
            "Iteration 93, loss = 0.53579302\n",
            "Iteration 94, loss = 0.53572805\n",
            "Iteration 95, loss = 0.53434664\n",
            "Iteration 96, loss = 0.53413889\n",
            "Iteration 97, loss = 0.53237050\n",
            "Iteration 98, loss = 0.53383550\n",
            "Iteration 99, loss = 0.53210448\n",
            "Iteration 100, loss = 0.53200131\n",
            "Iteration 101, loss = 0.53080702\n",
            "Iteration 102, loss = 0.53102290\n",
            "Iteration 103, loss = 0.52976591\n",
            "Iteration 104, loss = 0.53023008\n",
            "Iteration 105, loss = 0.52885415\n",
            "Iteration 106, loss = 0.52831132\n",
            "Iteration 107, loss = 0.52802277\n",
            "Iteration 108, loss = 0.52803646\n",
            "Iteration 109, loss = 0.52760665\n",
            "Iteration 110, loss = 0.52783843\n",
            "Iteration 111, loss = 0.52691337\n",
            "Iteration 112, loss = 0.52578592\n",
            "Iteration 113, loss = 0.52581203\n",
            "Iteration 114, loss = 0.52583234\n",
            "Iteration 115, loss = 0.52522416\n",
            "Iteration 116, loss = 0.52478688\n",
            "Iteration 117, loss = 0.52519251\n",
            "Iteration 118, loss = 0.52398719\n",
            "Iteration 119, loss = 0.52435142\n",
            "Iteration 120, loss = 0.52337038\n",
            "Iteration 121, loss = 0.52398796\n",
            "Iteration 122, loss = 0.52351545\n",
            "Iteration 123, loss = 0.52327707\n",
            "Iteration 124, loss = 0.52271199\n",
            "Iteration 125, loss = 0.52150749\n",
            "Iteration 126, loss = 0.52227335\n",
            "Iteration 127, loss = 0.52273400\n",
            "Iteration 128, loss = 0.52230244\n",
            "Iteration 129, loss = 0.52168108\n",
            "Iteration 130, loss = 0.51959755\n",
            "Iteration 131, loss = 0.52245750\n",
            "Iteration 132, loss = 0.52144434\n",
            "Iteration 133, loss = 0.52184169\n",
            "Iteration 134, loss = 0.52174619\n",
            "Iteration 135, loss = 0.52071697\n",
            "Iteration 136, loss = 0.52091961\n",
            "Iteration 137, loss = 0.52058453\n",
            "Iteration 138, loss = 0.52080099\n",
            "Iteration 139, loss = 0.52035825\n",
            "Iteration 140, loss = 0.52020852\n",
            "Iteration 141, loss = 0.51983175\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56753238\n",
            "Iteration 2, loss = 0.56492274\n",
            "Iteration 3, loss = 0.56390645\n",
            "Iteration 4, loss = 0.56417521\n",
            "Iteration 5, loss = 0.56486390\n",
            "Iteration 6, loss = 0.56397708\n",
            "Iteration 7, loss = 0.56522135\n",
            "Iteration 8, loss = 0.56450658\n",
            "Iteration 9, loss = 0.56405328\n",
            "Iteration 10, loss = 0.56435791\n",
            "Iteration 11, loss = 0.56388343\n",
            "Iteration 12, loss = 0.56452317\n",
            "Iteration 13, loss = 0.56431055\n",
            "Iteration 14, loss = 0.56482968\n",
            "Iteration 15, loss = 0.56443265\n",
            "Iteration 16, loss = 0.56363834\n",
            "Iteration 17, loss = 0.56463813\n",
            "Iteration 18, loss = 0.56362143\n",
            "Iteration 19, loss = 0.56437209\n",
            "Iteration 20, loss = 0.56291648\n",
            "Iteration 21, loss = 0.56310232\n",
            "Iteration 22, loss = 0.56314168\n",
            "Iteration 23, loss = 0.56328993\n",
            "Iteration 24, loss = 0.56284318\n",
            "Iteration 25, loss = 0.56382688\n",
            "Iteration 26, loss = 0.56347763\n",
            "Iteration 27, loss = 0.56248910\n",
            "Iteration 28, loss = 0.56306467\n",
            "Iteration 29, loss = 0.56312088\n",
            "Iteration 30, loss = 0.56236356\n",
            "Iteration 31, loss = 0.56227419\n",
            "Iteration 32, loss = 0.56171029\n",
            "Iteration 33, loss = 0.56216400\n",
            "Iteration 34, loss = 0.56277330\n",
            "Iteration 35, loss = 0.56162370\n",
            "Iteration 36, loss = 0.56155674\n",
            "Iteration 37, loss = 0.56221179\n",
            "Iteration 38, loss = 0.56147346\n",
            "Iteration 39, loss = 0.56146681\n",
            "Iteration 40, loss = 0.56121233\n",
            "Iteration 41, loss = 0.55971827\n",
            "Iteration 42, loss = 0.56110736\n",
            "Iteration 43, loss = 0.56068348\n",
            "Iteration 44, loss = 0.55979273\n",
            "Iteration 45, loss = 0.56049697\n",
            "Iteration 46, loss = 0.55945651\n",
            "Iteration 47, loss = 0.55975233\n",
            "Iteration 48, loss = 0.56043046\n",
            "Iteration 49, loss = 0.55911962\n",
            "Iteration 50, loss = 0.55877930\n",
            "Iteration 51, loss = 0.55876485\n",
            "Iteration 52, loss = 0.55886133\n",
            "Iteration 53, loss = 0.55860646\n",
            "Iteration 54, loss = 0.55802395\n",
            "Iteration 55, loss = 0.55717282\n",
            "Iteration 56, loss = 0.55787383\n",
            "Iteration 57, loss = 0.55732974\n",
            "Iteration 58, loss = 0.55692991\n",
            "Iteration 59, loss = 0.55667645\n",
            "Iteration 60, loss = 0.55592079\n",
            "Iteration 61, loss = 0.55527418\n",
            "Iteration 62, loss = 0.55499182\n",
            "Iteration 63, loss = 0.55389517\n",
            "Iteration 64, loss = 0.55336075\n",
            "Iteration 65, loss = 0.55273763\n",
            "Iteration 66, loss = 0.55278424\n",
            "Iteration 67, loss = 0.55204158\n",
            "Iteration 68, loss = 0.55156649\n",
            "Iteration 69, loss = 0.55198917\n",
            "Iteration 70, loss = 0.54999193\n",
            "Iteration 71, loss = 0.54774703\n",
            "Iteration 72, loss = 0.54827841\n",
            "Iteration 73, loss = 0.54863897\n",
            "Iteration 74, loss = 0.54713619\n",
            "Iteration 75, loss = 0.54644331\n",
            "Iteration 76, loss = 0.54592117\n",
            "Iteration 77, loss = 0.54502292\n",
            "Iteration 78, loss = 0.54393614\n",
            "Iteration 79, loss = 0.54355196\n",
            "Iteration 80, loss = 0.54345225\n",
            "Iteration 81, loss = 0.54098402\n",
            "Iteration 82, loss = 0.54159880\n",
            "Iteration 83, loss = 0.54110919\n",
            "Iteration 84, loss = 0.53957495\n",
            "Iteration 85, loss = 0.53852987\n",
            "Iteration 86, loss = 0.53834156\n",
            "Iteration 87, loss = 0.53774030\n",
            "Iteration 88, loss = 0.53722014\n",
            "Iteration 89, loss = 0.53594231\n",
            "Iteration 90, loss = 0.53617616\n",
            "Iteration 91, loss = 0.53589146\n",
            "Iteration 92, loss = 0.53507845\n",
            "Iteration 93, loss = 0.53403713\n",
            "Iteration 94, loss = 0.53380873\n",
            "Iteration 95, loss = 0.53288365\n",
            "Iteration 96, loss = 0.53269913\n",
            "Iteration 97, loss = 0.53248852\n",
            "Iteration 98, loss = 0.53134385\n",
            "Iteration 99, loss = 0.53175393\n",
            "Iteration 100, loss = 0.53055319\n",
            "Iteration 101, loss = 0.53103437\n",
            "Iteration 102, loss = 0.53041232\n",
            "Iteration 103, loss = 0.53031398\n",
            "Iteration 104, loss = 0.52991378\n",
            "Iteration 105, loss = 0.52907048\n",
            "Iteration 106, loss = 0.52939335\n",
            "Iteration 107, loss = 0.52910988\n",
            "Iteration 108, loss = 0.52860798\n",
            "Iteration 109, loss = 0.52800379\n",
            "Iteration 110, loss = 0.52757827\n",
            "Iteration 111, loss = 0.52796589\n",
            "Iteration 112, loss = 0.52714988\n",
            "Iteration 113, loss = 0.52743398\n",
            "Iteration 114, loss = 0.52699420\n",
            "Iteration 115, loss = 0.52712754\n",
            "Iteration 116, loss = 0.52774041\n",
            "Iteration 117, loss = 0.52652130\n",
            "Iteration 118, loss = 0.52653019\n",
            "Iteration 119, loss = 0.52649959\n",
            "Iteration 120, loss = 0.52569388\n",
            "Iteration 121, loss = 0.52562334\n",
            "Iteration 122, loss = 0.52602181\n",
            "Iteration 123, loss = 0.52602967\n",
            "Iteration 124, loss = 0.52585818\n",
            "Iteration 125, loss = 0.52619968\n",
            "Iteration 126, loss = 0.52551460\n",
            "Iteration 127, loss = 0.52557460\n",
            "Iteration 128, loss = 0.52484503\n",
            "Iteration 129, loss = 0.52436162\n",
            "Iteration 130, loss = 0.52483201\n",
            "Iteration 131, loss = 0.52411854\n",
            "Iteration 132, loss = 0.52387304\n",
            "Iteration 133, loss = 0.52498127\n",
            "Iteration 134, loss = 0.52357768\n",
            "Iteration 135, loss = 0.52469474\n",
            "Iteration 136, loss = 0.52360101\n",
            "Iteration 137, loss = 0.52389014\n",
            "Iteration 138, loss = 0.52437706\n",
            "Iteration 139, loss = 0.52345711\n",
            "Iteration 140, loss = 0.52387160\n",
            "Iteration 141, loss = 0.52340635\n",
            "Iteration 142, loss = 0.52328606\n",
            "Iteration 143, loss = 0.52351179\n",
            "Iteration 144, loss = 0.52368742\n",
            "Iteration 145, loss = 0.52314042\n",
            "Iteration 146, loss = 0.52336680\n",
            "Iteration 147, loss = 0.52344404\n",
            "Iteration 148, loss = 0.52317540\n",
            "Iteration 149, loss = 0.52223374\n",
            "Iteration 150, loss = 0.52332921\n",
            "Iteration 151, loss = 0.52295982\n",
            "Iteration 152, loss = 0.52181467\n",
            "Iteration 153, loss = 0.52295567\n",
            "Iteration 154, loss = 0.52248822\n",
            "Iteration 155, loss = 0.52256476\n",
            "Iteration 156, loss = 0.52182330\n",
            "Iteration 157, loss = 0.52187800\n",
            "Iteration 158, loss = 0.52240125\n",
            "Iteration 159, loss = 0.52273374\n",
            "Iteration 160, loss = 0.52241566\n",
            "Iteration 161, loss = 0.52199969\n",
            "Iteration 162, loss = 0.52307296\n",
            "Iteration 163, loss = 0.52212007\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57908954\n",
            "Iteration 2, loss = 0.56247188\n",
            "Iteration 3, loss = 0.56277287\n",
            "Iteration 4, loss = 0.56295415\n",
            "Iteration 5, loss = 0.56234226\n",
            "Iteration 6, loss = 0.56302539\n",
            "Iteration 7, loss = 0.56211901\n",
            "Iteration 8, loss = 0.56263370\n",
            "Iteration 9, loss = 0.56235859\n",
            "Iteration 10, loss = 0.56269880\n",
            "Iteration 11, loss = 0.56174646\n",
            "Iteration 12, loss = 0.56259572\n",
            "Iteration 13, loss = 0.56171686\n",
            "Iteration 14, loss = 0.56212047\n",
            "Iteration 15, loss = 0.56198843\n",
            "Iteration 16, loss = 0.56176978\n",
            "Iteration 17, loss = 0.56240182\n",
            "Iteration 18, loss = 0.56178661\n",
            "Iteration 19, loss = 0.56195941\n",
            "Iteration 20, loss = 0.56195745\n",
            "Iteration 21, loss = 0.56201488\n",
            "Iteration 22, loss = 0.56219007\n",
            "Iteration 23, loss = 0.56144974\n",
            "Iteration 24, loss = 0.56148797\n",
            "Iteration 25, loss = 0.56028898\n",
            "Iteration 26, loss = 0.56148651\n",
            "Iteration 27, loss = 0.56083005\n",
            "Iteration 28, loss = 0.56129028\n",
            "Iteration 29, loss = 0.56045498\n",
            "Iteration 30, loss = 0.56143309\n",
            "Iteration 31, loss = 0.56110933\n",
            "Iteration 32, loss = 0.56140617\n",
            "Iteration 33, loss = 0.56038437\n",
            "Iteration 34, loss = 0.56032981\n",
            "Iteration 35, loss = 0.56043392\n",
            "Iteration 36, loss = 0.56021914\n",
            "Iteration 37, loss = 0.55990788\n",
            "Iteration 38, loss = 0.55955926\n",
            "Iteration 39, loss = 0.55856261\n",
            "Iteration 40, loss = 0.56012384\n",
            "Iteration 41, loss = 0.56010312\n",
            "Iteration 42, loss = 0.55997775\n",
            "Iteration 43, loss = 0.55864363\n",
            "Iteration 44, loss = 0.55943466\n",
            "Iteration 45, loss = 0.55926698\n",
            "Iteration 46, loss = 0.55881706\n",
            "Iteration 47, loss = 0.55874422\n",
            "Iteration 48, loss = 0.55857800\n",
            "Iteration 49, loss = 0.55793715\n",
            "Iteration 50, loss = 0.55778861\n",
            "Iteration 51, loss = 0.55807516\n",
            "Iteration 52, loss = 0.55764120\n",
            "Iteration 53, loss = 0.55751187\n",
            "Iteration 54, loss = 0.55764092\n",
            "Iteration 55, loss = 0.55648297\n",
            "Iteration 56, loss = 0.55644795\n",
            "Iteration 57, loss = 0.55659281\n",
            "Iteration 58, loss = 0.55587875\n",
            "Iteration 59, loss = 0.55588574\n",
            "Iteration 60, loss = 0.55523186\n",
            "Iteration 61, loss = 0.55543718\n",
            "Iteration 62, loss = 0.55371490\n",
            "Iteration 63, loss = 0.55435622\n",
            "Iteration 64, loss = 0.55387878\n",
            "Iteration 65, loss = 0.55368865\n",
            "Iteration 66, loss = 0.55331927\n",
            "Iteration 67, loss = 0.55239787\n",
            "Iteration 68, loss = 0.55239561\n",
            "Iteration 69, loss = 0.55099113\n",
            "Iteration 70, loss = 0.55114037\n",
            "Iteration 71, loss = 0.55041675\n",
            "Iteration 72, loss = 0.54972170\n",
            "Iteration 73, loss = 0.55001675\n",
            "Iteration 74, loss = 0.54871068\n",
            "Iteration 75, loss = 0.54790741\n",
            "Iteration 76, loss = 0.54731488\n",
            "Iteration 77, loss = 0.54652263\n",
            "Iteration 78, loss = 0.54532436\n",
            "Iteration 79, loss = 0.54516349\n",
            "Iteration 80, loss = 0.54424135\n",
            "Iteration 81, loss = 0.54300715\n",
            "Iteration 82, loss = 0.54265566\n",
            "Iteration 83, loss = 0.54156043\n",
            "Iteration 84, loss = 0.54158137\n",
            "Iteration 85, loss = 0.54053650\n",
            "Iteration 86, loss = 0.53984977\n",
            "Iteration 87, loss = 0.53900730\n",
            "Iteration 88, loss = 0.53866744\n",
            "Iteration 89, loss = 0.53740163\n",
            "Iteration 90, loss = 0.53707323\n",
            "Iteration 91, loss = 0.53532690\n",
            "Iteration 92, loss = 0.53509091\n",
            "Iteration 93, loss = 0.53544116\n",
            "Iteration 94, loss = 0.53479036\n",
            "Iteration 95, loss = 0.53347039\n",
            "Iteration 96, loss = 0.53292371\n",
            "Iteration 97, loss = 0.53275310\n",
            "Iteration 98, loss = 0.53240345\n",
            "Iteration 99, loss = 0.53180481\n",
            "Iteration 100, loss = 0.53051959\n",
            "Iteration 101, loss = 0.53110782\n",
            "Iteration 102, loss = 0.53012081\n",
            "Iteration 103, loss = 0.53102489\n",
            "Iteration 104, loss = 0.52951948\n",
            "Iteration 105, loss = 0.52934285\n",
            "Iteration 106, loss = 0.53002787\n",
            "Iteration 107, loss = 0.52841531\n",
            "Iteration 108, loss = 0.52790774\n",
            "Iteration 109, loss = 0.52785487\n",
            "Iteration 110, loss = 0.52713187\n",
            "Iteration 111, loss = 0.52747020\n",
            "Iteration 112, loss = 0.52769326\n",
            "Iteration 113, loss = 0.52525277\n",
            "Iteration 114, loss = 0.52697191\n",
            "Iteration 115, loss = 0.52639201\n",
            "Iteration 116, loss = 0.52593336\n",
            "Iteration 117, loss = 0.52567676\n",
            "Iteration 118, loss = 0.52610198\n",
            "Iteration 119, loss = 0.52514116\n",
            "Iteration 120, loss = 0.52630150\n",
            "Iteration 121, loss = 0.52486585\n",
            "Iteration 122, loss = 0.52458946\n",
            "Iteration 123, loss = 0.52553208\n",
            "Iteration 124, loss = 0.52453060\n",
            "Iteration 125, loss = 0.52456353\n",
            "Iteration 126, loss = 0.52462279\n",
            "Iteration 127, loss = 0.52336275\n",
            "Iteration 128, loss = 0.52286731\n",
            "Iteration 129, loss = 0.52320614\n",
            "Iteration 130, loss = 0.52264407\n",
            "Iteration 131, loss = 0.52361145\n",
            "Iteration 132, loss = 0.52356641\n",
            "Iteration 133, loss = 0.52285822\n",
            "Iteration 134, loss = 0.52335146\n",
            "Iteration 135, loss = 0.52253556\n",
            "Iteration 136, loss = 0.52303381\n",
            "Iteration 137, loss = 0.52268340\n",
            "Iteration 138, loss = 0.52229138\n",
            "Iteration 139, loss = 0.52263913\n",
            "Iteration 140, loss = 0.52222253\n",
            "Iteration 141, loss = 0.52207935\n",
            "Iteration 142, loss = 0.52235134\n",
            "Iteration 143, loss = 0.52131215\n",
            "Iteration 144, loss = 0.52279794\n",
            "Iteration 145, loss = 0.52149269\n",
            "Iteration 146, loss = 0.52178085\n",
            "Iteration 147, loss = 0.52101777\n",
            "Iteration 148, loss = 0.52073012\n",
            "Iteration 149, loss = 0.52180460\n",
            "Iteration 150, loss = 0.52100951\n",
            "Iteration 151, loss = 0.52111993\n",
            "Iteration 152, loss = 0.52148930\n",
            "Iteration 153, loss = 0.52050671\n",
            "Iteration 154, loss = 0.52060817\n",
            "Iteration 155, loss = 0.52050830\n",
            "Iteration 156, loss = 0.52030041\n",
            "Iteration 157, loss = 0.52051206\n",
            "Iteration 158, loss = 0.51998510\n",
            "Iteration 159, loss = 0.52048899\n",
            "Iteration 160, loss = 0.52086116\n",
            "Iteration 161, loss = 0.52115958\n",
            "Iteration 162, loss = 0.52109218\n",
            "Iteration 163, loss = 0.52074020\n",
            "Iteration 164, loss = 0.52022603\n",
            "Iteration 165, loss = 0.51987700\n",
            "Iteration 166, loss = 0.52080141\n",
            "Iteration 167, loss = 0.52012930\n",
            "Iteration 168, loss = 0.52082386\n",
            "Iteration 169, loss = 0.52011310\n",
            "Iteration 170, loss = 0.51993749\n",
            "Iteration 171, loss = 0.52113464\n",
            "Iteration 172, loss = 0.52010115\n",
            "Iteration 173, loss = 0.52010839\n",
            "Iteration 174, loss = 0.51996408\n",
            "Iteration 175, loss = 0.51987689\n",
            "Iteration 176, loss = 0.51960503\n",
            "Iteration 177, loss = 0.51953861\n",
            "Iteration 178, loss = 0.51959758\n",
            "Iteration 179, loss = 0.51926131\n",
            "Iteration 180, loss = 0.52016717\n",
            "Iteration 181, loss = 0.52033065\n",
            "Iteration 182, loss = 0.52017952\n",
            "Iteration 183, loss = 0.51907446\n",
            "Iteration 184, loss = 0.51951683\n",
            "Iteration 185, loss = 0.51987305\n",
            "Iteration 186, loss = 0.51945181\n",
            "Iteration 187, loss = 0.51962639\n",
            "Iteration 188, loss = 0.51921927\n",
            "Iteration 189, loss = 0.52002624\n",
            "Iteration 190, loss = 0.51946565\n",
            "Iteration 191, loss = 0.51939941\n",
            "Iteration 192, loss = 0.51874890\n",
            "Iteration 193, loss = 0.51923265\n",
            "Iteration 194, loss = 0.51941658\n",
            "Iteration 195, loss = 0.51946106\n",
            "Iteration 196, loss = 0.51893603\n",
            "Iteration 197, loss = 0.51900332\n",
            "Iteration 198, loss = 0.51986491\n",
            "Iteration 199, loss = 0.51929287\n",
            "Iteration 200, loss = 0.51895894\n",
            "Iteration 201, loss = 0.51926373\n",
            "Iteration 202, loss = 0.51822233\n",
            "Iteration 203, loss = 0.51900649\n",
            "Iteration 204, loss = 0.51895314\n",
            "Iteration 205, loss = 0.51986313\n",
            "Iteration 206, loss = 0.51935343\n",
            "Iteration 207, loss = 0.51945774\n",
            "Iteration 208, loss = 0.51874935\n",
            "Iteration 209, loss = 0.51832115\n",
            "Iteration 210, loss = 0.51940810\n",
            "Iteration 211, loss = 0.51960796\n",
            "Iteration 212, loss = 0.51851775\n",
            "Iteration 213, loss = 0.51929715\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57248290\n",
            "Iteration 2, loss = 0.56252121\n",
            "Iteration 3, loss = 0.56157315\n",
            "Iteration 4, loss = 0.56341323\n",
            "Iteration 5, loss = 0.56336619\n",
            "Iteration 6, loss = 0.56291300\n",
            "Iteration 7, loss = 0.56228857\n",
            "Iteration 8, loss = 0.56245408\n",
            "Iteration 9, loss = 0.56326043\n",
            "Iteration 10, loss = 0.56275202\n",
            "Iteration 11, loss = 0.56221953\n",
            "Iteration 12, loss = 0.56165980\n",
            "Iteration 13, loss = 0.56227056\n",
            "Iteration 14, loss = 0.56302804\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "10\n",
            "Iteration 1, loss = 0.56529238\n",
            "Iteration 2, loss = 0.56350202\n",
            "Iteration 3, loss = 0.56335690\n",
            "Iteration 4, loss = 0.56355228\n",
            "Iteration 5, loss = 0.56361570\n",
            "Iteration 6, loss = 0.56381361\n",
            "Iteration 7, loss = 0.56309407\n",
            "Iteration 8, loss = 0.56254623\n",
            "Iteration 9, loss = 0.56324723\n",
            "Iteration 10, loss = 0.56238138\n",
            "Iteration 11, loss = 0.56277688\n",
            "Iteration 12, loss = 0.56269539\n",
            "Iteration 13, loss = 0.56294573\n",
            "Iteration 14, loss = 0.56275130\n",
            "Iteration 15, loss = 0.56179756\n",
            "Iteration 16, loss = 0.56256148\n",
            "Iteration 17, loss = 0.56266425\n",
            "Iteration 18, loss = 0.56225619\n",
            "Iteration 19, loss = 0.56246438\n",
            "Iteration 20, loss = 0.56219002\n",
            "Iteration 21, loss = 0.56223249\n",
            "Iteration 22, loss = 0.56257339\n",
            "Iteration 23, loss = 0.56195346\n",
            "Iteration 24, loss = 0.56241918\n",
            "Iteration 25, loss = 0.56213378\n",
            "Iteration 26, loss = 0.56157221\n",
            "Iteration 27, loss = 0.56123246\n",
            "Iteration 28, loss = 0.56173981\n",
            "Iteration 29, loss = 0.56207272\n",
            "Iteration 30, loss = 0.56139085\n",
            "Iteration 31, loss = 0.56188315\n",
            "Iteration 32, loss = 0.56129623\n",
            "Iteration 33, loss = 0.56127927\n",
            "Iteration 34, loss = 0.56154893\n",
            "Iteration 35, loss = 0.56176869\n",
            "Iteration 36, loss = 0.56074501\n",
            "Iteration 37, loss = 0.56112288\n",
            "Iteration 38, loss = 0.56050184\n",
            "Iteration 39, loss = 0.56092365\n",
            "Iteration 40, loss = 0.56070345\n",
            "Iteration 41, loss = 0.56002927\n",
            "Iteration 42, loss = 0.55968102\n",
            "Iteration 43, loss = 0.55955057\n",
            "Iteration 44, loss = 0.56086624\n",
            "Iteration 45, loss = 0.56053712\n",
            "Iteration 46, loss = 0.55938331\n",
            "Iteration 47, loss = 0.55958326\n",
            "Iteration 48, loss = 0.55886785\n",
            "Iteration 49, loss = 0.55816192\n",
            "Iteration 50, loss = 0.55915426\n",
            "Iteration 51, loss = 0.55905548\n",
            "Iteration 52, loss = 0.55884187\n",
            "Iteration 53, loss = 0.55679044\n",
            "Iteration 54, loss = 0.55843363\n",
            "Iteration 55, loss = 0.55759432\n",
            "Iteration 56, loss = 0.55708199\n",
            "Iteration 57, loss = 0.55721059\n",
            "Iteration 58, loss = 0.55675057\n",
            "Iteration 59, loss = 0.55705677\n",
            "Iteration 60, loss = 0.55589579\n",
            "Iteration 61, loss = 0.55542311\n",
            "Iteration 62, loss = 0.55458772\n",
            "Iteration 63, loss = 0.55389380\n",
            "Iteration 64, loss = 0.55442588\n",
            "Iteration 65, loss = 0.55333992\n",
            "Iteration 66, loss = 0.55278483\n",
            "Iteration 67, loss = 0.55378888\n",
            "Iteration 68, loss = 0.55263290\n",
            "Iteration 69, loss = 0.55201354\n",
            "Iteration 70, loss = 0.55146209\n",
            "Iteration 71, loss = 0.55117826\n",
            "Iteration 72, loss = 0.55053040\n",
            "Iteration 73, loss = 0.55047618\n",
            "Iteration 74, loss = 0.54888442\n",
            "Iteration 75, loss = 0.54928925\n",
            "Iteration 76, loss = 0.54933374\n",
            "Iteration 77, loss = 0.54766795\n",
            "Iteration 78, loss = 0.54685029\n",
            "Iteration 79, loss = 0.54704668\n",
            "Iteration 80, loss = 0.54534517\n",
            "Iteration 81, loss = 0.54488696\n",
            "Iteration 82, loss = 0.54379899\n",
            "Iteration 83, loss = 0.54356118\n",
            "Iteration 84, loss = 0.54359443\n",
            "Iteration 85, loss = 0.54276509\n",
            "Iteration 86, loss = 0.54172294\n",
            "Iteration 87, loss = 0.54165765\n",
            "Iteration 88, loss = 0.54093739\n",
            "Iteration 89, loss = 0.53977514\n",
            "Iteration 90, loss = 0.53898630\n",
            "Iteration 91, loss = 0.53845211\n",
            "Iteration 92, loss = 0.53802133\n",
            "Iteration 93, loss = 0.53791387\n",
            "Iteration 94, loss = 0.53634659\n",
            "Iteration 95, loss = 0.53617880\n",
            "Iteration 96, loss = 0.53612088\n",
            "Iteration 97, loss = 0.53633503\n",
            "Iteration 98, loss = 0.53485240\n",
            "Iteration 99, loss = 0.53518911\n",
            "Iteration 100, loss = 0.53441892\n",
            "Iteration 101, loss = 0.53331799\n",
            "Iteration 102, loss = 0.53315204\n",
            "Iteration 103, loss = 0.53323101\n",
            "Iteration 104, loss = 0.53287385\n",
            "Iteration 105, loss = 0.53222194\n",
            "Iteration 106, loss = 0.53178622\n",
            "Iteration 107, loss = 0.53204247\n",
            "Iteration 108, loss = 0.53095663\n",
            "Iteration 109, loss = 0.53098272\n",
            "Iteration 110, loss = 0.52965197\n",
            "Iteration 111, loss = 0.53039240\n",
            "Iteration 112, loss = 0.52999171\n",
            "Iteration 113, loss = 0.53038095\n",
            "Iteration 114, loss = 0.52967121\n",
            "Iteration 115, loss = 0.52860277\n",
            "Iteration 116, loss = 0.52927450\n",
            "Iteration 117, loss = 0.52949875\n",
            "Iteration 118, loss = 0.52821518\n",
            "Iteration 119, loss = 0.52897962\n",
            "Iteration 120, loss = 0.52779675\n",
            "Iteration 121, loss = 0.52778652\n",
            "Iteration 122, loss = 0.52793735\n",
            "Iteration 123, loss = 0.52651769\n",
            "Iteration 124, loss = 0.52730509\n",
            "Iteration 125, loss = 0.52714679\n",
            "Iteration 126, loss = 0.52604355\n",
            "Iteration 127, loss = 0.52719404\n",
            "Iteration 128, loss = 0.52706284\n",
            "Iteration 129, loss = 0.52618910\n",
            "Iteration 130, loss = 0.52667610\n",
            "Iteration 131, loss = 0.52645889\n",
            "Iteration 132, loss = 0.52577332\n",
            "Iteration 133, loss = 0.52580167\n",
            "Iteration 134, loss = 0.52487361\n",
            "Iteration 135, loss = 0.52652447\n",
            "Iteration 136, loss = 0.52601326\n",
            "Iteration 137, loss = 0.52579780\n",
            "Iteration 138, loss = 0.52539021\n",
            "Iteration 139, loss = 0.52592950\n",
            "Iteration 140, loss = 0.52544336\n",
            "Iteration 141, loss = 0.52496584\n",
            "Iteration 142, loss = 0.52472661\n",
            "Iteration 143, loss = 0.52510153\n",
            "Iteration 144, loss = 0.52476517\n",
            "Iteration 145, loss = 0.52487950\n",
            "Iteration 146, loss = 0.52483942\n",
            "Iteration 147, loss = 0.52491813\n",
            "Iteration 148, loss = 0.52411769\n",
            "Iteration 149, loss = 0.52393960\n",
            "Iteration 150, loss = 0.52479143\n",
            "Iteration 151, loss = 0.52454688\n",
            "Iteration 152, loss = 0.52376273\n",
            "Iteration 153, loss = 0.52382152\n",
            "Iteration 154, loss = 0.52443183\n",
            "Iteration 155, loss = 0.52400769\n",
            "Iteration 156, loss = 0.52431976\n",
            "Iteration 157, loss = 0.52402032\n",
            "Iteration 158, loss = 0.52387820\n",
            "Iteration 159, loss = 0.52269767\n",
            "Iteration 160, loss = 0.52344555\n",
            "Iteration 161, loss = 0.52393720\n",
            "Iteration 162, loss = 0.52393147\n",
            "Iteration 163, loss = 0.52518574\n",
            "Iteration 164, loss = 0.52390182\n",
            "Iteration 165, loss = 0.52394715\n",
            "Iteration 166, loss = 0.52406530\n",
            "Iteration 167, loss = 0.52297545\n",
            "Iteration 168, loss = 0.52312996\n",
            "Iteration 169, loss = 0.52213076\n",
            "Iteration 170, loss = 0.52308145\n",
            "Iteration 171, loss = 0.52366031\n",
            "Iteration 172, loss = 0.52207137\n",
            "Iteration 173, loss = 0.52340602\n",
            "Iteration 174, loss = 0.52302274\n",
            "Iteration 175, loss = 0.52351017\n",
            "Iteration 176, loss = 0.52387105\n",
            "Iteration 177, loss = 0.52309158\n",
            "Iteration 178, loss = 0.52293870\n",
            "Iteration 179, loss = 0.52268815\n",
            "Iteration 180, loss = 0.52246794\n",
            "Iteration 181, loss = 0.52231735\n",
            "Iteration 182, loss = 0.52320922\n",
            "Iteration 183, loss = 0.52221938\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57114309\n",
            "Iteration 2, loss = 0.56555426\n",
            "Iteration 3, loss = 0.56693765\n",
            "Iteration 4, loss = 0.56654967\n",
            "Iteration 5, loss = 0.56624399\n",
            "Iteration 6, loss = 0.56699703\n",
            "Iteration 7, loss = 0.56590524\n",
            "Iteration 8, loss = 0.56720923\n",
            "Iteration 9, loss = 0.56571490\n",
            "Iteration 10, loss = 0.56701867\n",
            "Iteration 11, loss = 0.56689525\n",
            "Iteration 12, loss = 0.56637217\n",
            "Iteration 13, loss = 0.56599615\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56925415\n",
            "Iteration 2, loss = 0.56290269\n",
            "Iteration 3, loss = 0.56231197\n",
            "Iteration 4, loss = 0.56192502\n",
            "Iteration 5, loss = 0.56258041\n",
            "Iteration 6, loss = 0.56305986\n",
            "Iteration 7, loss = 0.56211985\n",
            "Iteration 8, loss = 0.56227876\n",
            "Iteration 9, loss = 0.56188688\n",
            "Iteration 10, loss = 0.56257198\n",
            "Iteration 11, loss = 0.56186595\n",
            "Iteration 12, loss = 0.56176256\n",
            "Iteration 13, loss = 0.56112228\n",
            "Iteration 14, loss = 0.56202211\n",
            "Iteration 15, loss = 0.56168887\n",
            "Iteration 16, loss = 0.56182026\n",
            "Iteration 17, loss = 0.56063853\n",
            "Iteration 18, loss = 0.56152287\n",
            "Iteration 19, loss = 0.56195256\n",
            "Iteration 20, loss = 0.56186755\n",
            "Iteration 21, loss = 0.56151435\n",
            "Iteration 22, loss = 0.56127872\n",
            "Iteration 23, loss = 0.56117362\n",
            "Iteration 24, loss = 0.56078750\n",
            "Iteration 25, loss = 0.56069792\n",
            "Iteration 26, loss = 0.56058860\n",
            "Iteration 27, loss = 0.56099900\n",
            "Iteration 28, loss = 0.56118188\n",
            "Iteration 29, loss = 0.56068958\n",
            "Iteration 30, loss = 0.56071172\n",
            "Iteration 31, loss = 0.56035185\n",
            "Iteration 32, loss = 0.56005308\n",
            "Iteration 33, loss = 0.56039714\n",
            "Iteration 34, loss = 0.56053549\n",
            "Iteration 35, loss = 0.55977336\n",
            "Iteration 36, loss = 0.56036895\n",
            "Iteration 37, loss = 0.56035843\n",
            "Iteration 38, loss = 0.55950515\n",
            "Iteration 39, loss = 0.55976138\n",
            "Iteration 40, loss = 0.55894771\n",
            "Iteration 41, loss = 0.55937375\n",
            "Iteration 42, loss = 0.55906410\n",
            "Iteration 43, loss = 0.55845682\n",
            "Iteration 44, loss = 0.55851167\n",
            "Iteration 45, loss = 0.55865859\n",
            "Iteration 46, loss = 0.55797564\n",
            "Iteration 47, loss = 0.55855057\n",
            "Iteration 48, loss = 0.55862110\n",
            "Iteration 49, loss = 0.55742900\n",
            "Iteration 50, loss = 0.55833225\n",
            "Iteration 51, loss = 0.55738386\n",
            "Iteration 52, loss = 0.55646093\n",
            "Iteration 53, loss = 0.55684268\n",
            "Iteration 54, loss = 0.55735102\n",
            "Iteration 55, loss = 0.55664054\n",
            "Iteration 56, loss = 0.55662718\n",
            "Iteration 57, loss = 0.55587718\n",
            "Iteration 58, loss = 0.55601694\n",
            "Iteration 59, loss = 0.55489019\n",
            "Iteration 60, loss = 0.55490586\n",
            "Iteration 61, loss = 0.55498830\n",
            "Iteration 62, loss = 0.55336271\n",
            "Iteration 63, loss = 0.55408026\n",
            "Iteration 64, loss = 0.55310215\n",
            "Iteration 65, loss = 0.55287813\n",
            "Iteration 66, loss = 0.55191696\n",
            "Iteration 67, loss = 0.55204013\n",
            "Iteration 68, loss = 0.55098463\n",
            "Iteration 69, loss = 0.55109062\n",
            "Iteration 70, loss = 0.55007068\n",
            "Iteration 71, loss = 0.54922742\n",
            "Iteration 72, loss = 0.54913662\n",
            "Iteration 73, loss = 0.54861039\n",
            "Iteration 74, loss = 0.54720216\n",
            "Iteration 75, loss = 0.54714239\n",
            "Iteration 76, loss = 0.54634310\n",
            "Iteration 77, loss = 0.54510993\n",
            "Iteration 78, loss = 0.54480665\n",
            "Iteration 79, loss = 0.54423623\n",
            "Iteration 80, loss = 0.54344509\n",
            "Iteration 81, loss = 0.54212873\n",
            "Iteration 82, loss = 0.54175903\n",
            "Iteration 83, loss = 0.54109641\n",
            "Iteration 84, loss = 0.54041806\n",
            "Iteration 85, loss = 0.53991792\n",
            "Iteration 86, loss = 0.53840134\n",
            "Iteration 87, loss = 0.53765068\n",
            "Iteration 88, loss = 0.53661061\n",
            "Iteration 89, loss = 0.53630052\n",
            "Iteration 90, loss = 0.53587121\n",
            "Iteration 91, loss = 0.53547063\n",
            "Iteration 92, loss = 0.53439196\n",
            "Iteration 93, loss = 0.53372924\n",
            "Iteration 94, loss = 0.53431569\n",
            "Iteration 95, loss = 0.53297293\n",
            "Iteration 96, loss = 0.53141965\n",
            "Iteration 97, loss = 0.53133760\n",
            "Iteration 98, loss = 0.53082950\n",
            "Iteration 99, loss = 0.52997177\n",
            "Iteration 100, loss = 0.52941234\n",
            "Iteration 101, loss = 0.52834690\n",
            "Iteration 102, loss = 0.52903389\n",
            "Iteration 103, loss = 0.52795101\n",
            "Iteration 104, loss = 0.52816329\n",
            "Iteration 105, loss = 0.52704946\n",
            "Iteration 106, loss = 0.52801822\n",
            "Iteration 107, loss = 0.52636309\n",
            "Iteration 108, loss = 0.52725955\n",
            "Iteration 109, loss = 0.52670079\n",
            "Iteration 110, loss = 0.52543017\n",
            "Iteration 111, loss = 0.52601499\n",
            "Iteration 112, loss = 0.52547857\n",
            "Iteration 113, loss = 0.52429988\n",
            "Iteration 114, loss = 0.52552233\n",
            "Iteration 115, loss = 0.52463903\n",
            "Iteration 116, loss = 0.52437350\n",
            "Iteration 117, loss = 0.52281209\n",
            "Iteration 118, loss = 0.52444206\n",
            "Iteration 119, loss = 0.52427394\n",
            "Iteration 120, loss = 0.52343651\n",
            "Iteration 121, loss = 0.52358406\n",
            "Iteration 122, loss = 0.52356763\n",
            "Iteration 123, loss = 0.52272825\n",
            "Iteration 124, loss = 0.52290883\n",
            "Iteration 125, loss = 0.52095960\n",
            "Iteration 126, loss = 0.52290063\n",
            "Iteration 127, loss = 0.52161003\n",
            "Iteration 128, loss = 0.52253136\n",
            "Iteration 129, loss = 0.52104782\n",
            "Iteration 130, loss = 0.52174473\n",
            "Iteration 131, loss = 0.52112402\n",
            "Iteration 132, loss = 0.52107199\n",
            "Iteration 133, loss = 0.52065402\n",
            "Iteration 134, loss = 0.52119043\n",
            "Iteration 135, loss = 0.52034254\n",
            "Iteration 136, loss = 0.52081680\n",
            "Iteration 137, loss = 0.52051656\n",
            "Iteration 138, loss = 0.52021520\n",
            "Iteration 139, loss = 0.52084966\n",
            "Iteration 140, loss = 0.51937671\n",
            "Iteration 141, loss = 0.52059365\n",
            "Iteration 142, loss = 0.51993500\n",
            "Iteration 143, loss = 0.51966536\n",
            "Iteration 144, loss = 0.52013026\n",
            "Iteration 145, loss = 0.51945097\n",
            "Iteration 146, loss = 0.51895791\n",
            "Iteration 147, loss = 0.52011205\n",
            "Iteration 148, loss = 0.51970035\n",
            "Iteration 149, loss = 0.51889503\n",
            "Iteration 150, loss = 0.51917185\n",
            "Iteration 151, loss = 0.51996362\n",
            "Iteration 152, loss = 0.51928851\n",
            "Iteration 153, loss = 0.51969087\n",
            "Iteration 154, loss = 0.51991178\n",
            "Iteration 155, loss = 0.51900106\n",
            "Iteration 156, loss = 0.51835422\n",
            "Iteration 157, loss = 0.51838262\n",
            "Iteration 158, loss = 0.51785320\n",
            "Iteration 159, loss = 0.51906702\n",
            "Iteration 160, loss = 0.51908499\n",
            "Iteration 161, loss = 0.51767714\n",
            "Iteration 162, loss = 0.51943855\n",
            "Iteration 163, loss = 0.51863815\n",
            "Iteration 164, loss = 0.51824854\n",
            "Iteration 165, loss = 0.51897037\n",
            "Iteration 166, loss = 0.51888938\n",
            "Iteration 167, loss = 0.51729461\n",
            "Iteration 168, loss = 0.51804465\n",
            "Iteration 169, loss = 0.51840020\n",
            "Iteration 170, loss = 0.51753668\n",
            "Iteration 171, loss = 0.51814699\n",
            "Iteration 172, loss = 0.51727764\n",
            "Iteration 173, loss = 0.51820938\n",
            "Iteration 174, loss = 0.51745410\n",
            "Iteration 175, loss = 0.51757225\n",
            "Iteration 176, loss = 0.51826184\n",
            "Iteration 177, loss = 0.51823249\n",
            "Iteration 178, loss = 0.51799414\n",
            "Iteration 179, loss = 0.51823955\n",
            "Iteration 180, loss = 0.51745960\n",
            "Iteration 181, loss = 0.51669403\n",
            "Iteration 182, loss = 0.51818454\n",
            "Iteration 183, loss = 0.51792761\n",
            "Iteration 184, loss = 0.51740236\n",
            "Iteration 185, loss = 0.51703410\n",
            "Iteration 186, loss = 0.51737953\n",
            "Iteration 187, loss = 0.51812986\n",
            "Iteration 188, loss = 0.51737192\n",
            "Iteration 189, loss = 0.51729070\n",
            "Iteration 190, loss = 0.51683127\n",
            "Iteration 191, loss = 0.51721989\n",
            "Iteration 192, loss = 0.51726411\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56697449\n",
            "Iteration 2, loss = 0.56085271\n",
            "Iteration 3, loss = 0.56105831\n",
            "Iteration 4, loss = 0.56059058\n",
            "Iteration 5, loss = 0.56070574\n",
            "Iteration 6, loss = 0.56129779\n",
            "Iteration 7, loss = 0.56049897\n",
            "Iteration 8, loss = 0.56117520\n",
            "Iteration 9, loss = 0.56063660\n",
            "Iteration 10, loss = 0.56049745\n",
            "Iteration 11, loss = 0.56103286\n",
            "Iteration 12, loss = 0.56056635\n",
            "Iteration 13, loss = 0.56076940\n",
            "Iteration 14, loss = 0.56087218\n",
            "Iteration 15, loss = 0.56127904\n",
            "Iteration 16, loss = 0.55955993\n",
            "Iteration 17, loss = 0.56064573\n",
            "Iteration 18, loss = 0.56066724\n",
            "Iteration 19, loss = 0.56026529\n",
            "Iteration 20, loss = 0.56052831\n",
            "Iteration 21, loss = 0.56025889\n",
            "Iteration 22, loss = 0.56035824\n",
            "Iteration 23, loss = 0.55971622\n",
            "Iteration 24, loss = 0.56029046\n",
            "Iteration 25, loss = 0.56024192\n",
            "Iteration 26, loss = 0.55996339\n",
            "Iteration 27, loss = 0.56013191\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56744207\n",
            "Iteration 2, loss = 0.55731526\n",
            "Iteration 3, loss = 0.55746955\n",
            "Iteration 4, loss = 0.55754446\n",
            "Iteration 5, loss = 0.55755400\n",
            "Iteration 6, loss = 0.55681723\n",
            "Iteration 7, loss = 0.55690886\n",
            "Iteration 8, loss = 0.55747983\n",
            "Iteration 9, loss = 0.55730766\n",
            "Iteration 10, loss = 0.55747440\n",
            "Iteration 11, loss = 0.55726024\n",
            "Iteration 12, loss = 0.55630004\n",
            "Iteration 13, loss = 0.55730143\n",
            "Iteration 14, loss = 0.55733593\n",
            "Iteration 15, loss = 0.55827385\n",
            "Iteration 16, loss = 0.55678769\n",
            "Iteration 17, loss = 0.55722549\n",
            "Iteration 18, loss = 0.55732097\n",
            "Iteration 19, loss = 0.55726162\n",
            "Iteration 20, loss = 0.55757662\n",
            "Iteration 21, loss = 0.55645919\n",
            "Iteration 22, loss = 0.55637912\n",
            "Iteration 23, loss = 0.55666690\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56936038\n",
            "Iteration 2, loss = 0.56824870\n",
            "Iteration 3, loss = 0.56846299\n",
            "Iteration 4, loss = 0.56779316\n",
            "Iteration 5, loss = 0.56790288\n",
            "Iteration 6, loss = 0.56791916\n",
            "Iteration 7, loss = 0.56868186\n",
            "Iteration 8, loss = 0.56786774\n",
            "Iteration 9, loss = 0.56780941\n",
            "Iteration 10, loss = 0.56715646\n",
            "Iteration 11, loss = 0.56777579\n",
            "Iteration 12, loss = 0.56720078\n",
            "Iteration 13, loss = 0.56847185\n",
            "Iteration 14, loss = 0.56781688\n",
            "Iteration 15, loss = 0.56735034\n",
            "Iteration 16, loss = 0.56725706\n",
            "Iteration 17, loss = 0.56733188\n",
            "Iteration 18, loss = 0.56735853\n",
            "Iteration 19, loss = 0.56665893\n",
            "Iteration 20, loss = 0.56748298\n",
            "Iteration 21, loss = 0.56756159\n",
            "Iteration 22, loss = 0.56718959\n",
            "Iteration 23, loss = 0.56715017\n",
            "Iteration 24, loss = 0.56700393\n",
            "Iteration 25, loss = 0.56731722\n",
            "Iteration 26, loss = 0.56726878\n",
            "Iteration 27, loss = 0.56656723\n",
            "Iteration 28, loss = 0.56720434\n",
            "Iteration 29, loss = 0.56598079\n",
            "Iteration 30, loss = 0.56651130\n",
            "Iteration 31, loss = 0.56628065\n",
            "Iteration 32, loss = 0.56667210\n",
            "Iteration 33, loss = 0.56710777\n",
            "Iteration 34, loss = 0.56624198\n",
            "Iteration 35, loss = 0.56611104\n",
            "Iteration 36, loss = 0.56608624\n",
            "Iteration 37, loss = 0.56624003\n",
            "Iteration 38, loss = 0.56549419\n",
            "Iteration 39, loss = 0.56578678\n",
            "Iteration 40, loss = 0.56532280\n",
            "Iteration 41, loss = 0.56476300\n",
            "Iteration 42, loss = 0.56492557\n",
            "Iteration 43, loss = 0.56460184\n",
            "Iteration 44, loss = 0.56456436\n",
            "Iteration 45, loss = 0.56541856\n",
            "Iteration 46, loss = 0.56478199\n",
            "Iteration 47, loss = 0.56402667\n",
            "Iteration 48, loss = 0.56432818\n",
            "Iteration 49, loss = 0.56504274\n",
            "Iteration 50, loss = 0.56427828\n",
            "Iteration 51, loss = 0.56356079\n",
            "Iteration 52, loss = 0.56318729\n",
            "Iteration 53, loss = 0.56344784\n",
            "Iteration 54, loss = 0.56327076\n",
            "Iteration 55, loss = 0.56275171\n",
            "Iteration 56, loss = 0.56298189\n",
            "Iteration 57, loss = 0.56225572\n",
            "Iteration 58, loss = 0.56219053\n",
            "Iteration 59, loss = 0.56154565\n",
            "Iteration 60, loss = 0.56171739\n",
            "Iteration 61, loss = 0.56100325\n",
            "Iteration 62, loss = 0.56101927\n",
            "Iteration 63, loss = 0.55986985\n",
            "Iteration 64, loss = 0.56077131\n",
            "Iteration 65, loss = 0.55904445\n",
            "Iteration 66, loss = 0.55938216\n",
            "Iteration 67, loss = 0.55867163\n",
            "Iteration 68, loss = 0.55888353\n",
            "Iteration 69, loss = 0.55801101\n",
            "Iteration 70, loss = 0.55792352\n",
            "Iteration 71, loss = 0.55727527\n",
            "Iteration 72, loss = 0.55655032\n",
            "Iteration 73, loss = 0.55528712\n",
            "Iteration 74, loss = 0.55448831\n",
            "Iteration 75, loss = 0.55466998\n",
            "Iteration 76, loss = 0.55426650\n",
            "Iteration 77, loss = 0.55296072\n",
            "Iteration 78, loss = 0.55310416\n",
            "Iteration 79, loss = 0.55097673\n",
            "Iteration 80, loss = 0.55054860\n",
            "Iteration 81, loss = 0.55019259\n",
            "Iteration 82, loss = 0.54977433\n",
            "Iteration 83, loss = 0.54932928\n",
            "Iteration 84, loss = 0.54763824\n",
            "Iteration 85, loss = 0.54801053\n",
            "Iteration 86, loss = 0.54679100\n",
            "Iteration 87, loss = 0.54596826\n",
            "Iteration 88, loss = 0.54450241\n",
            "Iteration 89, loss = 0.54488931\n",
            "Iteration 90, loss = 0.54325063\n",
            "Iteration 91, loss = 0.54390749\n",
            "Iteration 92, loss = 0.54221148\n",
            "Iteration 93, loss = 0.54224623\n",
            "Iteration 94, loss = 0.54138514\n",
            "Iteration 95, loss = 0.54074459\n",
            "Iteration 96, loss = 0.54053204\n",
            "Iteration 97, loss = 0.53970210\n",
            "Iteration 98, loss = 0.53865506\n",
            "Iteration 99, loss = 0.53834086\n",
            "Iteration 100, loss = 0.53792385\n",
            "Iteration 101, loss = 0.53701134\n",
            "Iteration 102, loss = 0.53695200\n",
            "Iteration 103, loss = 0.53588309\n",
            "Iteration 104, loss = 0.53609629\n",
            "Iteration 105, loss = 0.53448389\n",
            "Iteration 106, loss = 0.53544207\n",
            "Iteration 107, loss = 0.53430294\n",
            "Iteration 108, loss = 0.53445993\n",
            "Iteration 109, loss = 0.53461916\n",
            "Iteration 110, loss = 0.53304445\n",
            "Iteration 111, loss = 0.53319443\n",
            "Iteration 112, loss = 0.53308495\n",
            "Iteration 113, loss = 0.53315623\n",
            "Iteration 114, loss = 0.53266364\n",
            "Iteration 115, loss = 0.53191944\n",
            "Iteration 116, loss = 0.53147197\n",
            "Iteration 117, loss = 0.53041994\n",
            "Iteration 118, loss = 0.53081406\n",
            "Iteration 119, loss = 0.53116070\n",
            "Iteration 120, loss = 0.53060527\n",
            "Iteration 121, loss = 0.52982635\n",
            "Iteration 122, loss = 0.53087165\n",
            "Iteration 123, loss = 0.53020567\n",
            "Iteration 124, loss = 0.53024245\n",
            "Iteration 125, loss = 0.52932597\n",
            "Iteration 126, loss = 0.52915901\n",
            "Iteration 127, loss = 0.52917861\n",
            "Iteration 128, loss = 0.52858311\n",
            "Iteration 129, loss = 0.52886460\n",
            "Iteration 130, loss = 0.52911009\n",
            "Iteration 131, loss = 0.52783581\n",
            "Iteration 132, loss = 0.52800159\n",
            "Iteration 133, loss = 0.52747103\n",
            "Iteration 134, loss = 0.52857234\n",
            "Iteration 135, loss = 0.52798470\n",
            "Iteration 136, loss = 0.52796159\n",
            "Iteration 137, loss = 0.52732676\n",
            "Iteration 138, loss = 0.52666621\n",
            "Iteration 139, loss = 0.52660582\n",
            "Iteration 140, loss = 0.52699551\n",
            "Iteration 141, loss = 0.52685964\n",
            "Iteration 142, loss = 0.52680209\n",
            "Iteration 143, loss = 0.52648883\n",
            "Iteration 144, loss = 0.52750801\n",
            "Iteration 145, loss = 0.52626566\n",
            "Iteration 146, loss = 0.52556819\n",
            "Iteration 147, loss = 0.52550644\n",
            "Iteration 148, loss = 0.52626565\n",
            "Iteration 149, loss = 0.52558534\n",
            "Iteration 150, loss = 0.52607868\n",
            "Iteration 151, loss = 0.52573076\n",
            "Iteration 152, loss = 0.52521709\n",
            "Iteration 153, loss = 0.52417354\n",
            "Iteration 154, loss = 0.52517611\n",
            "Iteration 155, loss = 0.52504641\n",
            "Iteration 156, loss = 0.52500946\n",
            "Iteration 157, loss = 0.52412274\n",
            "Iteration 158, loss = 0.52489358\n",
            "Iteration 159, loss = 0.52469313\n",
            "Iteration 160, loss = 0.52439740\n",
            "Iteration 161, loss = 0.52475464\n",
            "Iteration 162, loss = 0.52494940\n",
            "Iteration 163, loss = 0.52402163\n",
            "Iteration 164, loss = 0.52334214\n",
            "Iteration 165, loss = 0.52412381\n",
            "Iteration 166, loss = 0.52413846\n",
            "Iteration 167, loss = 0.52366776\n",
            "Iteration 168, loss = 0.52353480\n",
            "Iteration 169, loss = 0.52442263\n",
            "Iteration 170, loss = 0.52351535\n",
            "Iteration 171, loss = 0.52332983\n",
            "Iteration 172, loss = 0.52370195\n",
            "Iteration 173, loss = 0.52367153\n",
            "Iteration 174, loss = 0.52378976\n",
            "Iteration 175, loss = 0.52472032\n",
            "Iteration 176, loss = 0.52354654\n",
            "Iteration 177, loss = 0.52354108\n",
            "Iteration 178, loss = 0.52300006\n",
            "Iteration 179, loss = 0.52387064\n",
            "Iteration 180, loss = 0.52277406\n",
            "Iteration 181, loss = 0.52346696\n",
            "Iteration 182, loss = 0.52396509\n",
            "Iteration 183, loss = 0.52268176\n",
            "Iteration 184, loss = 0.52320652\n",
            "Iteration 185, loss = 0.52285027\n",
            "Iteration 186, loss = 0.52305965\n",
            "Iteration 187, loss = 0.52207391\n",
            "Iteration 188, loss = 0.52338997\n",
            "Iteration 189, loss = 0.52328803\n",
            "Iteration 190, loss = 0.52279225\n",
            "Iteration 191, loss = 0.52315771\n",
            "Iteration 192, loss = 0.52216199\n",
            "Iteration 193, loss = 0.52294144\n",
            "Iteration 194, loss = 0.52293221\n",
            "Iteration 195, loss = 0.52325524\n",
            "Iteration 196, loss = 0.52160371\n",
            "Iteration 197, loss = 0.52328151\n",
            "Iteration 198, loss = 0.52224573\n",
            "Iteration 199, loss = 0.52296619\n",
            "Iteration 200, loss = 0.52231613\n",
            "Iteration 201, loss = 0.52207908\n",
            "Iteration 202, loss = 0.52187179\n",
            "Iteration 203, loss = 0.52354418\n",
            "Iteration 204, loss = 0.52140232\n",
            "Iteration 205, loss = 0.52237080\n",
            "Iteration 206, loss = 0.52255586\n",
            "Iteration 207, loss = 0.52328939\n",
            "Iteration 208, loss = 0.52268145\n",
            "Iteration 209, loss = 0.52214117\n",
            "Iteration 210, loss = 0.52191114\n",
            "Iteration 211, loss = 0.52244862\n",
            "Iteration 212, loss = 0.52263576\n",
            "Iteration 213, loss = 0.52197283\n",
            "Iteration 214, loss = 0.52256797\n",
            "Iteration 215, loss = 0.52242747\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56241359\n",
            "Iteration 2, loss = 0.56230071\n",
            "Iteration 3, loss = 0.56155098\n",
            "Iteration 4, loss = 0.56232198\n",
            "Iteration 5, loss = 0.56178770\n",
            "Iteration 6, loss = 0.56103631\n",
            "Iteration 7, loss = 0.56140831\n",
            "Iteration 8, loss = 0.56125292\n",
            "Iteration 9, loss = 0.56121248\n",
            "Iteration 10, loss = 0.56148830\n",
            "Iteration 11, loss = 0.56214920\n",
            "Iteration 12, loss = 0.56149313\n",
            "Iteration 13, loss = 0.56146896\n",
            "Iteration 14, loss = 0.56056540\n",
            "Iteration 15, loss = 0.56221007\n",
            "Iteration 16, loss = 0.56130956\n",
            "Iteration 17, loss = 0.56131665\n",
            "Iteration 18, loss = 0.56149213\n",
            "Iteration 19, loss = 0.56087400\n",
            "Iteration 20, loss = 0.56068129\n",
            "Iteration 21, loss = 0.56086805\n",
            "Iteration 22, loss = 0.56039132\n",
            "Iteration 23, loss = 0.56124461\n",
            "Iteration 24, loss = 0.56004746\n",
            "Iteration 25, loss = 0.56047685\n",
            "Iteration 26, loss = 0.56139026\n",
            "Iteration 27, loss = 0.56022428\n",
            "Iteration 28, loss = 0.56083230\n",
            "Iteration 29, loss = 0.56051317\n",
            "Iteration 30, loss = 0.55998117\n",
            "Iteration 31, loss = 0.56048508\n",
            "Iteration 32, loss = 0.55983244\n",
            "Iteration 33, loss = 0.56011029\n",
            "Iteration 34, loss = 0.56063955\n",
            "Iteration 35, loss = 0.56006525\n",
            "Iteration 36, loss = 0.56046722\n",
            "Iteration 37, loss = 0.55918869\n",
            "Iteration 38, loss = 0.55988333\n",
            "Iteration 39, loss = 0.55990891\n",
            "Iteration 40, loss = 0.55942657\n",
            "Iteration 41, loss = 0.55971080\n",
            "Iteration 42, loss = 0.55883776\n",
            "Iteration 43, loss = 0.55887004\n",
            "Iteration 44, loss = 0.55934255\n",
            "Iteration 45, loss = 0.55923333\n",
            "Iteration 46, loss = 0.55897390\n",
            "Iteration 47, loss = 0.55912339\n",
            "Iteration 48, loss = 0.55815368\n",
            "Iteration 49, loss = 0.55880234\n",
            "Iteration 50, loss = 0.55873446\n",
            "Iteration 51, loss = 0.55800279\n",
            "Iteration 52, loss = 0.55808812\n",
            "Iteration 53, loss = 0.55867024\n",
            "Iteration 54, loss = 0.55746434\n",
            "Iteration 55, loss = 0.55783343\n",
            "Iteration 56, loss = 0.55718879\n",
            "Iteration 57, loss = 0.55713764\n",
            "Iteration 58, loss = 0.55731730\n",
            "Iteration 59, loss = 0.55698805\n",
            "Iteration 60, loss = 0.55671195\n",
            "Iteration 61, loss = 0.55582897\n",
            "Iteration 62, loss = 0.55622720\n",
            "Iteration 63, loss = 0.55582436\n",
            "Iteration 64, loss = 0.55499535\n",
            "Iteration 65, loss = 0.55571734\n",
            "Iteration 66, loss = 0.55554486\n",
            "Iteration 67, loss = 0.55410106\n",
            "Iteration 68, loss = 0.55441750\n",
            "Iteration 69, loss = 0.55388903\n",
            "Iteration 70, loss = 0.55316153\n",
            "Iteration 71, loss = 0.55327450\n",
            "Iteration 72, loss = 0.55335579\n",
            "Iteration 73, loss = 0.55220214\n",
            "Iteration 74, loss = 0.55147900\n",
            "Iteration 75, loss = 0.55123081\n",
            "Iteration 76, loss = 0.55166112\n",
            "Iteration 77, loss = 0.55056617\n",
            "Iteration 78, loss = 0.55038427\n",
            "Iteration 79, loss = 0.54913101\n",
            "Iteration 80, loss = 0.54868719\n",
            "Iteration 81, loss = 0.54707749\n",
            "Iteration 82, loss = 0.54683870\n",
            "Iteration 83, loss = 0.54719195\n",
            "Iteration 84, loss = 0.54576853\n",
            "Iteration 85, loss = 0.54601815\n",
            "Iteration 86, loss = 0.54485606\n",
            "Iteration 87, loss = 0.54495870\n",
            "Iteration 88, loss = 0.54350967\n",
            "Iteration 89, loss = 0.54299469\n",
            "Iteration 90, loss = 0.54336140\n",
            "Iteration 91, loss = 0.54203129\n",
            "Iteration 92, loss = 0.54130892\n",
            "Iteration 93, loss = 0.54072146\n",
            "Iteration 94, loss = 0.53938920\n",
            "Iteration 95, loss = 0.53950504\n",
            "Iteration 96, loss = 0.53860783\n",
            "Iteration 97, loss = 0.53778152\n",
            "Iteration 98, loss = 0.53787066\n",
            "Iteration 99, loss = 0.53771193\n",
            "Iteration 100, loss = 0.53609429\n",
            "Iteration 101, loss = 0.53549767\n",
            "Iteration 102, loss = 0.53529479\n",
            "Iteration 103, loss = 0.53498673\n",
            "Iteration 104, loss = 0.53446515\n",
            "Iteration 105, loss = 0.53360316\n",
            "Iteration 106, loss = 0.53372869\n",
            "Iteration 107, loss = 0.53256211\n",
            "Iteration 108, loss = 0.53250039\n",
            "Iteration 109, loss = 0.53185356\n",
            "Iteration 110, loss = 0.53160879\n",
            "Iteration 111, loss = 0.53117842\n",
            "Iteration 112, loss = 0.53098693\n",
            "Iteration 113, loss = 0.53056754\n",
            "Iteration 114, loss = 0.52960872\n",
            "Iteration 115, loss = 0.53001467\n",
            "Iteration 116, loss = 0.52976133\n",
            "Iteration 117, loss = 0.52896013\n",
            "Iteration 118, loss = 0.52886101\n",
            "Iteration 119, loss = 0.52824329\n",
            "Iteration 120, loss = 0.52779367\n",
            "Iteration 121, loss = 0.52751244\n",
            "Iteration 122, loss = 0.52804707\n",
            "Iteration 123, loss = 0.52689055\n",
            "Iteration 124, loss = 0.52769659\n",
            "Iteration 125, loss = 0.52749507\n",
            "Iteration 126, loss = 0.52664402\n",
            "Iteration 127, loss = 0.52707023\n",
            "Iteration 128, loss = 0.52681069\n",
            "Iteration 129, loss = 0.52599651\n",
            "Iteration 130, loss = 0.52670198\n",
            "Iteration 131, loss = 0.52602178\n",
            "Iteration 132, loss = 0.52564409\n",
            "Iteration 133, loss = 0.52563820\n",
            "Iteration 134, loss = 0.52551170\n",
            "Iteration 135, loss = 0.52472430\n",
            "Iteration 136, loss = 0.52391641\n",
            "Iteration 137, loss = 0.52540247\n",
            "Iteration 138, loss = 0.52457779\n",
            "Iteration 139, loss = 0.52457527\n",
            "Iteration 140, loss = 0.52484661\n",
            "Iteration 141, loss = 0.52344067\n",
            "Iteration 142, loss = 0.52360451\n",
            "Iteration 143, loss = 0.52336077\n",
            "Iteration 144, loss = 0.52361823\n",
            "Iteration 145, loss = 0.52361306\n",
            "Iteration 146, loss = 0.52231843\n",
            "Iteration 147, loss = 0.52356411\n",
            "Iteration 148, loss = 0.52201353\n",
            "Iteration 149, loss = 0.52241295\n",
            "Iteration 150, loss = 0.52339701\n",
            "Iteration 151, loss = 0.52215610\n",
            "Iteration 152, loss = 0.52263328\n",
            "Iteration 153, loss = 0.52227731\n",
            "Iteration 154, loss = 0.52255876\n",
            "Iteration 155, loss = 0.52146253\n",
            "Iteration 156, loss = 0.52220777\n",
            "Iteration 157, loss = 0.52313393\n",
            "Iteration 158, loss = 0.52112756\n",
            "Iteration 159, loss = 0.52148771\n",
            "Iteration 160, loss = 0.52158789\n",
            "Iteration 161, loss = 0.52158733\n",
            "Iteration 162, loss = 0.52189944\n",
            "Iteration 163, loss = 0.52151117\n",
            "Iteration 164, loss = 0.52072578\n",
            "Iteration 165, loss = 0.52042377\n",
            "Iteration 166, loss = 0.52168208\n",
            "Iteration 167, loss = 0.52116246\n",
            "Iteration 168, loss = 0.52101530\n",
            "Iteration 169, loss = 0.52078504\n",
            "Iteration 170, loss = 0.52159336\n",
            "Iteration 171, loss = 0.52101115\n",
            "Iteration 172, loss = 0.52156371\n",
            "Iteration 173, loss = 0.52034170\n",
            "Iteration 174, loss = 0.52103012\n",
            "Iteration 175, loss = 0.52142789\n",
            "Iteration 176, loss = 0.52114924\n",
            "Iteration 177, loss = 0.52059816\n",
            "Iteration 178, loss = 0.52054500\n",
            "Iteration 179, loss = 0.52036994\n",
            "Iteration 180, loss = 0.52017690\n",
            "Iteration 181, loss = 0.51999955\n",
            "Iteration 182, loss = 0.51995722\n",
            "Iteration 183, loss = 0.52085236\n",
            "Iteration 184, loss = 0.52010296\n",
            "Iteration 185, loss = 0.51946590\n",
            "Iteration 186, loss = 0.51971650\n",
            "Iteration 187, loss = 0.52045577\n",
            "Iteration 188, loss = 0.52040333\n",
            "Iteration 189, loss = 0.52013035\n",
            "Iteration 190, loss = 0.52042740\n",
            "Iteration 191, loss = 0.51900432\n",
            "Iteration 192, loss = 0.51913998\n",
            "Iteration 193, loss = 0.51988955\n",
            "Iteration 194, loss = 0.51989915\n",
            "Iteration 195, loss = 0.52058972\n",
            "Iteration 196, loss = 0.52036300\n",
            "Iteration 197, loss = 0.51950405\n",
            "Iteration 198, loss = 0.51970519\n",
            "Iteration 199, loss = 0.51894391\n",
            "Iteration 200, loss = 0.52034213\n",
            "Iteration 201, loss = 0.51927724\n",
            "Iteration 202, loss = 0.51994959\n",
            "Iteration 203, loss = 0.52026689\n",
            "Iteration 204, loss = 0.51938471\n",
            "Iteration 205, loss = 0.51947013\n",
            "Iteration 206, loss = 0.51984150\n",
            "Iteration 207, loss = 0.51926778\n",
            "Iteration 208, loss = 0.51918001\n",
            "Iteration 209, loss = 0.51930964\n",
            "Iteration 210, loss = 0.51955174\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56317639\n",
            "Iteration 2, loss = 0.56309254\n",
            "Iteration 3, loss = 0.56236158\n",
            "Iteration 4, loss = 0.56296936\n",
            "Iteration 5, loss = 0.56317013\n",
            "Iteration 6, loss = 0.56283078\n",
            "Iteration 7, loss = 0.56238445\n",
            "Iteration 8, loss = 0.56296061\n",
            "Iteration 9, loss = 0.56269808\n",
            "Iteration 10, loss = 0.56218466\n",
            "Iteration 11, loss = 0.56197126\n",
            "Iteration 12, loss = 0.56250071\n",
            "Iteration 13, loss = 0.56221547\n",
            "Iteration 14, loss = 0.56256025\n",
            "Iteration 15, loss = 0.56220087\n",
            "Iteration 16, loss = 0.56224760\n",
            "Iteration 17, loss = 0.56194387\n",
            "Iteration 18, loss = 0.56158035\n",
            "Iteration 19, loss = 0.56179901\n",
            "Iteration 20, loss = 0.56181103\n",
            "Iteration 21, loss = 0.56186700\n",
            "Iteration 22, loss = 0.56129912\n",
            "Iteration 23, loss = 0.56157150\n",
            "Iteration 24, loss = 0.56070695\n",
            "Iteration 25, loss = 0.56121901\n",
            "Iteration 26, loss = 0.56071568\n",
            "Iteration 27, loss = 0.56098441\n",
            "Iteration 28, loss = 0.56065428\n",
            "Iteration 29, loss = 0.56076460\n",
            "Iteration 30, loss = 0.56078222\n",
            "Iteration 31, loss = 0.56065079\n",
            "Iteration 32, loss = 0.56048198\n",
            "Iteration 33, loss = 0.56033490\n",
            "Iteration 34, loss = 0.56018134\n",
            "Iteration 35, loss = 0.56039657\n",
            "Iteration 36, loss = 0.55923554\n",
            "Iteration 37, loss = 0.55944719\n",
            "Iteration 38, loss = 0.55872075\n",
            "Iteration 39, loss = 0.55839387\n",
            "Iteration 40, loss = 0.55892853\n",
            "Iteration 41, loss = 0.55870269\n",
            "Iteration 42, loss = 0.55812704\n",
            "Iteration 43, loss = 0.55708068\n",
            "Iteration 44, loss = 0.55803931\n",
            "Iteration 45, loss = 0.55739495\n",
            "Iteration 46, loss = 0.55650132\n",
            "Iteration 47, loss = 0.55690387\n",
            "Iteration 48, loss = 0.55674482\n",
            "Iteration 49, loss = 0.55674976\n",
            "Iteration 50, loss = 0.55575596\n",
            "Iteration 51, loss = 0.55467662\n",
            "Iteration 52, loss = 0.55485958\n",
            "Iteration 53, loss = 0.55439084\n",
            "Iteration 54, loss = 0.55420375\n",
            "Iteration 55, loss = 0.55296912\n",
            "Iteration 56, loss = 0.55285029\n",
            "Iteration 57, loss = 0.55260672\n",
            "Iteration 58, loss = 0.55160615\n",
            "Iteration 59, loss = 0.55200800\n",
            "Iteration 60, loss = 0.55229619\n",
            "Iteration 61, loss = 0.54950900\n",
            "Iteration 62, loss = 0.54980652\n",
            "Iteration 63, loss = 0.54969897\n",
            "Iteration 64, loss = 0.54873244\n",
            "Iteration 65, loss = 0.54791010\n",
            "Iteration 66, loss = 0.54721641\n",
            "Iteration 67, loss = 0.54551459\n",
            "Iteration 68, loss = 0.54609260\n",
            "Iteration 69, loss = 0.54552079\n",
            "Iteration 70, loss = 0.54517832\n",
            "Iteration 71, loss = 0.54429816\n",
            "Iteration 72, loss = 0.54248493\n",
            "Iteration 73, loss = 0.54175232\n",
            "Iteration 74, loss = 0.54061799\n",
            "Iteration 75, loss = 0.54072193\n",
            "Iteration 76, loss = 0.53996120\n",
            "Iteration 77, loss = 0.53945897\n",
            "Iteration 78, loss = 0.53748628\n",
            "Iteration 79, loss = 0.53779583\n",
            "Iteration 80, loss = 0.53704439\n",
            "Iteration 81, loss = 0.53647957\n",
            "Iteration 82, loss = 0.53538187\n",
            "Iteration 83, loss = 0.53554706\n",
            "Iteration 84, loss = 0.53474774\n",
            "Iteration 85, loss = 0.53404699\n",
            "Iteration 86, loss = 0.53367859\n",
            "Iteration 87, loss = 0.53285471\n",
            "Iteration 88, loss = 0.53224484\n",
            "Iteration 89, loss = 0.53240195\n",
            "Iteration 90, loss = 0.53091772\n",
            "Iteration 91, loss = 0.53073376\n",
            "Iteration 92, loss = 0.53066649\n",
            "Iteration 93, loss = 0.53034424\n",
            "Iteration 94, loss = 0.52967521\n",
            "Iteration 95, loss = 0.52922539\n",
            "Iteration 96, loss = 0.52847911\n",
            "Iteration 97, loss = 0.52865756\n",
            "Iteration 98, loss = 0.52856113\n",
            "Iteration 99, loss = 0.52873355\n",
            "Iteration 100, loss = 0.52758456\n",
            "Iteration 101, loss = 0.52738505\n",
            "Iteration 102, loss = 0.52667213\n",
            "Iteration 103, loss = 0.52704444\n",
            "Iteration 104, loss = 0.52654946\n",
            "Iteration 105, loss = 0.52640348\n",
            "Iteration 106, loss = 0.52598968\n",
            "Iteration 107, loss = 0.52551278\n",
            "Iteration 108, loss = 0.52578092\n",
            "Iteration 109, loss = 0.52542185\n",
            "Iteration 110, loss = 0.52479847\n",
            "Iteration 111, loss = 0.52477852\n",
            "Iteration 112, loss = 0.52448595\n",
            "Iteration 113, loss = 0.52481422\n",
            "Iteration 114, loss = 0.52443236\n",
            "Iteration 115, loss = 0.52471747\n",
            "Iteration 116, loss = 0.52246956\n",
            "Iteration 117, loss = 0.52376694\n",
            "Iteration 118, loss = 0.52404646\n",
            "Iteration 119, loss = 0.52336800\n",
            "Iteration 120, loss = 0.52362402\n",
            "Iteration 121, loss = 0.52383563\n",
            "Iteration 122, loss = 0.52381346\n",
            "Iteration 123, loss = 0.52310492\n",
            "Iteration 124, loss = 0.52381516\n",
            "Iteration 125, loss = 0.52273804\n",
            "Iteration 126, loss = 0.52308085\n",
            "Iteration 127, loss = 0.52288807\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56737779\n",
            "Iteration 2, loss = 0.56629975\n",
            "Iteration 3, loss = 0.56791685\n",
            "Iteration 4, loss = 0.56654955\n",
            "Iteration 5, loss = 0.56655716\n",
            "Iteration 6, loss = 0.56681021\n",
            "Iteration 7, loss = 0.56657699\n",
            "Iteration 8, loss = 0.56648201\n",
            "Iteration 9, loss = 0.56730004\n",
            "Iteration 10, loss = 0.56765458\n",
            "Iteration 11, loss = 0.56690373\n",
            "Iteration 12, loss = 0.56629700\n",
            "Iteration 13, loss = 0.56606843\n",
            "Iteration 14, loss = 0.56637177\n",
            "Iteration 15, loss = 0.56593654\n",
            "Iteration 16, loss = 0.56595775\n",
            "Iteration 17, loss = 0.56694981\n",
            "Iteration 18, loss = 0.56483280\n",
            "Iteration 19, loss = 0.56596236\n",
            "Iteration 20, loss = 0.56584805\n",
            "Iteration 21, loss = 0.56589830\n",
            "Iteration 22, loss = 0.56584293\n",
            "Iteration 23, loss = 0.56543185\n",
            "Iteration 24, loss = 0.56429291\n",
            "Iteration 25, loss = 0.56575345\n",
            "Iteration 26, loss = 0.56630797\n",
            "Iteration 27, loss = 0.56619958\n",
            "Iteration 28, loss = 0.56557106\n",
            "Iteration 29, loss = 0.56547350\n",
            "Iteration 30, loss = 0.56448682\n",
            "Iteration 31, loss = 0.56569420\n",
            "Iteration 32, loss = 0.56612088\n",
            "Iteration 33, loss = 0.56507522\n",
            "Iteration 34, loss = 0.56464995\n",
            "Iteration 35, loss = 0.56506921\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56737950\n",
            "Iteration 2, loss = 0.55800624\n",
            "Iteration 3, loss = 0.55940532\n",
            "Iteration 4, loss = 0.55735767\n",
            "Iteration 5, loss = 0.55912014\n",
            "Iteration 6, loss = 0.55846476\n",
            "Iteration 7, loss = 0.55820787\n",
            "Iteration 8, loss = 0.55884258\n",
            "Iteration 9, loss = 0.55759453\n",
            "Iteration 10, loss = 0.55782413\n",
            "Iteration 11, loss = 0.55905836\n",
            "Iteration 12, loss = 0.55817305\n",
            "Iteration 13, loss = 0.55878558\n",
            "Iteration 14, loss = 0.55816508\n",
            "Iteration 15, loss = 0.55672740\n",
            "Iteration 16, loss = 0.55840798\n",
            "Iteration 17, loss = 0.55771045\n",
            "Iteration 18, loss = 0.55757752\n",
            "Iteration 19, loss = 0.55791807\n",
            "Iteration 20, loss = 0.55794303\n",
            "Iteration 21, loss = 0.55785388\n",
            "Iteration 22, loss = 0.55842383\n",
            "Iteration 23, loss = 0.55741632\n",
            "Iteration 24, loss = 0.55739684\n",
            "Iteration 25, loss = 0.55752498\n",
            "Iteration 26, loss = 0.55696015\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "11\n",
            "Iteration 1, loss = 0.56824388\n",
            "Iteration 2, loss = 0.56257170\n",
            "Iteration 3, loss = 0.56203477\n",
            "Iteration 4, loss = 0.56195812\n",
            "Iteration 5, loss = 0.56159552\n",
            "Iteration 6, loss = 0.56245388\n",
            "Iteration 7, loss = 0.56198306\n",
            "Iteration 8, loss = 0.56221823\n",
            "Iteration 9, loss = 0.56175618\n",
            "Iteration 10, loss = 0.56196728\n",
            "Iteration 11, loss = 0.56188461\n",
            "Iteration 12, loss = 0.56075467\n",
            "Iteration 13, loss = 0.56126735\n",
            "Iteration 14, loss = 0.56157788\n",
            "Iteration 15, loss = 0.56183740\n",
            "Iteration 16, loss = 0.56033713\n",
            "Iteration 17, loss = 0.56141581\n",
            "Iteration 18, loss = 0.56168471\n",
            "Iteration 19, loss = 0.56088388\n",
            "Iteration 20, loss = 0.56197193\n",
            "Iteration 21, loss = 0.56025749\n",
            "Iteration 22, loss = 0.56082763\n",
            "Iteration 23, loss = 0.56091363\n",
            "Iteration 24, loss = 0.56177103\n",
            "Iteration 25, loss = 0.56026281\n",
            "Iteration 26, loss = 0.56105631\n",
            "Iteration 27, loss = 0.56090571\n",
            "Iteration 28, loss = 0.56013501\n",
            "Iteration 29, loss = 0.56067247\n",
            "Iteration 30, loss = 0.55999156\n",
            "Iteration 31, loss = 0.56038245\n",
            "Iteration 32, loss = 0.56032636\n",
            "Iteration 33, loss = 0.56027670\n",
            "Iteration 34, loss = 0.55946769\n",
            "Iteration 35, loss = 0.55922790\n",
            "Iteration 36, loss = 0.55996751\n",
            "Iteration 37, loss = 0.55971290\n",
            "Iteration 38, loss = 0.55958112\n",
            "Iteration 39, loss = 0.55913096\n",
            "Iteration 40, loss = 0.55905192\n",
            "Iteration 41, loss = 0.55979930\n",
            "Iteration 42, loss = 0.55836073\n",
            "Iteration 43, loss = 0.55846860\n",
            "Iteration 44, loss = 0.55797930\n",
            "Iteration 45, loss = 0.55881828\n",
            "Iteration 46, loss = 0.55779802\n",
            "Iteration 47, loss = 0.55758546\n",
            "Iteration 48, loss = 0.55831422\n",
            "Iteration 49, loss = 0.55736504\n",
            "Iteration 50, loss = 0.55728156\n",
            "Iteration 51, loss = 0.55639818\n",
            "Iteration 52, loss = 0.55686678\n",
            "Iteration 53, loss = 0.55651896\n",
            "Iteration 54, loss = 0.55570794\n",
            "Iteration 55, loss = 0.55461090\n",
            "Iteration 56, loss = 0.55517911\n",
            "Iteration 57, loss = 0.55535047\n",
            "Iteration 58, loss = 0.55524386\n",
            "Iteration 59, loss = 0.55348343\n",
            "Iteration 60, loss = 0.55381447\n",
            "Iteration 61, loss = 0.55364010\n",
            "Iteration 62, loss = 0.55318172\n",
            "Iteration 63, loss = 0.55286052\n",
            "Iteration 64, loss = 0.55208498\n",
            "Iteration 65, loss = 0.55093464\n",
            "Iteration 66, loss = 0.55029642\n",
            "Iteration 67, loss = 0.54990413\n",
            "Iteration 68, loss = 0.55000130\n",
            "Iteration 69, loss = 0.54945383\n",
            "Iteration 70, loss = 0.54938686\n",
            "Iteration 71, loss = 0.54841838\n",
            "Iteration 72, loss = 0.54766609\n",
            "Iteration 73, loss = 0.54698958\n",
            "Iteration 74, loss = 0.54561748\n",
            "Iteration 75, loss = 0.54509253\n",
            "Iteration 76, loss = 0.54399018\n",
            "Iteration 77, loss = 0.54327800\n",
            "Iteration 78, loss = 0.54327113\n",
            "Iteration 79, loss = 0.54236732\n",
            "Iteration 80, loss = 0.54202725\n",
            "Iteration 81, loss = 0.54107218\n",
            "Iteration 82, loss = 0.54034838\n",
            "Iteration 83, loss = 0.53960778\n",
            "Iteration 84, loss = 0.53935054\n",
            "Iteration 85, loss = 0.53847449\n",
            "Iteration 86, loss = 0.53705132\n",
            "Iteration 87, loss = 0.53580218\n",
            "Iteration 88, loss = 0.53664169\n",
            "Iteration 89, loss = 0.53582477\n",
            "Iteration 90, loss = 0.53357115\n",
            "Iteration 91, loss = 0.53417864\n",
            "Iteration 92, loss = 0.53392169\n",
            "Iteration 93, loss = 0.53270453\n",
            "Iteration 94, loss = 0.53230293\n",
            "Iteration 95, loss = 0.53184771\n",
            "Iteration 96, loss = 0.53133911\n",
            "Iteration 97, loss = 0.53120837\n",
            "Iteration 98, loss = 0.53083094\n",
            "Iteration 99, loss = 0.52898991\n",
            "Iteration 100, loss = 0.52992570\n",
            "Iteration 101, loss = 0.52978124\n",
            "Iteration 102, loss = 0.52923697\n",
            "Iteration 103, loss = 0.52788933\n",
            "Iteration 104, loss = 0.52727479\n",
            "Iteration 105, loss = 0.52801394\n",
            "Iteration 106, loss = 0.52694416\n",
            "Iteration 107, loss = 0.52691316\n",
            "Iteration 108, loss = 0.52696970\n",
            "Iteration 109, loss = 0.52723421\n",
            "Iteration 110, loss = 0.52662630\n",
            "Iteration 111, loss = 0.52550625\n",
            "Iteration 112, loss = 0.52603223\n",
            "Iteration 113, loss = 0.52560591\n",
            "Iteration 114, loss = 0.52470123\n",
            "Iteration 115, loss = 0.52474932\n",
            "Iteration 116, loss = 0.52356785\n",
            "Iteration 117, loss = 0.52320135\n",
            "Iteration 118, loss = 0.52330001\n",
            "Iteration 119, loss = 0.52444382\n",
            "Iteration 120, loss = 0.52384577\n",
            "Iteration 121, loss = 0.52330842\n",
            "Iteration 122, loss = 0.52334606\n",
            "Iteration 123, loss = 0.52343039\n",
            "Iteration 124, loss = 0.52225257\n",
            "Iteration 125, loss = 0.52261488\n",
            "Iteration 126, loss = 0.52221838\n",
            "Iteration 127, loss = 0.52209004\n",
            "Iteration 128, loss = 0.52169883\n",
            "Iteration 129, loss = 0.52245974\n",
            "Iteration 130, loss = 0.52149094\n",
            "Iteration 131, loss = 0.52127846\n",
            "Iteration 132, loss = 0.52214909\n",
            "Iteration 133, loss = 0.52130885\n",
            "Iteration 134, loss = 0.52064612\n",
            "Iteration 135, loss = 0.51992241\n",
            "Iteration 136, loss = 0.52119753\n",
            "Iteration 137, loss = 0.52138440\n",
            "Iteration 138, loss = 0.52123398\n",
            "Iteration 139, loss = 0.52015843\n",
            "Iteration 140, loss = 0.52172276\n",
            "Iteration 141, loss = 0.51975184\n",
            "Iteration 142, loss = 0.52053018\n",
            "Iteration 143, loss = 0.51964619\n",
            "Iteration 144, loss = 0.51976493\n",
            "Iteration 145, loss = 0.51925047\n",
            "Iteration 146, loss = 0.51878316\n",
            "Iteration 147, loss = 0.51924116\n",
            "Iteration 148, loss = 0.51924997\n",
            "Iteration 149, loss = 0.51969118\n",
            "Iteration 150, loss = 0.51961452\n",
            "Iteration 151, loss = 0.51919659\n",
            "Iteration 152, loss = 0.51977265\n",
            "Iteration 153, loss = 0.51892949\n",
            "Iteration 154, loss = 0.51836816\n",
            "Iteration 155, loss = 0.51817209\n",
            "Iteration 156, loss = 0.51866339\n",
            "Iteration 157, loss = 0.51862987\n",
            "Iteration 158, loss = 0.51815634\n",
            "Iteration 159, loss = 0.51704393\n",
            "Iteration 160, loss = 0.51855532\n",
            "Iteration 161, loss = 0.51881426\n",
            "Iteration 162, loss = 0.51752767\n",
            "Iteration 163, loss = 0.51863625\n",
            "Iteration 164, loss = 0.51853453\n",
            "Iteration 165, loss = 0.51714764\n",
            "Iteration 166, loss = 0.51826471\n",
            "Iteration 167, loss = 0.51781646\n",
            "Iteration 168, loss = 0.51862098\n",
            "Iteration 169, loss = 0.51795309\n",
            "Iteration 170, loss = 0.51789394\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56992639\n",
            "Iteration 2, loss = 0.56597610\n",
            "Iteration 3, loss = 0.56629473\n",
            "Iteration 4, loss = 0.56602351\n",
            "Iteration 5, loss = 0.56555819\n",
            "Iteration 6, loss = 0.56546362\n",
            "Iteration 7, loss = 0.56538309\n",
            "Iteration 8, loss = 0.56452889\n",
            "Iteration 9, loss = 0.56554268\n",
            "Iteration 10, loss = 0.56446971\n",
            "Iteration 11, loss = 0.56565403\n",
            "Iteration 12, loss = 0.56545049\n",
            "Iteration 13, loss = 0.56544607\n",
            "Iteration 14, loss = 0.56517789\n",
            "Iteration 15, loss = 0.56513383\n",
            "Iteration 16, loss = 0.56549844\n",
            "Iteration 17, loss = 0.56528115\n",
            "Iteration 18, loss = 0.56525799\n",
            "Iteration 19, loss = 0.56542071\n",
            "Iteration 20, loss = 0.56505262\n",
            "Iteration 21, loss = 0.56476894\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56306418\n",
            "Iteration 2, loss = 0.56223319\n",
            "Iteration 3, loss = 0.56186785\n",
            "Iteration 4, loss = 0.56217432\n",
            "Iteration 5, loss = 0.56038451\n",
            "Iteration 6, loss = 0.56175829\n",
            "Iteration 7, loss = 0.56222886\n",
            "Iteration 8, loss = 0.56163829\n",
            "Iteration 9, loss = 0.56203953\n",
            "Iteration 10, loss = 0.56107496\n",
            "Iteration 11, loss = 0.56274903\n",
            "Iteration 12, loss = 0.56146165\n",
            "Iteration 13, loss = 0.56123941\n",
            "Iteration 14, loss = 0.56221185\n",
            "Iteration 15, loss = 0.56218654\n",
            "Iteration 16, loss = 0.56230903\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57255378\n",
            "Iteration 2, loss = 0.56499201\n",
            "Iteration 3, loss = 0.56492358\n",
            "Iteration 4, loss = 0.56362936\n",
            "Iteration 5, loss = 0.56465852\n",
            "Iteration 6, loss = 0.56523731\n",
            "Iteration 7, loss = 0.56440379\n",
            "Iteration 8, loss = 0.56415887\n",
            "Iteration 9, loss = 0.56413722\n",
            "Iteration 10, loss = 0.56477103\n",
            "Iteration 11, loss = 0.56475382\n",
            "Iteration 12, loss = 0.56437446\n",
            "Iteration 13, loss = 0.56467705\n",
            "Iteration 14, loss = 0.56500505\n",
            "Iteration 15, loss = 0.56417710\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56591413\n",
            "Iteration 2, loss = 0.55951676\n",
            "Iteration 3, loss = 0.55979398\n",
            "Iteration 4, loss = 0.55933963\n",
            "Iteration 5, loss = 0.55971944\n",
            "Iteration 6, loss = 0.55954410\n",
            "Iteration 7, loss = 0.55938806\n",
            "Iteration 8, loss = 0.55931569\n",
            "Iteration 9, loss = 0.55826037\n",
            "Iteration 10, loss = 0.56031712\n",
            "Iteration 11, loss = 0.55917815\n",
            "Iteration 12, loss = 0.55903061\n",
            "Iteration 13, loss = 0.55854910\n",
            "Iteration 14, loss = 0.56013219\n",
            "Iteration 15, loss = 0.55920647\n",
            "Iteration 16, loss = 0.55937525\n",
            "Iteration 17, loss = 0.55870045\n",
            "Iteration 18, loss = 0.55750004\n",
            "Iteration 19, loss = 0.55860723\n",
            "Iteration 20, loss = 0.55882452\n",
            "Iteration 21, loss = 0.55770270\n",
            "Iteration 22, loss = 0.55890661\n",
            "Iteration 23, loss = 0.55851737\n",
            "Iteration 24, loss = 0.55795627\n",
            "Iteration 25, loss = 0.55855037\n",
            "Iteration 26, loss = 0.55760979\n",
            "Iteration 27, loss = 0.55878238\n",
            "Iteration 28, loss = 0.55770958\n",
            "Iteration 29, loss = 0.55807372\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56911799\n",
            "Iteration 2, loss = 0.56070959\n",
            "Iteration 3, loss = 0.56075204\n",
            "Iteration 4, loss = 0.56128414\n",
            "Iteration 5, loss = 0.56169041\n",
            "Iteration 6, loss = 0.56073173\n",
            "Iteration 7, loss = 0.56157030\n",
            "Iteration 8, loss = 0.56035490\n",
            "Iteration 9, loss = 0.56138485\n",
            "Iteration 10, loss = 0.56014265\n",
            "Iteration 11, loss = 0.56124787\n",
            "Iteration 12, loss = 0.56105952\n",
            "Iteration 13, loss = 0.56105677\n",
            "Iteration 14, loss = 0.56139197\n",
            "Iteration 15, loss = 0.56065407\n",
            "Iteration 16, loss = 0.56062589\n",
            "Iteration 17, loss = 0.56018937\n",
            "Iteration 18, loss = 0.55969882\n",
            "Iteration 19, loss = 0.56091900\n",
            "Iteration 20, loss = 0.56042205\n",
            "Iteration 21, loss = 0.56080990\n",
            "Iteration 22, loss = 0.56098198\n",
            "Iteration 23, loss = 0.56032534\n",
            "Iteration 24, loss = 0.55977685\n",
            "Iteration 25, loss = 0.55997685\n",
            "Iteration 26, loss = 0.56021174\n",
            "Iteration 27, loss = 0.56005105\n",
            "Iteration 28, loss = 0.55948489\n",
            "Iteration 29, loss = 0.55988055\n",
            "Iteration 30, loss = 0.55961437\n",
            "Iteration 31, loss = 0.55971800\n",
            "Iteration 32, loss = 0.55939723\n",
            "Iteration 33, loss = 0.55899561\n",
            "Iteration 34, loss = 0.55943054\n",
            "Iteration 35, loss = 0.55880092\n",
            "Iteration 36, loss = 0.55898104\n",
            "Iteration 37, loss = 0.55949347\n",
            "Iteration 38, loss = 0.55968635\n",
            "Iteration 39, loss = 0.55864040\n",
            "Iteration 40, loss = 0.55812666\n",
            "Iteration 41, loss = 0.55892797\n",
            "Iteration 42, loss = 0.55827489\n",
            "Iteration 43, loss = 0.55850745\n",
            "Iteration 44, loss = 0.55813849\n",
            "Iteration 45, loss = 0.55832893\n",
            "Iteration 46, loss = 0.55800474\n",
            "Iteration 47, loss = 0.55732008\n",
            "Iteration 48, loss = 0.55814476\n",
            "Iteration 49, loss = 0.55696866\n",
            "Iteration 50, loss = 0.55637951\n",
            "Iteration 51, loss = 0.55674544\n",
            "Iteration 52, loss = 0.55667099\n",
            "Iteration 53, loss = 0.55636775\n",
            "Iteration 54, loss = 0.55600159\n",
            "Iteration 55, loss = 0.55548531\n",
            "Iteration 56, loss = 0.55571421\n",
            "Iteration 57, loss = 0.55514019\n",
            "Iteration 58, loss = 0.55500266\n",
            "Iteration 59, loss = 0.55366429\n",
            "Iteration 60, loss = 0.55472587\n",
            "Iteration 61, loss = 0.55358622\n",
            "Iteration 62, loss = 0.55344359\n",
            "Iteration 63, loss = 0.55301922\n",
            "Iteration 64, loss = 0.55207143\n",
            "Iteration 65, loss = 0.55232688\n",
            "Iteration 66, loss = 0.55211956\n",
            "Iteration 67, loss = 0.55092515\n",
            "Iteration 68, loss = 0.55152014\n",
            "Iteration 69, loss = 0.55018603\n",
            "Iteration 70, loss = 0.54941485\n",
            "Iteration 71, loss = 0.54974468\n",
            "Iteration 72, loss = 0.54926224\n",
            "Iteration 73, loss = 0.54799599\n",
            "Iteration 74, loss = 0.54810061\n",
            "Iteration 75, loss = 0.54713342\n",
            "Iteration 76, loss = 0.54688954\n",
            "Iteration 77, loss = 0.54610592\n",
            "Iteration 78, loss = 0.54542771\n",
            "Iteration 79, loss = 0.54451384\n",
            "Iteration 80, loss = 0.54403302\n",
            "Iteration 81, loss = 0.54339332\n",
            "Iteration 82, loss = 0.54207742\n",
            "Iteration 83, loss = 0.54140864\n",
            "Iteration 84, loss = 0.54063686\n",
            "Iteration 85, loss = 0.53958783\n",
            "Iteration 86, loss = 0.53922148\n",
            "Iteration 87, loss = 0.53990692\n",
            "Iteration 88, loss = 0.53823540\n",
            "Iteration 89, loss = 0.53709237\n",
            "Iteration 90, loss = 0.53595356\n",
            "Iteration 91, loss = 0.53565807\n",
            "Iteration 92, loss = 0.53560428\n",
            "Iteration 93, loss = 0.53509194\n",
            "Iteration 94, loss = 0.53531552\n",
            "Iteration 95, loss = 0.53315931\n",
            "Iteration 96, loss = 0.53298513\n",
            "Iteration 97, loss = 0.53182387\n",
            "Iteration 98, loss = 0.53109684\n",
            "Iteration 99, loss = 0.53078094\n",
            "Iteration 100, loss = 0.53068270\n",
            "Iteration 101, loss = 0.53040587\n",
            "Iteration 102, loss = 0.53019393\n",
            "Iteration 103, loss = 0.52932071\n",
            "Iteration 104, loss = 0.52955427\n",
            "Iteration 105, loss = 0.52865377\n",
            "Iteration 106, loss = 0.52834693\n",
            "Iteration 107, loss = 0.52749624\n",
            "Iteration 108, loss = 0.52815288\n",
            "Iteration 109, loss = 0.52810602\n",
            "Iteration 110, loss = 0.52827707\n",
            "Iteration 111, loss = 0.52683191\n",
            "Iteration 112, loss = 0.52612615\n",
            "Iteration 113, loss = 0.52688595\n",
            "Iteration 114, loss = 0.52584520\n",
            "Iteration 115, loss = 0.52560787\n",
            "Iteration 116, loss = 0.52554079\n",
            "Iteration 117, loss = 0.52430414\n",
            "Iteration 118, loss = 0.52516548\n",
            "Iteration 119, loss = 0.52452490\n",
            "Iteration 120, loss = 0.52426554\n",
            "Iteration 121, loss = 0.52406763\n",
            "Iteration 122, loss = 0.52429354\n",
            "Iteration 123, loss = 0.52396287\n",
            "Iteration 124, loss = 0.52335468\n",
            "Iteration 125, loss = 0.52316161\n",
            "Iteration 126, loss = 0.52397759\n",
            "Iteration 127, loss = 0.52423758\n",
            "Iteration 128, loss = 0.52270445\n",
            "Iteration 129, loss = 0.52289887\n",
            "Iteration 130, loss = 0.52336964\n",
            "Iteration 131, loss = 0.52267391\n",
            "Iteration 132, loss = 0.52183224\n",
            "Iteration 133, loss = 0.52279286\n",
            "Iteration 134, loss = 0.52220820\n",
            "Iteration 135, loss = 0.52208316\n",
            "Iteration 136, loss = 0.52195760\n",
            "Iteration 137, loss = 0.52220665\n",
            "Iteration 138, loss = 0.52202171\n",
            "Iteration 139, loss = 0.52163950\n",
            "Iteration 140, loss = 0.52179124\n",
            "Iteration 141, loss = 0.52183058\n",
            "Iteration 142, loss = 0.52123518\n",
            "Iteration 143, loss = 0.52139436\n",
            "Iteration 144, loss = 0.52165984\n",
            "Iteration 145, loss = 0.52025833\n",
            "Iteration 146, loss = 0.52145169\n",
            "Iteration 147, loss = 0.52006140\n",
            "Iteration 148, loss = 0.52055886\n",
            "Iteration 149, loss = 0.52087098\n",
            "Iteration 150, loss = 0.52076302\n",
            "Iteration 151, loss = 0.52087127\n",
            "Iteration 152, loss = 0.52091706\n",
            "Iteration 153, loss = 0.52057536\n",
            "Iteration 154, loss = 0.52069524\n",
            "Iteration 155, loss = 0.52022453\n",
            "Iteration 156, loss = 0.51978219\n",
            "Iteration 157, loss = 0.52030468\n",
            "Iteration 158, loss = 0.52010051\n",
            "Iteration 159, loss = 0.52042722\n",
            "Iteration 160, loss = 0.51922585\n",
            "Iteration 161, loss = 0.51885006\n",
            "Iteration 162, loss = 0.52020980\n",
            "Iteration 163, loss = 0.52004011\n",
            "Iteration 164, loss = 0.52000015\n",
            "Iteration 165, loss = 0.51999941\n",
            "Iteration 166, loss = 0.51966306\n",
            "Iteration 167, loss = 0.52013753\n",
            "Iteration 168, loss = 0.51974786\n",
            "Iteration 169, loss = 0.51943977\n",
            "Iteration 170, loss = 0.51965093\n",
            "Iteration 171, loss = 0.51902825\n",
            "Iteration 172, loss = 0.51882207\n",
            "Iteration 173, loss = 0.51816800\n",
            "Iteration 174, loss = 0.51852049\n",
            "Iteration 175, loss = 0.51903631\n",
            "Iteration 176, loss = 0.51923198\n",
            "Iteration 177, loss = 0.51859392\n",
            "Iteration 178, loss = 0.51921980\n",
            "Iteration 179, loss = 0.51890006\n",
            "Iteration 180, loss = 0.51899559\n",
            "Iteration 181, loss = 0.51852959\n",
            "Iteration 182, loss = 0.51766187\n",
            "Iteration 183, loss = 0.51800150\n",
            "Iteration 184, loss = 0.51752760\n",
            "Iteration 185, loss = 0.51890413\n",
            "Iteration 186, loss = 0.51872090\n",
            "Iteration 187, loss = 0.51872120\n",
            "Iteration 188, loss = 0.51881669\n",
            "Iteration 189, loss = 0.51882406\n",
            "Iteration 190, loss = 0.51889138\n",
            "Iteration 191, loss = 0.51839379\n",
            "Iteration 192, loss = 0.51915392\n",
            "Iteration 193, loss = 0.51833684\n",
            "Iteration 194, loss = 0.51946655\n",
            "Iteration 195, loss = 0.51871845\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56839188\n",
            "Iteration 2, loss = 0.56404261\n",
            "Iteration 3, loss = 0.56326373\n",
            "Iteration 4, loss = 0.56306465\n",
            "Iteration 5, loss = 0.56256933\n",
            "Iteration 6, loss = 0.56283390\n",
            "Iteration 7, loss = 0.56280652\n",
            "Iteration 8, loss = 0.56228829\n",
            "Iteration 9, loss = 0.56253141\n",
            "Iteration 10, loss = 0.56294246\n",
            "Iteration 11, loss = 0.56262874\n",
            "Iteration 12, loss = 0.56313349\n",
            "Iteration 13, loss = 0.56172512\n",
            "Iteration 14, loss = 0.56248600\n",
            "Iteration 15, loss = 0.56323615\n",
            "Iteration 16, loss = 0.56191293\n",
            "Iteration 17, loss = 0.56217287\n",
            "Iteration 18, loss = 0.56245327\n",
            "Iteration 19, loss = 0.56215366\n",
            "Iteration 20, loss = 0.56234986\n",
            "Iteration 21, loss = 0.56172171\n",
            "Iteration 22, loss = 0.56190364\n",
            "Iteration 23, loss = 0.56126355\n",
            "Iteration 24, loss = 0.56214641\n",
            "Iteration 25, loss = 0.56215319\n",
            "Iteration 26, loss = 0.56149346\n",
            "Iteration 27, loss = 0.56208135\n",
            "Iteration 28, loss = 0.56154415\n",
            "Iteration 29, loss = 0.56120025\n",
            "Iteration 30, loss = 0.56085667\n",
            "Iteration 31, loss = 0.56160045\n",
            "Iteration 32, loss = 0.56116128\n",
            "Iteration 33, loss = 0.56063473\n",
            "Iteration 34, loss = 0.56066790\n",
            "Iteration 35, loss = 0.56033885\n",
            "Iteration 36, loss = 0.56095445\n",
            "Iteration 37, loss = 0.56037236\n",
            "Iteration 38, loss = 0.56053026\n",
            "Iteration 39, loss = 0.55935036\n",
            "Iteration 40, loss = 0.56007171\n",
            "Iteration 41, loss = 0.55948375\n",
            "Iteration 42, loss = 0.55999370\n",
            "Iteration 43, loss = 0.55974452\n",
            "Iteration 44, loss = 0.55898583\n",
            "Iteration 45, loss = 0.55942616\n",
            "Iteration 46, loss = 0.55903009\n",
            "Iteration 47, loss = 0.55832021\n",
            "Iteration 48, loss = 0.55789887\n",
            "Iteration 49, loss = 0.55717140\n",
            "Iteration 50, loss = 0.55804517\n",
            "Iteration 51, loss = 0.55782518\n",
            "Iteration 52, loss = 0.55753658\n",
            "Iteration 53, loss = 0.55680554\n",
            "Iteration 54, loss = 0.55657355\n",
            "Iteration 55, loss = 0.55706008\n",
            "Iteration 56, loss = 0.55631620\n",
            "Iteration 57, loss = 0.55591278\n",
            "Iteration 58, loss = 0.55585305\n",
            "Iteration 59, loss = 0.55444265\n",
            "Iteration 60, loss = 0.55375643\n",
            "Iteration 61, loss = 0.55530795\n",
            "Iteration 62, loss = 0.55321936\n",
            "Iteration 63, loss = 0.55419380\n",
            "Iteration 64, loss = 0.55336758\n",
            "Iteration 65, loss = 0.55276058\n",
            "Iteration 66, loss = 0.55306561\n",
            "Iteration 67, loss = 0.55263759\n",
            "Iteration 68, loss = 0.55146913\n",
            "Iteration 69, loss = 0.55067238\n",
            "Iteration 70, loss = 0.54988330\n",
            "Iteration 71, loss = 0.54915444\n",
            "Iteration 72, loss = 0.54907428\n",
            "Iteration 73, loss = 0.54781975\n",
            "Iteration 74, loss = 0.54801686\n",
            "Iteration 75, loss = 0.54704004\n",
            "Iteration 76, loss = 0.54576923\n",
            "Iteration 77, loss = 0.54592245\n",
            "Iteration 78, loss = 0.54517557\n",
            "Iteration 79, loss = 0.54332549\n",
            "Iteration 80, loss = 0.54400338\n",
            "Iteration 81, loss = 0.54260623\n",
            "Iteration 82, loss = 0.54213916\n",
            "Iteration 83, loss = 0.54103666\n",
            "Iteration 84, loss = 0.53967153\n",
            "Iteration 85, loss = 0.53955381\n",
            "Iteration 86, loss = 0.53816338\n",
            "Iteration 87, loss = 0.53824195\n",
            "Iteration 88, loss = 0.53757218\n",
            "Iteration 89, loss = 0.53730629\n",
            "Iteration 90, loss = 0.53658229\n",
            "Iteration 91, loss = 0.53589808\n",
            "Iteration 92, loss = 0.53507948\n",
            "Iteration 93, loss = 0.53496887\n",
            "Iteration 94, loss = 0.53332361\n",
            "Iteration 95, loss = 0.53309334\n",
            "Iteration 96, loss = 0.53285052\n",
            "Iteration 97, loss = 0.53280492\n",
            "Iteration 98, loss = 0.53228260\n",
            "Iteration 99, loss = 0.53166598\n",
            "Iteration 100, loss = 0.53090413\n",
            "Iteration 101, loss = 0.53071216\n",
            "Iteration 102, loss = 0.53041026\n",
            "Iteration 103, loss = 0.52990780\n",
            "Iteration 104, loss = 0.52933331\n",
            "Iteration 105, loss = 0.52916935\n",
            "Iteration 106, loss = 0.52850736\n",
            "Iteration 107, loss = 0.52941410\n",
            "Iteration 108, loss = 0.52814899\n",
            "Iteration 109, loss = 0.52748815\n",
            "Iteration 110, loss = 0.52804307\n",
            "Iteration 111, loss = 0.52671362\n",
            "Iteration 112, loss = 0.52769590\n",
            "Iteration 113, loss = 0.52692818\n",
            "Iteration 114, loss = 0.52611391\n",
            "Iteration 115, loss = 0.52701593\n",
            "Iteration 116, loss = 0.52632922\n",
            "Iteration 117, loss = 0.52531489\n",
            "Iteration 118, loss = 0.52643854\n",
            "Iteration 119, loss = 0.52514145\n",
            "Iteration 120, loss = 0.52689115\n",
            "Iteration 121, loss = 0.52551177\n",
            "Iteration 122, loss = 0.52611309\n",
            "Iteration 123, loss = 0.52432017\n",
            "Iteration 124, loss = 0.52531365\n",
            "Iteration 125, loss = 0.52446381\n",
            "Iteration 126, loss = 0.52384826\n",
            "Iteration 127, loss = 0.52232310\n",
            "Iteration 128, loss = 0.52360986\n",
            "Iteration 129, loss = 0.52401572\n",
            "Iteration 130, loss = 0.52391947\n",
            "Iteration 131, loss = 0.52354361\n",
            "Iteration 132, loss = 0.52282841\n",
            "Iteration 133, loss = 0.52377093\n",
            "Iteration 134, loss = 0.52424148\n",
            "Iteration 135, loss = 0.52312890\n",
            "Iteration 136, loss = 0.52248765\n",
            "Iteration 137, loss = 0.52244480\n",
            "Iteration 138, loss = 0.52243815\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57151515\n",
            "Iteration 2, loss = 0.56492825\n",
            "Iteration 3, loss = 0.56471152\n",
            "Iteration 4, loss = 0.56521865\n",
            "Iteration 5, loss = 0.56516860\n",
            "Iteration 6, loss = 0.56456213\n",
            "Iteration 7, loss = 0.56503367\n",
            "Iteration 8, loss = 0.56493091\n",
            "Iteration 9, loss = 0.56463361\n",
            "Iteration 10, loss = 0.56453159\n",
            "Iteration 11, loss = 0.56458533\n",
            "Iteration 12, loss = 0.56441718\n",
            "Iteration 13, loss = 0.56431337\n",
            "Iteration 14, loss = 0.56474407\n",
            "Iteration 15, loss = 0.56439482\n",
            "Iteration 16, loss = 0.56434010\n",
            "Iteration 17, loss = 0.56482081\n",
            "Iteration 18, loss = 0.56411982\n",
            "Iteration 19, loss = 0.56426449\n",
            "Iteration 20, loss = 0.56342936\n",
            "Iteration 21, loss = 0.56373293\n",
            "Iteration 22, loss = 0.56289784\n",
            "Iteration 23, loss = 0.56444487\n",
            "Iteration 24, loss = 0.56398868\n",
            "Iteration 25, loss = 0.56421851\n",
            "Iteration 26, loss = 0.56393295\n",
            "Iteration 27, loss = 0.56387565\n",
            "Iteration 28, loss = 0.56362788\n",
            "Iteration 29, loss = 0.56399547\n",
            "Iteration 30, loss = 0.56286538\n",
            "Iteration 31, loss = 0.56360294\n",
            "Iteration 32, loss = 0.56324221\n",
            "Iteration 33, loss = 0.56392136\n",
            "Iteration 34, loss = 0.56277421\n",
            "Iteration 35, loss = 0.56382446\n",
            "Iteration 36, loss = 0.56335822\n",
            "Iteration 37, loss = 0.56230884\n",
            "Iteration 38, loss = 0.56255247\n",
            "Iteration 39, loss = 0.56250281\n",
            "Iteration 40, loss = 0.56330916\n",
            "Iteration 41, loss = 0.56318677\n",
            "Iteration 42, loss = 0.56232580\n",
            "Iteration 43, loss = 0.56256333\n",
            "Iteration 44, loss = 0.56347375\n",
            "Iteration 45, loss = 0.56221792\n",
            "Iteration 46, loss = 0.56223284\n",
            "Iteration 47, loss = 0.56189785\n",
            "Iteration 48, loss = 0.56208898\n",
            "Iteration 49, loss = 0.56184355\n",
            "Iteration 50, loss = 0.56183406\n",
            "Iteration 51, loss = 0.56193320\n",
            "Iteration 52, loss = 0.56116677\n",
            "Iteration 53, loss = 0.56060416\n",
            "Iteration 54, loss = 0.56079773\n",
            "Iteration 55, loss = 0.56026527\n",
            "Iteration 56, loss = 0.56058778\n",
            "Iteration 57, loss = 0.56036809\n",
            "Iteration 58, loss = 0.55942177\n",
            "Iteration 59, loss = 0.55961695\n",
            "Iteration 60, loss = 0.55978376\n",
            "Iteration 61, loss = 0.55832971\n",
            "Iteration 62, loss = 0.55868217\n",
            "Iteration 63, loss = 0.55848795\n",
            "Iteration 64, loss = 0.55809371\n",
            "Iteration 65, loss = 0.55729331\n",
            "Iteration 66, loss = 0.55745466\n",
            "Iteration 67, loss = 0.55669828\n",
            "Iteration 68, loss = 0.55664302\n",
            "Iteration 69, loss = 0.55593881\n",
            "Iteration 70, loss = 0.55534207\n",
            "Iteration 71, loss = 0.55523259\n",
            "Iteration 72, loss = 0.55552282\n",
            "Iteration 73, loss = 0.55385306\n",
            "Iteration 74, loss = 0.55440662\n",
            "Iteration 75, loss = 0.55311518\n",
            "Iteration 76, loss = 0.55286675\n",
            "Iteration 77, loss = 0.55198281\n",
            "Iteration 78, loss = 0.55242976\n",
            "Iteration 79, loss = 0.55100371\n",
            "Iteration 80, loss = 0.55016590\n",
            "Iteration 81, loss = 0.55008317\n",
            "Iteration 82, loss = 0.54917831\n",
            "Iteration 83, loss = 0.54911508\n",
            "Iteration 84, loss = 0.54745472\n",
            "Iteration 85, loss = 0.54789072\n",
            "Iteration 86, loss = 0.54575245\n",
            "Iteration 87, loss = 0.54520745\n",
            "Iteration 88, loss = 0.54558134\n",
            "Iteration 89, loss = 0.54403624\n",
            "Iteration 90, loss = 0.54301484\n",
            "Iteration 91, loss = 0.54209658\n",
            "Iteration 92, loss = 0.54267914\n",
            "Iteration 93, loss = 0.54163286\n",
            "Iteration 94, loss = 0.54096384\n",
            "Iteration 95, loss = 0.53988073\n",
            "Iteration 96, loss = 0.53952463\n",
            "Iteration 97, loss = 0.53912190\n",
            "Iteration 98, loss = 0.53818912\n",
            "Iteration 99, loss = 0.53809684\n",
            "Iteration 100, loss = 0.53784142\n",
            "Iteration 101, loss = 0.53741296\n",
            "Iteration 102, loss = 0.53690922\n",
            "Iteration 103, loss = 0.53567306\n",
            "Iteration 104, loss = 0.53677343\n",
            "Iteration 105, loss = 0.53537642\n",
            "Iteration 106, loss = 0.53545351\n",
            "Iteration 107, loss = 0.53414395\n",
            "Iteration 108, loss = 0.53404822\n",
            "Iteration 109, loss = 0.53356144\n",
            "Iteration 110, loss = 0.53360739\n",
            "Iteration 111, loss = 0.53329523\n",
            "Iteration 112, loss = 0.53235161\n",
            "Iteration 113, loss = 0.53191263\n",
            "Iteration 114, loss = 0.53170236\n",
            "Iteration 115, loss = 0.53189983\n",
            "Iteration 116, loss = 0.53239285\n",
            "Iteration 117, loss = 0.53100469\n",
            "Iteration 118, loss = 0.53139629\n",
            "Iteration 119, loss = 0.53004742\n",
            "Iteration 120, loss = 0.53035225\n",
            "Iteration 121, loss = 0.53055864\n",
            "Iteration 122, loss = 0.53075770\n",
            "Iteration 123, loss = 0.52938436\n",
            "Iteration 124, loss = 0.52881540\n",
            "Iteration 125, loss = 0.52918893\n",
            "Iteration 126, loss = 0.52930589\n",
            "Iteration 127, loss = 0.53020930\n",
            "Iteration 128, loss = 0.52880740\n",
            "Iteration 129, loss = 0.52871525\n",
            "Iteration 130, loss = 0.52926826\n",
            "Iteration 131, loss = 0.52934137\n",
            "Iteration 132, loss = 0.52913314\n",
            "Iteration 133, loss = 0.52876412\n",
            "Iteration 134, loss = 0.52786904\n",
            "Iteration 135, loss = 0.52750671\n",
            "Iteration 136, loss = 0.52781493\n",
            "Iteration 137, loss = 0.52792401\n",
            "Iteration 138, loss = 0.52758892\n",
            "Iteration 139, loss = 0.52696958\n",
            "Iteration 140, loss = 0.52736596\n",
            "Iteration 141, loss = 0.52686533\n",
            "Iteration 142, loss = 0.52743585\n",
            "Iteration 143, loss = 0.52625080\n",
            "Iteration 144, loss = 0.52694392\n",
            "Iteration 145, loss = 0.52652152\n",
            "Iteration 146, loss = 0.52700789\n",
            "Iteration 147, loss = 0.52631050\n",
            "Iteration 148, loss = 0.52648231\n",
            "Iteration 149, loss = 0.52542386\n",
            "Iteration 150, loss = 0.52729823\n",
            "Iteration 151, loss = 0.52592048\n",
            "Iteration 152, loss = 0.52626612\n",
            "Iteration 153, loss = 0.52583760\n",
            "Iteration 154, loss = 0.52584024\n",
            "Iteration 155, loss = 0.52577648\n",
            "Iteration 156, loss = 0.52598206\n",
            "Iteration 157, loss = 0.52522073\n",
            "Iteration 158, loss = 0.52519500\n",
            "Iteration 159, loss = 0.52524141\n",
            "Iteration 160, loss = 0.52543189\n",
            "Iteration 161, loss = 0.52576924\n",
            "Iteration 162, loss = 0.52546373\n",
            "Iteration 163, loss = 0.52470065\n",
            "Iteration 164, loss = 0.52508224\n",
            "Iteration 165, loss = 0.52465765\n",
            "Iteration 166, loss = 0.52443666\n",
            "Iteration 167, loss = 0.52538313\n",
            "Iteration 168, loss = 0.52528720\n",
            "Iteration 169, loss = 0.52438045\n",
            "Iteration 170, loss = 0.52496874\n",
            "Iteration 171, loss = 0.52427987\n",
            "Iteration 172, loss = 0.52423718\n",
            "Iteration 173, loss = 0.52497979\n",
            "Iteration 174, loss = 0.52411208\n",
            "Iteration 175, loss = 0.52401386\n",
            "Iteration 176, loss = 0.52347310\n",
            "Iteration 177, loss = 0.52516047\n",
            "Iteration 178, loss = 0.52513681\n",
            "Iteration 179, loss = 0.52331245\n",
            "Iteration 180, loss = 0.52450215\n",
            "Iteration 181, loss = 0.52412390\n",
            "Iteration 182, loss = 0.52433906\n",
            "Iteration 183, loss = 0.52368918\n",
            "Iteration 184, loss = 0.52347141\n",
            "Iteration 185, loss = 0.52365514\n",
            "Iteration 186, loss = 0.52357053\n",
            "Iteration 187, loss = 0.52348765\n",
            "Iteration 188, loss = 0.52436996\n",
            "Iteration 189, loss = 0.52254327\n",
            "Iteration 190, loss = 0.52469520\n",
            "Iteration 191, loss = 0.52419743\n",
            "Iteration 192, loss = 0.52376629\n",
            "Iteration 193, loss = 0.52342285\n",
            "Iteration 194, loss = 0.52382837\n",
            "Iteration 195, loss = 0.52360911\n",
            "Iteration 196, loss = 0.52223419\n",
            "Iteration 197, loss = 0.52396677\n",
            "Iteration 198, loss = 0.52365184\n",
            "Iteration 199, loss = 0.52365263\n",
            "Iteration 200, loss = 0.52406883\n",
            "Iteration 201, loss = 0.52326654\n",
            "Iteration 202, loss = 0.52325066\n",
            "Iteration 203, loss = 0.52332599\n",
            "Iteration 204, loss = 0.52361598\n",
            "Iteration 205, loss = 0.52256961\n",
            "Iteration 206, loss = 0.52349116\n",
            "Iteration 207, loss = 0.52325353\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56611562\n",
            "Iteration 2, loss = 0.56410250\n",
            "Iteration 3, loss = 0.56354665\n",
            "Iteration 4, loss = 0.56249858\n",
            "Iteration 5, loss = 0.56352289\n",
            "Iteration 6, loss = 0.56380778\n",
            "Iteration 7, loss = 0.56387414\n",
            "Iteration 8, loss = 0.56293644\n",
            "Iteration 9, loss = 0.56335227\n",
            "Iteration 10, loss = 0.56370980\n",
            "Iteration 11, loss = 0.56327399\n",
            "Iteration 12, loss = 0.56326910\n",
            "Iteration 13, loss = 0.56313292\n",
            "Iteration 14, loss = 0.56382202\n",
            "Iteration 15, loss = 0.56304601\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56885957\n",
            "Iteration 2, loss = 0.56215656\n",
            "Iteration 3, loss = 0.56170839\n",
            "Iteration 4, loss = 0.56134706\n",
            "Iteration 5, loss = 0.56165892\n",
            "Iteration 6, loss = 0.56232529\n",
            "Iteration 7, loss = 0.56216776\n",
            "Iteration 8, loss = 0.56130712\n",
            "Iteration 9, loss = 0.56253784\n",
            "Iteration 10, loss = 0.56168526\n",
            "Iteration 11, loss = 0.56112578\n",
            "Iteration 12, loss = 0.56153220\n",
            "Iteration 13, loss = 0.56169990\n",
            "Iteration 14, loss = 0.56122463\n",
            "Iteration 15, loss = 0.56118241\n",
            "Iteration 16, loss = 0.56075409\n",
            "Iteration 17, loss = 0.56203250\n",
            "Iteration 18, loss = 0.56085265\n",
            "Iteration 19, loss = 0.56124929\n",
            "Iteration 20, loss = 0.56069582\n",
            "Iteration 21, loss = 0.56036737\n",
            "Iteration 22, loss = 0.56138124\n",
            "Iteration 23, loss = 0.56081706\n",
            "Iteration 24, loss = 0.56057830\n",
            "Iteration 25, loss = 0.56078696\n",
            "Iteration 26, loss = 0.56062318\n",
            "Iteration 27, loss = 0.56096114\n",
            "Iteration 28, loss = 0.56039544\n",
            "Iteration 29, loss = 0.56012891\n",
            "Iteration 30, loss = 0.55992486\n",
            "Iteration 31, loss = 0.56085700\n",
            "Iteration 32, loss = 0.56051666\n",
            "Iteration 33, loss = 0.55963320\n",
            "Iteration 34, loss = 0.56097607\n",
            "Iteration 35, loss = 0.55878781\n",
            "Iteration 36, loss = 0.55989567\n",
            "Iteration 37, loss = 0.56008397\n",
            "Iteration 38, loss = 0.55985763\n",
            "Iteration 39, loss = 0.55967064\n",
            "Iteration 40, loss = 0.55849212\n",
            "Iteration 41, loss = 0.55999733\n",
            "Iteration 42, loss = 0.55892785\n",
            "Iteration 43, loss = 0.55957874\n",
            "Iteration 44, loss = 0.55961209\n",
            "Iteration 45, loss = 0.55859707\n",
            "Iteration 46, loss = 0.55876409\n",
            "Iteration 47, loss = 0.55900614\n",
            "Iteration 48, loss = 0.55782856\n",
            "Iteration 49, loss = 0.55790772\n",
            "Iteration 50, loss = 0.55760958\n",
            "Iteration 51, loss = 0.55778203\n",
            "Iteration 52, loss = 0.55776565\n",
            "Iteration 53, loss = 0.55898683\n",
            "Iteration 54, loss = 0.55705787\n",
            "Iteration 55, loss = 0.55750613\n",
            "Iteration 56, loss = 0.55647187\n",
            "Iteration 57, loss = 0.55558867\n",
            "Iteration 58, loss = 0.55702167\n",
            "Iteration 59, loss = 0.55573274\n",
            "Iteration 60, loss = 0.55710932\n",
            "Iteration 61, loss = 0.55540264\n",
            "Iteration 62, loss = 0.55481257\n",
            "Iteration 63, loss = 0.55511091\n",
            "Iteration 64, loss = 0.55508687\n",
            "Iteration 65, loss = 0.55496568\n",
            "Iteration 66, loss = 0.55378042\n",
            "Iteration 67, loss = 0.55402964\n",
            "Iteration 68, loss = 0.55286893\n",
            "Iteration 69, loss = 0.55260167\n",
            "Iteration 70, loss = 0.55183532\n",
            "Iteration 71, loss = 0.55277771\n",
            "Iteration 72, loss = 0.55121470\n",
            "Iteration 73, loss = 0.55109467\n",
            "Iteration 74, loss = 0.55011361\n",
            "Iteration 75, loss = 0.54999706\n",
            "Iteration 76, loss = 0.54915660\n",
            "Iteration 77, loss = 0.54911663\n",
            "Iteration 78, loss = 0.54816402\n",
            "Iteration 79, loss = 0.54756893\n",
            "Iteration 80, loss = 0.54642492\n",
            "Iteration 81, loss = 0.54623683\n",
            "Iteration 82, loss = 0.54428431\n",
            "Iteration 83, loss = 0.54426345\n",
            "Iteration 84, loss = 0.54396188\n",
            "Iteration 85, loss = 0.54290282\n",
            "Iteration 86, loss = 0.54270107\n",
            "Iteration 87, loss = 0.54171582\n",
            "Iteration 88, loss = 0.54108574\n",
            "Iteration 89, loss = 0.54062729\n",
            "Iteration 90, loss = 0.53928753\n",
            "Iteration 91, loss = 0.53873290\n",
            "Iteration 92, loss = 0.53746171\n",
            "Iteration 93, loss = 0.53692065\n",
            "Iteration 94, loss = 0.53689664\n",
            "Iteration 95, loss = 0.53565155\n",
            "Iteration 96, loss = 0.53501557\n",
            "Iteration 97, loss = 0.53538113\n",
            "Iteration 98, loss = 0.53370090\n",
            "Iteration 99, loss = 0.53281852\n",
            "Iteration 100, loss = 0.53325285\n",
            "Iteration 101, loss = 0.53276327\n",
            "Iteration 102, loss = 0.53129603\n",
            "Iteration 103, loss = 0.53124152\n",
            "Iteration 104, loss = 0.53129670\n",
            "Iteration 105, loss = 0.53009622\n",
            "Iteration 106, loss = 0.52920129\n",
            "Iteration 107, loss = 0.52943734\n",
            "Iteration 108, loss = 0.52884923\n",
            "Iteration 109, loss = 0.52696648\n",
            "Iteration 110, loss = 0.52795950\n",
            "Iteration 111, loss = 0.52742338\n",
            "Iteration 112, loss = 0.52722248\n",
            "Iteration 113, loss = 0.52629529\n",
            "Iteration 114, loss = 0.52666380\n",
            "Iteration 115, loss = 0.52625842\n",
            "Iteration 116, loss = 0.52608700\n",
            "Iteration 117, loss = 0.52599254\n",
            "Iteration 118, loss = 0.52571094\n",
            "Iteration 119, loss = 0.52604536\n",
            "Iteration 120, loss = 0.52485131\n",
            "Iteration 121, loss = 0.52440938\n",
            "Iteration 122, loss = 0.52437832\n",
            "Iteration 123, loss = 0.52337273\n",
            "Iteration 124, loss = 0.52414555\n",
            "Iteration 125, loss = 0.52392417\n",
            "Iteration 126, loss = 0.52427752\n",
            "Iteration 127, loss = 0.52314300\n",
            "Iteration 128, loss = 0.52373507\n",
            "Iteration 129, loss = 0.52332777\n",
            "Iteration 130, loss = 0.52302307\n",
            "Iteration 131, loss = 0.52329321\n",
            "Iteration 132, loss = 0.52265345\n",
            "Iteration 133, loss = 0.52224375\n",
            "Iteration 134, loss = 0.52163608\n",
            "Iteration 135, loss = 0.52308416\n",
            "Iteration 136, loss = 0.52097702\n",
            "Iteration 137, loss = 0.52222580\n",
            "Iteration 138, loss = 0.52193116\n",
            "Iteration 139, loss = 0.52136756\n",
            "Iteration 140, loss = 0.52087050\n",
            "Iteration 141, loss = 0.52040285\n",
            "Iteration 142, loss = 0.52152297\n",
            "Iteration 143, loss = 0.52160682\n",
            "Iteration 144, loss = 0.52038539\n",
            "Iteration 145, loss = 0.52143891\n",
            "Iteration 146, loss = 0.52087454\n",
            "Iteration 147, loss = 0.52058758\n",
            "Iteration 148, loss = 0.52005720\n",
            "Iteration 149, loss = 0.52001185\n",
            "Iteration 150, loss = 0.52086528\n",
            "Iteration 151, loss = 0.52067847\n",
            "Iteration 152, loss = 0.52036547\n",
            "Iteration 153, loss = 0.51961886\n",
            "Iteration 154, loss = 0.51973715\n",
            "Iteration 155, loss = 0.52063609\n",
            "Iteration 156, loss = 0.51969210\n",
            "Iteration 157, loss = 0.52016539\n",
            "Iteration 158, loss = 0.52052866\n",
            "Iteration 159, loss = 0.51896361\n",
            "Iteration 160, loss = 0.51944108\n",
            "Iteration 161, loss = 0.51976739\n",
            "Iteration 162, loss = 0.52009174\n",
            "Iteration 163, loss = 0.51879844\n",
            "Iteration 164, loss = 0.51948179\n",
            "Iteration 165, loss = 0.51830162\n",
            "Iteration 166, loss = 0.51916884\n",
            "Iteration 167, loss = 0.51835312\n",
            "Iteration 168, loss = 0.51954118\n",
            "Iteration 169, loss = 0.51910784\n",
            "Iteration 170, loss = 0.51965789\n",
            "Iteration 171, loss = 0.51911873\n",
            "Iteration 172, loss = 0.51810475\n",
            "Iteration 173, loss = 0.51861456\n",
            "Iteration 174, loss = 0.51840815\n",
            "Iteration 175, loss = 0.51891540\n",
            "Iteration 176, loss = 0.51802761\n",
            "Iteration 177, loss = 0.51903973\n",
            "Iteration 178, loss = 0.51813254\n",
            "Iteration 179, loss = 0.51786244\n",
            "Iteration 180, loss = 0.51880570\n",
            "Iteration 181, loss = 0.51846150\n",
            "Iteration 182, loss = 0.51839373\n",
            "Iteration 183, loss = 0.51870217\n",
            "Iteration 184, loss = 0.51721256\n",
            "Iteration 185, loss = 0.51831947\n",
            "Iteration 186, loss = 0.51774521\n",
            "Iteration 187, loss = 0.51772255\n",
            "Iteration 188, loss = 0.51715367\n",
            "Iteration 189, loss = 0.51944137\n",
            "Iteration 190, loss = 0.51803768\n",
            "Iteration 191, loss = 0.51751446\n",
            "Iteration 192, loss = 0.51737571\n",
            "Iteration 193, loss = 0.51944125\n",
            "Iteration 194, loss = 0.51771886\n",
            "Iteration 195, loss = 0.51784886\n",
            "Iteration 196, loss = 0.51756024\n",
            "Iteration 197, loss = 0.51779183\n",
            "Iteration 198, loss = 0.51818670\n",
            "Iteration 199, loss = 0.51779303\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "12\n",
            "Iteration 1, loss = 0.56770155\n",
            "Iteration 2, loss = 0.56425456\n",
            "Iteration 3, loss = 0.56412263\n",
            "Iteration 4, loss = 0.56359256\n",
            "Iteration 5, loss = 0.56428145\n",
            "Iteration 6, loss = 0.56412662\n",
            "Iteration 7, loss = 0.56372578\n",
            "Iteration 8, loss = 0.56412762\n",
            "Iteration 9, loss = 0.56338941\n",
            "Iteration 10, loss = 0.56263952\n",
            "Iteration 11, loss = 0.56372890\n",
            "Iteration 12, loss = 0.56428139\n",
            "Iteration 13, loss = 0.56316627\n",
            "Iteration 14, loss = 0.56333125\n",
            "Iteration 15, loss = 0.56383577\n",
            "Iteration 16, loss = 0.56343098\n",
            "Iteration 17, loss = 0.56231227\n",
            "Iteration 18, loss = 0.56318122\n",
            "Iteration 19, loss = 0.56317387\n",
            "Iteration 20, loss = 0.56411338\n",
            "Iteration 21, loss = 0.56304082\n",
            "Iteration 22, loss = 0.56339562\n",
            "Iteration 23, loss = 0.56292194\n",
            "Iteration 24, loss = 0.56292967\n",
            "Iteration 25, loss = 0.56254143\n",
            "Iteration 26, loss = 0.56310764\n",
            "Iteration 27, loss = 0.56295670\n",
            "Iteration 28, loss = 0.56315965\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56786380\n",
            "Iteration 2, loss = 0.56637600\n",
            "Iteration 3, loss = 0.56691014\n",
            "Iteration 4, loss = 0.56690430\n",
            "Iteration 5, loss = 0.56621074\n",
            "Iteration 6, loss = 0.56679058\n",
            "Iteration 7, loss = 0.56665964\n",
            "Iteration 8, loss = 0.56688648\n",
            "Iteration 9, loss = 0.56656932\n",
            "Iteration 10, loss = 0.56601317\n",
            "Iteration 11, loss = 0.56632135\n",
            "Iteration 12, loss = 0.56608209\n",
            "Iteration 13, loss = 0.56700636\n",
            "Iteration 14, loss = 0.56570913\n",
            "Iteration 15, loss = 0.56661777\n",
            "Iteration 16, loss = 0.56638259\n",
            "Iteration 17, loss = 0.56705421\n",
            "Iteration 18, loss = 0.56619431\n",
            "Iteration 19, loss = 0.56595697\n",
            "Iteration 20, loss = 0.56585688\n",
            "Iteration 21, loss = 0.56555935\n",
            "Iteration 22, loss = 0.56583522\n",
            "Iteration 23, loss = 0.56491115\n",
            "Iteration 24, loss = 0.56536718\n",
            "Iteration 25, loss = 0.56612588\n",
            "Iteration 26, loss = 0.56601814\n",
            "Iteration 27, loss = 0.56525328\n",
            "Iteration 28, loss = 0.56487649\n",
            "Iteration 29, loss = 0.56563306\n",
            "Iteration 30, loss = 0.56563700\n",
            "Iteration 31, loss = 0.56544067\n",
            "Iteration 32, loss = 0.56541185\n",
            "Iteration 33, loss = 0.56383435\n",
            "Iteration 34, loss = 0.56571847\n",
            "Iteration 35, loss = 0.56470250\n",
            "Iteration 36, loss = 0.56471757\n",
            "Iteration 37, loss = 0.56486381\n",
            "Iteration 38, loss = 0.56404243\n",
            "Iteration 39, loss = 0.56438073\n",
            "Iteration 40, loss = 0.56353397\n",
            "Iteration 41, loss = 0.56437796\n",
            "Iteration 42, loss = 0.56367102\n",
            "Iteration 43, loss = 0.56402108\n",
            "Iteration 44, loss = 0.56396913\n",
            "Iteration 45, loss = 0.56338697\n",
            "Iteration 46, loss = 0.56371229\n",
            "Iteration 47, loss = 0.56249924\n",
            "Iteration 48, loss = 0.56256134\n",
            "Iteration 49, loss = 0.56339868\n",
            "Iteration 50, loss = 0.56297945\n",
            "Iteration 51, loss = 0.56318551\n",
            "Iteration 52, loss = 0.56245646\n",
            "Iteration 53, loss = 0.56234975\n",
            "Iteration 54, loss = 0.56150201\n",
            "Iteration 55, loss = 0.56184946\n",
            "Iteration 56, loss = 0.56116811\n",
            "Iteration 57, loss = 0.56188756\n",
            "Iteration 58, loss = 0.56111281\n",
            "Iteration 59, loss = 0.56054245\n",
            "Iteration 60, loss = 0.56041944\n",
            "Iteration 61, loss = 0.56025041\n",
            "Iteration 62, loss = 0.55946196\n",
            "Iteration 63, loss = 0.55890110\n",
            "Iteration 64, loss = 0.55878366\n",
            "Iteration 65, loss = 0.55854864\n",
            "Iteration 66, loss = 0.55824735\n",
            "Iteration 67, loss = 0.55752889\n",
            "Iteration 68, loss = 0.55688859\n",
            "Iteration 69, loss = 0.55704656\n",
            "Iteration 70, loss = 0.55589648\n",
            "Iteration 71, loss = 0.55639430\n",
            "Iteration 72, loss = 0.55524580\n",
            "Iteration 73, loss = 0.55333829\n",
            "Iteration 74, loss = 0.55316281\n",
            "Iteration 75, loss = 0.55316507\n",
            "Iteration 76, loss = 0.55278786\n",
            "Iteration 77, loss = 0.55217330\n",
            "Iteration 78, loss = 0.55062972\n",
            "Iteration 79, loss = 0.54992888\n",
            "Iteration 80, loss = 0.54951817\n",
            "Iteration 81, loss = 0.54856864\n",
            "Iteration 82, loss = 0.54826428\n",
            "Iteration 83, loss = 0.54737193\n",
            "Iteration 84, loss = 0.54627037\n",
            "Iteration 85, loss = 0.54580005\n",
            "Iteration 86, loss = 0.54521250\n",
            "Iteration 87, loss = 0.54462364\n",
            "Iteration 88, loss = 0.54311480\n",
            "Iteration 89, loss = 0.54255247\n",
            "Iteration 90, loss = 0.54179454\n",
            "Iteration 91, loss = 0.54100867\n",
            "Iteration 92, loss = 0.54026035\n",
            "Iteration 93, loss = 0.54005615\n",
            "Iteration 94, loss = 0.53885180\n",
            "Iteration 95, loss = 0.53831011\n",
            "Iteration 96, loss = 0.53682903\n",
            "Iteration 97, loss = 0.53712762\n",
            "Iteration 98, loss = 0.53635399\n",
            "Iteration 99, loss = 0.53573047\n",
            "Iteration 100, loss = 0.53488073\n",
            "Iteration 101, loss = 0.53501848\n",
            "Iteration 102, loss = 0.53421622\n",
            "Iteration 103, loss = 0.53392758\n",
            "Iteration 104, loss = 0.53317243\n",
            "Iteration 105, loss = 0.53369951\n",
            "Iteration 106, loss = 0.53160213\n",
            "Iteration 107, loss = 0.53189343\n",
            "Iteration 108, loss = 0.53092255\n",
            "Iteration 109, loss = 0.53123417\n",
            "Iteration 110, loss = 0.53117969\n",
            "Iteration 111, loss = 0.52999302\n",
            "Iteration 112, loss = 0.52991411\n",
            "Iteration 113, loss = 0.52997531\n",
            "Iteration 114, loss = 0.52938018\n",
            "Iteration 115, loss = 0.52792681\n",
            "Iteration 116, loss = 0.52877818\n",
            "Iteration 117, loss = 0.52876751\n",
            "Iteration 118, loss = 0.52856133\n",
            "Iteration 119, loss = 0.52862041\n",
            "Iteration 120, loss = 0.52739111\n",
            "Iteration 121, loss = 0.52826861\n",
            "Iteration 122, loss = 0.52847564\n",
            "Iteration 123, loss = 0.52811560\n",
            "Iteration 124, loss = 0.52823292\n",
            "Iteration 125, loss = 0.52665908\n",
            "Iteration 126, loss = 0.52643953\n",
            "Iteration 127, loss = 0.52618624\n",
            "Iteration 128, loss = 0.52533073\n",
            "Iteration 129, loss = 0.52585645\n",
            "Iteration 130, loss = 0.52660109\n",
            "Iteration 131, loss = 0.52531177\n",
            "Iteration 132, loss = 0.52504152\n",
            "Iteration 133, loss = 0.52517905\n",
            "Iteration 134, loss = 0.52489684\n",
            "Iteration 135, loss = 0.52435942\n",
            "Iteration 136, loss = 0.52481123\n",
            "Iteration 137, loss = 0.52423794\n",
            "Iteration 138, loss = 0.52463967\n",
            "Iteration 139, loss = 0.52412341\n",
            "Iteration 140, loss = 0.52425097\n",
            "Iteration 141, loss = 0.52538304\n",
            "Iteration 142, loss = 0.52312980\n",
            "Iteration 143, loss = 0.52346196\n",
            "Iteration 144, loss = 0.52320467\n",
            "Iteration 145, loss = 0.52271503\n",
            "Iteration 146, loss = 0.52326158\n",
            "Iteration 147, loss = 0.52360984\n",
            "Iteration 148, loss = 0.52258603\n",
            "Iteration 149, loss = 0.52305761\n",
            "Iteration 150, loss = 0.52300023\n",
            "Iteration 151, loss = 0.52188107\n",
            "Iteration 152, loss = 0.52385601\n",
            "Iteration 153, loss = 0.52219231\n",
            "Iteration 154, loss = 0.52246908\n",
            "Iteration 155, loss = 0.52267051\n",
            "Iteration 156, loss = 0.52287442\n",
            "Iteration 157, loss = 0.52207191\n",
            "Iteration 158, loss = 0.52269828\n",
            "Iteration 159, loss = 0.52189235\n",
            "Iteration 160, loss = 0.52267214\n",
            "Iteration 161, loss = 0.52261553\n",
            "Iteration 162, loss = 0.52245600\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56971546\n",
            "Iteration 2, loss = 0.56367522\n",
            "Iteration 3, loss = 0.56326018\n",
            "Iteration 4, loss = 0.56426714\n",
            "Iteration 5, loss = 0.56404204\n",
            "Iteration 6, loss = 0.56405842\n",
            "Iteration 7, loss = 0.56411288\n",
            "Iteration 8, loss = 0.56381647\n",
            "Iteration 9, loss = 0.56354022\n",
            "Iteration 10, loss = 0.56331282\n",
            "Iteration 11, loss = 0.56403953\n",
            "Iteration 12, loss = 0.56389273\n",
            "Iteration 13, loss = 0.56342591\n",
            "Iteration 14, loss = 0.56328450\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56622598\n",
            "Iteration 2, loss = 0.56316730\n",
            "Iteration 3, loss = 0.56177020\n",
            "Iteration 4, loss = 0.56276873\n",
            "Iteration 5, loss = 0.56287644\n",
            "Iteration 6, loss = 0.56288733\n",
            "Iteration 7, loss = 0.56303979\n",
            "Iteration 8, loss = 0.56287879\n",
            "Iteration 9, loss = 0.56228309\n",
            "Iteration 10, loss = 0.56276753\n",
            "Iteration 11, loss = 0.56277174\n",
            "Iteration 12, loss = 0.56193410\n",
            "Iteration 13, loss = 0.56251114\n",
            "Iteration 14, loss = 0.56230135\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56603484\n",
            "Iteration 2, loss = 0.55973867\n",
            "Iteration 3, loss = 0.55953061\n",
            "Iteration 4, loss = 0.55980122\n",
            "Iteration 5, loss = 0.55980966\n",
            "Iteration 6, loss = 0.56009083\n",
            "Iteration 7, loss = 0.56001054\n",
            "Iteration 8, loss = 0.55976788\n",
            "Iteration 9, loss = 0.55950791\n",
            "Iteration 10, loss = 0.55915655\n",
            "Iteration 11, loss = 0.55959535\n",
            "Iteration 12, loss = 0.55957667\n",
            "Iteration 13, loss = 0.55887909\n",
            "Iteration 14, loss = 0.55931773\n",
            "Iteration 15, loss = 0.55925114\n",
            "Iteration 16, loss = 0.55910637\n",
            "Iteration 17, loss = 0.55924602\n",
            "Iteration 18, loss = 0.55848439\n",
            "Iteration 19, loss = 0.55897120\n",
            "Iteration 20, loss = 0.55937817\n",
            "Iteration 21, loss = 0.55872650\n",
            "Iteration 22, loss = 0.55806993\n",
            "Iteration 23, loss = 0.55938114\n",
            "Iteration 24, loss = 0.55864921\n",
            "Iteration 25, loss = 0.55901144\n",
            "Iteration 26, loss = 0.55847687\n",
            "Iteration 27, loss = 0.55871565\n",
            "Iteration 28, loss = 0.55852461\n",
            "Iteration 29, loss = 0.55783060\n",
            "Iteration 30, loss = 0.55840287\n",
            "Iteration 31, loss = 0.55782254\n",
            "Iteration 32, loss = 0.55748616\n",
            "Iteration 33, loss = 0.55761560\n",
            "Iteration 34, loss = 0.55871224\n",
            "Iteration 35, loss = 0.55702186\n",
            "Iteration 36, loss = 0.55692262\n",
            "Iteration 37, loss = 0.55833055\n",
            "Iteration 38, loss = 0.55754882\n",
            "Iteration 39, loss = 0.55781877\n",
            "Iteration 40, loss = 0.55730781\n",
            "Iteration 41, loss = 0.55692885\n",
            "Iteration 42, loss = 0.55695551\n",
            "Iteration 43, loss = 0.55627092\n",
            "Iteration 44, loss = 0.55626845\n",
            "Iteration 45, loss = 0.55659569\n",
            "Iteration 46, loss = 0.55670453\n",
            "Iteration 47, loss = 0.55608766\n",
            "Iteration 48, loss = 0.55600432\n",
            "Iteration 49, loss = 0.55596554\n",
            "Iteration 50, loss = 0.55596011\n",
            "Iteration 51, loss = 0.55425106\n",
            "Iteration 52, loss = 0.55532353\n",
            "Iteration 53, loss = 0.55512637\n",
            "Iteration 54, loss = 0.55473684\n",
            "Iteration 55, loss = 0.55424680\n",
            "Iteration 56, loss = 0.55419518\n",
            "Iteration 57, loss = 0.55426989\n",
            "Iteration 58, loss = 0.55394507\n",
            "Iteration 59, loss = 0.55379676\n",
            "Iteration 60, loss = 0.55285742\n",
            "Iteration 61, loss = 0.55386871\n",
            "Iteration 62, loss = 0.55281164\n",
            "Iteration 63, loss = 0.55139590\n",
            "Iteration 64, loss = 0.55186215\n",
            "Iteration 65, loss = 0.55108653\n",
            "Iteration 66, loss = 0.55111343\n",
            "Iteration 67, loss = 0.54995759\n",
            "Iteration 68, loss = 0.54950180\n",
            "Iteration 69, loss = 0.54933576\n",
            "Iteration 70, loss = 0.54905969\n",
            "Iteration 71, loss = 0.54856470\n",
            "Iteration 72, loss = 0.54726639\n",
            "Iteration 73, loss = 0.54821171\n",
            "Iteration 74, loss = 0.54706222\n",
            "Iteration 75, loss = 0.54660155\n",
            "Iteration 76, loss = 0.54603762\n",
            "Iteration 77, loss = 0.54640107\n",
            "Iteration 78, loss = 0.54470233\n",
            "Iteration 79, loss = 0.54449131\n",
            "Iteration 80, loss = 0.54306314\n",
            "Iteration 81, loss = 0.54226667\n",
            "Iteration 82, loss = 0.54157564\n",
            "Iteration 83, loss = 0.54086331\n",
            "Iteration 84, loss = 0.53944190\n",
            "Iteration 85, loss = 0.53900177\n",
            "Iteration 86, loss = 0.53967851\n",
            "Iteration 87, loss = 0.53752792\n",
            "Iteration 88, loss = 0.53687291\n",
            "Iteration 89, loss = 0.53621240\n",
            "Iteration 90, loss = 0.53592363\n",
            "Iteration 91, loss = 0.53461655\n",
            "Iteration 92, loss = 0.53356661\n",
            "Iteration 93, loss = 0.53310010\n",
            "Iteration 94, loss = 0.53275599\n",
            "Iteration 95, loss = 0.53226667\n",
            "Iteration 96, loss = 0.53078105\n",
            "Iteration 97, loss = 0.52978248\n",
            "Iteration 98, loss = 0.52968137\n",
            "Iteration 99, loss = 0.52986789\n",
            "Iteration 100, loss = 0.52888851\n",
            "Iteration 101, loss = 0.52754703\n",
            "Iteration 102, loss = 0.52724678\n",
            "Iteration 103, loss = 0.52748800\n",
            "Iteration 104, loss = 0.52693273\n",
            "Iteration 105, loss = 0.52661800\n",
            "Iteration 106, loss = 0.52479766\n",
            "Iteration 107, loss = 0.52490963\n",
            "Iteration 108, loss = 0.52485105\n",
            "Iteration 109, loss = 0.52562438\n",
            "Iteration 110, loss = 0.52350773\n",
            "Iteration 111, loss = 0.52343673\n",
            "Iteration 112, loss = 0.52306818\n",
            "Iteration 113, loss = 0.52305979\n",
            "Iteration 114, loss = 0.52229955\n",
            "Iteration 115, loss = 0.52265493\n",
            "Iteration 116, loss = 0.52321648\n",
            "Iteration 117, loss = 0.52230036\n",
            "Iteration 118, loss = 0.52210572\n",
            "Iteration 119, loss = 0.52178347\n",
            "Iteration 120, loss = 0.52050425\n",
            "Iteration 121, loss = 0.52132342\n",
            "Iteration 122, loss = 0.51986724\n",
            "Iteration 123, loss = 0.52065834\n",
            "Iteration 124, loss = 0.52054526\n",
            "Iteration 125, loss = 0.52080232\n",
            "Iteration 126, loss = 0.52052999\n",
            "Iteration 127, loss = 0.51971902\n",
            "Iteration 128, loss = 0.51885062\n",
            "Iteration 129, loss = 0.51978414\n",
            "Iteration 130, loss = 0.51806477\n",
            "Iteration 131, loss = 0.51827654\n",
            "Iteration 132, loss = 0.51777564\n",
            "Iteration 133, loss = 0.51798806\n",
            "Iteration 134, loss = 0.51696879\n",
            "Iteration 135, loss = 0.51885025\n",
            "Iteration 136, loss = 0.51790339\n",
            "Iteration 137, loss = 0.51693683\n",
            "Iteration 138, loss = 0.51762245\n",
            "Iteration 139, loss = 0.51762987\n",
            "Iteration 140, loss = 0.51692909\n",
            "Iteration 141, loss = 0.51651011\n",
            "Iteration 142, loss = 0.51834743\n",
            "Iteration 143, loss = 0.51697622\n",
            "Iteration 144, loss = 0.51663878\n",
            "Iteration 145, loss = 0.51567067\n",
            "Iteration 146, loss = 0.51657145\n",
            "Iteration 147, loss = 0.51657233\n",
            "Iteration 148, loss = 0.51628531\n",
            "Iteration 149, loss = 0.51485744\n",
            "Iteration 150, loss = 0.51627726\n",
            "Iteration 151, loss = 0.51568175\n",
            "Iteration 152, loss = 0.51555028\n",
            "Iteration 153, loss = 0.51564593\n",
            "Iteration 154, loss = 0.51575188\n",
            "Iteration 155, loss = 0.51531460\n",
            "Iteration 156, loss = 0.51519951\n",
            "Iteration 157, loss = 0.51573914\n",
            "Iteration 158, loss = 0.51396780\n",
            "Iteration 159, loss = 0.51567226\n",
            "Iteration 160, loss = 0.51514789\n",
            "Iteration 161, loss = 0.51465324\n",
            "Iteration 162, loss = 0.51486384\n",
            "Iteration 163, loss = 0.51477241\n",
            "Iteration 164, loss = 0.51478387\n",
            "Iteration 165, loss = 0.51536669\n",
            "Iteration 166, loss = 0.51466090\n",
            "Iteration 167, loss = 0.51485707\n",
            "Iteration 168, loss = 0.51423355\n",
            "Iteration 169, loss = 0.51387498\n",
            "Iteration 170, loss = 0.51388813\n",
            "Iteration 171, loss = 0.51355139\n",
            "Iteration 172, loss = 0.51410180\n",
            "Iteration 173, loss = 0.51434739\n",
            "Iteration 174, loss = 0.51335272\n",
            "Iteration 175, loss = 0.51388634\n",
            "Iteration 176, loss = 0.51330187\n",
            "Iteration 177, loss = 0.51510104\n",
            "Iteration 178, loss = 0.51343326\n",
            "Iteration 179, loss = 0.51326366\n",
            "Iteration 180, loss = 0.51332560\n",
            "Iteration 181, loss = 0.51348227\n",
            "Iteration 182, loss = 0.51342414\n",
            "Iteration 183, loss = 0.51322978\n",
            "Iteration 184, loss = 0.51356997\n",
            "Iteration 185, loss = 0.51359307\n",
            "Iteration 186, loss = 0.51318435\n",
            "Iteration 187, loss = 0.51249408\n",
            "Iteration 188, loss = 0.51276013\n",
            "Iteration 189, loss = 0.51314497\n",
            "Iteration 190, loss = 0.51256825\n",
            "Iteration 191, loss = 0.51318214\n",
            "Iteration 192, loss = 0.51263857\n",
            "Iteration 193, loss = 0.51312304\n",
            "Iteration 194, loss = 0.51285245\n",
            "Iteration 195, loss = 0.51225047\n",
            "Iteration 196, loss = 0.51370052\n",
            "Iteration 197, loss = 0.51198380\n",
            "Iteration 198, loss = 0.51370172\n",
            "Iteration 199, loss = 0.51193698\n",
            "Iteration 200, loss = 0.51228597\n",
            "Iteration 201, loss = 0.51241547\n",
            "Iteration 202, loss = 0.51196991\n",
            "Iteration 203, loss = 0.51251711\n",
            "Iteration 204, loss = 0.51290960\n",
            "Iteration 205, loss = 0.51254204\n",
            "Iteration 206, loss = 0.51294920\n",
            "Iteration 207, loss = 0.51321529\n",
            "Iteration 208, loss = 0.51227585\n",
            "Iteration 209, loss = 0.51281973\n",
            "Iteration 210, loss = 0.51219251\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56310783\n",
            "Iteration 2, loss = 0.55940383\n",
            "Iteration 3, loss = 0.55880581\n",
            "Iteration 4, loss = 0.55996725\n",
            "Iteration 5, loss = 0.55824886\n",
            "Iteration 6, loss = 0.55936304\n",
            "Iteration 7, loss = 0.55859697\n",
            "Iteration 8, loss = 0.56020569\n",
            "Iteration 9, loss = 0.55932010\n",
            "Iteration 10, loss = 0.55917299\n",
            "Iteration 11, loss = 0.55888855\n",
            "Iteration 12, loss = 0.55909742\n",
            "Iteration 13, loss = 0.55864261\n",
            "Iteration 14, loss = 0.55798984\n",
            "Iteration 15, loss = 0.55900471\n",
            "Iteration 16, loss = 0.55861587\n",
            "Iteration 17, loss = 0.55850407\n",
            "Iteration 18, loss = 0.55855360\n",
            "Iteration 19, loss = 0.55838159\n",
            "Iteration 20, loss = 0.55856972\n",
            "Iteration 21, loss = 0.55771217\n",
            "Iteration 22, loss = 0.55829635\n",
            "Iteration 23, loss = 0.55744269\n",
            "Iteration 24, loss = 0.55893176\n",
            "Iteration 25, loss = 0.55882923\n",
            "Iteration 26, loss = 0.55770338\n",
            "Iteration 27, loss = 0.55723943\n",
            "Iteration 28, loss = 0.55701423\n",
            "Iteration 29, loss = 0.55720817\n",
            "Iteration 30, loss = 0.55697370\n",
            "Iteration 31, loss = 0.55701409\n",
            "Iteration 32, loss = 0.55729415\n",
            "Iteration 33, loss = 0.55748058\n",
            "Iteration 34, loss = 0.55689249\n",
            "Iteration 35, loss = 0.55604429\n",
            "Iteration 36, loss = 0.55711697\n",
            "Iteration 37, loss = 0.55643040\n",
            "Iteration 38, loss = 0.55693759\n",
            "Iteration 39, loss = 0.55592425\n",
            "Iteration 40, loss = 0.55661466\n",
            "Iteration 41, loss = 0.55583836\n",
            "Iteration 42, loss = 0.55669888\n",
            "Iteration 43, loss = 0.55622238\n",
            "Iteration 44, loss = 0.55609672\n",
            "Iteration 45, loss = 0.55558802\n",
            "Iteration 46, loss = 0.55513711\n",
            "Iteration 47, loss = 0.55567512\n",
            "Iteration 48, loss = 0.55464761\n",
            "Iteration 49, loss = 0.55499808\n",
            "Iteration 50, loss = 0.55453450\n",
            "Iteration 51, loss = 0.55461216\n",
            "Iteration 52, loss = 0.55406616\n",
            "Iteration 53, loss = 0.55355940\n",
            "Iteration 54, loss = 0.55355757\n",
            "Iteration 55, loss = 0.55299637\n",
            "Iteration 56, loss = 0.55248084\n",
            "Iteration 57, loss = 0.55259308\n",
            "Iteration 58, loss = 0.55208180\n",
            "Iteration 59, loss = 0.55186030\n",
            "Iteration 60, loss = 0.55137341\n",
            "Iteration 61, loss = 0.55146505\n",
            "Iteration 62, loss = 0.55146046\n",
            "Iteration 63, loss = 0.55084521\n",
            "Iteration 64, loss = 0.54985466\n",
            "Iteration 65, loss = 0.54982242\n",
            "Iteration 66, loss = 0.54902236\n",
            "Iteration 67, loss = 0.54918034\n",
            "Iteration 68, loss = 0.54841850\n",
            "Iteration 69, loss = 0.54748954\n",
            "Iteration 70, loss = 0.54758192\n",
            "Iteration 71, loss = 0.54736136\n",
            "Iteration 72, loss = 0.54525514\n",
            "Iteration 73, loss = 0.54475489\n",
            "Iteration 74, loss = 0.54458424\n",
            "Iteration 75, loss = 0.54415607\n",
            "Iteration 76, loss = 0.54300003\n",
            "Iteration 77, loss = 0.54325107\n",
            "Iteration 78, loss = 0.54232251\n",
            "Iteration 79, loss = 0.54113317\n",
            "Iteration 80, loss = 0.54014126\n",
            "Iteration 81, loss = 0.53985311\n",
            "Iteration 82, loss = 0.53887986\n",
            "Iteration 83, loss = 0.53877780\n",
            "Iteration 84, loss = 0.53924006\n",
            "Iteration 85, loss = 0.53666553\n",
            "Iteration 86, loss = 0.53566205\n",
            "Iteration 87, loss = 0.53618274\n",
            "Iteration 88, loss = 0.53400995\n",
            "Iteration 89, loss = 0.53451026\n",
            "Iteration 90, loss = 0.53366379\n",
            "Iteration 91, loss = 0.53217493\n",
            "Iteration 92, loss = 0.53202601\n",
            "Iteration 93, loss = 0.53213917\n",
            "Iteration 94, loss = 0.53088005\n",
            "Iteration 95, loss = 0.53054328\n",
            "Iteration 96, loss = 0.52966878\n",
            "Iteration 97, loss = 0.52938752\n",
            "Iteration 98, loss = 0.52865770\n",
            "Iteration 99, loss = 0.52816734\n",
            "Iteration 100, loss = 0.52768043\n",
            "Iteration 101, loss = 0.52730216\n",
            "Iteration 102, loss = 0.52793236\n",
            "Iteration 103, loss = 0.52726656\n",
            "Iteration 104, loss = 0.52635922\n",
            "Iteration 105, loss = 0.52649890\n",
            "Iteration 106, loss = 0.52669355\n",
            "Iteration 107, loss = 0.52599835\n",
            "Iteration 108, loss = 0.52530452\n",
            "Iteration 109, loss = 0.52537104\n",
            "Iteration 110, loss = 0.52525427\n",
            "Iteration 111, loss = 0.52509778\n",
            "Iteration 112, loss = 0.52358024\n",
            "Iteration 113, loss = 0.52578324\n",
            "Iteration 114, loss = 0.52376729\n",
            "Iteration 115, loss = 0.52390125\n",
            "Iteration 116, loss = 0.52389680\n",
            "Iteration 117, loss = 0.52311170\n",
            "Iteration 118, loss = 0.52359109\n",
            "Iteration 119, loss = 0.52337610\n",
            "Iteration 120, loss = 0.52289984\n",
            "Iteration 121, loss = 0.52271909\n",
            "Iteration 122, loss = 0.52244950\n",
            "Iteration 123, loss = 0.52314860\n",
            "Iteration 124, loss = 0.52206128\n",
            "Iteration 125, loss = 0.52139701\n",
            "Iteration 126, loss = 0.52131030\n",
            "Iteration 127, loss = 0.52106063\n",
            "Iteration 128, loss = 0.52186936\n",
            "Iteration 129, loss = 0.52177524\n",
            "Iteration 130, loss = 0.52172083\n",
            "Iteration 131, loss = 0.52132403\n",
            "Iteration 132, loss = 0.52081549\n",
            "Iteration 133, loss = 0.52013167\n",
            "Iteration 134, loss = 0.52104365\n",
            "Iteration 135, loss = 0.52152764\n",
            "Iteration 136, loss = 0.51994930\n",
            "Iteration 137, loss = 0.52044199\n",
            "Iteration 138, loss = 0.51999719\n",
            "Iteration 139, loss = 0.51996392\n",
            "Iteration 140, loss = 0.52017593\n",
            "Iteration 141, loss = 0.52009478\n",
            "Iteration 142, loss = 0.51954166\n",
            "Iteration 143, loss = 0.51979597\n",
            "Iteration 144, loss = 0.51908727\n",
            "Iteration 145, loss = 0.51945366\n",
            "Iteration 146, loss = 0.51915767\n",
            "Iteration 147, loss = 0.51942603\n",
            "Iteration 148, loss = 0.51911597\n",
            "Iteration 149, loss = 0.51861864\n",
            "Iteration 150, loss = 0.51973863\n",
            "Iteration 151, loss = 0.51927819\n",
            "Iteration 152, loss = 0.51873754\n",
            "Iteration 153, loss = 0.51925575\n",
            "Iteration 154, loss = 0.51822600\n",
            "Iteration 155, loss = 0.51815500\n",
            "Iteration 156, loss = 0.51822151\n",
            "Iteration 157, loss = 0.51876011\n",
            "Iteration 158, loss = 0.51895556\n",
            "Iteration 159, loss = 0.51724442\n",
            "Iteration 160, loss = 0.51788842\n",
            "Iteration 161, loss = 0.51927301\n",
            "Iteration 162, loss = 0.51772225\n",
            "Iteration 163, loss = 0.51839515\n",
            "Iteration 164, loss = 0.51711514\n",
            "Iteration 165, loss = 0.51762427\n",
            "Iteration 166, loss = 0.51742999\n",
            "Iteration 167, loss = 0.51737554\n",
            "Iteration 168, loss = 0.51797336\n",
            "Iteration 169, loss = 0.51736316\n",
            "Iteration 170, loss = 0.51865752\n",
            "Iteration 171, loss = 0.51740159\n",
            "Iteration 172, loss = 0.51816598\n",
            "Iteration 173, loss = 0.51747597\n",
            "Iteration 174, loss = 0.51752494\n",
            "Iteration 175, loss = 0.51773329\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56929875\n",
            "Iteration 2, loss = 0.56444884\n",
            "Iteration 3, loss = 0.56438779\n",
            "Iteration 4, loss = 0.56374381\n",
            "Iteration 5, loss = 0.56385590\n",
            "Iteration 6, loss = 0.56419871\n",
            "Iteration 7, loss = 0.56449134\n",
            "Iteration 8, loss = 0.56403525\n",
            "Iteration 9, loss = 0.56396342\n",
            "Iteration 10, loss = 0.56351318\n",
            "Iteration 11, loss = 0.56370401\n",
            "Iteration 12, loss = 0.56398921\n",
            "Iteration 13, loss = 0.56337421\n",
            "Iteration 14, loss = 0.56359709\n",
            "Iteration 15, loss = 0.56419554\n",
            "Iteration 16, loss = 0.56327134\n",
            "Iteration 17, loss = 0.56302106\n",
            "Iteration 18, loss = 0.56338915\n",
            "Iteration 19, loss = 0.56325141\n",
            "Iteration 20, loss = 0.56376354\n",
            "Iteration 21, loss = 0.56322107\n",
            "Iteration 22, loss = 0.56362479\n",
            "Iteration 23, loss = 0.56382756\n",
            "Iteration 24, loss = 0.56290984\n",
            "Iteration 25, loss = 0.56274637\n",
            "Iteration 26, loss = 0.56262652\n",
            "Iteration 27, loss = 0.56265680\n",
            "Iteration 28, loss = 0.56249561\n",
            "Iteration 29, loss = 0.56233389\n",
            "Iteration 30, loss = 0.56328892\n",
            "Iteration 31, loss = 0.56187510\n",
            "Iteration 32, loss = 0.56245066\n",
            "Iteration 33, loss = 0.56229117\n",
            "Iteration 34, loss = 0.56224761\n",
            "Iteration 35, loss = 0.56269465\n",
            "Iteration 36, loss = 0.56196561\n",
            "Iteration 37, loss = 0.56159951\n",
            "Iteration 38, loss = 0.56182563\n",
            "Iteration 39, loss = 0.56135414\n",
            "Iteration 40, loss = 0.56098160\n",
            "Iteration 41, loss = 0.56219129\n",
            "Iteration 42, loss = 0.56118190\n",
            "Iteration 43, loss = 0.56129533\n",
            "Iteration 44, loss = 0.56136736\n",
            "Iteration 45, loss = 0.56091036\n",
            "Iteration 46, loss = 0.56076502\n",
            "Iteration 47, loss = 0.56014645\n",
            "Iteration 48, loss = 0.56084509\n",
            "Iteration 49, loss = 0.56003455\n",
            "Iteration 50, loss = 0.56035496\n",
            "Iteration 51, loss = 0.55970248\n",
            "Iteration 52, loss = 0.55976334\n",
            "Iteration 53, loss = 0.55964550\n",
            "Iteration 54, loss = 0.55936591\n",
            "Iteration 55, loss = 0.55933081\n",
            "Iteration 56, loss = 0.55862493\n",
            "Iteration 57, loss = 0.55906317\n",
            "Iteration 58, loss = 0.55891189\n",
            "Iteration 59, loss = 0.55831988\n",
            "Iteration 60, loss = 0.55655311\n",
            "Iteration 61, loss = 0.55714664\n",
            "Iteration 62, loss = 0.55790663\n",
            "Iteration 63, loss = 0.55569769\n",
            "Iteration 64, loss = 0.55661599\n",
            "Iteration 65, loss = 0.55709704\n",
            "Iteration 66, loss = 0.55463010\n",
            "Iteration 67, loss = 0.55531792\n",
            "Iteration 68, loss = 0.55436690\n",
            "Iteration 69, loss = 0.55415080\n",
            "Iteration 70, loss = 0.55373361\n",
            "Iteration 71, loss = 0.55314208\n",
            "Iteration 72, loss = 0.55296439\n",
            "Iteration 73, loss = 0.55186584\n",
            "Iteration 74, loss = 0.55116209\n",
            "Iteration 75, loss = 0.55052405\n",
            "Iteration 76, loss = 0.55033443\n",
            "Iteration 77, loss = 0.54921221\n",
            "Iteration 78, loss = 0.54898819\n",
            "Iteration 79, loss = 0.54749686\n",
            "Iteration 80, loss = 0.54739865\n",
            "Iteration 81, loss = 0.54674116\n",
            "Iteration 82, loss = 0.54633584\n",
            "Iteration 83, loss = 0.54485904\n",
            "Iteration 84, loss = 0.54529133\n",
            "Iteration 85, loss = 0.54417360\n",
            "Iteration 86, loss = 0.54278277\n",
            "Iteration 87, loss = 0.54211696\n",
            "Iteration 88, loss = 0.54150556\n",
            "Iteration 89, loss = 0.54123713\n",
            "Iteration 90, loss = 0.54033309\n",
            "Iteration 91, loss = 0.53930132\n",
            "Iteration 92, loss = 0.53762828\n",
            "Iteration 93, loss = 0.53759814\n",
            "Iteration 94, loss = 0.53745756\n",
            "Iteration 95, loss = 0.53677634\n",
            "Iteration 96, loss = 0.53558270\n",
            "Iteration 97, loss = 0.53540326\n",
            "Iteration 98, loss = 0.53483379\n",
            "Iteration 99, loss = 0.53448857\n",
            "Iteration 100, loss = 0.53365511\n",
            "Iteration 101, loss = 0.53299547\n",
            "Iteration 102, loss = 0.53223052\n",
            "Iteration 103, loss = 0.53233516\n",
            "Iteration 104, loss = 0.53179527\n",
            "Iteration 105, loss = 0.53144068\n",
            "Iteration 106, loss = 0.53168308\n",
            "Iteration 107, loss = 0.53174512\n",
            "Iteration 108, loss = 0.53036918\n",
            "Iteration 109, loss = 0.52983676\n",
            "Iteration 110, loss = 0.52910758\n",
            "Iteration 111, loss = 0.52911443\n",
            "Iteration 112, loss = 0.52939671\n",
            "Iteration 113, loss = 0.52922479\n",
            "Iteration 114, loss = 0.52861199\n",
            "Iteration 115, loss = 0.52745908\n",
            "Iteration 116, loss = 0.52840592\n",
            "Iteration 117, loss = 0.52688015\n",
            "Iteration 118, loss = 0.52785813\n",
            "Iteration 119, loss = 0.52733560\n",
            "Iteration 120, loss = 0.52701033\n",
            "Iteration 121, loss = 0.52751718\n",
            "Iteration 122, loss = 0.52631889\n",
            "Iteration 123, loss = 0.52666432\n",
            "Iteration 124, loss = 0.52649484\n",
            "Iteration 125, loss = 0.52662786\n",
            "Iteration 126, loss = 0.52466591\n",
            "Iteration 127, loss = 0.52594878\n",
            "Iteration 128, loss = 0.52526268\n",
            "Iteration 129, loss = 0.52518592\n",
            "Iteration 130, loss = 0.52493717\n",
            "Iteration 131, loss = 0.52494870\n",
            "Iteration 132, loss = 0.52471581\n",
            "Iteration 133, loss = 0.52457180\n",
            "Iteration 134, loss = 0.52474202\n",
            "Iteration 135, loss = 0.52447825\n",
            "Iteration 136, loss = 0.52407477\n",
            "Iteration 137, loss = 0.52453002\n",
            "Iteration 138, loss = 0.52420732\n",
            "Iteration 139, loss = 0.52369888\n",
            "Iteration 140, loss = 0.52440787\n",
            "Iteration 141, loss = 0.52326385\n",
            "Iteration 142, loss = 0.52294276\n",
            "Iteration 143, loss = 0.52353572\n",
            "Iteration 144, loss = 0.52300611\n",
            "Iteration 145, loss = 0.52369146\n",
            "Iteration 146, loss = 0.52267570\n",
            "Iteration 147, loss = 0.52220929\n",
            "Iteration 148, loss = 0.52298638\n",
            "Iteration 149, loss = 0.52178376\n",
            "Iteration 150, loss = 0.52280023\n",
            "Iteration 151, loss = 0.52150799\n",
            "Iteration 152, loss = 0.52134125\n",
            "Iteration 153, loss = 0.52219765\n",
            "Iteration 154, loss = 0.52241829\n",
            "Iteration 155, loss = 0.52259834\n",
            "Iteration 156, loss = 0.52180275\n",
            "Iteration 157, loss = 0.52151023\n",
            "Iteration 158, loss = 0.52094909\n",
            "Iteration 159, loss = 0.52160574\n",
            "Iteration 160, loss = 0.52122410\n",
            "Iteration 161, loss = 0.52144849\n",
            "Iteration 162, loss = 0.52150498\n",
            "Iteration 163, loss = 0.52137015\n",
            "Iteration 164, loss = 0.52183955\n",
            "Iteration 165, loss = 0.52155606\n",
            "Iteration 166, loss = 0.52080191\n",
            "Iteration 167, loss = 0.52165550\n",
            "Iteration 168, loss = 0.52122570\n",
            "Iteration 169, loss = 0.52101553\n",
            "Iteration 170, loss = 0.52093268\n",
            "Iteration 171, loss = 0.52107396\n",
            "Iteration 172, loss = 0.52215148\n",
            "Iteration 173, loss = 0.51983176\n",
            "Iteration 174, loss = 0.52118824\n",
            "Iteration 175, loss = 0.52015251\n",
            "Iteration 176, loss = 0.52169991\n",
            "Iteration 177, loss = 0.52019005\n",
            "Iteration 178, loss = 0.52047360\n",
            "Iteration 179, loss = 0.51988185\n",
            "Iteration 180, loss = 0.52080276\n",
            "Iteration 181, loss = 0.52122291\n",
            "Iteration 182, loss = 0.51933863\n",
            "Iteration 183, loss = 0.52060641\n",
            "Iteration 184, loss = 0.52029828\n",
            "Iteration 185, loss = 0.52017917\n",
            "Iteration 186, loss = 0.52012075\n",
            "Iteration 187, loss = 0.52003526\n",
            "Iteration 188, loss = 0.52029522\n",
            "Iteration 189, loss = 0.51992158\n",
            "Iteration 190, loss = 0.52014260\n",
            "Iteration 191, loss = 0.52002261\n",
            "Iteration 192, loss = 0.51973692\n",
            "Iteration 193, loss = 0.51944736\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56914925\n",
            "Iteration 2, loss = 0.55595126\n",
            "Iteration 3, loss = 0.55689724\n",
            "Iteration 4, loss = 0.55586207\n",
            "Iteration 5, loss = 0.55678534\n",
            "Iteration 6, loss = 0.55630802\n",
            "Iteration 7, loss = 0.55620236\n",
            "Iteration 8, loss = 0.55612394\n",
            "Iteration 9, loss = 0.55655656\n",
            "Iteration 10, loss = 0.55594633\n",
            "Iteration 11, loss = 0.55533239\n",
            "Iteration 12, loss = 0.55518094\n",
            "Iteration 13, loss = 0.55582402\n",
            "Iteration 14, loss = 0.55643752\n",
            "Iteration 15, loss = 0.55531644\n",
            "Iteration 16, loss = 0.55545776\n",
            "Iteration 17, loss = 0.55570606\n",
            "Iteration 18, loss = 0.55522699\n",
            "Iteration 19, loss = 0.55542459\n",
            "Iteration 20, loss = 0.55511687\n",
            "Iteration 21, loss = 0.55539532\n",
            "Iteration 22, loss = 0.55524326\n",
            "Iteration 23, loss = 0.55563749\n",
            "Iteration 24, loss = 0.55469755\n",
            "Iteration 25, loss = 0.55512048\n",
            "Iteration 26, loss = 0.55581185\n",
            "Iteration 27, loss = 0.55522777\n",
            "Iteration 28, loss = 0.55490438\n",
            "Iteration 29, loss = 0.55445641\n",
            "Iteration 30, loss = 0.55498664\n",
            "Iteration 31, loss = 0.55519780\n",
            "Iteration 32, loss = 0.55513555\n",
            "Iteration 33, loss = 0.55507940\n",
            "Iteration 34, loss = 0.55455991\n",
            "Iteration 35, loss = 0.55460627\n",
            "Iteration 36, loss = 0.55453650\n",
            "Iteration 37, loss = 0.55523285\n",
            "Iteration 38, loss = 0.55459520\n",
            "Iteration 39, loss = 0.55469523\n",
            "Iteration 40, loss = 0.55501854\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57197545\n",
            "Iteration 2, loss = 0.56142386\n",
            "Iteration 3, loss = 0.56157251\n",
            "Iteration 4, loss = 0.56152941\n",
            "Iteration 5, loss = 0.56061565\n",
            "Iteration 6, loss = 0.56102690\n",
            "Iteration 7, loss = 0.56130672\n",
            "Iteration 8, loss = 0.56059151\n",
            "Iteration 9, loss = 0.56044083\n",
            "Iteration 10, loss = 0.56052147\n",
            "Iteration 11, loss = 0.56123246\n",
            "Iteration 12, loss = 0.56060294\n",
            "Iteration 13, loss = 0.56069760\n",
            "Iteration 14, loss = 0.56076146\n",
            "Iteration 15, loss = 0.56113350\n",
            "Iteration 16, loss = 0.56208145\n",
            "Iteration 17, loss = 0.56029821\n",
            "Iteration 18, loss = 0.55943302\n",
            "Iteration 19, loss = 0.55998130\n",
            "Iteration 20, loss = 0.56081972\n",
            "Iteration 21, loss = 0.56024940\n",
            "Iteration 22, loss = 0.55973241\n",
            "Iteration 23, loss = 0.56015701\n",
            "Iteration 24, loss = 0.55947753\n",
            "Iteration 25, loss = 0.55959282\n",
            "Iteration 26, loss = 0.56015308\n",
            "Iteration 27, loss = 0.56027867\n",
            "Iteration 28, loss = 0.55928819\n",
            "Iteration 29, loss = 0.55953123\n",
            "Iteration 30, loss = 0.56000053\n",
            "Iteration 31, loss = 0.56019232\n",
            "Iteration 32, loss = 0.55939799\n",
            "Iteration 33, loss = 0.55971669\n",
            "Iteration 34, loss = 0.55940813\n",
            "Iteration 35, loss = 0.55979417\n",
            "Iteration 36, loss = 0.55820891\n",
            "Iteration 37, loss = 0.55881091\n",
            "Iteration 38, loss = 0.55814604\n",
            "Iteration 39, loss = 0.55891644\n",
            "Iteration 40, loss = 0.55898074\n",
            "Iteration 41, loss = 0.55850706\n",
            "Iteration 42, loss = 0.55879958\n",
            "Iteration 43, loss = 0.55840037\n",
            "Iteration 44, loss = 0.55798057\n",
            "Iteration 45, loss = 0.55856256\n",
            "Iteration 46, loss = 0.55775221\n",
            "Iteration 47, loss = 0.55773292\n",
            "Iteration 48, loss = 0.55715834\n",
            "Iteration 49, loss = 0.55699175\n",
            "Iteration 50, loss = 0.55695502\n",
            "Iteration 51, loss = 0.55715041\n",
            "Iteration 52, loss = 0.55619771\n",
            "Iteration 53, loss = 0.55636173\n",
            "Iteration 54, loss = 0.55546408\n",
            "Iteration 55, loss = 0.55590171\n",
            "Iteration 56, loss = 0.55549160\n",
            "Iteration 57, loss = 0.55505373\n",
            "Iteration 58, loss = 0.55431672\n",
            "Iteration 59, loss = 0.55451730\n",
            "Iteration 60, loss = 0.55408021\n",
            "Iteration 61, loss = 0.55461108\n",
            "Iteration 62, loss = 0.55391348\n",
            "Iteration 63, loss = 0.55277909\n",
            "Iteration 64, loss = 0.55281689\n",
            "Iteration 65, loss = 0.55337934\n",
            "Iteration 66, loss = 0.55163044\n",
            "Iteration 67, loss = 0.55170233\n",
            "Iteration 68, loss = 0.55160120\n",
            "Iteration 69, loss = 0.55008605\n",
            "Iteration 70, loss = 0.55009469\n",
            "Iteration 71, loss = 0.54959549\n",
            "Iteration 72, loss = 0.54847841\n",
            "Iteration 73, loss = 0.54892081\n",
            "Iteration 74, loss = 0.54805903\n",
            "Iteration 75, loss = 0.54734820\n",
            "Iteration 76, loss = 0.54681455\n",
            "Iteration 77, loss = 0.54608909\n",
            "Iteration 78, loss = 0.54503330\n",
            "Iteration 79, loss = 0.54518705\n",
            "Iteration 80, loss = 0.54360019\n",
            "Iteration 81, loss = 0.54327643\n",
            "Iteration 82, loss = 0.54251118\n",
            "Iteration 83, loss = 0.54023334\n",
            "Iteration 84, loss = 0.54089456\n",
            "Iteration 85, loss = 0.53958789\n",
            "Iteration 86, loss = 0.53885778\n",
            "Iteration 87, loss = 0.53783287\n",
            "Iteration 88, loss = 0.53744049\n",
            "Iteration 89, loss = 0.53770650\n",
            "Iteration 90, loss = 0.53669472\n",
            "Iteration 91, loss = 0.53572230\n",
            "Iteration 92, loss = 0.53466329\n",
            "Iteration 93, loss = 0.53375052\n",
            "Iteration 94, loss = 0.53370003\n",
            "Iteration 95, loss = 0.53337741\n",
            "Iteration 96, loss = 0.53312028\n",
            "Iteration 97, loss = 0.53283955\n",
            "Iteration 98, loss = 0.53120839\n",
            "Iteration 99, loss = 0.53120774\n",
            "Iteration 100, loss = 0.53039297\n",
            "Iteration 101, loss = 0.53103271\n",
            "Iteration 102, loss = 0.52919839\n",
            "Iteration 103, loss = 0.52903047\n",
            "Iteration 104, loss = 0.52830220\n",
            "Iteration 105, loss = 0.52825955\n",
            "Iteration 106, loss = 0.52802105\n",
            "Iteration 107, loss = 0.52757498\n",
            "Iteration 108, loss = 0.52723539\n",
            "Iteration 109, loss = 0.52704348\n",
            "Iteration 110, loss = 0.52696245\n",
            "Iteration 111, loss = 0.52671547\n",
            "Iteration 112, loss = 0.52555998\n",
            "Iteration 113, loss = 0.52646634\n",
            "Iteration 114, loss = 0.52580215\n",
            "Iteration 115, loss = 0.52529320\n",
            "Iteration 116, loss = 0.52433384\n",
            "Iteration 117, loss = 0.52511000\n",
            "Iteration 118, loss = 0.52431026\n",
            "Iteration 119, loss = 0.52549473\n",
            "Iteration 120, loss = 0.52431057\n",
            "Iteration 121, loss = 0.52547033\n",
            "Iteration 122, loss = 0.52303421\n",
            "Iteration 123, loss = 0.52388578\n",
            "Iteration 124, loss = 0.52304039\n",
            "Iteration 125, loss = 0.52363656\n",
            "Iteration 126, loss = 0.52310058\n",
            "Iteration 127, loss = 0.52237834\n",
            "Iteration 128, loss = 0.52148679\n",
            "Iteration 129, loss = 0.52418775\n",
            "Iteration 130, loss = 0.52218041\n",
            "Iteration 131, loss = 0.52299592\n",
            "Iteration 132, loss = 0.52204589\n",
            "Iteration 133, loss = 0.52212462\n",
            "Iteration 134, loss = 0.52121424\n",
            "Iteration 135, loss = 0.52091704\n",
            "Iteration 136, loss = 0.52164036\n",
            "Iteration 137, loss = 0.52203149\n",
            "Iteration 138, loss = 0.52126568\n",
            "Iteration 139, loss = 0.52098970\n",
            "Iteration 140, loss = 0.52104825\n",
            "Iteration 141, loss = 0.52079611\n",
            "Iteration 142, loss = 0.52040297\n",
            "Iteration 143, loss = 0.52124818\n",
            "Iteration 144, loss = 0.52063006\n",
            "Iteration 145, loss = 0.52057926\n",
            "Iteration 146, loss = 0.52060772\n",
            "Iteration 147, loss = 0.52051620\n",
            "Iteration 148, loss = 0.51989831\n",
            "Iteration 149, loss = 0.51972504\n",
            "Iteration 150, loss = 0.51897185\n",
            "Iteration 151, loss = 0.52119659\n",
            "Iteration 152, loss = 0.52010748\n",
            "Iteration 153, loss = 0.51962368\n",
            "Iteration 154, loss = 0.52026312\n",
            "Iteration 155, loss = 0.52077578\n",
            "Iteration 156, loss = 0.52064251\n",
            "Iteration 157, loss = 0.51887549\n",
            "Iteration 158, loss = 0.51946033\n",
            "Iteration 159, loss = 0.51997868\n",
            "Iteration 160, loss = 0.51861372\n",
            "Iteration 161, loss = 0.51845557\n",
            "Iteration 162, loss = 0.51889178\n",
            "Iteration 163, loss = 0.51943612\n",
            "Iteration 164, loss = 0.51868310\n",
            "Iteration 165, loss = 0.51833028\n",
            "Iteration 166, loss = 0.51977839\n",
            "Iteration 167, loss = 0.51855545\n",
            "Iteration 168, loss = 0.52003245\n",
            "Iteration 169, loss = 0.51862200\n",
            "Iteration 170, loss = 0.51843871\n",
            "Iteration 171, loss = 0.51877930\n",
            "Iteration 172, loss = 0.51800779\n",
            "Iteration 173, loss = 0.51909798\n",
            "Iteration 174, loss = 0.51866353\n",
            "Iteration 175, loss = 0.51782748\n",
            "Iteration 176, loss = 0.51825907\n",
            "Iteration 177, loss = 0.51801848\n",
            "Iteration 178, loss = 0.51875726\n",
            "Iteration 179, loss = 0.51770743\n",
            "Iteration 180, loss = 0.51732149\n",
            "Iteration 181, loss = 0.51864686\n",
            "Iteration 182, loss = 0.51863816\n",
            "Iteration 183, loss = 0.51840638\n",
            "Iteration 184, loss = 0.51821367\n",
            "Iteration 185, loss = 0.51787962\n",
            "Iteration 186, loss = 0.51766673\n",
            "Iteration 187, loss = 0.51815672\n",
            "Iteration 188, loss = 0.51778753\n",
            "Iteration 189, loss = 0.51767837\n",
            "Iteration 190, loss = 0.51807342\n",
            "Iteration 191, loss = 0.51706426\n",
            "Iteration 192, loss = 0.51860855\n",
            "Iteration 193, loss = 0.51773231\n",
            "Iteration 194, loss = 0.51899161\n",
            "Iteration 195, loss = 0.51855294\n",
            "Iteration 196, loss = 0.51838810\n",
            "Iteration 197, loss = 0.51746574\n",
            "Iteration 198, loss = 0.51751885\n",
            "Iteration 199, loss = 0.51723218\n",
            "Iteration 200, loss = 0.51826066\n",
            "Iteration 201, loss = 0.51794972\n",
            "Iteration 202, loss = 0.51782594\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57078672\n",
            "Iteration 2, loss = 0.56986591\n",
            "Iteration 3, loss = 0.57048010\n",
            "Iteration 4, loss = 0.57061939\n",
            "Iteration 5, loss = 0.57072850\n",
            "Iteration 6, loss = 0.57127560\n",
            "Iteration 7, loss = 0.57088415\n",
            "Iteration 8, loss = 0.56977481\n",
            "Iteration 9, loss = 0.57027437\n",
            "Iteration 10, loss = 0.57013123\n",
            "Iteration 11, loss = 0.56999433\n",
            "Iteration 12, loss = 0.57000169\n",
            "Iteration 13, loss = 0.57037208\n",
            "Iteration 14, loss = 0.57024636\n",
            "Iteration 15, loss = 0.57064098\n",
            "Iteration 16, loss = 0.57012014\n",
            "Iteration 17, loss = 0.56971044\n",
            "Iteration 18, loss = 0.56985121\n",
            "Iteration 19, loss = 0.56924864\n",
            "Iteration 20, loss = 0.57024015\n",
            "Iteration 21, loss = 0.57028863\n",
            "Iteration 22, loss = 0.56970951\n",
            "Iteration 23, loss = 0.56891620\n",
            "Iteration 24, loss = 0.56900006\n",
            "Iteration 25, loss = 0.56946635\n",
            "Iteration 26, loss = 0.56974295\n",
            "Iteration 27, loss = 0.56980622\n",
            "Iteration 28, loss = 0.56917637\n",
            "Iteration 29, loss = 0.56947403\n",
            "Iteration 30, loss = 0.56924313\n",
            "Iteration 31, loss = 0.56964547\n",
            "Iteration 32, loss = 0.56870628\n",
            "Iteration 33, loss = 0.56884613\n",
            "Iteration 34, loss = 0.56989595\n",
            "Iteration 35, loss = 0.56845218\n",
            "Iteration 36, loss = 0.56859862\n",
            "Iteration 37, loss = 0.56876573\n",
            "Iteration 38, loss = 0.56915578\n",
            "Iteration 39, loss = 0.56764359\n",
            "Iteration 40, loss = 0.56849314\n",
            "Iteration 41, loss = 0.56833362\n",
            "Iteration 42, loss = 0.56811511\n",
            "Iteration 43, loss = 0.56820759\n",
            "Iteration 44, loss = 0.56783919\n",
            "Iteration 45, loss = 0.56883613\n",
            "Iteration 46, loss = 0.56699237\n",
            "Iteration 47, loss = 0.56778063\n",
            "Iteration 48, loss = 0.56631873\n",
            "Iteration 49, loss = 0.56752065\n",
            "Iteration 50, loss = 0.56608763\n",
            "Iteration 51, loss = 0.56704586\n",
            "Iteration 52, loss = 0.56652661\n",
            "Iteration 53, loss = 0.56709681\n",
            "Iteration 54, loss = 0.56611226\n",
            "Iteration 55, loss = 0.56608640\n",
            "Iteration 56, loss = 0.56514887\n",
            "Iteration 57, loss = 0.56560845\n",
            "Iteration 58, loss = 0.56512466\n",
            "Iteration 59, loss = 0.56455227\n",
            "Iteration 60, loss = 0.56455012\n",
            "Iteration 61, loss = 0.56466817\n",
            "Iteration 62, loss = 0.56405654\n",
            "Iteration 63, loss = 0.56483243\n",
            "Iteration 64, loss = 0.56307162\n",
            "Iteration 65, loss = 0.56303958\n",
            "Iteration 66, loss = 0.56310495\n",
            "Iteration 67, loss = 0.56186475\n",
            "Iteration 68, loss = 0.56255942\n",
            "Iteration 69, loss = 0.56238249\n",
            "Iteration 70, loss = 0.56153301\n",
            "Iteration 71, loss = 0.56094403\n",
            "Iteration 72, loss = 0.56082541\n",
            "Iteration 73, loss = 0.55932393\n",
            "Iteration 74, loss = 0.55924135\n",
            "Iteration 75, loss = 0.55914233\n",
            "Iteration 76, loss = 0.55850689\n",
            "Iteration 77, loss = 0.55812658\n",
            "Iteration 78, loss = 0.55771548\n",
            "Iteration 79, loss = 0.55712875\n",
            "Iteration 80, loss = 0.55561598\n",
            "Iteration 81, loss = 0.55585541\n",
            "Iteration 82, loss = 0.55480994\n",
            "Iteration 83, loss = 0.55309646\n",
            "Iteration 84, loss = 0.55350594\n",
            "Iteration 85, loss = 0.55197861\n",
            "Iteration 86, loss = 0.55196110\n",
            "Iteration 87, loss = 0.55116578\n",
            "Iteration 88, loss = 0.55043942\n",
            "Iteration 89, loss = 0.54896103\n",
            "Iteration 90, loss = 0.54868650\n",
            "Iteration 91, loss = 0.54706243\n",
            "Iteration 92, loss = 0.54647935\n",
            "Iteration 93, loss = 0.54628532\n",
            "Iteration 94, loss = 0.54551279\n",
            "Iteration 95, loss = 0.54500218\n",
            "Iteration 96, loss = 0.54422165\n",
            "Iteration 97, loss = 0.54341200\n",
            "Iteration 98, loss = 0.54357678\n",
            "Iteration 99, loss = 0.54237846\n",
            "Iteration 100, loss = 0.54137353\n",
            "Iteration 101, loss = 0.54085895\n",
            "Iteration 102, loss = 0.54063911\n",
            "Iteration 103, loss = 0.54017703\n",
            "Iteration 104, loss = 0.53941309\n",
            "Iteration 105, loss = 0.53912942\n",
            "Iteration 106, loss = 0.53815707\n",
            "Iteration 107, loss = 0.53691880\n",
            "Iteration 108, loss = 0.53708248\n",
            "Iteration 109, loss = 0.53693629\n",
            "Iteration 110, loss = 0.53568673\n",
            "Iteration 111, loss = 0.53683955\n",
            "Iteration 112, loss = 0.53681114\n",
            "Iteration 113, loss = 0.53600349\n",
            "Iteration 114, loss = 0.53530676\n",
            "Iteration 115, loss = 0.53530714\n",
            "Iteration 116, loss = 0.53429204\n",
            "Iteration 117, loss = 0.53428633\n",
            "Iteration 118, loss = 0.53347395\n",
            "Iteration 119, loss = 0.53332694\n",
            "Iteration 120, loss = 0.53267596\n",
            "Iteration 121, loss = 0.53419371\n",
            "Iteration 122, loss = 0.53277930\n",
            "Iteration 123, loss = 0.53196181\n",
            "Iteration 124, loss = 0.53277130\n",
            "Iteration 125, loss = 0.53180178\n",
            "Iteration 126, loss = 0.53238803\n",
            "Iteration 127, loss = 0.53256424\n",
            "Iteration 128, loss = 0.53283850\n",
            "Iteration 129, loss = 0.53114626\n",
            "Iteration 130, loss = 0.53153372\n",
            "Iteration 131, loss = 0.53125912\n",
            "Iteration 132, loss = 0.53086036\n",
            "Iteration 133, loss = 0.53088892\n",
            "Iteration 134, loss = 0.53128384\n",
            "Iteration 135, loss = 0.53069811\n",
            "Iteration 136, loss = 0.52941927\n",
            "Iteration 137, loss = 0.52977142\n",
            "Iteration 138, loss = 0.53024540\n",
            "Iteration 139, loss = 0.53046246\n",
            "Iteration 140, loss = 0.53076277\n",
            "Iteration 141, loss = 0.52946262\n",
            "Iteration 142, loss = 0.53006316\n",
            "Iteration 143, loss = 0.52936088\n",
            "Iteration 144, loss = 0.52867272\n",
            "Iteration 145, loss = 0.52939327\n",
            "Iteration 146, loss = 0.52861876\n",
            "Iteration 147, loss = 0.52798527\n",
            "Iteration 148, loss = 0.52907151\n",
            "Iteration 149, loss = 0.52920248\n",
            "Iteration 150, loss = 0.52792063\n",
            "Iteration 151, loss = 0.52888888\n",
            "Iteration 152, loss = 0.52783063\n",
            "Iteration 153, loss = 0.52859199\n",
            "Iteration 154, loss = 0.52792764\n",
            "Iteration 155, loss = 0.52831062\n",
            "Iteration 156, loss = 0.52811032\n",
            "Iteration 157, loss = 0.52690761\n",
            "Iteration 158, loss = 0.52743788\n",
            "Iteration 159, loss = 0.52753777\n",
            "Iteration 160, loss = 0.52635895\n",
            "Iteration 161, loss = 0.52732856\n",
            "Iteration 162, loss = 0.52659037\n",
            "Iteration 163, loss = 0.52704516\n",
            "Iteration 164, loss = 0.52687225\n",
            "Iteration 165, loss = 0.52704544\n",
            "Iteration 166, loss = 0.52696403\n",
            "Iteration 167, loss = 0.52711268\n",
            "Iteration 168, loss = 0.52653001\n",
            "Iteration 169, loss = 0.52654652\n",
            "Iteration 170, loss = 0.52790553\n",
            "Iteration 171, loss = 0.52640428\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "13\n",
            "Iteration 1, loss = 0.57016690\n",
            "Iteration 2, loss = 0.56398517\n",
            "Iteration 3, loss = 0.56385972\n",
            "Iteration 4, loss = 0.56364082\n",
            "Iteration 5, loss = 0.56299374\n",
            "Iteration 6, loss = 0.56419868\n",
            "Iteration 7, loss = 0.56383587\n",
            "Iteration 8, loss = 0.56329988\n",
            "Iteration 9, loss = 0.56318124\n",
            "Iteration 10, loss = 0.56439074\n",
            "Iteration 11, loss = 0.56269431\n",
            "Iteration 12, loss = 0.56342883\n",
            "Iteration 13, loss = 0.56374299\n",
            "Iteration 14, loss = 0.56301510\n",
            "Iteration 15, loss = 0.56299786\n",
            "Iteration 16, loss = 0.56344897\n",
            "Iteration 17, loss = 0.56321648\n",
            "Iteration 18, loss = 0.56289172\n",
            "Iteration 19, loss = 0.56215673\n",
            "Iteration 20, loss = 0.56303089\n",
            "Iteration 21, loss = 0.56294628\n",
            "Iteration 22, loss = 0.56264555\n",
            "Iteration 23, loss = 0.56363149\n",
            "Iteration 24, loss = 0.56206279\n",
            "Iteration 25, loss = 0.56247072\n",
            "Iteration 26, loss = 0.56188610\n",
            "Iteration 27, loss = 0.56247141\n",
            "Iteration 28, loss = 0.56211063\n",
            "Iteration 29, loss = 0.56274742\n",
            "Iteration 30, loss = 0.56188809\n",
            "Iteration 31, loss = 0.56115115\n",
            "Iteration 32, loss = 0.56143614\n",
            "Iteration 33, loss = 0.56120077\n",
            "Iteration 34, loss = 0.56184893\n",
            "Iteration 35, loss = 0.56113756\n",
            "Iteration 36, loss = 0.56193830\n",
            "Iteration 37, loss = 0.56148979\n",
            "Iteration 38, loss = 0.56119406\n",
            "Iteration 39, loss = 0.56146942\n",
            "Iteration 40, loss = 0.56066376\n",
            "Iteration 41, loss = 0.56083413\n",
            "Iteration 42, loss = 0.55996941\n",
            "Iteration 43, loss = 0.56034737\n",
            "Iteration 44, loss = 0.55971374\n",
            "Iteration 45, loss = 0.55930865\n",
            "Iteration 46, loss = 0.55983847\n",
            "Iteration 47, loss = 0.55931913\n",
            "Iteration 48, loss = 0.56009009\n",
            "Iteration 49, loss = 0.55981378\n",
            "Iteration 50, loss = 0.55910808\n",
            "Iteration 51, loss = 0.56030734\n",
            "Iteration 52, loss = 0.55881062\n",
            "Iteration 53, loss = 0.55790680\n",
            "Iteration 54, loss = 0.55819022\n",
            "Iteration 55, loss = 0.55757196\n",
            "Iteration 56, loss = 0.55745186\n",
            "Iteration 57, loss = 0.55704420\n",
            "Iteration 58, loss = 0.55655324\n",
            "Iteration 59, loss = 0.55583557\n",
            "Iteration 60, loss = 0.55687641\n",
            "Iteration 61, loss = 0.55558500\n",
            "Iteration 62, loss = 0.55545094\n",
            "Iteration 63, loss = 0.55470473\n",
            "Iteration 64, loss = 0.55486840\n",
            "Iteration 65, loss = 0.55364970\n",
            "Iteration 66, loss = 0.55323936\n",
            "Iteration 67, loss = 0.55292429\n",
            "Iteration 68, loss = 0.55201938\n",
            "Iteration 69, loss = 0.55102191\n",
            "Iteration 70, loss = 0.55076893\n",
            "Iteration 71, loss = 0.55146604\n",
            "Iteration 72, loss = 0.55002410\n",
            "Iteration 73, loss = 0.54943528\n",
            "Iteration 74, loss = 0.54842860\n",
            "Iteration 75, loss = 0.54873120\n",
            "Iteration 76, loss = 0.54835080\n",
            "Iteration 77, loss = 0.54716279\n",
            "Iteration 78, loss = 0.54625610\n",
            "Iteration 79, loss = 0.54548471\n",
            "Iteration 80, loss = 0.54409580\n",
            "Iteration 81, loss = 0.54379870\n",
            "Iteration 82, loss = 0.54316446\n",
            "Iteration 83, loss = 0.54230378\n",
            "Iteration 84, loss = 0.54242064\n",
            "Iteration 85, loss = 0.54131787\n",
            "Iteration 86, loss = 0.54032063\n",
            "Iteration 87, loss = 0.54019861\n",
            "Iteration 88, loss = 0.53961672\n",
            "Iteration 89, loss = 0.53882110\n",
            "Iteration 90, loss = 0.53755769\n",
            "Iteration 91, loss = 0.53723785\n",
            "Iteration 92, loss = 0.53722543\n",
            "Iteration 93, loss = 0.53705479\n",
            "Iteration 94, loss = 0.53624267\n",
            "Iteration 95, loss = 0.53469185\n",
            "Iteration 96, loss = 0.53531257\n",
            "Iteration 97, loss = 0.53425275\n",
            "Iteration 98, loss = 0.53408388\n",
            "Iteration 99, loss = 0.53339529\n",
            "Iteration 100, loss = 0.53372369\n",
            "Iteration 101, loss = 0.53309495\n",
            "Iteration 102, loss = 0.53114792\n",
            "Iteration 103, loss = 0.53150392\n",
            "Iteration 104, loss = 0.53128709\n",
            "Iteration 105, loss = 0.53105493\n",
            "Iteration 106, loss = 0.53084129\n",
            "Iteration 107, loss = 0.53058209\n",
            "Iteration 108, loss = 0.53095501\n",
            "Iteration 109, loss = 0.52958097\n",
            "Iteration 110, loss = 0.52940367\n",
            "Iteration 111, loss = 0.52826388\n",
            "Iteration 112, loss = 0.52924375\n",
            "Iteration 113, loss = 0.52928220\n",
            "Iteration 114, loss = 0.52849772\n",
            "Iteration 115, loss = 0.52866565\n",
            "Iteration 116, loss = 0.52831686\n",
            "Iteration 117, loss = 0.52800365\n",
            "Iteration 118, loss = 0.52839269\n",
            "Iteration 119, loss = 0.52788939\n",
            "Iteration 120, loss = 0.52780940\n",
            "Iteration 121, loss = 0.52785196\n",
            "Iteration 122, loss = 0.52746186\n",
            "Iteration 123, loss = 0.52719110\n",
            "Iteration 124, loss = 0.52684488\n",
            "Iteration 125, loss = 0.52657340\n",
            "Iteration 126, loss = 0.52573678\n",
            "Iteration 127, loss = 0.52659997\n",
            "Iteration 128, loss = 0.52668455\n",
            "Iteration 129, loss = 0.52559347\n",
            "Iteration 130, loss = 0.52591840\n",
            "Iteration 131, loss = 0.52633255\n",
            "Iteration 132, loss = 0.52526875\n",
            "Iteration 133, loss = 0.52694009\n",
            "Iteration 134, loss = 0.52635409\n",
            "Iteration 135, loss = 0.52566516\n",
            "Iteration 136, loss = 0.52526198\n",
            "Iteration 137, loss = 0.52408262\n",
            "Iteration 138, loss = 0.52508836\n",
            "Iteration 139, loss = 0.52499593\n",
            "Iteration 140, loss = 0.52423528\n",
            "Iteration 141, loss = 0.52470085\n",
            "Iteration 142, loss = 0.52413102\n",
            "Iteration 143, loss = 0.52462611\n",
            "Iteration 144, loss = 0.52376025\n",
            "Iteration 145, loss = 0.52467502\n",
            "Iteration 146, loss = 0.52450998\n",
            "Iteration 147, loss = 0.52398536\n",
            "Iteration 148, loss = 0.52338323\n",
            "Iteration 149, loss = 0.52354157\n",
            "Iteration 150, loss = 0.52371261\n",
            "Iteration 151, loss = 0.52390544\n",
            "Iteration 152, loss = 0.52394711\n",
            "Iteration 153, loss = 0.52360296\n",
            "Iteration 154, loss = 0.52320622\n",
            "Iteration 155, loss = 0.52334728\n",
            "Iteration 156, loss = 0.52255824\n",
            "Iteration 157, loss = 0.52415326\n",
            "Iteration 158, loss = 0.52384214\n",
            "Iteration 159, loss = 0.52416006\n",
            "Iteration 160, loss = 0.52276304\n",
            "Iteration 161, loss = 0.52405377\n",
            "Iteration 162, loss = 0.52358496\n",
            "Iteration 163, loss = 0.52256662\n",
            "Iteration 164, loss = 0.52276008\n",
            "Iteration 165, loss = 0.52267991\n",
            "Iteration 166, loss = 0.52330294\n",
            "Iteration 167, loss = 0.52285113\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56636199\n",
            "Iteration 2, loss = 0.56360424\n",
            "Iteration 3, loss = 0.56287928\n",
            "Iteration 4, loss = 0.56356648\n",
            "Iteration 5, loss = 0.56340811\n",
            "Iteration 6, loss = 0.56302024\n",
            "Iteration 7, loss = 0.56270552\n",
            "Iteration 8, loss = 0.56281645\n",
            "Iteration 9, loss = 0.56366255\n",
            "Iteration 10, loss = 0.56321745\n",
            "Iteration 11, loss = 0.56262961\n",
            "Iteration 12, loss = 0.56323204\n",
            "Iteration 13, loss = 0.56284987\n",
            "Iteration 14, loss = 0.56279309\n",
            "Iteration 15, loss = 0.56300958\n",
            "Iteration 16, loss = 0.56246462\n",
            "Iteration 17, loss = 0.56290068\n",
            "Iteration 18, loss = 0.56226629\n",
            "Iteration 19, loss = 0.56282622\n",
            "Iteration 20, loss = 0.56246155\n",
            "Iteration 21, loss = 0.56237462\n",
            "Iteration 22, loss = 0.56279174\n",
            "Iteration 23, loss = 0.56251441\n",
            "Iteration 24, loss = 0.56203187\n",
            "Iteration 25, loss = 0.56179351\n",
            "Iteration 26, loss = 0.56211129\n",
            "Iteration 27, loss = 0.56222217\n",
            "Iteration 28, loss = 0.56081661\n",
            "Iteration 29, loss = 0.56149036\n",
            "Iteration 30, loss = 0.56244669\n",
            "Iteration 31, loss = 0.56060688\n",
            "Iteration 32, loss = 0.56191916\n",
            "Iteration 33, loss = 0.56168752\n",
            "Iteration 34, loss = 0.56201157\n",
            "Iteration 35, loss = 0.56223618\n",
            "Iteration 36, loss = 0.56178335\n",
            "Iteration 37, loss = 0.56117542\n",
            "Iteration 38, loss = 0.56043903\n",
            "Iteration 39, loss = 0.56110815\n",
            "Iteration 40, loss = 0.56148264\n",
            "Iteration 41, loss = 0.56156540\n",
            "Iteration 42, loss = 0.56167700\n",
            "Iteration 43, loss = 0.56092948\n",
            "Iteration 44, loss = 0.56091833\n",
            "Iteration 45, loss = 0.56024677\n",
            "Iteration 46, loss = 0.56061449\n",
            "Iteration 47, loss = 0.56041603\n",
            "Iteration 48, loss = 0.56068056\n",
            "Iteration 49, loss = 0.55942549\n",
            "Iteration 50, loss = 0.55894550\n",
            "Iteration 51, loss = 0.55999048\n",
            "Iteration 52, loss = 0.56050091\n",
            "Iteration 53, loss = 0.55930325\n",
            "Iteration 54, loss = 0.55985241\n",
            "Iteration 55, loss = 0.55992641\n",
            "Iteration 56, loss = 0.55930682\n",
            "Iteration 57, loss = 0.55922398\n",
            "Iteration 58, loss = 0.55825981\n",
            "Iteration 59, loss = 0.55814685\n",
            "Iteration 60, loss = 0.55923030\n",
            "Iteration 61, loss = 0.55816558\n",
            "Iteration 62, loss = 0.55847937\n",
            "Iteration 63, loss = 0.55845350\n",
            "Iteration 64, loss = 0.55839779\n",
            "Iteration 65, loss = 0.55797716\n",
            "Iteration 66, loss = 0.55719551\n",
            "Iteration 67, loss = 0.55522342\n",
            "Iteration 68, loss = 0.55707657\n",
            "Iteration 69, loss = 0.55666257\n",
            "Iteration 70, loss = 0.55526372\n",
            "Iteration 71, loss = 0.55636104\n",
            "Iteration 72, loss = 0.55510933\n",
            "Iteration 73, loss = 0.55521247\n",
            "Iteration 74, loss = 0.55466638\n",
            "Iteration 75, loss = 0.55459229\n",
            "Iteration 76, loss = 0.55347667\n",
            "Iteration 77, loss = 0.55286270\n",
            "Iteration 78, loss = 0.55355312\n",
            "Iteration 79, loss = 0.55170089\n",
            "Iteration 80, loss = 0.55235236\n",
            "Iteration 81, loss = 0.55093019\n",
            "Iteration 82, loss = 0.55135079\n",
            "Iteration 83, loss = 0.55053210\n",
            "Iteration 84, loss = 0.54953144\n",
            "Iteration 85, loss = 0.54925707\n",
            "Iteration 86, loss = 0.54841950\n",
            "Iteration 87, loss = 0.54782422\n",
            "Iteration 88, loss = 0.54692989\n",
            "Iteration 89, loss = 0.54679130\n",
            "Iteration 90, loss = 0.54524489\n",
            "Iteration 91, loss = 0.54424507\n",
            "Iteration 92, loss = 0.54455015\n",
            "Iteration 93, loss = 0.54362766\n",
            "Iteration 94, loss = 0.54267082\n",
            "Iteration 95, loss = 0.54179347\n",
            "Iteration 96, loss = 0.54131993\n",
            "Iteration 97, loss = 0.54056812\n",
            "Iteration 98, loss = 0.54045954\n",
            "Iteration 99, loss = 0.53928794\n",
            "Iteration 100, loss = 0.53879434\n",
            "Iteration 101, loss = 0.53793003\n",
            "Iteration 102, loss = 0.53729851\n",
            "Iteration 103, loss = 0.53619292\n",
            "Iteration 104, loss = 0.53552912\n",
            "Iteration 105, loss = 0.53603244\n",
            "Iteration 106, loss = 0.53491321\n",
            "Iteration 107, loss = 0.53478700\n",
            "Iteration 108, loss = 0.53348372\n",
            "Iteration 109, loss = 0.53367633\n",
            "Iteration 110, loss = 0.53373518\n",
            "Iteration 111, loss = 0.53252640\n",
            "Iteration 112, loss = 0.53187085\n",
            "Iteration 113, loss = 0.53015255\n",
            "Iteration 114, loss = 0.53124059\n",
            "Iteration 115, loss = 0.52919505\n",
            "Iteration 116, loss = 0.53141629\n",
            "Iteration 117, loss = 0.52964139\n",
            "Iteration 118, loss = 0.52972695\n",
            "Iteration 119, loss = 0.52998484\n",
            "Iteration 120, loss = 0.52887765\n",
            "Iteration 121, loss = 0.52809801\n",
            "Iteration 122, loss = 0.52816938\n",
            "Iteration 123, loss = 0.52815108\n",
            "Iteration 124, loss = 0.52846775\n",
            "Iteration 125, loss = 0.52775053\n",
            "Iteration 126, loss = 0.52681030\n",
            "Iteration 127, loss = 0.52701703\n",
            "Iteration 128, loss = 0.52625146\n",
            "Iteration 129, loss = 0.52652137\n",
            "Iteration 130, loss = 0.52647955\n",
            "Iteration 131, loss = 0.52562345\n",
            "Iteration 132, loss = 0.52681814\n",
            "Iteration 133, loss = 0.52561549\n",
            "Iteration 134, loss = 0.52495056\n",
            "Iteration 135, loss = 0.52572996\n",
            "Iteration 136, loss = 0.52486275\n",
            "Iteration 137, loss = 0.52501596\n",
            "Iteration 138, loss = 0.52465052\n",
            "Iteration 139, loss = 0.52442456\n",
            "Iteration 140, loss = 0.52491313\n",
            "Iteration 141, loss = 0.52396201\n",
            "Iteration 142, loss = 0.52395641\n",
            "Iteration 143, loss = 0.52323295\n",
            "Iteration 144, loss = 0.52307648\n",
            "Iteration 145, loss = 0.52441416\n",
            "Iteration 146, loss = 0.52304582\n",
            "Iteration 147, loss = 0.52261136\n",
            "Iteration 148, loss = 0.52324593\n",
            "Iteration 149, loss = 0.52327387\n",
            "Iteration 150, loss = 0.52363794\n",
            "Iteration 151, loss = 0.52222129\n",
            "Iteration 152, loss = 0.52298154\n",
            "Iteration 153, loss = 0.52190385\n",
            "Iteration 154, loss = 0.52186005\n",
            "Iteration 155, loss = 0.52254937\n",
            "Iteration 156, loss = 0.52221443\n",
            "Iteration 157, loss = 0.52307304\n",
            "Iteration 158, loss = 0.52254328\n",
            "Iteration 159, loss = 0.52194061\n",
            "Iteration 160, loss = 0.52210210\n",
            "Iteration 161, loss = 0.52199344\n",
            "Iteration 162, loss = 0.52257129\n",
            "Iteration 163, loss = 0.52148141\n",
            "Iteration 164, loss = 0.52201363\n",
            "Iteration 165, loss = 0.52139015\n",
            "Iteration 166, loss = 0.52165618\n",
            "Iteration 167, loss = 0.52174414\n",
            "Iteration 168, loss = 0.52133280\n",
            "Iteration 169, loss = 0.52086055\n",
            "Iteration 170, loss = 0.52161357\n",
            "Iteration 171, loss = 0.52070204\n",
            "Iteration 172, loss = 0.52067373\n",
            "Iteration 173, loss = 0.52062632\n",
            "Iteration 174, loss = 0.52080975\n",
            "Iteration 175, loss = 0.51966843\n",
            "Iteration 176, loss = 0.52119352\n",
            "Iteration 177, loss = 0.52118247\n",
            "Iteration 178, loss = 0.52085738\n",
            "Iteration 179, loss = 0.52032010\n",
            "Iteration 180, loss = 0.52066205\n",
            "Iteration 181, loss = 0.52036955\n",
            "Iteration 182, loss = 0.51937248\n",
            "Iteration 183, loss = 0.52025639\n",
            "Iteration 184, loss = 0.52059370\n",
            "Iteration 185, loss = 0.52062344\n",
            "Iteration 186, loss = 0.51994925\n",
            "Iteration 187, loss = 0.52005516\n",
            "Iteration 188, loss = 0.52056438\n",
            "Iteration 189, loss = 0.52017243\n",
            "Iteration 190, loss = 0.52016358\n",
            "Iteration 191, loss = 0.51988158\n",
            "Iteration 192, loss = 0.52042883\n",
            "Iteration 193, loss = 0.52028997\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56734157\n",
            "Iteration 2, loss = 0.55745006\n",
            "Iteration 3, loss = 0.55733298\n",
            "Iteration 4, loss = 0.55687506\n",
            "Iteration 5, loss = 0.55665885\n",
            "Iteration 6, loss = 0.55675660\n",
            "Iteration 7, loss = 0.55602372\n",
            "Iteration 8, loss = 0.55722078\n",
            "Iteration 9, loss = 0.55578115\n",
            "Iteration 10, loss = 0.55642973\n",
            "Iteration 11, loss = 0.55673217\n",
            "Iteration 12, loss = 0.55561450\n",
            "Iteration 13, loss = 0.55648886\n",
            "Iteration 14, loss = 0.55665898\n",
            "Iteration 15, loss = 0.55632741\n",
            "Iteration 16, loss = 0.55605006\n",
            "Iteration 17, loss = 0.55701747\n",
            "Iteration 18, loss = 0.55633886\n",
            "Iteration 19, loss = 0.55604257\n",
            "Iteration 20, loss = 0.55657254\n",
            "Iteration 21, loss = 0.55618157\n",
            "Iteration 22, loss = 0.55602752\n",
            "Iteration 23, loss = 0.55532515\n",
            "Iteration 24, loss = 0.55651070\n",
            "Iteration 25, loss = 0.55573224\n",
            "Iteration 26, loss = 0.55554722\n",
            "Iteration 27, loss = 0.55577736\n",
            "Iteration 28, loss = 0.55484725\n",
            "Iteration 29, loss = 0.55626388\n",
            "Iteration 30, loss = 0.55465143\n",
            "Iteration 31, loss = 0.55568325\n",
            "Iteration 32, loss = 0.55511602\n",
            "Iteration 33, loss = 0.55577022\n",
            "Iteration 34, loss = 0.55537664\n",
            "Iteration 35, loss = 0.55545683\n",
            "Iteration 36, loss = 0.55542102\n",
            "Iteration 37, loss = 0.55491021\n",
            "Iteration 38, loss = 0.55525209\n",
            "Iteration 39, loss = 0.55506544\n",
            "Iteration 40, loss = 0.55411251\n",
            "Iteration 41, loss = 0.55527089\n",
            "Iteration 42, loss = 0.55490003\n",
            "Iteration 43, loss = 0.55414212\n",
            "Iteration 44, loss = 0.55477205\n",
            "Iteration 45, loss = 0.55442616\n",
            "Iteration 46, loss = 0.55392921\n",
            "Iteration 47, loss = 0.55393124\n",
            "Iteration 48, loss = 0.55424985\n",
            "Iteration 49, loss = 0.55466583\n",
            "Iteration 50, loss = 0.55393977\n",
            "Iteration 51, loss = 0.55424342\n",
            "Iteration 52, loss = 0.55397002\n",
            "Iteration 53, loss = 0.55270318\n",
            "Iteration 54, loss = 0.55335083\n",
            "Iteration 55, loss = 0.55332382\n",
            "Iteration 56, loss = 0.55331286\n",
            "Iteration 57, loss = 0.55312942\n",
            "Iteration 58, loss = 0.55308875\n",
            "Iteration 59, loss = 0.55270767\n",
            "Iteration 60, loss = 0.55215503\n",
            "Iteration 61, loss = 0.55226161\n",
            "Iteration 62, loss = 0.55210307\n",
            "Iteration 63, loss = 0.55092122\n",
            "Iteration 64, loss = 0.55174150\n",
            "Iteration 65, loss = 0.55129591\n",
            "Iteration 66, loss = 0.55107090\n",
            "Iteration 67, loss = 0.55107925\n",
            "Iteration 68, loss = 0.55050295\n",
            "Iteration 69, loss = 0.54949454\n",
            "Iteration 70, loss = 0.55067131\n",
            "Iteration 71, loss = 0.54942026\n",
            "Iteration 72, loss = 0.54967236\n",
            "Iteration 73, loss = 0.54899094\n",
            "Iteration 74, loss = 0.54834031\n",
            "Iteration 75, loss = 0.54768632\n",
            "Iteration 76, loss = 0.54678995\n",
            "Iteration 77, loss = 0.54691321\n",
            "Iteration 78, loss = 0.54663918\n",
            "Iteration 79, loss = 0.54671824\n",
            "Iteration 80, loss = 0.54589591\n",
            "Iteration 81, loss = 0.54500587\n",
            "Iteration 82, loss = 0.54481729\n",
            "Iteration 83, loss = 0.54440550\n",
            "Iteration 84, loss = 0.54399372\n",
            "Iteration 85, loss = 0.54344696\n",
            "Iteration 86, loss = 0.54222899\n",
            "Iteration 87, loss = 0.54148036\n",
            "Iteration 88, loss = 0.54059122\n",
            "Iteration 89, loss = 0.54024975\n",
            "Iteration 90, loss = 0.54004306\n",
            "Iteration 91, loss = 0.53918274\n",
            "Iteration 92, loss = 0.53833296\n",
            "Iteration 93, loss = 0.53741336\n",
            "Iteration 94, loss = 0.53785733\n",
            "Iteration 95, loss = 0.53587908\n",
            "Iteration 96, loss = 0.53610410\n",
            "Iteration 97, loss = 0.53546041\n",
            "Iteration 98, loss = 0.53478703\n",
            "Iteration 99, loss = 0.53332263\n",
            "Iteration 100, loss = 0.53268011\n",
            "Iteration 101, loss = 0.53243593\n",
            "Iteration 102, loss = 0.53097888\n",
            "Iteration 103, loss = 0.53127949\n",
            "Iteration 104, loss = 0.53040808\n",
            "Iteration 105, loss = 0.52984287\n",
            "Iteration 106, loss = 0.52913895\n",
            "Iteration 107, loss = 0.52846798\n",
            "Iteration 108, loss = 0.52755947\n",
            "Iteration 109, loss = 0.52700493\n",
            "Iteration 110, loss = 0.52749202\n",
            "Iteration 111, loss = 0.52619951\n",
            "Iteration 112, loss = 0.52653209\n",
            "Iteration 113, loss = 0.52483419\n",
            "Iteration 114, loss = 0.52485201\n",
            "Iteration 115, loss = 0.52523787\n",
            "Iteration 116, loss = 0.52442106\n",
            "Iteration 117, loss = 0.52433158\n",
            "Iteration 118, loss = 0.52366344\n",
            "Iteration 119, loss = 0.52304457\n",
            "Iteration 120, loss = 0.52296857\n",
            "Iteration 121, loss = 0.52326277\n",
            "Iteration 122, loss = 0.52200517\n",
            "Iteration 123, loss = 0.52222412\n",
            "Iteration 124, loss = 0.52138878\n",
            "Iteration 125, loss = 0.52115563\n",
            "Iteration 126, loss = 0.52172783\n",
            "Iteration 127, loss = 0.52151155\n",
            "Iteration 128, loss = 0.52205109\n",
            "Iteration 129, loss = 0.51967180\n",
            "Iteration 130, loss = 0.52019156\n",
            "Iteration 131, loss = 0.52009372\n",
            "Iteration 132, loss = 0.52057349\n",
            "Iteration 133, loss = 0.51950441\n",
            "Iteration 134, loss = 0.51901032\n",
            "Iteration 135, loss = 0.51953656\n",
            "Iteration 136, loss = 0.51947961\n",
            "Iteration 137, loss = 0.51889289\n",
            "Iteration 138, loss = 0.51875061\n",
            "Iteration 139, loss = 0.52030798\n",
            "Iteration 140, loss = 0.51927663\n",
            "Iteration 141, loss = 0.51880821\n",
            "Iteration 142, loss = 0.51828605\n",
            "Iteration 143, loss = 0.51803278\n",
            "Iteration 144, loss = 0.51740585\n",
            "Iteration 145, loss = 0.51746581\n",
            "Iteration 146, loss = 0.51762625\n",
            "Iteration 147, loss = 0.51723831\n",
            "Iteration 148, loss = 0.51784603\n",
            "Iteration 149, loss = 0.51677119\n",
            "Iteration 150, loss = 0.51650823\n",
            "Iteration 151, loss = 0.51711959\n",
            "Iteration 152, loss = 0.51680709\n",
            "Iteration 153, loss = 0.51688569\n",
            "Iteration 154, loss = 0.51655272\n",
            "Iteration 155, loss = 0.51724765\n",
            "Iteration 156, loss = 0.51643352\n",
            "Iteration 157, loss = 0.51635197\n",
            "Iteration 158, loss = 0.51598834\n",
            "Iteration 159, loss = 0.51637389\n",
            "Iteration 160, loss = 0.51597178\n",
            "Iteration 161, loss = 0.51599201\n",
            "Iteration 162, loss = 0.51582164\n",
            "Iteration 163, loss = 0.51698426\n",
            "Iteration 164, loss = 0.51565066\n",
            "Iteration 165, loss = 0.51586415\n",
            "Iteration 166, loss = 0.51579120\n",
            "Iteration 167, loss = 0.51587246\n",
            "Iteration 168, loss = 0.51587872\n",
            "Iteration 169, loss = 0.51510296\n",
            "Iteration 170, loss = 0.51507506\n",
            "Iteration 171, loss = 0.51521224\n",
            "Iteration 172, loss = 0.51554265\n",
            "Iteration 173, loss = 0.51411498\n",
            "Iteration 174, loss = 0.51555793\n",
            "Iteration 175, loss = 0.51419414\n",
            "Iteration 176, loss = 0.51425858\n",
            "Iteration 177, loss = 0.51461243\n",
            "Iteration 178, loss = 0.51475329\n",
            "Iteration 179, loss = 0.51534989\n",
            "Iteration 180, loss = 0.51435648\n",
            "Iteration 181, loss = 0.51419908\n",
            "Iteration 182, loss = 0.51313641\n",
            "Iteration 183, loss = 0.51456723\n",
            "Iteration 184, loss = 0.51466980\n",
            "Iteration 185, loss = 0.51389856\n",
            "Iteration 186, loss = 0.51458037\n",
            "Iteration 187, loss = 0.51430081\n",
            "Iteration 188, loss = 0.51394345\n",
            "Iteration 189, loss = 0.51443126\n",
            "Iteration 190, loss = 0.51416930\n",
            "Iteration 191, loss = 0.51379050\n",
            "Iteration 192, loss = 0.51438503\n",
            "Iteration 193, loss = 0.51471860\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57321147\n",
            "Iteration 2, loss = 0.56239960\n",
            "Iteration 3, loss = 0.56294622\n",
            "Iteration 4, loss = 0.56308558\n",
            "Iteration 5, loss = 0.56239536\n",
            "Iteration 6, loss = 0.56208804\n",
            "Iteration 7, loss = 0.56189813\n",
            "Iteration 8, loss = 0.56165971\n",
            "Iteration 9, loss = 0.56228430\n",
            "Iteration 10, loss = 0.56261635\n",
            "Iteration 11, loss = 0.56206100\n",
            "Iteration 12, loss = 0.56255948\n",
            "Iteration 13, loss = 0.56170586\n",
            "Iteration 14, loss = 0.56308525\n",
            "Iteration 15, loss = 0.56117606\n",
            "Iteration 16, loss = 0.56229261\n",
            "Iteration 17, loss = 0.56238029\n",
            "Iteration 18, loss = 0.56186885\n",
            "Iteration 19, loss = 0.56149499\n",
            "Iteration 20, loss = 0.56193816\n",
            "Iteration 21, loss = 0.56112439\n",
            "Iteration 22, loss = 0.56157401\n",
            "Iteration 23, loss = 0.56099223\n",
            "Iteration 24, loss = 0.56146849\n",
            "Iteration 25, loss = 0.56174746\n",
            "Iteration 26, loss = 0.56118523\n",
            "Iteration 27, loss = 0.56181385\n",
            "Iteration 28, loss = 0.56089677\n",
            "Iteration 29, loss = 0.56105028\n",
            "Iteration 30, loss = 0.56024089\n",
            "Iteration 31, loss = 0.56134996\n",
            "Iteration 32, loss = 0.56102108\n",
            "Iteration 33, loss = 0.56100598\n",
            "Iteration 34, loss = 0.56054626\n",
            "Iteration 35, loss = 0.56054340\n",
            "Iteration 36, loss = 0.56000520\n",
            "Iteration 37, loss = 0.56047128\n",
            "Iteration 38, loss = 0.56037551\n",
            "Iteration 39, loss = 0.55967742\n",
            "Iteration 40, loss = 0.56004599\n",
            "Iteration 41, loss = 0.55951608\n",
            "Iteration 42, loss = 0.55997854\n",
            "Iteration 43, loss = 0.55935975\n",
            "Iteration 44, loss = 0.55965751\n",
            "Iteration 45, loss = 0.55923811\n",
            "Iteration 46, loss = 0.55927967\n",
            "Iteration 47, loss = 0.55772285\n",
            "Iteration 48, loss = 0.55908977\n",
            "Iteration 49, loss = 0.55883924\n",
            "Iteration 50, loss = 0.55837500\n",
            "Iteration 51, loss = 0.55846176\n",
            "Iteration 52, loss = 0.55859852\n",
            "Iteration 53, loss = 0.55768901\n",
            "Iteration 54, loss = 0.55768379\n",
            "Iteration 55, loss = 0.55747571\n",
            "Iteration 56, loss = 0.55652590\n",
            "Iteration 57, loss = 0.55729769\n",
            "Iteration 58, loss = 0.55594075\n",
            "Iteration 59, loss = 0.55637698\n",
            "Iteration 60, loss = 0.55607082\n",
            "Iteration 61, loss = 0.55505633\n",
            "Iteration 62, loss = 0.55508229\n",
            "Iteration 63, loss = 0.55450615\n",
            "Iteration 64, loss = 0.55395068\n",
            "Iteration 65, loss = 0.55394023\n",
            "Iteration 66, loss = 0.55327258\n",
            "Iteration 67, loss = 0.55283671\n",
            "Iteration 68, loss = 0.55273329\n",
            "Iteration 69, loss = 0.55209201\n",
            "Iteration 70, loss = 0.55155678\n",
            "Iteration 71, loss = 0.55106376\n",
            "Iteration 72, loss = 0.55023314\n",
            "Iteration 73, loss = 0.54957796\n",
            "Iteration 74, loss = 0.54909194\n",
            "Iteration 75, loss = 0.54763312\n",
            "Iteration 76, loss = 0.54700949\n",
            "Iteration 77, loss = 0.54681365\n",
            "Iteration 78, loss = 0.54654057\n",
            "Iteration 79, loss = 0.54559848\n",
            "Iteration 80, loss = 0.54434120\n",
            "Iteration 81, loss = 0.54388094\n",
            "Iteration 82, loss = 0.54263143\n",
            "Iteration 83, loss = 0.54181038\n",
            "Iteration 84, loss = 0.54082301\n",
            "Iteration 85, loss = 0.53981687\n",
            "Iteration 86, loss = 0.53894285\n",
            "Iteration 87, loss = 0.53882462\n",
            "Iteration 88, loss = 0.53748662\n",
            "Iteration 89, loss = 0.53699321\n",
            "Iteration 90, loss = 0.53658885\n",
            "Iteration 91, loss = 0.53484663\n",
            "Iteration 92, loss = 0.53452868\n",
            "Iteration 93, loss = 0.53421903\n",
            "Iteration 94, loss = 0.53256805\n",
            "Iteration 95, loss = 0.53298608\n",
            "Iteration 96, loss = 0.53209392\n",
            "Iteration 97, loss = 0.53138136\n",
            "Iteration 98, loss = 0.53052664\n",
            "Iteration 99, loss = 0.53069423\n",
            "Iteration 100, loss = 0.53003559\n",
            "Iteration 101, loss = 0.52947483\n",
            "Iteration 102, loss = 0.52801948\n",
            "Iteration 103, loss = 0.52873148\n",
            "Iteration 104, loss = 0.52781419\n",
            "Iteration 105, loss = 0.52707960\n",
            "Iteration 106, loss = 0.52720248\n",
            "Iteration 107, loss = 0.52610168\n",
            "Iteration 108, loss = 0.52583906\n",
            "Iteration 109, loss = 0.52539172\n",
            "Iteration 110, loss = 0.52539931\n",
            "Iteration 111, loss = 0.52566092\n",
            "Iteration 112, loss = 0.52448361\n",
            "Iteration 113, loss = 0.52387025\n",
            "Iteration 114, loss = 0.52395887\n",
            "Iteration 115, loss = 0.52424314\n",
            "Iteration 116, loss = 0.52408031\n",
            "Iteration 117, loss = 0.52309081\n",
            "Iteration 118, loss = 0.52365083\n",
            "Iteration 119, loss = 0.52315926\n",
            "Iteration 120, loss = 0.52270520\n",
            "Iteration 121, loss = 0.52155153\n",
            "Iteration 122, loss = 0.52223188\n",
            "Iteration 123, loss = 0.52248049\n",
            "Iteration 124, loss = 0.52171038\n",
            "Iteration 125, loss = 0.52224321\n",
            "Iteration 126, loss = 0.52167999\n",
            "Iteration 127, loss = 0.52116705\n",
            "Iteration 128, loss = 0.52137948\n",
            "Iteration 129, loss = 0.52111181\n",
            "Iteration 130, loss = 0.52035142\n",
            "Iteration 131, loss = 0.52109494\n",
            "Iteration 132, loss = 0.51951843\n",
            "Iteration 133, loss = 0.52017223\n",
            "Iteration 134, loss = 0.51913382\n",
            "Iteration 135, loss = 0.51954189\n",
            "Iteration 136, loss = 0.51965137\n",
            "Iteration 137, loss = 0.51955035\n",
            "Iteration 138, loss = 0.52000034\n",
            "Iteration 139, loss = 0.51922961\n",
            "Iteration 140, loss = 0.51846706\n",
            "Iteration 141, loss = 0.51895856\n",
            "Iteration 142, loss = 0.51956302\n",
            "Iteration 143, loss = 0.51836408\n",
            "Iteration 144, loss = 0.51899105\n",
            "Iteration 145, loss = 0.51860470\n",
            "Iteration 146, loss = 0.51846386\n",
            "Iteration 147, loss = 0.51829715\n",
            "Iteration 148, loss = 0.51783012\n",
            "Iteration 149, loss = 0.51847345\n",
            "Iteration 150, loss = 0.51808510\n",
            "Iteration 151, loss = 0.51815250\n",
            "Iteration 152, loss = 0.51688663\n",
            "Iteration 153, loss = 0.51824434\n",
            "Iteration 154, loss = 0.51769797\n",
            "Iteration 155, loss = 0.51654122\n",
            "Iteration 156, loss = 0.51802292\n",
            "Iteration 157, loss = 0.51726054\n",
            "Iteration 158, loss = 0.51711428\n",
            "Iteration 159, loss = 0.51705701\n",
            "Iteration 160, loss = 0.51736803\n",
            "Iteration 161, loss = 0.51698429\n",
            "Iteration 162, loss = 0.51705253\n",
            "Iteration 163, loss = 0.51700762\n",
            "Iteration 164, loss = 0.51720992\n",
            "Iteration 165, loss = 0.51641647\n",
            "Iteration 166, loss = 0.51664840\n",
            "Iteration 167, loss = 0.51682131\n",
            "Iteration 168, loss = 0.51650243\n",
            "Iteration 169, loss = 0.51645996\n",
            "Iteration 170, loss = 0.51711885\n",
            "Iteration 171, loss = 0.51689303\n",
            "Iteration 172, loss = 0.51597158\n",
            "Iteration 173, loss = 0.51634800\n",
            "Iteration 174, loss = 0.51670123\n",
            "Iteration 175, loss = 0.51668197\n",
            "Iteration 176, loss = 0.51587067\n",
            "Iteration 177, loss = 0.51590662\n",
            "Iteration 178, loss = 0.51573336\n",
            "Iteration 179, loss = 0.51649692\n",
            "Iteration 180, loss = 0.51610554\n",
            "Iteration 181, loss = 0.51582613\n",
            "Iteration 182, loss = 0.51613591\n",
            "Iteration 183, loss = 0.51558699\n",
            "Iteration 184, loss = 0.51694904\n",
            "Iteration 185, loss = 0.51512381\n",
            "Iteration 186, loss = 0.51547090\n",
            "Iteration 187, loss = 0.51500920\n",
            "Iteration 188, loss = 0.51648399\n",
            "Iteration 189, loss = 0.51573276\n",
            "Iteration 190, loss = 0.51656263\n",
            "Iteration 191, loss = 0.51573684\n",
            "Iteration 192, loss = 0.51646487\n",
            "Iteration 193, loss = 0.51612779\n",
            "Iteration 194, loss = 0.51466925\n",
            "Iteration 195, loss = 0.51486140\n",
            "Iteration 196, loss = 0.51517595\n",
            "Iteration 197, loss = 0.51414661\n",
            "Iteration 198, loss = 0.51481228\n",
            "Iteration 199, loss = 0.51534203\n",
            "Iteration 200, loss = 0.51460409\n",
            "Iteration 201, loss = 0.51497492\n",
            "Iteration 202, loss = 0.51556998\n",
            "Iteration 203, loss = 0.51403748\n",
            "Iteration 204, loss = 0.51561278\n",
            "Iteration 205, loss = 0.51592984\n",
            "Iteration 206, loss = 0.51537028\n",
            "Iteration 207, loss = 0.51465813\n",
            "Iteration 208, loss = 0.51476612\n",
            "Iteration 209, loss = 0.51474447\n",
            "Iteration 210, loss = 0.51563191\n",
            "Iteration 211, loss = 0.51543089\n",
            "Iteration 212, loss = 0.51542497\n",
            "Iteration 213, loss = 0.51417349\n",
            "Iteration 214, loss = 0.51636225\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56888120\n",
            "Iteration 2, loss = 0.56246182\n",
            "Iteration 3, loss = 0.56220944\n",
            "Iteration 4, loss = 0.56168790\n",
            "Iteration 5, loss = 0.56207936\n",
            "Iteration 6, loss = 0.56206488\n",
            "Iteration 7, loss = 0.56147770\n",
            "Iteration 8, loss = 0.56166939\n",
            "Iteration 9, loss = 0.56113904\n",
            "Iteration 10, loss = 0.56137911\n",
            "Iteration 11, loss = 0.56182954\n",
            "Iteration 12, loss = 0.56274277\n",
            "Iteration 13, loss = 0.56096711\n",
            "Iteration 14, loss = 0.56111932\n",
            "Iteration 15, loss = 0.56194031\n",
            "Iteration 16, loss = 0.56115408\n",
            "Iteration 17, loss = 0.56153716\n",
            "Iteration 18, loss = 0.56128073\n",
            "Iteration 19, loss = 0.56141275\n",
            "Iteration 20, loss = 0.56145265\n",
            "Iteration 21, loss = 0.56142145\n",
            "Iteration 22, loss = 0.56137044\n",
            "Iteration 23, loss = 0.56132078\n",
            "Iteration 24, loss = 0.56150878\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56819395\n",
            "Iteration 2, loss = 0.56188500\n",
            "Iteration 3, loss = 0.56232118\n",
            "Iteration 4, loss = 0.56223012\n",
            "Iteration 5, loss = 0.56248752\n",
            "Iteration 6, loss = 0.56144329\n",
            "Iteration 7, loss = 0.56235201\n",
            "Iteration 8, loss = 0.56174077\n",
            "Iteration 9, loss = 0.56266227\n",
            "Iteration 10, loss = 0.56116994\n",
            "Iteration 11, loss = 0.56209852\n",
            "Iteration 12, loss = 0.56135639\n",
            "Iteration 13, loss = 0.56172773\n",
            "Iteration 14, loss = 0.56232977\n",
            "Iteration 15, loss = 0.56200477\n",
            "Iteration 16, loss = 0.56258952\n",
            "Iteration 17, loss = 0.56141689\n",
            "Iteration 18, loss = 0.56189396\n",
            "Iteration 19, loss = 0.56152782\n",
            "Iteration 20, loss = 0.56161200\n",
            "Iteration 21, loss = 0.56129614\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57346338\n",
            "Iteration 2, loss = 0.56528320\n",
            "Iteration 3, loss = 0.56580100\n",
            "Iteration 4, loss = 0.56580097\n",
            "Iteration 5, loss = 0.56507157\n",
            "Iteration 6, loss = 0.56487770\n",
            "Iteration 7, loss = 0.56483335\n",
            "Iteration 8, loss = 0.56540541\n",
            "Iteration 9, loss = 0.56533698\n",
            "Iteration 10, loss = 0.56488098\n",
            "Iteration 11, loss = 0.56584438\n",
            "Iteration 12, loss = 0.56459046\n",
            "Iteration 13, loss = 0.56465801\n",
            "Iteration 14, loss = 0.56482644\n",
            "Iteration 15, loss = 0.56458409\n",
            "Iteration 16, loss = 0.56515277\n",
            "Iteration 17, loss = 0.56437942\n",
            "Iteration 18, loss = 0.56483189\n",
            "Iteration 19, loss = 0.56429143\n",
            "Iteration 20, loss = 0.56467655\n",
            "Iteration 21, loss = 0.56458363\n",
            "Iteration 22, loss = 0.56475868\n",
            "Iteration 23, loss = 0.56426348\n",
            "Iteration 24, loss = 0.56412622\n",
            "Iteration 25, loss = 0.56431361\n",
            "Iteration 26, loss = 0.56450576\n",
            "Iteration 27, loss = 0.56385022\n",
            "Iteration 28, loss = 0.56353297\n",
            "Iteration 29, loss = 0.56380498\n",
            "Iteration 30, loss = 0.56388094\n",
            "Iteration 31, loss = 0.56338682\n",
            "Iteration 32, loss = 0.56278238\n",
            "Iteration 33, loss = 0.56326975\n",
            "Iteration 34, loss = 0.56422856\n",
            "Iteration 35, loss = 0.56295721\n",
            "Iteration 36, loss = 0.56300568\n",
            "Iteration 37, loss = 0.56317941\n",
            "Iteration 38, loss = 0.56299611\n",
            "Iteration 39, loss = 0.56243289\n",
            "Iteration 40, loss = 0.56231525\n",
            "Iteration 41, loss = 0.56299507\n",
            "Iteration 42, loss = 0.56212625\n",
            "Iteration 43, loss = 0.56245960\n",
            "Iteration 44, loss = 0.56265811\n",
            "Iteration 45, loss = 0.56226645\n",
            "Iteration 46, loss = 0.56255809\n",
            "Iteration 47, loss = 0.56225317\n",
            "Iteration 48, loss = 0.56220478\n",
            "Iteration 49, loss = 0.56203860\n",
            "Iteration 50, loss = 0.56149075\n",
            "Iteration 51, loss = 0.56241852\n",
            "Iteration 52, loss = 0.56130189\n",
            "Iteration 53, loss = 0.56038199\n",
            "Iteration 54, loss = 0.55998400\n",
            "Iteration 55, loss = 0.56055628\n",
            "Iteration 56, loss = 0.56052832\n",
            "Iteration 57, loss = 0.56018991\n",
            "Iteration 58, loss = 0.56044392\n",
            "Iteration 59, loss = 0.55939249\n",
            "Iteration 60, loss = 0.56000224\n",
            "Iteration 61, loss = 0.55874515\n",
            "Iteration 62, loss = 0.55838477\n",
            "Iteration 63, loss = 0.55908497\n",
            "Iteration 64, loss = 0.55850882\n",
            "Iteration 65, loss = 0.55803822\n",
            "Iteration 66, loss = 0.55781380\n",
            "Iteration 67, loss = 0.55672028\n",
            "Iteration 68, loss = 0.55682949\n",
            "Iteration 69, loss = 0.55802678\n",
            "Iteration 70, loss = 0.55587295\n",
            "Iteration 71, loss = 0.55556692\n",
            "Iteration 72, loss = 0.55500670\n",
            "Iteration 73, loss = 0.55437086\n",
            "Iteration 74, loss = 0.55354498\n",
            "Iteration 75, loss = 0.55352114\n",
            "Iteration 76, loss = 0.55309162\n",
            "Iteration 77, loss = 0.55263255\n",
            "Iteration 78, loss = 0.55176734\n",
            "Iteration 79, loss = 0.55163133\n",
            "Iteration 80, loss = 0.54994779\n",
            "Iteration 81, loss = 0.54966477\n",
            "Iteration 82, loss = 0.54798472\n",
            "Iteration 83, loss = 0.54893814\n",
            "Iteration 84, loss = 0.54845307\n",
            "Iteration 85, loss = 0.54802755\n",
            "Iteration 86, loss = 0.54711036\n",
            "Iteration 87, loss = 0.54615104\n",
            "Iteration 88, loss = 0.54525681\n",
            "Iteration 89, loss = 0.54434094\n",
            "Iteration 90, loss = 0.54335725\n",
            "Iteration 91, loss = 0.54243673\n",
            "Iteration 92, loss = 0.54217972\n",
            "Iteration 93, loss = 0.54105535\n",
            "Iteration 94, loss = 0.54010306\n",
            "Iteration 95, loss = 0.53968319\n",
            "Iteration 96, loss = 0.54018190\n",
            "Iteration 97, loss = 0.53742231\n",
            "Iteration 98, loss = 0.53836372\n",
            "Iteration 99, loss = 0.53602635\n",
            "Iteration 100, loss = 0.53621853\n",
            "Iteration 101, loss = 0.53580073\n",
            "Iteration 102, loss = 0.53545374\n",
            "Iteration 103, loss = 0.53516287\n",
            "Iteration 104, loss = 0.53508235\n",
            "Iteration 105, loss = 0.53390263\n",
            "Iteration 106, loss = 0.53296219\n",
            "Iteration 107, loss = 0.53340734\n",
            "Iteration 108, loss = 0.53268838\n",
            "Iteration 109, loss = 0.53164868\n",
            "Iteration 110, loss = 0.53227962\n",
            "Iteration 111, loss = 0.53161478\n",
            "Iteration 112, loss = 0.53149978\n",
            "Iteration 113, loss = 0.53124731\n",
            "Iteration 114, loss = 0.53017389\n",
            "Iteration 115, loss = 0.53053726\n",
            "Iteration 116, loss = 0.53011932\n",
            "Iteration 117, loss = 0.52974636\n",
            "Iteration 118, loss = 0.52880388\n",
            "Iteration 119, loss = 0.52852025\n",
            "Iteration 120, loss = 0.52901101\n",
            "Iteration 121, loss = 0.52867575\n",
            "Iteration 122, loss = 0.52815982\n",
            "Iteration 123, loss = 0.52818342\n",
            "Iteration 124, loss = 0.52770994\n",
            "Iteration 125, loss = 0.52780041\n",
            "Iteration 126, loss = 0.52736826\n",
            "Iteration 127, loss = 0.52648427\n",
            "Iteration 128, loss = 0.52691497\n",
            "Iteration 129, loss = 0.52614510\n",
            "Iteration 130, loss = 0.52656095\n",
            "Iteration 131, loss = 0.52584883\n",
            "Iteration 132, loss = 0.52584075\n",
            "Iteration 133, loss = 0.52661532\n",
            "Iteration 134, loss = 0.52539108\n",
            "Iteration 135, loss = 0.52584888\n",
            "Iteration 136, loss = 0.52567729\n",
            "Iteration 137, loss = 0.52568202\n",
            "Iteration 1, loss = 0.57628652\n",
            "Iteration 2, loss = 0.56735073\n",
            "Iteration 3, loss = 0.56665685\n",
            "Iteration 4, loss = 0.56743463\n",
            "Iteration 5, loss = 0.56730442\n",
            "Iteration 6, loss = 0.56687586\n",
            "Iteration 7, loss = 0.56704248\n",
            "Iteration 8, loss = 0.56773463\n",
            "Iteration 9, loss = 0.56730859\n",
            "Iteration 10, loss = 0.56741882\n",
            "Iteration 11, loss = 0.56732067\n",
            "Iteration 12, loss = 0.56702516\n",
            "Iteration 13, loss = 0.56708091\n",
            "Iteration 14, loss = 0.56699920\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56297623\n",
            "Iteration 2, loss = 0.56107383\n",
            "Iteration 3, loss = 0.56140636\n",
            "Iteration 4, loss = 0.56039621\n",
            "Iteration 5, loss = 0.56126674\n",
            "Iteration 6, loss = 0.56138459\n",
            "Iteration 7, loss = 0.56043640\n",
            "Iteration 8, loss = 0.56067314\n",
            "Iteration 9, loss = 0.56177300\n",
            "Iteration 10, loss = 0.56042999\n",
            "Iteration 11, loss = 0.56110128\n",
            "Iteration 12, loss = 0.56102036\n",
            "Iteration 13, loss = 0.56018620\n",
            "Iteration 14, loss = 0.56072940\n",
            "Iteration 15, loss = 0.56035790\n",
            "Iteration 16, loss = 0.55963514\n",
            "Iteration 17, loss = 0.56006744\n",
            "Iteration 18, loss = 0.56021172\n",
            "Iteration 19, loss = 0.55987929\n",
            "Iteration 20, loss = 0.55912039\n",
            "Iteration 21, loss = 0.55951726\n",
            "Iteration 22, loss = 0.55966479\n",
            "Iteration 23, loss = 0.55984150\n",
            "Iteration 24, loss = 0.55977654\n",
            "Iteration 25, loss = 0.55985586\n",
            "Iteration 26, loss = 0.55979344\n",
            "Iteration 27, loss = 0.55915916\n",
            "Iteration 28, loss = 0.55930455\n",
            "Iteration 29, loss = 0.55972783\n",
            "Iteration 30, loss = 0.55929445\n",
            "Iteration 31, loss = 0.55869778\n",
            "Iteration 32, loss = 0.55950735\n",
            "Iteration 33, loss = 0.55871737\n",
            "Iteration 34, loss = 0.55923003\n",
            "Iteration 35, loss = 0.55840819\n",
            "Iteration 36, loss = 0.55942838\n",
            "Iteration 37, loss = 0.55848453\n",
            "Iteration 38, loss = 0.55861728\n",
            "Iteration 39, loss = 0.55861676\n",
            "Iteration 40, loss = 0.55792307\n",
            "Iteration 41, loss = 0.55906349\n",
            "Iteration 42, loss = 0.55859543\n",
            "Iteration 43, loss = 0.55652017\n",
            "Iteration 44, loss = 0.55676696\n",
            "Iteration 45, loss = 0.55743494\n",
            "Iteration 46, loss = 0.55692886\n",
            "Iteration 47, loss = 0.55651908\n",
            "Iteration 48, loss = 0.55665921\n",
            "Iteration 49, loss = 0.55646280\n",
            "Iteration 50, loss = 0.55617281\n",
            "Iteration 51, loss = 0.55640094\n",
            "Iteration 52, loss = 0.55590404\n",
            "Iteration 53, loss = 0.55610289\n",
            "Iteration 54, loss = 0.55469626\n",
            "Iteration 55, loss = 0.55499168\n",
            "Iteration 56, loss = 0.55516818\n",
            "Iteration 57, loss = 0.55416502\n",
            "Iteration 58, loss = 0.55405610\n",
            "Iteration 59, loss = 0.55372894\n",
            "Iteration 60, loss = 0.55430486\n",
            "Iteration 61, loss = 0.55284284\n",
            "Iteration 62, loss = 0.55185013\n",
            "Iteration 63, loss = 0.55216175\n",
            "Iteration 64, loss = 0.55232632\n",
            "Iteration 65, loss = 0.55067069\n",
            "Iteration 66, loss = 0.55074609\n",
            "Iteration 67, loss = 0.55037668\n",
            "Iteration 68, loss = 0.54998665\n",
            "Iteration 69, loss = 0.54943688\n",
            "Iteration 70, loss = 0.54816343\n",
            "Iteration 71, loss = 0.54741741\n",
            "Iteration 72, loss = 0.54705693\n",
            "Iteration 73, loss = 0.54779015\n",
            "Iteration 74, loss = 0.54612577\n",
            "Iteration 75, loss = 0.54558867\n",
            "Iteration 76, loss = 0.54521247\n",
            "Iteration 77, loss = 0.54384845\n",
            "Iteration 78, loss = 0.54353359\n",
            "Iteration 79, loss = 0.54326394\n",
            "Iteration 80, loss = 0.54214598\n",
            "Iteration 81, loss = 0.54223925\n",
            "Iteration 82, loss = 0.54097472\n",
            "Iteration 83, loss = 0.53995800\n",
            "Iteration 84, loss = 0.53948156\n",
            "Iteration 85, loss = 0.53960904\n",
            "Iteration 86, loss = 0.53827561\n",
            "Iteration 87, loss = 0.53782691\n",
            "Iteration 88, loss = 0.53631703\n",
            "Iteration 89, loss = 0.53638186\n",
            "Iteration 90, loss = 0.53532684\n",
            "Iteration 91, loss = 0.53546888\n",
            "Iteration 92, loss = 0.53567606\n",
            "Iteration 93, loss = 0.53409123\n",
            "Iteration 94, loss = 0.53358271\n",
            "Iteration 95, loss = 0.53335957\n",
            "Iteration 96, loss = 0.53287890\n",
            "Iteration 97, loss = 0.53197849\n",
            "Iteration 98, loss = 0.53196720\n",
            "Iteration 99, loss = 0.53112421\n",
            "Iteration 100, loss = 0.53155200\n",
            "Iteration 101, loss = 0.53066614\n",
            "Iteration 102, loss = 0.53050596\n",
            "Iteration 103, loss = 0.52993299\n",
            "Iteration 104, loss = 0.52946193\n",
            "Iteration 105, loss = 0.52943482\n",
            "Iteration 106, loss = 0.52897565\n",
            "Iteration 107, loss = 0.52819403\n",
            "Iteration 108, loss = 0.52845188\n",
            "Iteration 109, loss = 0.52845236\n",
            "Iteration 110, loss = 0.52866481\n",
            "Iteration 111, loss = 0.52701173\n",
            "Iteration 112, loss = 0.52627607\n",
            "Iteration 113, loss = 0.52687692\n",
            "Iteration 114, loss = 0.52602078\n",
            "Iteration 115, loss = 0.52674881\n",
            "Iteration 116, loss = 0.52620317\n",
            "Iteration 117, loss = 0.52691876\n",
            "Iteration 118, loss = 0.52611033\n",
            "Iteration 119, loss = 0.52641838\n",
            "Iteration 120, loss = 0.52636694\n",
            "Iteration 121, loss = 0.52580762\n",
            "Iteration 122, loss = 0.52593798\n",
            "Iteration 123, loss = 0.52501275\n",
            "Iteration 124, loss = 0.52505595\n",
            "Iteration 125, loss = 0.52437644\n",
            "Iteration 126, loss = 0.52500713\n",
            "Iteration 127, loss = 0.52477612\n",
            "Iteration 128, loss = 0.52524256\n",
            "Iteration 129, loss = 0.52433278\n",
            "Iteration 130, loss = 0.52276448\n",
            "Iteration 131, loss = 0.52375765\n",
            "Iteration 132, loss = 0.52354180\n",
            "Iteration 133, loss = 0.52371723\n",
            "Iteration 134, loss = 0.52332746\n",
            "Iteration 135, loss = 0.52368758\n",
            "Iteration 136, loss = 0.52324347\n",
            "Iteration 137, loss = 0.52274719\n",
            "Iteration 138, loss = 0.52372144\n",
            "Iteration 139, loss = 0.52324184\n",
            "Iteration 140, loss = 0.52257603\n",
            "Iteration 141, loss = 0.52312104\n",
            "Iteration 142, loss = 0.52255922\n",
            "Iteration 143, loss = 0.52241834\n",
            "Iteration 144, loss = 0.52161397\n",
            "Iteration 145, loss = 0.52293552\n",
            "Iteration 146, loss = 0.52128484\n",
            "Iteration 147, loss = 0.52358649\n",
            "Iteration 148, loss = 0.52168988\n",
            "Iteration 149, loss = 0.52176686\n",
            "Iteration 150, loss = 0.52172684\n",
            "Iteration 151, loss = 0.52194691\n",
            "Iteration 152, loss = 0.52149004\n",
            "Iteration 153, loss = 0.52152387\n",
            "Iteration 154, loss = 0.52207252\n",
            "Iteration 155, loss = 0.52179796\n",
            "Iteration 156, loss = 0.52097423\n",
            "Iteration 157, loss = 0.52170757\n",
            "Iteration 158, loss = 0.52098602\n",
            "Iteration 159, loss = 0.52191269\n",
            "Iteration 160, loss = 0.52134808\n",
            "Iteration 161, loss = 0.52081638\n",
            "Iteration 162, loss = 0.52086774\n",
            "Iteration 163, loss = 0.52165040\n",
            "Iteration 164, loss = 0.52108099\n",
            "Iteration 165, loss = 0.52054446\n",
            "Iteration 166, loss = 0.51985905\n",
            "Iteration 167, loss = 0.52100785\n",
            "Iteration 168, loss = 0.52068693\n",
            "Iteration 169, loss = 0.52071738\n",
            "Iteration 170, loss = 0.52032676\n",
            "Iteration 171, loss = 0.52038239\n",
            "Iteration 172, loss = 0.52012625\n",
            "Iteration 173, loss = 0.52037598\n",
            "Iteration 174, loss = 0.52020082\n",
            "Iteration 175, loss = 0.51812266\n",
            "Iteration 176, loss = 0.52007288\n",
            "Iteration 177, loss = 0.52009002\n",
            "Iteration 178, loss = 0.52012549\n",
            "Iteration 179, loss = 0.51974075\n",
            "Iteration 180, loss = 0.52013470\n",
            "Iteration 181, loss = 0.51870208\n",
            "Iteration 182, loss = 0.51953036\n",
            "Iteration 183, loss = 0.51945223\n",
            "Iteration 184, loss = 0.51949334\n",
            "Iteration 185, loss = 0.52110371\n",
            "Iteration 186, loss = 0.51944155\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56543802\n",
            "Iteration 2, loss = 0.56507135\n",
            "Iteration 3, loss = 0.56516243\n",
            "Iteration 4, loss = 0.56446720\n",
            "Iteration 5, loss = 0.56490229\n",
            "Iteration 6, loss = 0.56450652\n",
            "Iteration 7, loss = 0.56380447\n",
            "Iteration 8, loss = 0.56442236\n",
            "Iteration 9, loss = 0.56410627\n",
            "Iteration 10, loss = 0.56468844\n",
            "Iteration 11, loss = 0.56374530\n",
            "Iteration 12, loss = 0.56494433\n",
            "Iteration 13, loss = 0.56442468\n",
            "Iteration 14, loss = 0.56428646\n",
            "Iteration 15, loss = 0.56375239\n",
            "Iteration 16, loss = 0.56352299\n",
            "Iteration 17, loss = 0.56288184\n",
            "Iteration 18, loss = 0.56319621\n",
            "Iteration 19, loss = 0.56343298\n",
            "Iteration 20, loss = 0.56283994\n",
            "Iteration 21, loss = 0.56386099\n",
            "Iteration 22, loss = 0.56268546\n",
            "Iteration 23, loss = 0.56276736\n",
            "Iteration 24, loss = 0.56259541\n",
            "Iteration 25, loss = 0.56302202\n",
            "Iteration 26, loss = 0.56256362\n",
            "Iteration 27, loss = 0.56251429\n",
            "Iteration 28, loss = 0.56339884\n",
            "Iteration 29, loss = 0.56203756\n",
            "Iteration 30, loss = 0.56201044\n",
            "Iteration 31, loss = 0.56174359\n",
            "Iteration 32, loss = 0.56180330\n",
            "Iteration 33, loss = 0.56204562\n",
            "Iteration 34, loss = 0.56135344\n",
            "Iteration 35, loss = 0.56124194\n",
            "Iteration 36, loss = 0.56115797\n",
            "Iteration 37, loss = 0.56053537\n",
            "Iteration 38, loss = 0.55916712\n",
            "Iteration 39, loss = 0.56017153\n",
            "Iteration 40, loss = 0.55943034\n",
            "Iteration 41, loss = 0.56041694\n",
            "Iteration 42, loss = 0.55963439\n",
            "Iteration 43, loss = 0.55865410\n",
            "Iteration 44, loss = 0.56015076\n",
            "Iteration 45, loss = 0.55913156\n",
            "Iteration 46, loss = 0.55789219\n",
            "Iteration 47, loss = 0.55863597\n",
            "Iteration 48, loss = 0.55802752\n",
            "Iteration 49, loss = 0.55700495\n",
            "Iteration 50, loss = 0.55808693\n",
            "Iteration 51, loss = 0.55616034\n",
            "Iteration 52, loss = 0.55623390\n",
            "Iteration 53, loss = 0.55546716\n",
            "Iteration 54, loss = 0.55505419\n",
            "Iteration 55, loss = 0.55451861\n",
            "Iteration 56, loss = 0.55425685\n",
            "Iteration 57, loss = 0.55332167\n",
            "Iteration 58, loss = 0.55324768\n",
            "Iteration 59, loss = 0.55227936\n",
            "Iteration 60, loss = 0.55181188\n",
            "Iteration 61, loss = 0.55170403\n",
            "Iteration 62, loss = 0.55011931\n",
            "Iteration 63, loss = 0.54936539\n",
            "Iteration 64, loss = 0.54867498\n",
            "Iteration 65, loss = 0.54955107\n",
            "Iteration 66, loss = 0.54837253\n",
            "Iteration 67, loss = 0.54725251\n",
            "Iteration 68, loss = 0.54615318\n",
            "Iteration 69, loss = 0.54514164\n",
            "Iteration 70, loss = 0.54421351\n",
            "Iteration 71, loss = 0.54406565\n",
            "Iteration 72, loss = 0.54393712\n",
            "Iteration 73, loss = 0.54210731\n",
            "Iteration 74, loss = 0.54127005\n",
            "Iteration 75, loss = 0.54163214\n",
            "Iteration 76, loss = 0.53995803\n",
            "Iteration 77, loss = 0.53993288\n",
            "Iteration 78, loss = 0.53835179\n",
            "Iteration 79, loss = 0.53840202\n",
            "Iteration 80, loss = 0.53753365\n",
            "Iteration 81, loss = 0.53711341\n",
            "Iteration 82, loss = 0.53632635\n",
            "Iteration 83, loss = 0.53507391\n",
            "Iteration 84, loss = 0.53438871\n",
            "Iteration 85, loss = 0.53474964\n",
            "Iteration 86, loss = 0.53378296\n",
            "Iteration 87, loss = 0.53351243\n",
            "Iteration 88, loss = 0.53145099\n",
            "Iteration 89, loss = 0.53250825\n",
            "Iteration 90, loss = 0.53248191\n",
            "Iteration 91, loss = 0.53110399\n",
            "Iteration 92, loss = 0.53127721\n",
            "Iteration 93, loss = 0.53084263\n",
            "Iteration 94, loss = 0.53070723\n",
            "Iteration 95, loss = 0.53064135\n",
            "Iteration 96, loss = 0.52966529\n",
            "Iteration 97, loss = 0.53049997\n",
            "Iteration 98, loss = 0.53014949\n",
            "Iteration 99, loss = 0.52905854\n",
            "Iteration 100, loss = 0.52880332\n",
            "Iteration 101, loss = 0.52866986\n",
            "Iteration 102, loss = 0.52844915\n",
            "Iteration 103, loss = 0.52714483\n",
            "Iteration 104, loss = 0.52795097\n",
            "Iteration 105, loss = 0.52803014\n",
            "Iteration 106, loss = 0.52714599\n",
            "Iteration 107, loss = 0.52644218\n",
            "Iteration 108, loss = 0.52705739\n",
            "Iteration 109, loss = 0.52618514\n",
            "Iteration 110, loss = 0.52717012\n",
            "Iteration 111, loss = 0.52620966\n",
            "Iteration 112, loss = 0.52542434\n",
            "Iteration 113, loss = 0.52591644\n",
            "Iteration 114, loss = 0.52587089\n",
            "Iteration 115, loss = 0.52502818\n",
            "Iteration 116, loss = 0.52443140\n",
            "Iteration 117, loss = 0.52506296\n",
            "Iteration 118, loss = 0.52497885\n",
            "Iteration 119, loss = 0.52432870\n",
            "Iteration 120, loss = 0.52451909\n",
            "Iteration 121, loss = 0.52469253\n",
            "Iteration 122, loss = 0.52397623\n",
            "Iteration 123, loss = 0.52488402\n",
            "Iteration 124, loss = 0.52397469\n",
            "Iteration 125, loss = 0.52392990\n",
            "Iteration 126, loss = 0.52350125\n",
            "Iteration 127, loss = 0.52402253\n",
            "Iteration 128, loss = 0.52412110\n",
            "Iteration 129, loss = 0.52309764\n",
            "Iteration 130, loss = 0.52442272\n",
            "Iteration 131, loss = 0.52218382\n",
            "Iteration 132, loss = 0.52336305\n",
            "Iteration 133, loss = 0.52305192\n",
            "Iteration 134, loss = 0.52302952\n",
            "Iteration 135, loss = 0.52293413\n",
            "Iteration 136, loss = 0.52303027\n",
            "Iteration 137, loss = 0.52276334\n",
            "Iteration 138, loss = 0.52219792\n",
            "Iteration 139, loss = 0.52230375\n",
            "Iteration 140, loss = 0.52238504\n",
            "Iteration 141, loss = 0.52200127\n",
            "Iteration 142, loss = 0.52134876\n",
            "Iteration 143, loss = 0.52262557\n",
            "Iteration 144, loss = 0.52118425\n",
            "Iteration 145, loss = 0.52130573\n",
            "Iteration 146, loss = 0.52203699\n",
            "Iteration 147, loss = 0.52212884\n",
            "Iteration 148, loss = 0.52109150\n",
            "Iteration 149, loss = 0.52196600\n",
            "Iteration 150, loss = 0.52128688\n",
            "Iteration 151, loss = 0.52253463\n",
            "Iteration 152, loss = 0.52088743\n",
            "Iteration 153, loss = 0.52133156\n",
            "Iteration 154, loss = 0.52079016\n",
            "Iteration 155, loss = 0.52079594\n",
            "Iteration 156, loss = 0.52140318\n",
            "Iteration 157, loss = 0.52121245\n",
            "Iteration 158, loss = 0.52081073\n",
            "Iteration 159, loss = 0.52039488\n",
            "Iteration 160, loss = 0.52104455\n",
            "Iteration 161, loss = 0.51983076\n",
            "Iteration 162, loss = 0.52024663\n",
            "Iteration 163, loss = 0.52053704\n",
            "Iteration 164, loss = 0.52028202\n",
            "Iteration 165, loss = 0.52189859\n",
            "Iteration 166, loss = 0.52064335\n",
            "Iteration 167, loss = 0.52070298\n",
            "Iteration 168, loss = 0.52102748\n",
            "Iteration 169, loss = 0.51985115\n",
            "Iteration 170, loss = 0.52003923\n",
            "Iteration 171, loss = 0.52030482\n",
            "Iteration 172, loss = 0.52120944\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "14\n",
            "Iteration 1, loss = 0.56837784\n",
            "Iteration 2, loss = 0.56433105\n",
            "Iteration 3, loss = 0.56516576\n",
            "Iteration 4, loss = 0.56350028\n",
            "Iteration 5, loss = 0.56454891\n",
            "Iteration 6, loss = 0.56489920\n",
            "Iteration 7, loss = 0.56488396\n",
            "Iteration 8, loss = 0.56416742\n",
            "Iteration 9, loss = 0.56490575\n",
            "Iteration 10, loss = 0.56437035\n",
            "Iteration 11, loss = 0.56444742\n",
            "Iteration 12, loss = 0.56323663\n",
            "Iteration 13, loss = 0.56363795\n",
            "Iteration 14, loss = 0.56406837\n",
            "Iteration 15, loss = 0.56377432\n",
            "Iteration 16, loss = 0.56436627\n",
            "Iteration 17, loss = 0.56394656\n",
            "Iteration 18, loss = 0.56395294\n",
            "Iteration 19, loss = 0.56392057\n",
            "Iteration 20, loss = 0.56445476\n",
            "Iteration 21, loss = 0.56327765\n",
            "Iteration 22, loss = 0.56351405\n",
            "Iteration 23, loss = 0.56361688\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56580541\n",
            "Iteration 2, loss = 0.56365718\n",
            "Iteration 3, loss = 0.56398475\n",
            "Iteration 4, loss = 0.56386134\n",
            "Iteration 5, loss = 0.56465803\n",
            "Iteration 6, loss = 0.56393799\n",
            "Iteration 7, loss = 0.56378409\n",
            "Iteration 8, loss = 0.56367082\n",
            "Iteration 9, loss = 0.56440432\n",
            "Iteration 10, loss = 0.56386981\n",
            "Iteration 11, loss = 0.56379051\n",
            "Iteration 12, loss = 0.56468489\n",
            "Iteration 13, loss = 0.56432211\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56748143\n",
            "Iteration 2, loss = 0.56273896\n",
            "Iteration 3, loss = 0.56322068\n",
            "Iteration 4, loss = 0.56322810\n",
            "Iteration 5, loss = 0.56257565\n",
            "Iteration 6, loss = 0.56279006\n",
            "Iteration 7, loss = 0.56267035\n",
            "Iteration 8, loss = 0.56280096\n",
            "Iteration 9, loss = 0.56220276\n",
            "Iteration 10, loss = 0.56171687\n",
            "Iteration 11, loss = 0.56311257\n",
            "Iteration 12, loss = 0.56192642\n",
            "Iteration 13, loss = 0.56291830\n",
            "Iteration 14, loss = 0.56283000\n",
            "Iteration 15, loss = 0.56197881\n",
            "Iteration 16, loss = 0.56279818\n",
            "Iteration 17, loss = 0.56184739\n",
            "Iteration 18, loss = 0.56225072\n",
            "Iteration 19, loss = 0.56231655\n",
            "Iteration 20, loss = 0.56223803\n",
            "Iteration 21, loss = 0.56197051\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56875930\n",
            "Iteration 2, loss = 0.56480940\n",
            "Iteration 3, loss = 0.56508402\n",
            "Iteration 4, loss = 0.56443987\n",
            "Iteration 5, loss = 0.56540945\n",
            "Iteration 6, loss = 0.56551193\n",
            "Iteration 7, loss = 0.56484691\n",
            "Iteration 8, loss = 0.56478949\n",
            "Iteration 9, loss = 0.56382894\n",
            "Iteration 10, loss = 0.56433173\n",
            "Iteration 11, loss = 0.56437545\n",
            "Iteration 12, loss = 0.56507786\n",
            "Iteration 13, loss = 0.56475677\n",
            "Iteration 14, loss = 0.56432303\n",
            "Iteration 15, loss = 0.56483657\n",
            "Iteration 16, loss = 0.56406652\n",
            "Iteration 17, loss = 0.56472229\n",
            "Iteration 18, loss = 0.56377530\n",
            "Iteration 19, loss = 0.56443249\n",
            "Iteration 20, loss = 0.56283236\n",
            "Iteration 21, loss = 0.56378375\n",
            "Iteration 22, loss = 0.56385842\n",
            "Iteration 23, loss = 0.56432119\n",
            "Iteration 24, loss = 0.56362906\n",
            "Iteration 25, loss = 0.56391925\n",
            "Iteration 26, loss = 0.56406538\n",
            "Iteration 27, loss = 0.56319732\n",
            "Iteration 28, loss = 0.56349223\n",
            "Iteration 29, loss = 0.56405302\n",
            "Iteration 30, loss = 0.56340548\n",
            "Iteration 31, loss = 0.56321984\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57181850\n",
            "Iteration 2, loss = 0.56141626\n",
            "Iteration 3, loss = 0.56149334\n",
            "Iteration 4, loss = 0.56219456\n",
            "Iteration 5, loss = 0.56068265\n",
            "Iteration 6, loss = 0.56142225\n",
            "Iteration 7, loss = 0.56100177\n",
            "Iteration 8, loss = 0.56157767\n",
            "Iteration 9, loss = 0.56153757\n",
            "Iteration 10, loss = 0.56138793\n",
            "Iteration 11, loss = 0.56165268\n",
            "Iteration 12, loss = 0.56106279\n",
            "Iteration 13, loss = 0.56133429\n",
            "Iteration 14, loss = 0.56085046\n",
            "Iteration 15, loss = 0.56113452\n",
            "Iteration 16, loss = 0.56112293\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.58039701\n",
            "Iteration 2, loss = 0.56551130\n",
            "Iteration 3, loss = 0.56604728\n",
            "Iteration 4, loss = 0.56597793\n",
            "Iteration 5, loss = 0.56669852\n",
            "Iteration 6, loss = 0.56603316\n",
            "Iteration 7, loss = 0.56587728\n",
            "Iteration 8, loss = 0.56592248\n",
            "Iteration 9, loss = 0.56624110\n",
            "Iteration 10, loss = 0.56516584\n",
            "Iteration 11, loss = 0.56572350\n",
            "Iteration 12, loss = 0.56570929\n",
            "Iteration 13, loss = 0.56549870\n",
            "Iteration 14, loss = 0.56543498\n",
            "Iteration 15, loss = 0.56573111\n",
            "Iteration 16, loss = 0.56571278\n",
            "Iteration 17, loss = 0.56566851\n",
            "Iteration 18, loss = 0.56463246\n",
            "Iteration 19, loss = 0.56624964\n",
            "Iteration 20, loss = 0.56590399\n",
            "Iteration 21, loss = 0.56534806\n",
            "Iteration 22, loss = 0.56462250\n",
            "Iteration 23, loss = 0.56590089\n",
            "Iteration 24, loss = 0.56536716\n",
            "Iteration 25, loss = 0.56545154\n",
            "Iteration 26, loss = 0.56487174\n",
            "Iteration 27, loss = 0.56531758\n",
            "Iteration 28, loss = 0.56537256\n",
            "Iteration 29, loss = 0.56555163\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57080983\n",
            "Iteration 2, loss = 0.56210598\n",
            "Iteration 3, loss = 0.56143485\n",
            "Iteration 4, loss = 0.56171944\n",
            "Iteration 5, loss = 0.56188211\n",
            "Iteration 6, loss = 0.56269781\n",
            "Iteration 7, loss = 0.56150641\n",
            "Iteration 8, loss = 0.56197150\n",
            "Iteration 9, loss = 0.56138002\n",
            "Iteration 10, loss = 0.56057783\n",
            "Iteration 11, loss = 0.56165146\n",
            "Iteration 12, loss = 0.56044270\n",
            "Iteration 13, loss = 0.56138768\n",
            "Iteration 14, loss = 0.56107454\n",
            "Iteration 15, loss = 0.56130623\n",
            "Iteration 16, loss = 0.56122248\n",
            "Iteration 17, loss = 0.56124830\n",
            "Iteration 18, loss = 0.56163867\n",
            "Iteration 19, loss = 0.56071324\n",
            "Iteration 20, loss = 0.56166704\n",
            "Iteration 21, loss = 0.56120860\n",
            "Iteration 22, loss = 0.56107959\n",
            "Iteration 23, loss = 0.56059211\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56576728\n",
            "Iteration 2, loss = 0.56380230\n",
            "Iteration 3, loss = 0.56333786\n",
            "Iteration 4, loss = 0.56359969\n",
            "Iteration 5, loss = 0.56315712\n",
            "Iteration 6, loss = 0.56350831\n",
            "Iteration 7, loss = 0.56338524\n",
            "Iteration 8, loss = 0.56405431\n",
            "Iteration 9, loss = 0.56314456\n",
            "Iteration 10, loss = 0.56339891\n",
            "Iteration 11, loss = 0.56304825\n",
            "Iteration 12, loss = 0.56360400\n",
            "Iteration 13, loss = 0.56283154\n",
            "Iteration 14, loss = 0.56285986\n",
            "Iteration 15, loss = 0.56383465\n",
            "Iteration 16, loss = 0.56355982\n",
            "Iteration 17, loss = 0.56311564\n",
            "Iteration 18, loss = 0.56267789\n",
            "Iteration 19, loss = 0.56320359\n",
            "Iteration 20, loss = 0.56215588\n",
            "Iteration 21, loss = 0.56297759\n",
            "Iteration 22, loss = 0.56253078\n",
            "Iteration 23, loss = 0.56276497\n",
            "Iteration 24, loss = 0.56241987\n",
            "Iteration 25, loss = 0.56288231\n",
            "Iteration 26, loss = 0.56178796\n",
            "Iteration 27, loss = 0.56215714\n",
            "Iteration 28, loss = 0.56210133\n",
            "Iteration 29, loss = 0.56145488\n",
            "Iteration 30, loss = 0.56235387\n",
            "Iteration 31, loss = 0.56261819\n",
            "Iteration 32, loss = 0.56196921\n",
            "Iteration 33, loss = 0.56196006\n",
            "Iteration 34, loss = 0.56119724\n",
            "Iteration 35, loss = 0.56160855\n",
            "Iteration 36, loss = 0.56110695\n",
            "Iteration 37, loss = 0.56145128\n",
            "Iteration 38, loss = 0.56083173\n",
            "Iteration 39, loss = 0.56106439\n",
            "Iteration 40, loss = 0.56045254\n",
            "Iteration 41, loss = 0.55975399\n",
            "Iteration 42, loss = 0.56026245\n",
            "Iteration 43, loss = 0.56017211\n",
            "Iteration 44, loss = 0.55993170\n",
            "Iteration 45, loss = 0.56034304\n",
            "Iteration 46, loss = 0.55967454\n",
            "Iteration 47, loss = 0.55990729\n",
            "Iteration 48, loss = 0.55947792\n",
            "Iteration 49, loss = 0.55934792\n",
            "Iteration 50, loss = 0.55880379\n",
            "Iteration 51, loss = 0.55870452\n",
            "Iteration 52, loss = 0.55824700\n",
            "Iteration 53, loss = 0.55795014\n",
            "Iteration 54, loss = 0.55799487\n",
            "Iteration 55, loss = 0.55712000\n",
            "Iteration 56, loss = 0.55820436\n",
            "Iteration 57, loss = 0.55722068\n",
            "Iteration 58, loss = 0.55667585\n",
            "Iteration 59, loss = 0.55649486\n",
            "Iteration 60, loss = 0.55586882\n",
            "Iteration 61, loss = 0.55550842\n",
            "Iteration 62, loss = 0.55504737\n",
            "Iteration 63, loss = 0.55482918\n",
            "Iteration 64, loss = 0.55346032\n",
            "Iteration 65, loss = 0.55323468\n",
            "Iteration 66, loss = 0.55354318\n",
            "Iteration 67, loss = 0.55314583\n",
            "Iteration 68, loss = 0.55227991\n",
            "Iteration 69, loss = 0.55063507\n",
            "Iteration 70, loss = 0.55014259\n",
            "Iteration 71, loss = 0.55016363\n",
            "Iteration 72, loss = 0.54951550\n",
            "Iteration 73, loss = 0.54944946\n",
            "Iteration 74, loss = 0.54748802\n",
            "Iteration 75, loss = 0.54738047\n",
            "Iteration 76, loss = 0.54489641\n",
            "Iteration 77, loss = 0.54581914\n",
            "Iteration 78, loss = 0.54388758\n",
            "Iteration 79, loss = 0.54320222\n",
            "Iteration 80, loss = 0.54278383\n",
            "Iteration 81, loss = 0.54211131\n",
            "Iteration 82, loss = 0.54050931\n",
            "Iteration 83, loss = 0.53988651\n",
            "Iteration 84, loss = 0.53924015\n",
            "Iteration 85, loss = 0.53812062\n",
            "Iteration 86, loss = 0.53736104\n",
            "Iteration 87, loss = 0.53685709\n",
            "Iteration 88, loss = 0.53667391\n",
            "Iteration 89, loss = 0.53511380\n",
            "Iteration 90, loss = 0.53411366\n",
            "Iteration 91, loss = 0.53299037\n",
            "Iteration 92, loss = 0.53273376\n",
            "Iteration 93, loss = 0.53211721\n",
            "Iteration 94, loss = 0.53164288\n",
            "Iteration 95, loss = 0.53150239\n",
            "Iteration 96, loss = 0.53041296\n",
            "Iteration 97, loss = 0.52899801\n",
            "Iteration 98, loss = 0.52911344\n",
            "Iteration 99, loss = 0.52967178\n",
            "Iteration 100, loss = 0.52849235\n",
            "Iteration 101, loss = 0.52746478\n",
            "Iteration 102, loss = 0.52731436\n",
            "Iteration 103, loss = 0.52748715\n",
            "Iteration 104, loss = 0.52695630\n",
            "Iteration 105, loss = 0.52612270\n",
            "Iteration 106, loss = 0.52615921\n",
            "Iteration 107, loss = 0.52489174\n",
            "Iteration 108, loss = 0.52447956\n",
            "Iteration 109, loss = 0.52491607\n",
            "Iteration 110, loss = 0.52438010\n",
            "Iteration 111, loss = 0.52390829\n",
            "Iteration 112, loss = 0.52397556\n",
            "Iteration 113, loss = 0.52318930\n",
            "Iteration 114, loss = 0.52340308\n",
            "Iteration 115, loss = 0.52333525\n",
            "Iteration 116, loss = 0.52333533\n",
            "Iteration 117, loss = 0.52120108\n",
            "Iteration 118, loss = 0.52283635\n",
            "Iteration 119, loss = 0.52329003\n",
            "Iteration 120, loss = 0.52249319\n",
            "Iteration 121, loss = 0.52244855\n",
            "Iteration 122, loss = 0.52126675\n",
            "Iteration 123, loss = 0.52145857\n",
            "Iteration 124, loss = 0.52103675\n",
            "Iteration 125, loss = 0.52046888\n",
            "Iteration 126, loss = 0.52101399\n",
            "Iteration 127, loss = 0.52081156\n",
            "Iteration 128, loss = 0.51979795\n",
            "Iteration 129, loss = 0.52048872\n",
            "Iteration 130, loss = 0.52051468\n",
            "Iteration 131, loss = 0.51899811\n",
            "Iteration 132, loss = 0.52077795\n",
            "Iteration 133, loss = 0.52051813\n",
            "Iteration 134, loss = 0.51944558\n",
            "Iteration 135, loss = 0.51924299\n",
            "Iteration 136, loss = 0.51997140\n",
            "Iteration 137, loss = 0.51972175\n",
            "Iteration 138, loss = 0.51839332\n",
            "Iteration 139, loss = 0.51906882\n",
            "Iteration 140, loss = 0.51826429\n",
            "Iteration 141, loss = 0.51849105\n",
            "Iteration 142, loss = 0.51806728\n",
            "Iteration 143, loss = 0.51891896\n",
            "Iteration 144, loss = 0.51810012\n",
            "Iteration 145, loss = 0.51832593\n",
            "Iteration 146, loss = 0.51795881\n",
            "Iteration 147, loss = 0.51799603\n",
            "Iteration 148, loss = 0.51805623\n",
            "Iteration 149, loss = 0.51825836\n",
            "Iteration 150, loss = 0.51797730\n",
            "Iteration 151, loss = 0.51775945\n",
            "Iteration 152, loss = 0.51690165\n",
            "Iteration 153, loss = 0.51785685\n",
            "Iteration 154, loss = 0.51784707\n",
            "Iteration 155, loss = 0.51801068\n",
            "Iteration 156, loss = 0.51830589\n",
            "Iteration 157, loss = 0.51874873\n",
            "Iteration 158, loss = 0.51745902\n",
            "Iteration 159, loss = 0.51750620\n",
            "Iteration 160, loss = 0.51793549\n",
            "Iteration 161, loss = 0.51720971\n",
            "Iteration 162, loss = 0.51669065\n",
            "Iteration 163, loss = 0.51627397\n",
            "Iteration 164, loss = 0.51676073\n",
            "Iteration 165, loss = 0.51730730\n",
            "Iteration 166, loss = 0.51709314\n",
            "Iteration 167, loss = 0.51697294\n",
            "Iteration 168, loss = 0.51783520\n",
            "Iteration 169, loss = 0.51611679\n",
            "Iteration 170, loss = 0.51757221\n",
            "Iteration 171, loss = 0.51645828\n",
            "Iteration 172, loss = 0.51798501\n",
            "Iteration 173, loss = 0.51732719\n",
            "Iteration 174, loss = 0.51666993\n",
            "Iteration 175, loss = 0.51656230\n",
            "Iteration 176, loss = 0.51683326\n",
            "Iteration 177, loss = 0.51597907\n",
            "Iteration 178, loss = 0.51738194\n",
            "Iteration 179, loss = 0.51698231\n",
            "Iteration 180, loss = 0.51635272\n",
            "Iteration 181, loss = 0.51662624\n",
            "Iteration 182, loss = 0.51622912\n",
            "Iteration 183, loss = 0.51625149\n",
            "Iteration 184, loss = 0.51641236\n",
            "Iteration 185, loss = 0.51591460\n",
            "Iteration 186, loss = 0.51660559\n",
            "Iteration 187, loss = 0.51690508\n",
            "Iteration 188, loss = 0.51549288\n",
            "Iteration 189, loss = 0.51630892\n",
            "Iteration 190, loss = 0.51563943\n",
            "Iteration 191, loss = 0.51656290\n",
            "Iteration 192, loss = 0.51553960\n",
            "Iteration 193, loss = 0.51583494\n",
            "Iteration 194, loss = 0.51634337\n",
            "Iteration 195, loss = 0.51608322\n",
            "Iteration 196, loss = 0.51592695\n",
            "Iteration 197, loss = 0.51579973\n",
            "Iteration 198, loss = 0.51647077\n",
            "Iteration 199, loss = 0.51600849\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56820166\n",
            "Iteration 2, loss = 0.56132634\n",
            "Iteration 3, loss = 0.56130446\n",
            "Iteration 4, loss = 0.56139476\n",
            "Iteration 5, loss = 0.56011502\n",
            "Iteration 6, loss = 0.56077261\n",
            "Iteration 7, loss = 0.56090943\n",
            "Iteration 8, loss = 0.55958111\n",
            "Iteration 9, loss = 0.56029497\n",
            "Iteration 10, loss = 0.56096704\n",
            "Iteration 11, loss = 0.56068848\n",
            "Iteration 12, loss = 0.56078633\n",
            "Iteration 13, loss = 0.55994255\n",
            "Iteration 14, loss = 0.56031304\n",
            "Iteration 15, loss = 0.56066221\n",
            "Iteration 16, loss = 0.56056118\n",
            "Iteration 17, loss = 0.56112903\n",
            "Iteration 18, loss = 0.56011903\n",
            "Iteration 19, loss = 0.56026525\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56338041\n",
            "Iteration 2, loss = 0.55873811\n",
            "Iteration 3, loss = 0.55926268\n",
            "Iteration 4, loss = 0.55778474\n",
            "Iteration 5, loss = 0.55941563\n",
            "Iteration 6, loss = 0.55880787\n",
            "Iteration 7, loss = 0.55804562\n",
            "Iteration 8, loss = 0.55781053\n",
            "Iteration 9, loss = 0.55856318\n",
            "Iteration 10, loss = 0.55781708\n",
            "Iteration 11, loss = 0.55864472\n",
            "Iteration 12, loss = 0.55821020\n",
            "Iteration 13, loss = 0.55873758\n",
            "Iteration 14, loss = 0.55714354\n",
            "Iteration 15, loss = 0.55727202\n",
            "Iteration 16, loss = 0.55818416\n",
            "Iteration 17, loss = 0.55831077\n",
            "Iteration 18, loss = 0.55773280\n",
            "Iteration 19, loss = 0.55720567\n",
            "Iteration 20, loss = 0.55916873\n",
            "Iteration 21, loss = 0.55745075\n",
            "Iteration 22, loss = 0.55746827\n",
            "Iteration 23, loss = 0.55728199\n",
            "Iteration 24, loss = 0.55716117\n",
            "Iteration 25, loss = 0.55720892\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "15\n",
            "Iteration 1, loss = 0.56278175\n",
            "Iteration 2, loss = 0.56140467\n",
            "Iteration 3, loss = 0.56133238\n",
            "Iteration 4, loss = 0.56137805\n",
            "Iteration 5, loss = 0.56158161\n",
            "Iteration 6, loss = 0.56119664\n",
            "Iteration 7, loss = 0.56105992\n",
            "Iteration 8, loss = 0.56108620\n",
            "Iteration 9, loss = 0.56056017\n",
            "Iteration 10, loss = 0.56113822\n",
            "Iteration 11, loss = 0.56098803\n",
            "Iteration 12, loss = 0.56133872\n",
            "Iteration 13, loss = 0.56042511\n",
            "Iteration 14, loss = 0.56062483\n",
            "Iteration 15, loss = 0.55962226\n",
            "Iteration 16, loss = 0.56114367\n",
            "Iteration 17, loss = 0.56018725\n",
            "Iteration 18, loss = 0.56074935\n",
            "Iteration 19, loss = 0.56047619\n",
            "Iteration 20, loss = 0.56114254\n",
            "Iteration 21, loss = 0.55980027\n",
            "Iteration 22, loss = 0.56072112\n",
            "Iteration 23, loss = 0.56041838\n",
            "Iteration 24, loss = 0.55971336\n",
            "Iteration 25, loss = 0.56048543\n",
            "Iteration 26, loss = 0.56039257\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56890447\n",
            "Iteration 2, loss = 0.56607760\n",
            "Iteration 3, loss = 0.56595978\n",
            "Iteration 4, loss = 0.56647118\n",
            "Iteration 5, loss = 0.56617024\n",
            "Iteration 6, loss = 0.56534731\n",
            "Iteration 7, loss = 0.56538161\n",
            "Iteration 8, loss = 0.56509756\n",
            "Iteration 9, loss = 0.56614710\n",
            "Iteration 10, loss = 0.56522664\n",
            "Iteration 11, loss = 0.56540677\n",
            "Iteration 12, loss = 0.56541779\n",
            "Iteration 13, loss = 0.56595688\n",
            "Iteration 14, loss = 0.56530321\n",
            "Iteration 15, loss = 0.56483209\n",
            "Iteration 16, loss = 0.56581869\n",
            "Iteration 17, loss = 0.56481188\n",
            "Iteration 18, loss = 0.56432553\n",
            "Iteration 19, loss = 0.56489569\n",
            "Iteration 20, loss = 0.56543676\n",
            "Iteration 21, loss = 0.56546508\n",
            "Iteration 22, loss = 0.56498551\n",
            "Iteration 23, loss = 0.56438902\n",
            "Iteration 24, loss = 0.56452082\n",
            "Iteration 25, loss = 0.56348681\n",
            "Iteration 26, loss = 0.56533480\n",
            "Iteration 27, loss = 0.56472799\n",
            "Iteration 28, loss = 0.56434689\n",
            "Iteration 29, loss = 0.56378513\n",
            "Iteration 30, loss = 0.56432594\n",
            "Iteration 31, loss = 0.56333504\n",
            "Iteration 32, loss = 0.56549394\n",
            "Iteration 33, loss = 0.56415222\n",
            "Iteration 34, loss = 0.56366181\n",
            "Iteration 35, loss = 0.56355715\n",
            "Iteration 36, loss = 0.56291598\n",
            "Iteration 37, loss = 0.56393030\n",
            "Iteration 38, loss = 0.56322308\n",
            "Iteration 39, loss = 0.56343977\n",
            "Iteration 40, loss = 0.56322623\n",
            "Iteration 41, loss = 0.56219508\n",
            "Iteration 42, loss = 0.56245969\n",
            "Iteration 43, loss = 0.56187694\n",
            "Iteration 44, loss = 0.56206079\n",
            "Iteration 45, loss = 0.56303885\n",
            "Iteration 46, loss = 0.56198685\n",
            "Iteration 47, loss = 0.56170981\n",
            "Iteration 48, loss = 0.56106112\n",
            "Iteration 49, loss = 0.56080711\n",
            "Iteration 50, loss = 0.56063605\n",
            "Iteration 51, loss = 0.56134064\n",
            "Iteration 52, loss = 0.56119758\n",
            "Iteration 53, loss = 0.56041405\n",
            "Iteration 54, loss = 0.55987880\n",
            "Iteration 55, loss = 0.55990380\n",
            "Iteration 56, loss = 0.55969061\n",
            "Iteration 57, loss = 0.55962778\n",
            "Iteration 58, loss = 0.55968703\n",
            "Iteration 59, loss = 0.55897770\n",
            "Iteration 60, loss = 0.55915210\n",
            "Iteration 61, loss = 0.55888297\n",
            "Iteration 62, loss = 0.55779693\n",
            "Iteration 63, loss = 0.55785771\n",
            "Iteration 64, loss = 0.55665827\n",
            "Iteration 65, loss = 0.55641271\n",
            "Iteration 66, loss = 0.55647682\n",
            "Iteration 67, loss = 0.55557395\n",
            "Iteration 68, loss = 0.55515572\n",
            "Iteration 69, loss = 0.55410879\n",
            "Iteration 70, loss = 0.55429005\n",
            "Iteration 71, loss = 0.55327082\n",
            "Iteration 72, loss = 0.55307738\n",
            "Iteration 73, loss = 0.55254969\n",
            "Iteration 74, loss = 0.55118811\n",
            "Iteration 75, loss = 0.55112442\n",
            "Iteration 76, loss = 0.55074813\n",
            "Iteration 77, loss = 0.54946019\n",
            "Iteration 78, loss = 0.54935908\n",
            "Iteration 79, loss = 0.54879148\n",
            "Iteration 80, loss = 0.54799024\n",
            "Iteration 81, loss = 0.54689759\n",
            "Iteration 82, loss = 0.54673006\n",
            "Iteration 83, loss = 0.54521060\n",
            "Iteration 84, loss = 0.54419083\n",
            "Iteration 85, loss = 0.54399991\n",
            "Iteration 86, loss = 0.54257274\n",
            "Iteration 87, loss = 0.54315474\n",
            "Iteration 88, loss = 0.54202329\n",
            "Iteration 89, loss = 0.54137115\n",
            "Iteration 90, loss = 0.54063547\n",
            "Iteration 91, loss = 0.53934581\n",
            "Iteration 92, loss = 0.53946707\n",
            "Iteration 93, loss = 0.53718946\n",
            "Iteration 94, loss = 0.53684404\n",
            "Iteration 95, loss = 0.53697057\n",
            "Iteration 96, loss = 0.53681856\n",
            "Iteration 97, loss = 0.53662006\n",
            "Iteration 98, loss = 0.53621196\n",
            "Iteration 99, loss = 0.53647012\n",
            "Iteration 100, loss = 0.53592974\n",
            "Iteration 101, loss = 0.53457682\n",
            "Iteration 102, loss = 0.53380556\n",
            "Iteration 103, loss = 0.53371887\n",
            "Iteration 104, loss = 0.53357271\n",
            "Iteration 105, loss = 0.53367237\n",
            "Iteration 106, loss = 0.53305746\n",
            "Iteration 107, loss = 0.53149909\n",
            "Iteration 108, loss = 0.53148027\n",
            "Iteration 109, loss = 0.53193099\n",
            "Iteration 110, loss = 0.53223508\n",
            "Iteration 111, loss = 0.52997818\n",
            "Iteration 112, loss = 0.53105325\n",
            "Iteration 113, loss = 0.52987395\n",
            "Iteration 114, loss = 0.52943036\n",
            "Iteration 115, loss = 0.53014686\n",
            "Iteration 116, loss = 0.52965817\n",
            "Iteration 117, loss = 0.52927122\n",
            "Iteration 118, loss = 0.53030718\n",
            "Iteration 119, loss = 0.52843221\n",
            "Iteration 120, loss = 0.52842066\n",
            "Iteration 121, loss = 0.52886499\n",
            "Iteration 122, loss = 0.52847606\n",
            "Iteration 123, loss = 0.52775832\n",
            "Iteration 124, loss = 0.52818137\n",
            "Iteration 125, loss = 0.52765343\n",
            "Iteration 126, loss = 0.52812309\n",
            "Iteration 127, loss = 0.52706600\n",
            "Iteration 128, loss = 0.52717837\n",
            "Iteration 129, loss = 0.52728742\n",
            "Iteration 130, loss = 0.52663484\n",
            "Iteration 131, loss = 0.52679913\n",
            "Iteration 132, loss = 0.52554884\n",
            "Iteration 133, loss = 0.52597735\n",
            "Iteration 134, loss = 0.52720524\n",
            "Iteration 135, loss = 0.52654344\n",
            "Iteration 136, loss = 0.52616308\n",
            "Iteration 137, loss = 0.52577569\n",
            "Iteration 138, loss = 0.52588554\n",
            "Iteration 139, loss = 0.52510182\n",
            "Iteration 140, loss = 0.52605183\n",
            "Iteration 141, loss = 0.52474617\n",
            "Iteration 142, loss = 0.52521470\n",
            "Iteration 143, loss = 0.52372527\n",
            "Iteration 144, loss = 0.52469471\n",
            "Iteration 145, loss = 0.52400250\n",
            "Iteration 146, loss = 0.52495336\n",
            "Iteration 147, loss = 0.52490946\n",
            "Iteration 148, loss = 0.52460191\n",
            "Iteration 149, loss = 0.52422865\n",
            "Iteration 150, loss = 0.52443467\n",
            "Iteration 151, loss = 0.52285151\n",
            "Iteration 152, loss = 0.52383098\n",
            "Iteration 153, loss = 0.52433540\n",
            "Iteration 154, loss = 0.52412356\n",
            "Iteration 155, loss = 0.52260975\n",
            "Iteration 156, loss = 0.52457523\n",
            "Iteration 157, loss = 0.52244092\n",
            "Iteration 158, loss = 0.52411854\n",
            "Iteration 159, loss = 0.52360483\n",
            "Iteration 160, loss = 0.52392863\n",
            "Iteration 161, loss = 0.52379186\n",
            "Iteration 162, loss = 0.52228658\n",
            "Iteration 163, loss = 0.52326943\n",
            "Iteration 164, loss = 0.52257762\n",
            "Iteration 165, loss = 0.52294340\n",
            "Iteration 166, loss = 0.52334246\n",
            "Iteration 167, loss = 0.52318817\n",
            "Iteration 168, loss = 0.52334725\n",
            "Iteration 169, loss = 0.52250429\n",
            "Iteration 170, loss = 0.52280324\n",
            "Iteration 171, loss = 0.52294947\n",
            "Iteration 172, loss = 0.52185834\n",
            "Iteration 173, loss = 0.52243752\n",
            "Iteration 174, loss = 0.52259073\n",
            "Iteration 175, loss = 0.52277608\n",
            "Iteration 176, loss = 0.52273781\n",
            "Iteration 177, loss = 0.52214413\n",
            "Iteration 178, loss = 0.52223706\n",
            "Iteration 179, loss = 0.52213988\n",
            "Iteration 180, loss = 0.52193282\n",
            "Iteration 181, loss = 0.52255469\n",
            "Iteration 182, loss = 0.52202506\n",
            "Iteration 183, loss = 0.52172418\n",
            "Iteration 184, loss = 0.52249360\n",
            "Iteration 185, loss = 0.52200523\n",
            "Iteration 186, loss = 0.52233937\n",
            "Iteration 187, loss = 0.52274303\n",
            "Iteration 188, loss = 0.52211582\n",
            "Iteration 189, loss = 0.52185380\n",
            "Iteration 190, loss = 0.52242359\n",
            "Iteration 191, loss = 0.52095725\n",
            "Iteration 192, loss = 0.52147172\n",
            "Iteration 193, loss = 0.52210249\n",
            "Iteration 194, loss = 0.52174236\n",
            "Iteration 195, loss = 0.52163289\n",
            "Iteration 196, loss = 0.52133222\n",
            "Iteration 197, loss = 0.52226766\n",
            "Iteration 198, loss = 0.52247618\n",
            "Iteration 199, loss = 0.52152747\n",
            "Iteration 200, loss = 0.52148202\n",
            "Iteration 201, loss = 0.52210643\n",
            "Iteration 202, loss = 0.52245002\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57027887\n",
            "Iteration 2, loss = 0.56139579\n",
            "Iteration 3, loss = 0.56050583\n",
            "Iteration 4, loss = 0.56162805\n",
            "Iteration 5, loss = 0.56074323\n",
            "Iteration 6, loss = 0.56145371\n",
            "Iteration 7, loss = 0.56126570\n",
            "Iteration 8, loss = 0.56101754\n",
            "Iteration 9, loss = 0.56078890\n",
            "Iteration 10, loss = 0.56036835\n",
            "Iteration 11, loss = 0.56126916\n",
            "Iteration 12, loss = 0.56056245\n",
            "Iteration 13, loss = 0.56147930\n",
            "Iteration 14, loss = 0.56047730\n",
            "Iteration 15, loss = 0.56025679\n",
            "Iteration 16, loss = 0.56011042\n",
            "Iteration 17, loss = 0.56063411\n",
            "Iteration 18, loss = 0.55992116\n",
            "Iteration 19, loss = 0.56024467\n",
            "Iteration 20, loss = 0.56022860\n",
            "Iteration 21, loss = 0.56005579\n",
            "Iteration 22, loss = 0.56015160\n",
            "Iteration 23, loss = 0.55913666\n",
            "Iteration 24, loss = 0.56078768\n",
            "Iteration 25, loss = 0.56009934\n",
            "Iteration 26, loss = 0.55912322\n",
            "Iteration 27, loss = 0.55979724\n",
            "Iteration 28, loss = 0.55974114\n",
            "Iteration 29, loss = 0.55893677\n",
            "Iteration 30, loss = 0.55980750\n",
            "Iteration 31, loss = 0.55960662\n",
            "Iteration 32, loss = 0.55949459\n",
            "Iteration 33, loss = 0.55962383\n",
            "Iteration 34, loss = 0.55951621\n",
            "Iteration 35, loss = 0.56013266\n",
            "Iteration 36, loss = 0.55905644\n",
            "Iteration 37, loss = 0.55971443\n",
            "Iteration 38, loss = 0.55905323\n",
            "Iteration 39, loss = 0.55861039\n",
            "Iteration 40, loss = 0.55888534\n",
            "Iteration 41, loss = 0.55885182\n",
            "Iteration 42, loss = 0.55935194\n",
            "Iteration 43, loss = 0.55828273\n",
            "Iteration 44, loss = 0.55865506\n",
            "Iteration 45, loss = 0.55841091\n",
            "Iteration 46, loss = 0.55766352\n",
            "Iteration 47, loss = 0.55857529\n",
            "Iteration 48, loss = 0.55855605\n",
            "Iteration 49, loss = 0.55735301\n",
            "Iteration 50, loss = 0.55790094\n",
            "Iteration 51, loss = 0.55766420\n",
            "Iteration 52, loss = 0.55779829\n",
            "Iteration 53, loss = 0.55715277\n",
            "Iteration 54, loss = 0.55788822\n",
            "Iteration 55, loss = 0.55723882\n",
            "Iteration 56, loss = 0.55716280\n",
            "Iteration 57, loss = 0.55685313\n",
            "Iteration 58, loss = 0.55718571\n",
            "Iteration 59, loss = 0.55698510\n",
            "Iteration 60, loss = 0.55603580\n",
            "Iteration 61, loss = 0.55594262\n",
            "Iteration 62, loss = 0.55609122\n",
            "Iteration 63, loss = 0.55507345\n",
            "Iteration 64, loss = 0.55527474\n",
            "Iteration 65, loss = 0.55540838\n",
            "Iteration 66, loss = 0.55422522\n",
            "Iteration 67, loss = 0.55471521\n",
            "Iteration 68, loss = 0.55421368\n",
            "Iteration 69, loss = 0.55361490\n",
            "Iteration 70, loss = 0.55313360\n",
            "Iteration 71, loss = 0.55268583\n",
            "Iteration 72, loss = 0.55300840\n",
            "Iteration 73, loss = 0.55263959\n",
            "Iteration 74, loss = 0.55274648\n",
            "Iteration 75, loss = 0.55228619\n",
            "Iteration 76, loss = 0.55101371\n",
            "Iteration 77, loss = 0.55049585\n",
            "Iteration 78, loss = 0.55051453\n",
            "Iteration 79, loss = 0.55014061\n",
            "Iteration 80, loss = 0.54915188\n",
            "Iteration 81, loss = 0.54902837\n",
            "Iteration 82, loss = 0.54862429\n",
            "Iteration 83, loss = 0.54758125\n",
            "Iteration 84, loss = 0.54708725\n",
            "Iteration 85, loss = 0.54676174\n",
            "Iteration 86, loss = 0.54593580\n",
            "Iteration 87, loss = 0.54506485\n",
            "Iteration 88, loss = 0.54550236\n",
            "Iteration 89, loss = 0.54392151\n",
            "Iteration 90, loss = 0.54349104\n",
            "Iteration 91, loss = 0.54339944\n",
            "Iteration 92, loss = 0.54193854\n",
            "Iteration 93, loss = 0.54164125\n",
            "Iteration 94, loss = 0.54051891\n",
            "Iteration 95, loss = 0.54037805\n",
            "Iteration 96, loss = 0.53920912\n",
            "Iteration 97, loss = 0.53866348\n",
            "Iteration 98, loss = 0.53777059\n",
            "Iteration 99, loss = 0.53778650\n",
            "Iteration 100, loss = 0.53670116\n",
            "Iteration 101, loss = 0.53608823\n",
            "Iteration 102, loss = 0.53524375\n",
            "Iteration 103, loss = 0.53421815\n",
            "Iteration 104, loss = 0.53342861\n",
            "Iteration 105, loss = 0.53389048\n",
            "Iteration 106, loss = 0.53237177\n",
            "Iteration 107, loss = 0.53213899\n",
            "Iteration 108, loss = 0.53184344\n",
            "Iteration 109, loss = 0.53194731\n",
            "Iteration 110, loss = 0.53171167\n",
            "Iteration 111, loss = 0.53115853\n",
            "Iteration 112, loss = 0.53070286\n",
            "Iteration 113, loss = 0.52969976\n",
            "Iteration 114, loss = 0.52934258\n",
            "Iteration 115, loss = 0.52994153\n",
            "Iteration 116, loss = 0.52843815\n",
            "Iteration 117, loss = 0.52787474\n",
            "Iteration 118, loss = 0.52882470\n",
            "Iteration 119, loss = 0.52870671\n",
            "Iteration 120, loss = 0.52707388\n",
            "Iteration 121, loss = 0.52744137\n",
            "Iteration 122, loss = 0.52812427\n",
            "Iteration 123, loss = 0.52682303\n",
            "Iteration 124, loss = 0.52665403\n",
            "Iteration 125, loss = 0.52695120\n",
            "Iteration 126, loss = 0.52574189\n",
            "Iteration 127, loss = 0.52636581\n",
            "Iteration 128, loss = 0.52578988\n",
            "Iteration 129, loss = 0.52558436\n",
            "Iteration 130, loss = 0.52509598\n",
            "Iteration 131, loss = 0.52553465\n",
            "Iteration 132, loss = 0.52460765\n",
            "Iteration 133, loss = 0.52525362\n",
            "Iteration 134, loss = 0.52434753\n",
            "Iteration 135, loss = 0.52351017\n",
            "Iteration 136, loss = 0.52415889\n",
            "Iteration 137, loss = 0.52442682\n",
            "Iteration 138, loss = 0.52344496\n",
            "Iteration 139, loss = 0.52318375\n",
            "Iteration 140, loss = 0.52375574\n",
            "Iteration 141, loss = 0.52328445\n",
            "Iteration 142, loss = 0.52322090\n",
            "Iteration 143, loss = 0.52229873\n",
            "Iteration 144, loss = 0.52285248\n",
            "Iteration 145, loss = 0.52329321\n",
            "Iteration 146, loss = 0.52266813\n",
            "Iteration 147, loss = 0.52208747\n",
            "Iteration 148, loss = 0.52195628\n",
            "Iteration 149, loss = 0.52239425\n",
            "Iteration 150, loss = 0.52078906\n",
            "Iteration 151, loss = 0.52188605\n",
            "Iteration 152, loss = 0.52233607\n",
            "Iteration 153, loss = 0.52149622\n",
            "Iteration 154, loss = 0.52134566\n",
            "Iteration 155, loss = 0.51973034\n",
            "Iteration 156, loss = 0.52104719\n",
            "Iteration 157, loss = 0.52107185\n",
            "Iteration 158, loss = 0.52128350\n",
            "Iteration 159, loss = 0.52152494\n",
            "Iteration 160, loss = 0.52112387\n",
            "Iteration 161, loss = 0.52019458\n",
            "Iteration 162, loss = 0.52048242\n",
            "Iteration 163, loss = 0.52022164\n",
            "Iteration 164, loss = 0.51990560\n",
            "Iteration 165, loss = 0.52009431\n",
            "Iteration 166, loss = 0.52028312\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56604079\n",
            "Iteration 2, loss = 0.56397725\n",
            "Iteration 3, loss = 0.56385144\n",
            "Iteration 4, loss = 0.56267727\n",
            "Iteration 5, loss = 0.56365636\n",
            "Iteration 6, loss = 0.56399193\n",
            "Iteration 7, loss = 0.56261920\n",
            "Iteration 8, loss = 0.56316107\n",
            "Iteration 9, loss = 0.56344685\n",
            "Iteration 10, loss = 0.56361320\n",
            "Iteration 11, loss = 0.56323023\n",
            "Iteration 12, loss = 0.56313724\n",
            "Iteration 13, loss = 0.56303790\n",
            "Iteration 14, loss = 0.56264576\n",
            "Iteration 15, loss = 0.56307137\n",
            "Iteration 16, loss = 0.56243632\n",
            "Iteration 17, loss = 0.56296420\n",
            "Iteration 18, loss = 0.56276943\n",
            "Iteration 19, loss = 0.56239297\n",
            "Iteration 20, loss = 0.56278819\n",
            "Iteration 21, loss = 0.56192116\n",
            "Iteration 22, loss = 0.56225160\n",
            "Iteration 23, loss = 0.56244060\n",
            "Iteration 24, loss = 0.56199066\n",
            "Iteration 25, loss = 0.56216930\n",
            "Iteration 26, loss = 0.56281733\n",
            "Iteration 27, loss = 0.56172103\n",
            "Iteration 28, loss = 0.56211751\n",
            "Iteration 29, loss = 0.56135420\n",
            "Iteration 30, loss = 0.56110510\n",
            "Iteration 31, loss = 0.56239527\n",
            "Iteration 32, loss = 0.56177845\n",
            "Iteration 33, loss = 0.56194827\n",
            "Iteration 34, loss = 0.56141420\n",
            "Iteration 35, loss = 0.56220173\n",
            "Iteration 36, loss = 0.56124503\n",
            "Iteration 37, loss = 0.56128811\n",
            "Iteration 38, loss = 0.56215031\n",
            "Iteration 39, loss = 0.56168064\n",
            "Iteration 40, loss = 0.56181443\n",
            "Iteration 41, loss = 0.56115942\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57433419\n",
            "Iteration 2, loss = 0.56583530\n",
            "Iteration 3, loss = 0.56510385\n",
            "Iteration 4, loss = 0.56450094\n",
            "Iteration 5, loss = 0.56594755\n",
            "Iteration 6, loss = 0.56505289\n",
            "Iteration 7, loss = 0.56523955\n",
            "Iteration 8, loss = 0.56505276\n",
            "Iteration 9, loss = 0.56509849\n",
            "Iteration 10, loss = 0.56531516\n",
            "Iteration 11, loss = 0.56458937\n",
            "Iteration 12, loss = 0.56508573\n",
            "Iteration 13, loss = 0.56436794\n",
            "Iteration 14, loss = 0.56406880\n",
            "Iteration 15, loss = 0.56463569\n",
            "Iteration 16, loss = 0.56461881\n",
            "Iteration 17, loss = 0.56445979\n",
            "Iteration 18, loss = 0.56455497\n",
            "Iteration 19, loss = 0.56484497\n",
            "Iteration 20, loss = 0.56419127\n",
            "Iteration 21, loss = 0.56449918\n",
            "Iteration 22, loss = 0.56431966\n",
            "Iteration 23, loss = 0.56465384\n",
            "Iteration 24, loss = 0.56223747\n",
            "Iteration 25, loss = 0.56359423\n",
            "Iteration 26, loss = 0.56343246\n",
            "Iteration 27, loss = 0.56340936\n",
            "Iteration 28, loss = 0.56316513\n",
            "Iteration 29, loss = 0.56346771\n",
            "Iteration 30, loss = 0.56418142\n",
            "Iteration 31, loss = 0.56317234\n",
            "Iteration 32, loss = 0.56319990\n",
            "Iteration 33, loss = 0.56319265\n",
            "Iteration 34, loss = 0.56272140\n",
            "Iteration 35, loss = 0.56282504\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56520978\n",
            "Iteration 2, loss = 0.56507632\n",
            "Iteration 3, loss = 0.56399136\n",
            "Iteration 4, loss = 0.56469940\n",
            "Iteration 5, loss = 0.56509037\n",
            "Iteration 6, loss = 0.56443056\n",
            "Iteration 7, loss = 0.56424128\n",
            "Iteration 8, loss = 0.56463803\n",
            "Iteration 9, loss = 0.56432936\n",
            "Iteration 10, loss = 0.56443839\n",
            "Iteration 11, loss = 0.56382947\n",
            "Iteration 12, loss = 0.56403154\n",
            "Iteration 13, loss = 0.56516428\n",
            "Iteration 14, loss = 0.56410623\n",
            "Iteration 15, loss = 0.56354425\n",
            "Iteration 16, loss = 0.56487136\n",
            "Iteration 17, loss = 0.56435792\n",
            "Iteration 18, loss = 0.56394046\n",
            "Iteration 19, loss = 0.56317576\n",
            "Iteration 20, loss = 0.56371799\n",
            "Iteration 21, loss = 0.56399960\n",
            "Iteration 22, loss = 0.56370055\n",
            "Iteration 23, loss = 0.56368863\n",
            "Iteration 24, loss = 0.56490931\n",
            "Iteration 25, loss = 0.56336551\n",
            "Iteration 26, loss = 0.56370392\n",
            "Iteration 27, loss = 0.56323503\n",
            "Iteration 28, loss = 0.56363150\n",
            "Iteration 29, loss = 0.56378147\n",
            "Iteration 30, loss = 0.56376669\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57242256\n",
            "Iteration 2, loss = 0.55959986\n",
            "Iteration 3, loss = 0.55932988\n",
            "Iteration 4, loss = 0.56046936\n",
            "Iteration 5, loss = 0.55963018\n",
            "Iteration 6, loss = 0.55988267\n",
            "Iteration 7, loss = 0.55961348\n",
            "Iteration 8, loss = 0.55981969\n",
            "Iteration 9, loss = 0.55984332\n",
            "Iteration 10, loss = 0.55970527\n",
            "Iteration 11, loss = 0.55914732\n",
            "Iteration 12, loss = 0.55954711\n",
            "Iteration 13, loss = 0.55977313\n",
            "Iteration 14, loss = 0.55884324\n",
            "Iteration 15, loss = 0.55964058\n",
            "Iteration 16, loss = 0.55913845\n",
            "Iteration 17, loss = 0.55875079\n",
            "Iteration 18, loss = 0.55896299\n",
            "Iteration 19, loss = 0.55945295\n",
            "Iteration 20, loss = 0.55877512\n",
            "Iteration 21, loss = 0.55918050\n",
            "Iteration 22, loss = 0.55838897\n",
            "Iteration 23, loss = 0.56059220\n",
            "Iteration 24, loss = 0.55846846\n",
            "Iteration 25, loss = 0.55863145\n",
            "Iteration 26, loss = 0.55942542\n",
            "Iteration 27, loss = 0.55844236\n",
            "Iteration 28, loss = 0.55885896\n",
            "Iteration 29, loss = 0.55873841\n",
            "Iteration 30, loss = 0.55859302\n",
            "Iteration 31, loss = 0.55768218\n",
            "Iteration 32, loss = 0.55883828\n",
            "Iteration 33, loss = 0.55876954\n",
            "Iteration 34, loss = 0.55830362\n",
            "Iteration 35, loss = 0.55780338\n",
            "Iteration 36, loss = 0.55792944\n",
            "Iteration 37, loss = 0.55882373\n",
            "Iteration 38, loss = 0.55881479\n",
            "Iteration 39, loss = 0.55875711\n",
            "Iteration 40, loss = 0.55804424\n",
            "Iteration 41, loss = 0.55753441\n",
            "Iteration 42, loss = 0.55801972\n",
            "Iteration 43, loss = 0.55803420\n",
            "Iteration 44, loss = 0.55818339\n",
            "Iteration 45, loss = 0.55814431\n",
            "Iteration 46, loss = 0.55751893\n",
            "Iteration 47, loss = 0.55796976\n",
            "Iteration 48, loss = 0.55748793\n",
            "Iteration 49, loss = 0.55686316\n",
            "Iteration 50, loss = 0.55774469\n",
            "Iteration 51, loss = 0.55685192\n",
            "Iteration 52, loss = 0.55678789\n",
            "Iteration 53, loss = 0.55666256\n",
            "Iteration 54, loss = 0.55637709\n",
            "Iteration 55, loss = 0.55694005\n",
            "Iteration 56, loss = 0.55704458\n",
            "Iteration 57, loss = 0.55649912\n",
            "Iteration 58, loss = 0.55651757\n",
            "Iteration 59, loss = 0.55648681\n",
            "Iteration 60, loss = 0.55529855\n",
            "Iteration 61, loss = 0.55564005\n",
            "Iteration 62, loss = 0.55549112\n",
            "Iteration 63, loss = 0.55593228\n",
            "Iteration 64, loss = 0.55477588\n",
            "Iteration 65, loss = 0.55514058\n",
            "Iteration 66, loss = 0.55557400\n",
            "Iteration 67, loss = 0.55476893\n",
            "Iteration 68, loss = 0.55485661\n",
            "Iteration 69, loss = 0.55413679\n",
            "Iteration 70, loss = 0.55365378\n",
            "Iteration 71, loss = 0.55410706\n",
            "Iteration 72, loss = 0.55354271\n",
            "Iteration 73, loss = 0.55289860\n",
            "Iteration 74, loss = 0.55278474\n",
            "Iteration 75, loss = 0.55259493\n",
            "Iteration 76, loss = 0.55313027\n",
            "Iteration 77, loss = 0.55243815\n",
            "Iteration 78, loss = 0.55162042\n",
            "Iteration 79, loss = 0.55126992\n",
            "Iteration 80, loss = 0.55063726\n",
            "Iteration 81, loss = 0.55049237\n",
            "Iteration 82, loss = 0.55035186\n",
            "Iteration 83, loss = 0.54943873\n",
            "Iteration 84, loss = 0.54945010\n",
            "Iteration 85, loss = 0.54890270\n",
            "Iteration 86, loss = 0.54827623\n",
            "Iteration 87, loss = 0.54819337\n",
            "Iteration 88, loss = 0.54746586\n",
            "Iteration 89, loss = 0.54726726\n",
            "Iteration 90, loss = 0.54630446\n",
            "Iteration 91, loss = 0.54583669\n",
            "Iteration 92, loss = 0.54498747\n",
            "Iteration 93, loss = 0.54498794\n",
            "Iteration 94, loss = 0.54410368\n",
            "Iteration 95, loss = 0.54372938\n",
            "Iteration 96, loss = 0.54272350\n",
            "Iteration 97, loss = 0.54321045\n",
            "Iteration 98, loss = 0.54190432\n",
            "Iteration 99, loss = 0.54136542\n",
            "Iteration 100, loss = 0.54098760\n",
            "Iteration 101, loss = 0.54003630\n",
            "Iteration 102, loss = 0.53949997\n",
            "Iteration 103, loss = 0.53934682\n",
            "Iteration 104, loss = 0.53821562\n",
            "Iteration 105, loss = 0.53718664\n",
            "Iteration 106, loss = 0.53716518\n",
            "Iteration 107, loss = 0.53631267\n",
            "Iteration 108, loss = 0.53588332\n",
            "Iteration 109, loss = 0.53538725\n",
            "Iteration 110, loss = 0.53521034\n",
            "Iteration 111, loss = 0.53476183\n",
            "Iteration 112, loss = 0.53403497\n",
            "Iteration 113, loss = 0.53283246\n",
            "Iteration 114, loss = 0.53343436\n",
            "Iteration 115, loss = 0.53278999\n",
            "Iteration 116, loss = 0.53187507\n",
            "Iteration 117, loss = 0.53144557\n",
            "Iteration 118, loss = 0.53058941\n",
            "Iteration 119, loss = 0.53024429\n",
            "Iteration 120, loss = 0.53055573\n",
            "Iteration 121, loss = 0.53089130\n",
            "Iteration 122, loss = 0.52938769\n",
            "Iteration 123, loss = 0.52989722\n",
            "Iteration 124, loss = 0.52934930\n",
            "Iteration 125, loss = 0.52913424\n",
            "Iteration 126, loss = 0.52892162\n",
            "Iteration 127, loss = 0.52831819\n",
            "Iteration 128, loss = 0.52847187\n",
            "Iteration 129, loss = 0.52766703\n",
            "Iteration 130, loss = 0.52772176\n",
            "Iteration 131, loss = 0.52699623\n",
            "Iteration 132, loss = 0.52789133\n",
            "Iteration 133, loss = 0.52661848\n",
            "Iteration 134, loss = 0.52665335\n",
            "Iteration 135, loss = 0.52651796\n",
            "Iteration 136, loss = 0.52676821\n",
            "Iteration 137, loss = 0.52772340\n",
            "Iteration 138, loss = 0.52699830\n",
            "Iteration 139, loss = 0.52590083\n",
            "Iteration 140, loss = 0.52611054\n",
            "Iteration 141, loss = 0.52538201\n",
            "Iteration 142, loss = 0.52528850\n",
            "Iteration 143, loss = 0.52459469\n",
            "Iteration 144, loss = 0.52502396\n",
            "Iteration 145, loss = 0.52513620\n",
            "Iteration 146, loss = 0.52507566\n",
            "Iteration 147, loss = 0.52485472\n",
            "Iteration 148, loss = 0.52437726\n",
            "Iteration 149, loss = 0.52432951\n",
            "Iteration 150, loss = 0.52371515\n",
            "Iteration 151, loss = 0.52401563\n",
            "Iteration 152, loss = 0.52361489\n",
            "Iteration 153, loss = 0.52333453\n",
            "Iteration 154, loss = 0.52344426\n",
            "Iteration 155, loss = 0.52325926\n",
            "Iteration 156, loss = 0.52294757\n",
            "Iteration 157, loss = 0.52267853\n",
            "Iteration 158, loss = 0.52291691\n",
            "Iteration 159, loss = 0.52334176\n",
            "Iteration 160, loss = 0.52221048\n",
            "Iteration 161, loss = 0.52344781\n",
            "Iteration 162, loss = 0.52240254\n",
            "Iteration 163, loss = 0.52222101\n",
            "Iteration 164, loss = 0.52193987\n",
            "Iteration 165, loss = 0.52224917\n",
            "Iteration 166, loss = 0.52189681\n",
            "Iteration 167, loss = 0.52261559\n",
            "Iteration 168, loss = 0.52134589\n",
            "Iteration 169, loss = 0.52244709\n",
            "Iteration 170, loss = 0.52190868\n",
            "Iteration 171, loss = 0.52152728\n",
            "Iteration 172, loss = 0.52240656\n",
            "Iteration 173, loss = 0.52193786\n",
            "Iteration 174, loss = 0.52204269\n",
            "Iteration 175, loss = 0.52080777\n",
            "Iteration 176, loss = 0.52235093\n",
            "Iteration 177, loss = 0.52069761\n",
            "Iteration 178, loss = 0.52232192\n",
            "Iteration 179, loss = 0.52140328\n",
            "Iteration 180, loss = 0.52073555\n",
            "Iteration 181, loss = 0.52095487\n",
            "Iteration 182, loss = 0.52126815\n",
            "Iteration 183, loss = 0.52162527\n",
            "Iteration 184, loss = 0.52041365\n",
            "Iteration 185, loss = 0.52073403\n",
            "Iteration 186, loss = 0.52068367\n",
            "Iteration 187, loss = 0.52205797\n",
            "Iteration 188, loss = 0.52050091\n",
            "Iteration 189, loss = 0.52083275\n",
            "Iteration 190, loss = 0.52132419\n",
            "Iteration 191, loss = 0.52097609\n",
            "Iteration 192, loss = 0.52123913\n",
            "Iteration 193, loss = 0.52122854\n",
            "Iteration 194, loss = 0.52057070\n",
            "Iteration 195, loss = 0.52166035\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56888493\n",
            "Iteration 2, loss = 0.56717908\n",
            "Iteration 3, loss = 0.56722321\n",
            "Iteration 4, loss = 0.56759086\n",
            "Iteration 5, loss = 0.56781529\n",
            "Iteration 6, loss = 0.56762255\n",
            "Iteration 7, loss = 0.56680638\n",
            "Iteration 8, loss = 0.56667297\n",
            "Iteration 9, loss = 0.56766848\n",
            "Iteration 10, loss = 0.56707681\n",
            "Iteration 11, loss = 0.56704514\n",
            "Iteration 12, loss = 0.56720488\n",
            "Iteration 13, loss = 0.56774542\n",
            "Iteration 14, loss = 0.56679420\n",
            "Iteration 15, loss = 0.56707327\n",
            "Iteration 16, loss = 0.56744885\n",
            "Iteration 17, loss = 0.56745965\n",
            "Iteration 18, loss = 0.56708679\n",
            "Iteration 19, loss = 0.56719508\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57652678\n",
            "Iteration 2, loss = 0.56053117\n",
            "Iteration 3, loss = 0.56055114\n",
            "Iteration 4, loss = 0.56031735\n",
            "Iteration 5, loss = 0.55916692\n",
            "Iteration 6, loss = 0.55992103\n",
            "Iteration 7, loss = 0.56003357\n",
            "Iteration 8, loss = 0.55943440\n",
            "Iteration 9, loss = 0.55966086\n",
            "Iteration 10, loss = 0.56008600\n",
            "Iteration 11, loss = 0.55945522\n",
            "Iteration 12, loss = 0.56002842\n",
            "Iteration 13, loss = 0.55940146\n",
            "Iteration 14, loss = 0.55935609\n",
            "Iteration 15, loss = 0.55918512\n",
            "Iteration 16, loss = 0.55949363\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56702619\n",
            "Iteration 2, loss = 0.56131612\n",
            "Iteration 3, loss = 0.56150718\n",
            "Iteration 4, loss = 0.56111797\n",
            "Iteration 5, loss = 0.56158279\n",
            "Iteration 6, loss = 0.56178870\n",
            "Iteration 7, loss = 0.56117288\n",
            "Iteration 8, loss = 0.56083049\n",
            "Iteration 9, loss = 0.56070529\n",
            "Iteration 10, loss = 0.56086934\n",
            "Iteration 11, loss = 0.55985662\n",
            "Iteration 12, loss = 0.56156815\n",
            "Iteration 13, loss = 0.56096664\n",
            "Iteration 14, loss = 0.56091701\n",
            "Iteration 15, loss = 0.56018213\n",
            "Iteration 16, loss = 0.56070593\n",
            "Iteration 17, loss = 0.56038445\n",
            "Iteration 18, loss = 0.56065955\n",
            "Iteration 19, loss = 0.56094902\n",
            "Iteration 20, loss = 0.55972780\n",
            "Iteration 21, loss = 0.56016263\n",
            "Iteration 22, loss = 0.56016647\n",
            "Iteration 23, loss = 0.56056618\n",
            "Iteration 24, loss = 0.55966433\n",
            "Iteration 25, loss = 0.56036638\n",
            "Iteration 26, loss = 0.56009520\n",
            "Iteration 27, loss = 0.56063960\n",
            "Iteration 28, loss = 0.56020324\n",
            "Iteration 29, loss = 0.55991959\n",
            "Iteration 30, loss = 0.55971137\n",
            "Iteration 31, loss = 0.55956728\n",
            "Iteration 32, loss = 0.55911701\n",
            "Iteration 33, loss = 0.55956508\n",
            "Iteration 34, loss = 0.55937020\n",
            "Iteration 35, loss = 0.55935640\n",
            "Iteration 36, loss = 0.55929164\n",
            "Iteration 37, loss = 0.55913234\n",
            "Iteration 38, loss = 0.55801578\n",
            "Iteration 39, loss = 0.55895606\n",
            "Iteration 40, loss = 0.55868184\n",
            "Iteration 41, loss = 0.55789040\n",
            "Iteration 42, loss = 0.55828617\n",
            "Iteration 43, loss = 0.55811236\n",
            "Iteration 44, loss = 0.55782910\n",
            "Iteration 45, loss = 0.55745274\n",
            "Iteration 46, loss = 0.55882377\n",
            "Iteration 47, loss = 0.55810444\n",
            "Iteration 48, loss = 0.55882231\n",
            "Iteration 49, loss = 0.55682531\n",
            "Iteration 50, loss = 0.55686204\n",
            "Iteration 51, loss = 0.55709237\n",
            "Iteration 52, loss = 0.55633689\n",
            "Iteration 53, loss = 0.55662942\n",
            "Iteration 54, loss = 0.55577713\n",
            "Iteration 55, loss = 0.55649773\n",
            "Iteration 56, loss = 0.55571202\n",
            "Iteration 57, loss = 0.55486327\n",
            "Iteration 58, loss = 0.55571558\n",
            "Iteration 59, loss = 0.55494690\n",
            "Iteration 60, loss = 0.55480979\n",
            "Iteration 61, loss = 0.55435497\n",
            "Iteration 62, loss = 0.55518092\n",
            "Iteration 63, loss = 0.55447172\n",
            "Iteration 64, loss = 0.55404379\n",
            "Iteration 65, loss = 0.55253131\n",
            "Iteration 66, loss = 0.55324577\n",
            "Iteration 67, loss = 0.55266363\n",
            "Iteration 68, loss = 0.55172480\n",
            "Iteration 69, loss = 0.55104757\n",
            "Iteration 70, loss = 0.55115038\n",
            "Iteration 71, loss = 0.55034969\n",
            "Iteration 72, loss = 0.54988988\n",
            "Iteration 73, loss = 0.54938221\n",
            "Iteration 74, loss = 0.54826772\n",
            "Iteration 75, loss = 0.54855363\n",
            "Iteration 76, loss = 0.54732592\n",
            "Iteration 77, loss = 0.54684451\n",
            "Iteration 78, loss = 0.54640367\n",
            "Iteration 79, loss = 0.54559377\n",
            "Iteration 80, loss = 0.54468961\n",
            "Iteration 81, loss = 0.54427742\n",
            "Iteration 82, loss = 0.54343584\n",
            "Iteration 83, loss = 0.54250335\n",
            "Iteration 84, loss = 0.54245487\n",
            "Iteration 85, loss = 0.54141886\n",
            "Iteration 86, loss = 0.54083584\n",
            "Iteration 87, loss = 0.53934317\n",
            "Iteration 88, loss = 0.53967373\n",
            "Iteration 89, loss = 0.53787201\n",
            "Iteration 90, loss = 0.53700066\n",
            "Iteration 91, loss = 0.53652549\n",
            "Iteration 92, loss = 0.53509119\n",
            "Iteration 93, loss = 0.53524799\n",
            "Iteration 94, loss = 0.53373576\n",
            "Iteration 95, loss = 0.53433472\n",
            "Iteration 96, loss = 0.53276161\n",
            "Iteration 97, loss = 0.53229014\n",
            "Iteration 98, loss = 0.53185181\n",
            "Iteration 99, loss = 0.53099364\n",
            "Iteration 100, loss = 0.53012552\n",
            "Iteration 101, loss = 0.52961163\n",
            "Iteration 102, loss = 0.52866863\n",
            "Iteration 103, loss = 0.52864222\n",
            "Iteration 104, loss = 0.52875906\n",
            "Iteration 105, loss = 0.52704772\n",
            "Iteration 106, loss = 0.52708196\n",
            "Iteration 107, loss = 0.52662661\n",
            "Iteration 108, loss = 0.52597821\n",
            "Iteration 109, loss = 0.52685486\n",
            "Iteration 110, loss = 0.52498014\n",
            "Iteration 111, loss = 0.52598935\n",
            "Iteration 112, loss = 0.52509501\n",
            "Iteration 113, loss = 0.52485443\n",
            "Iteration 114, loss = 0.52432084\n",
            "Iteration 115, loss = 0.52412139\n",
            "Iteration 116, loss = 0.52410346\n",
            "Iteration 117, loss = 0.52406148\n",
            "Iteration 118, loss = 0.52352300\n",
            "Iteration 119, loss = 0.52349369\n",
            "Iteration 120, loss = 0.52322686\n",
            "Iteration 121, loss = 0.52239756\n",
            "Iteration 122, loss = 0.52306048\n",
            "Iteration 123, loss = 0.52174143\n",
            "Iteration 124, loss = 0.52243619\n",
            "Iteration 125, loss = 0.52143151\n",
            "Iteration 126, loss = 0.52116873\n",
            "Iteration 127, loss = 0.52179843\n",
            "Iteration 128, loss = 0.52163063\n",
            "Iteration 129, loss = 0.52056023\n",
            "Iteration 130, loss = 0.52219260\n",
            "Iteration 131, loss = 0.52076042\n",
            "Iteration 132, loss = 0.51975519\n",
            "Iteration 133, loss = 0.52074722\n",
            "Iteration 134, loss = 0.52000247\n",
            "Iteration 135, loss = 0.52097280\n",
            "Iteration 136, loss = 0.52017454\n",
            "Iteration 137, loss = 0.51994447\n",
            "Iteration 138, loss = 0.51955754\n",
            "Iteration 139, loss = 0.51955923\n",
            "Iteration 140, loss = 0.51922083\n",
            "Iteration 141, loss = 0.51953861\n",
            "Iteration 142, loss = 0.51972153\n",
            "Iteration 143, loss = 0.51891890\n",
            "Iteration 144, loss = 0.51878041\n",
            "Iteration 145, loss = 0.51871879\n",
            "Iteration 146, loss = 0.51902915\n",
            "Iteration 147, loss = 0.51901511\n",
            "Iteration 148, loss = 0.51888396\n",
            "Iteration 149, loss = 0.51870231\n",
            "Iteration 150, loss = 0.51922779\n",
            "Iteration 151, loss = 0.51955836\n",
            "Iteration 152, loss = 0.51880561\n",
            "Iteration 153, loss = 0.51882845\n",
            "Iteration 154, loss = 0.51791897\n",
            "Iteration 155, loss = 0.51858869\n",
            "Iteration 156, loss = 0.51734591\n",
            "Iteration 157, loss = 0.51765088\n",
            "Iteration 158, loss = 0.51731190\n",
            "Iteration 159, loss = 0.51716389\n",
            "Iteration 160, loss = 0.51713110\n",
            "Iteration 161, loss = 0.51722381\n",
            "Iteration 162, loss = 0.51800013\n",
            "Iteration 163, loss = 0.51800715\n",
            "Iteration 164, loss = 0.51725465\n",
            "Iteration 165, loss = 0.51743195\n",
            "Iteration 166, loss = 0.51691905\n",
            "Iteration 167, loss = 0.51777838\n",
            "Iteration 168, loss = 0.51653223\n",
            "Iteration 169, loss = 0.51760796\n",
            "Iteration 170, loss = 0.51804437\n",
            "Iteration 171, loss = 0.51748151\n",
            "Iteration 172, loss = 0.51819521\n",
            "Iteration 173, loss = 0.51689520\n",
            "Iteration 174, loss = 0.51694020\n",
            "Iteration 175, loss = 0.51650502\n",
            "Iteration 176, loss = 0.51638104\n",
            "16\n",
            "Iteration 1, loss = 0.56364397\n",
            "Iteration 2, loss = 0.55614821\n",
            "Iteration 3, loss = 0.55654601\n",
            "Iteration 4, loss = 0.55585666\n",
            "Iteration 5, loss = 0.55663906\n",
            "Iteration 6, loss = 0.55676869\n",
            "Iteration 7, loss = 0.55646141\n",
            "Iteration 8, loss = 0.55675420\n",
            "Iteration 9, loss = 0.55628963\n",
            "Iteration 10, loss = 0.55657583\n",
            "Iteration 11, loss = 0.55527639\n",
            "Iteration 12, loss = 0.55666957\n",
            "Iteration 13, loss = 0.55698254\n",
            "Iteration 14, loss = 0.55594364\n",
            "Iteration 15, loss = 0.55477326\n",
            "Iteration 16, loss = 0.55598286\n",
            "Iteration 17, loss = 0.55604441\n",
            "Iteration 18, loss = 0.55597299\n",
            "Iteration 19, loss = 0.55531468\n",
            "Iteration 20, loss = 0.55579141\n",
            "Iteration 21, loss = 0.55560762\n",
            "Iteration 22, loss = 0.55551018\n",
            "Iteration 23, loss = 0.55488119\n",
            "Iteration 24, loss = 0.55523763\n",
            "Iteration 25, loss = 0.55468535\n",
            "Iteration 26, loss = 0.55506388\n",
            "Iteration 27, loss = 0.55545479\n",
            "Iteration 28, loss = 0.55492567\n",
            "Iteration 29, loss = 0.55573419\n",
            "Iteration 30, loss = 0.55481609\n",
            "Iteration 31, loss = 0.55500550\n",
            "Iteration 32, loss = 0.55516509\n",
            "Iteration 33, loss = 0.55474209\n",
            "Iteration 34, loss = 0.55502089\n",
            "Iteration 35, loss = 0.55465209\n",
            "Iteration 36, loss = 0.55464422\n",
            "Iteration 37, loss = 0.55477113\n",
            "Iteration 38, loss = 0.55465540\n",
            "Iteration 39, loss = 0.55414925\n",
            "Iteration 40, loss = 0.55458227\n",
            "Iteration 41, loss = 0.55419145\n",
            "Iteration 42, loss = 0.55259104\n",
            "Iteration 43, loss = 0.55413351\n",
            "Iteration 44, loss = 0.55324109\n",
            "Iteration 45, loss = 0.55334234\n",
            "Iteration 46, loss = 0.55359619\n",
            "Iteration 47, loss = 0.55326777\n",
            "Iteration 48, loss = 0.55424415\n",
            "Iteration 49, loss = 0.55303306\n",
            "Iteration 50, loss = 0.55309063\n",
            "Iteration 51, loss = 0.55225701\n",
            "Iteration 52, loss = 0.55313395\n",
            "Iteration 53, loss = 0.55234321\n",
            "Iteration 54, loss = 0.55217900\n",
            "Iteration 55, loss = 0.55195643\n",
            "Iteration 56, loss = 0.55163139\n",
            "Iteration 57, loss = 0.55141480\n",
            "Iteration 58, loss = 0.55077005\n",
            "Iteration 59, loss = 0.55052778\n",
            "Iteration 60, loss = 0.55116383\n",
            "Iteration 61, loss = 0.55066543\n",
            "Iteration 62, loss = 0.55010091\n",
            "Iteration 63, loss = 0.54968699\n",
            "Iteration 64, loss = 0.54909192\n",
            "Iteration 65, loss = 0.54889983\n",
            "Iteration 66, loss = 0.54871926\n",
            "Iteration 67, loss = 0.54852623\n",
            "Iteration 1, loss = 0.57761250\n",
            "Iteration 2, loss = 0.56527952\n",
            "Iteration 3, loss = 0.56508460\n",
            "Iteration 4, loss = 0.56492125\n",
            "Iteration 5, loss = 0.56486880\n",
            "Iteration 6, loss = 0.56459046\n",
            "Iteration 7, loss = 0.56526455\n",
            "Iteration 8, loss = 0.56442112\n",
            "Iteration 9, loss = 0.56473307\n",
            "Iteration 10, loss = 0.56471850\n",
            "Iteration 11, loss = 0.56539520\n",
            "Iteration 12, loss = 0.56512045\n",
            "Iteration 13, loss = 0.56483530\n",
            "Iteration 14, loss = 0.56537108\n",
            "Iteration 15, loss = 0.56399220\n",
            "Iteration 16, loss = 0.56517380\n",
            "Iteration 17, loss = 0.56447520\n",
            "Iteration 18, loss = 0.56360921\n",
            "Iteration 19, loss = 0.56480970\n",
            "Iteration 20, loss = 0.56430298\n",
            "Iteration 21, loss = 0.56479171\n",
            "Iteration 22, loss = 0.56470294\n",
            "Iteration 23, loss = 0.56443488\n",
            "Iteration 24, loss = 0.56396901\n",
            "Iteration 25, loss = 0.56540487\n",
            "Iteration 26, loss = 0.56477061\n",
            "Iteration 27, loss = 0.56419635\n",
            "Iteration 28, loss = 0.56398162\n",
            "Iteration 29, loss = 0.56375691\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.57757514\n",
            "Iteration 2, loss = 0.56381124\n",
            "Iteration 3, loss = 0.56343801\n",
            "Iteration 4, loss = 0.56361240\n",
            "Iteration 5, loss = 0.56406630\n",
            "Iteration 6, loss = 0.56456698\n",
            "Iteration 7, loss = 0.56369921\n",
            "Iteration 8, loss = 0.56311397\n",
            "Iteration 9, loss = 0.56405024\n",
            "Iteration 10, loss = 0.56366867\n",
            "Iteration 11, loss = 0.56346290\n",
            "Iteration 12, loss = 0.56257306\n",
            "Iteration 13, loss = 0.56349529\n",
            "Iteration 14, loss = 0.56356286\n",
            "Iteration 15, loss = 0.56269023\n",
            "Iteration 16, loss = 0.56357265\n",
            "Iteration 17, loss = 0.56326094\n",
            "Iteration 18, loss = 0.56390597\n",
            "Iteration 19, loss = 0.56294683\n",
            "Iteration 20, loss = 0.56277801\n",
            "Iteration 21, loss = 0.56320285\n",
            "Iteration 22, loss = 0.56294694\n",
            "Iteration 23, loss = 0.56315315\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.56911313\n",
            "Iteration 2, loss = 0.56035606\n",
            "Iteration 3, loss = 0.55960529\n",
            "Iteration 4, loss = 0.56007371\n",
            "Iteration 5, loss = 0.56015980\n",
            "Iteration 6, loss = 0.56053541\n",
            "Iteration 7, loss = 0.56016909\n",
            "Iteration 8, loss = 0.55937676\n",
            "Iteration 9, loss = 0.55967806\n"
          ]
        }
      ],
      "source": [
        "resultados_arvore = []\n",
        "resultados_random_forest = []\n",
        "resultados_knn = []\n",
        "resultados_logistica = []\n",
        "resultados_svm = []\n",
        "resultados_rede_neural = []\n",
        "\n",
        "for i in range(30):\n",
        "  print(i)\n",
        "  kfold = KFold(n_splits = 10, shuffle = True, random_state = i)\n",
        "  # n_splits = folds (9 treinamento e 1 teste)\n",
        "  # shuffle = True para misturar os dados\n",
        "  arvore = DecisionTreeClassifier(criterion = 'log_loss', min_samples_leaf = 1, min_samples_split = 2 , splitter = 'best')\n",
        "  scores = cross_val_score(arvore, x, y, cv = kfold)\n",
        "  resultados_arvore.append(scores.mean())\n",
        "\n",
        "  random_forest = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', class_weight = \"balanced\",\n",
        "                                       min_samples_leaf = 5, min_samples_split = 5)\n",
        "  scores = cross_val_score(random_forest, x, y, cv = kfold)\n",
        "  resultados_random_forest.append(scores.mean())\n",
        "\n",
        "  knn = KNeighborsClassifier(n_neighbors = 20, p = 1)\n",
        "  scores = cross_val_score(knn, x, y, cv = kfold)\n",
        "  resultados_knn.append(scores.mean())\n",
        "\n",
        "  logistica = LogisticRegression(C = 1.0, solver = 'lbfgs', tol = 0.00001, class_weight = 'balanced')\n",
        "  scores = cross_val_score(logistica, x, y, cv = kfold)\n",
        "  resultados_logistica.append(scores.mean())\n",
        "\n",
        "  svm = SVC(kernel = 'rbf', C = 1.0, tol = 0.001)\n",
        "  scores = cross_val_score(svm, x, y, cv = kfold)\n",
        "  resultados_svm.append(scores.mean())\n",
        "\n",
        "  rede_neural = MLPClassifier(max_iter = 1000, verbose = True, tol = 0.0000100,\n",
        "                                   solver = 'sgd', activation = 'logistic',\n",
        "                                   hidden_layer_sizes = (100, 100),\n",
        "                                   batch_size = 10)\n",
        "  scores = cross_val_score(rede_neural, x, y, cv = kfold)\n",
        "  resultados_rede_neural.append(scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OA7902mFnfw"
      },
      "outputs": [],
      "source": [
        "resultados = pd.DataFrame({'Arvore': resultados_arvore, 'Random forest': resultados_random_forest,\n",
        "                           'KNN': resultados_knn, 'Logistica': resultados_logistica,\n",
        "                           'SVM': resultados_svm, 'Rede neural': resultados_rede_neural})\n",
        "resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIvbuzLtFqHL"
      },
      "outputs": [],
      "source": [
        "resultados.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpB6buaJFrjc"
      },
      "outputs": [],
      "source": [
        "resultados.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKXVNxaJFtdK"
      },
      "outputs": [],
      "source": [
        "# coeficiente de variação (CV = desvio padrao / media * 100)\n",
        "(resultados.std() / resultados.mean()) * 100"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
